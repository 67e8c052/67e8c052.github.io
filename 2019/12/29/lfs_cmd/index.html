<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per second. Contention">
<meta property="og:type" content="article">
<meta property="og:title" content="lfs command">
<meta property="og:url" content="http://example.com/2019/12/29/lfs_cmd/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per second. Contention">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-12-29T07:41:16.000Z">
<meta property="article:modified_time" content="2023-03-08T02:56:48.888Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="fs">
<meta property="article:tag" content="filesys">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>lfs command</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2020/03/03/crash/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2019/11/26/xfs/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2019/12/29/lfs_cmd/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2019/12/29/lfs_cmd/&text=lfs command"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2019/12/29/lfs_cmd/&is_video=false&description=lfs command"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=lfs command&body=Check out this article: http://example.com/2019/12/29/lfs_cmd/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2019/12/29/lfs_cmd/&name=lfs command&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2019/12/29/lfs_cmd/&t=lfs command"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#NET"><span class="toc-number">1.</span> <span class="toc-text">NET</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#UDSP"><span class="toc-number">1.1.</span> <span class="toc-text">UDSP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#project-quota"><span class="toc-number">1.2.</span> <span class="toc-text">project quota</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#User-group-quota"><span class="toc-number">2.</span> <span class="toc-text">User group quota</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#re-writeconf"><span class="toc-number">2.1.</span> <span class="toc-text">re-writeconf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#changelog"><span class="toc-number">2.2.</span> <span class="toc-text">changelog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kdump"><span class="toc-number">2.3.</span> <span class="toc-text">kdump</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><span class="toc-number"></span> <span class="toc-text">1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><span class="toc-number"></span> <span class="toc-text">1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#reaadonly-mount"><span class="toc-number">0.1.</span> <span class="toc-text">reaadonly mount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DNE"><span class="toc-number">0.2.</span> <span class="toc-text">DNE</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ALL"><span class="toc-number">1.</span> <span class="toc-text">ALL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collect-log"><span class="toc-number">2.</span> <span class="toc-text">collect log</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#list-all-parameters"><span class="toc-number">2.1.</span> <span class="toc-text">list all parameters</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDS"><span class="toc-number">3.</span> <span class="toc-text">MDS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor"><span class="toc-number">3.1.</span> <span class="toc-text">monitor</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OSS"><span class="toc-number">4.</span> <span class="toc-text">OSS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Monitor"><span class="toc-number">4.1.</span> <span class="toc-text">Monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#format"><span class="toc-number">4.2.</span> <span class="toc-text">format</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#change-ipaddr"><span class="toc-number">4.3.</span> <span class="toc-text">change ipaddr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Enable-large-dir-feature-enable-on-the-2-15-X"><span class="toc-number">4.4.</span> <span class="toc-text">Enable large_dir feature, enable on the 2.15.X</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EC"><span class="toc-number">4.5.</span> <span class="toc-text">EC</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client"><span class="toc-number">5.</span> <span class="toc-text">Client</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor-1"><span class="toc-number">5.1.</span> <span class="toc-text">monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#complie-client"><span class="toc-number">5.2.</span> <span class="toc-text">complie client</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ubuntu-install-client"><span class="toc-number">5.2.1.</span> <span class="toc-text">ubuntu install client</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#get-the-status"><span class="toc-number">5.3.</span> <span class="toc-text">get the status</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backup-and-recovery-lfs-ZFS-OST"><span class="toc-number">6.</span> <span class="toc-text">Backup and recovery lfs ZFS OST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-multiple-ethernet-port-for-diff-LAN"><span class="toc-number">7.</span> <span class="toc-text">lfs multiple ethernet port for diff LAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#trace-the-kernel-sock"><span class="toc-number">8.</span> <span class="toc-text">trace the kernel sock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-magic-num"><span class="toc-number">9.</span> <span class="toc-text">lfs magic num</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rocev2-test"><span class="toc-number">10.</span> <span class="toc-text">rocev2 test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DOM"><span class="toc-number">11.</span> <span class="toc-text">DOM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lnet-health"><span class="toc-number">11.1.</span> <span class="toc-text">lnet health</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OPA-setting"><span class="toc-number">11.2.</span> <span class="toc-text">OPA setting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llmount"><span class="toc-number">11.3.</span> <span class="toc-text">llmount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Skip-recovery"><span class="toc-number">11.4.</span> <span class="toc-text">Skip recovery</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-migarate"><span class="toc-number">11.5.</span> <span class="toc-text">lfs migarate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Job-status"><span class="toc-number">11.6.</span> <span class="toc-text">Job status</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-fid-and-path"><span class="toc-number">11.7.</span> <span class="toc-text">lfs fid and path</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#trace-with-debugfs"><span class="toc-number">11.8.</span> <span class="toc-text">trace with debugfs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llog-reader"><span class="toc-number">11.9.</span> <span class="toc-text">llog_reader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-12-8-client-performance-degraded-in-the-lustre-2-15-0-server"><span class="toc-number">12.</span> <span class="toc-text">2.12.8 client performance degraded in the lustre 2.15.0 server</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-zfs-direct-IO-support"><span class="toc-number">12.1.</span> <span class="toc-text">lfs zfs direct IO support</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Not-wait-the-zfs-sync"><span class="toc-number">13.</span> <span class="toc-text">Not wait the zfs sync</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#errors"><span class="toc-number">14.</span> <span class="toc-text">errors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#network"><span class="toc-number">14.1.</span> <span class="toc-text">network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Build-lfs-MASTER-with-zfs-under-almalinux-8-7"><span class="toc-number">15.</span> <span class="toc-text">Build lfs MASTER with zfs under almalinux 8.7</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#kernel-modules"><span class="toc-number">15.1.</span> <span class="toc-text">kernel modules</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#idmapped-mounts"><span class="toc-number">15.2.</span> <span class="toc-text">idmapped mounts</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        lfs command
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">John Doe</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-12-29T07:41:16.000Z" itemprop="datePublished">2019-12-29</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Storage/">Storage</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/filesys/" rel="tag">filesys</a>, <a class="tag-link-link" href="/tags/fs/" rel="tag">fs</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <ul>
<li><a target="_blank" rel="noopener" href="https://hpcf.umbc.edu/general-productivity/lfs-best-practices/">The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per second. Contention slows the performance of your applications and weakens the overall health of the lfs filesystem</a><ul>
<li>Avoid Using ls -l</li>
<li>Avoid Having a Large Number of Files in a Single Directory</li>
<li>Avoid Accessing Small Files</li>
<li>Avoid Repetitive “stat” Operations</li>
<li>Avoid Having Multiple Processes Open the Same File(s) at the Same Time</li>
<li>Avoid Repetitive Open&#x2F;Close Operations</li>
</ul>
</li>
</ul>
<h3 id="NET"><a href="#NET" class="headerlink" title="NET"></a>NET</h3><h4 id="UDSP"><a href="#UDSP" class="headerlink" title="UDSP"></a><a target="_blank" rel="noopener" href="https://wiki.whamcloud.com/display/LNet/UDSP+Behaviour">UDSP</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ lnetctl net show</span><br><span class="line">net:</span><br><span class="line">    - net <span class="built_in">type</span>: lo</span><br><span class="line">      <span class="built_in">local</span> NI(s):</span><br><span class="line">        - nid: 0@lo</span><br><span class="line">          status: up</span><br><span class="line">    - net <span class="built_in">type</span>: tcp</span><br><span class="line">      <span class="built_in">local</span> NI(s):</span><br><span class="line">        - nid: 192.168.122.110@tcp</span><br><span class="line">          status: up</span><br><span class="line">          interfaces:</span><br><span class="line">              0: eth0</span><br><span class="line">        - nid: 192.168.122.253@tcp</span><br><span class="line">          status: up</span><br><span class="line">          interfaces:</span><br><span class="line">              0: eth1</span><br><span class="line">    - net <span class="built_in">type</span>: tcp1</span><br><span class="line">      <span class="built_in">local</span> NI(s):</span><br><span class="line">        - nid: 192.168.122.238@tcp1</span><br><span class="line">          status: up</span><br><span class="line">          interfaces:</span><br><span class="line">              0: eth2</span><br><span class="line"></span><br><span class="line"><span class="comment">#Prioritize one of PeerA NIDs on tcp</span></span><br><span class="line">$ lnetctl udsp add --src 192.168.122.110@tcp</span><br><span class="line"></span><br><span class="line"><span class="comment">#Prioritize tcp1 as source on PeerA</span></span><br><span class="line">$ lnetctl udsp add --src tcp1</span><br><span class="line"></span><br><span class="line">$ lnetctl udsp show</span><br><span class="line">    - idx: 0</span><br><span class="line"></span><br><span class="line">$ lnetctl udsp del 0</span><br><span class="line"></span><br><span class="line">$ lnetctl net show -v 4 | grep put</span><br><span class="line">$ lnetctl peer show -v 4</span><br></pre></td></tr></table></figure>

<h4 id="project-quota"><a href="#project-quota" class="headerlink" title="project quota"></a>project quota</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">ldiskfs mds$ tune2fs -O project /dev/md1</span><br><span class="line"></span><br><span class="line">mds$ lctl conf_param  <span class="variable">$FNAME</span>.quota.ost=ugp</span><br><span class="line"></span><br><span class="line">mds$ lctl get_param osd-*.*.quota_slave.info</span><br><span class="line">osd-ldiskfs.testfs-MDT0000.quota_slave.info=</span><br><span class="line">target name:    testfs-MDT0000</span><br><span class="line">pool ID:        0</span><br><span class="line"><span class="built_in">type</span>:           md</span><br><span class="line">quota enabled:  ugp</span><br><span class="line">conn to master: setup</span><br><span class="line">space acct:     ugp</span><br><span class="line">user uptodate:  glb[1],slv[1],reint[0]</span><br><span class="line">group uptodate: glb[1],slv[1],reint[0]</span><br><span class="line">project uptodate: glb[1],slv[1],reint[0]</span><br><span class="line"></span><br><span class="line">client $  mount.lustre xx.xx.xx.xx@tcp:/fsname /testfs -o localflock,user_xattr</span><br><span class="line">client $  <span class="built_in">mkdir</span> /testfs/prj3</span><br><span class="line">client $  chattr +P /testfs/prj3</span><br><span class="line">client $  chattr -p 3 /testfs/prj3</span><br><span class="line">client $  lfs quota -p 3 /testfs/prj3</span><br><span class="line">client $  lfs setquota -p 1 -b 20G -B 21G -i 10000 -I 11000 /testfs/prj3</span><br><span class="line">client $  <span class="built_in">chown</span> nfsnobody.nfsnobody /testfs/prj3</span><br><span class="line">client $  su - nfsnobody -s /bin/bash -c <span class="string">&#x27;dd if=/dev/zero of=/testfs/prj3/test3 bs=1M count=20000&#x27;</span></span><br><span class="line"><span class="built_in">dd</span>: error writing ‘/testfs/prj3/test3’: Disk quota exceeded</span><br><span class="line">301+0 records <span class="keyword">in</span></span><br><span class="line">300+0 records out</span><br><span class="line">314572800 bytes (315 MB) copied, 5.22756 s, 60.2 MB/s</span><br><span class="line"></span><br><span class="line">client $  lfs quota -p 3 -h /testfs</span><br><span class="line">Disk quotas <span class="keyword">for</span> prj 3 (pid 3):</span><br><span class="line">     Filesystem    used   quota   <span class="built_in">limit</span>   grace   files   quota   <span class="built_in">limit</span>   grace</span><br><span class="line">          /testfs  20.09G*    20G     21G    none       4   10000   11000       -</span><br><span class="line"></span><br><span class="line">client $  <span class="built_in">mkdir</span> /testfs/prj3/test100</span><br><span class="line">client $  <span class="built_in">chown</span> games.games /testfs/prj3/test100</span><br><span class="line">client $  su - games -s /bin/bash -c <span class="string">&#x27;dd if=/dev/zero of=/testfs/prj3/test100/test0 bs=1M count=20000&#x27;</span></span><br><span class="line"><span class="built_in">dd</span>: error writing ‘/testfs/prj3/test100/test0’: Disk quota exceeded</span><br><span class="line">2+0 records <span class="keyword">in</span></span><br><span class="line">1+0 records out</span><br><span class="line">1445888 bytes (1.4 MB) copied, 0.0689412 s, 21.0 MB/s</span><br></pre></td></tr></table></figure>

<h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## you can &#x27;t use lctl set_param, it &#x27;s not work</span></span><br><span class="line">mds $ lctl conf_param/set_param -P fsname.quota.ost|mdt=u|g|p|ugp|none</span><br><span class="line"></span><br><span class="line"><span class="comment">#enable</span></span><br><span class="line">mds $ lctl conf_param  <span class="variable">$FNAME</span>.quota.ost=ugp</span><br><span class="line">mds $ lctl conf_param  <span class="variable">$FNAME</span>.quota.mdt=ugp</span><br><span class="line"></span><br><span class="line">mds $ <span class="built_in">cat</span> /proc/fs/lfs/osd-ldiskfs/<span class="variable">$FNAME</span>-MDT0000/quota_slave/info</span><br><span class="line">quota enabled:  <span class="string">&quot;ug&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#disable quota</span></span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.ost=none</span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FNAME</span>.quota.mdt=none</span><br><span class="line"></span><br><span class="line"><span class="comment">### lctl set_param -P must reboot, no -P not work, if you don&#x27; t reboot ,you have too use conf_param</span></span><br><span class="line"></span><br><span class="line">client $ lfs setquota -u user1 -b 307200 -B 309200 -i 10000 -I 11000 /mnt/lfs</span><br><span class="line">client $ lfs setquota -g group1 -b 5120000 -B 5150000 -i 100000 -I 101000 /mnt/lfs</span><br><span class="line"></span><br><span class="line">client $ lfs quota -u user1 -v /mnt/lfs</span><br><span class="line">client $ lfs quota -t -p /mnt/lfs</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure>

<h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mds$ tunefs.lfs --writeconf /dev/sdx</span><br><span class="line">oss$ tunefs.lfs --writeconf /dev/ost0</span><br></pre></td></tr></table></figure>

<h4 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm_control=enabled</span><br><span class="line">$ lctl set_param -P  mdt.$FNAME-MDT0000.hsm_control=enabled</span><br><span class="line">$ lctl set_param mdt.$FNAME-MDT0000.hsm.max_requests=8</span><br><span class="line"></span><br><span class="line"># create user</span><br><span class="line">$ lctl --device $FNAME-MDT0000 changelog_register</span><br><span class="line"># del user</span><br><span class="line">$ lctl --device fsname-MDT0000 changelog_deregister cl1</span><br><span class="line"># Get the size</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT0000.changelog_users mdd.$FNAME-MDT0000.changelog_size</span><br><span class="line"></span><br><span class="line"># changelog mask</span><br><span class="line">$ lctl set_param mdd.$FNAME-MDT*.changelog_mask=MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">$ lctl get_param mdd.$FNAME-MDT*.changelog_mask</span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line">$ FSNAME=testfs; lctl set_param mdd.$FSNAME-MDT0000.changelog_mask=&quot;UNLNK RMDIR RENME RNMTO NOPEN&quot;</span><br><span class="line">mdd.testfs-MDT0000.changelog_mask=UNLNK RMDIR RENME RNMTO NOPEN</span><br><span class="line">$ lctl get_param mdd.$FSNAME-MDT0000.changelog_mask</span><br><span class="line">mdd.testfs-MDT0000.changelog_mask=MARK UNLNK RMDIR RENME RNMTO NOPEN</span><br><span class="line"></span><br><span class="line">#Audit just the changelog extention</span><br><span class="line">#To have a fully functional Changelogs-based audit facility, some additional Changelog record types must be enabled, to be able to record events such as OPEN, ATIME, GETXATTR and DENIED OPEN.</span><br><span class="line">mds# lctl set_param mdd.lustre-MDT0000.changelog_mask=ALL</span><br><span class="line">mdd.seb-MDT0000.changelog_mask=ALL</span><br><span class="line"></span><br><span class="line">To prevent nodes pertaining to a nodemap to generate Changelog entries</span><br><span class="line">$ lctl nodemap_modify --name nm1 --property audit_mode --value 0</span><br><span class="line">$ lctl set_param mdd.lustre-MDT0000.changelog_deniednext=120</span><br><span class="line">mdd.seb-MDT0000.changelog_deniednext=120</span><br><span class="line">$ lctl get_param mdd.lustre-MDT0000.changelog_deniednext</span><br><span class="line">mdd.seb-MDT0000.changelog_deniednext=120</span><br><span class="line"></span><br><span class="line"># Get the changelog</span><br><span class="line">$ lfs changelog $FNAME-MDT0000 &gt; lfs-changelog</span><br><span class="line">$ fs changelog $fsname-MDT0000 [startrec [endrec]]</span><br><span class="line"></span><br><span class="line"># clear all</span><br><span class="line">$ lctl changelog_clear mdt_name userid endrec</span><br><span class="line"></span><br><span class="line">MARK    Internal recordkeeping</span><br><span class="line">CREAT   Regular file creation</span><br><span class="line">MKDIR   Directory creation</span><br><span class="line">HLINK   Hard link</span><br><span class="line">SLINK   Soft link</span><br><span class="line">MKNOD   Other file creation</span><br><span class="line">UNLNK   Regular file removal</span><br><span class="line">RMDIR   Directory removal</span><br><span class="line">RENME   Rename, original</span><br><span class="line">RNMTO   Rename, final</span><br><span class="line">OPEN    Open</span><br><span class="line">CLOSE   Close</span><br><span class="line">LYOUT   Layout change</span><br><span class="line">TRUNC   Regular file truncated</span><br><span class="line">SATTR   Attribute change</span><br><span class="line">XATTR   Extended attribute change (setxattr)</span><br><span class="line">HSM     HSM specific event</span><br><span class="line">MTIME   MTIME change</span><br><span class="line">CTIME   CTIME change</span><br><span class="line">ATIME   ATIME change</span><br><span class="line">MIGRT   Migration event</span><br><span class="line">FLRW    File Level Replication: file initially written</span><br><span class="line">RESYNC  File Level Replication: file re-synced</span><br><span class="line">GXATR   Extended attribute access (getxattr)</span><br><span class="line">NOPEN   Denied open</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">#### kdump</span><br></pre></td></tr></table></figure>

<h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat &#x2F;etc&#x2F;kdump.conf<br>nfs my.nfsserver.example.org:&#x2F;path&#x2F;to&#x2F;expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p>
<h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:&#x75;&#115;&#101;&#x72;&#x40;&#109;&#121;&#46;&#x73;&#101;&#x72;&#x76;&#x65;&#x72;&#x2e;&#101;&#x78;&#x61;&#109;&#112;&#x6c;&#101;&#46;&#111;&#x72;&#x67;">&#x75;&#115;&#101;&#x72;&#x40;&#109;&#121;&#46;&#x73;&#101;&#x72;&#x76;&#x65;&#x72;&#x2e;&#101;&#x78;&#x61;&#109;&#112;&#x6c;&#101;&#46;&#111;&#x72;&#x67;</a>:&#x2F;dest&#x2F;path<br>#By default, uses ssh key at &#x2F;root&#x2F;.ssh&#x2F;kdump_id_rsa<br>#core_collector makedumpfile <options></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### Disable </span><br></pre></td></tr></table></figure>
<p>#Notify a device that user cl1 no longer needs records (up toand including 3)<br>$ lfs changelog_clear $FNAME-MDT0000 cl1 3</p>
<p>#To stop changelogs, changelog_mask should be set to MARK only<br>$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;MARK<br>mdd.lfs-MDT0000.changelog_mask&#x3D;MARK</p>
<p>#or youcan set it -all<br>$ lctl set_param mdd.$FNAME-MDT0000.changelog_mask&#x3D;-all</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### FSCK</span><br><span class="line">```bash</span><br><span class="line">Dec 29 14:11:32 mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode 5384166 in dir #145170469</span><br><span class="line">Dec 29 14:11:32 mookie kernel: Remounting filesystem read-only</span><br></pre></td></tr></table></figure>

<ul>
<li>Flush the journal<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ umount /lfs</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /lfs</span><br><span class="line">$ umount /lfs</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT&#x2F;OSTs in parallel</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Check only mode</span><br><span class="line">$ e2fsck -fn /dev/sdx</span><br><span class="line"></span><br><span class="line"># Prudent mode</span><br><span class="line">$ e2fsck -fp /dev/sdx</span><br><span class="line"></span><br><span class="line"># Answer yes</span><br><span class="line">$ e2fsck -fy /dev/sdx</span><br></pre></td></tr></table></figure>


<h4 id="reaadonly-mount"><a href="#reaadonly-mount" class="headerlink" title="reaadonly mount"></a>reaadonly mount</h4><p>add “-o nosvc”  </p>
<h4 id="DNE"><a href="#DNE" class="headerlink" title="DNE"></a>DNE</h4><p>disable DNE balance   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setdirstripe -D -c 1 -i 0 --max-inherit=1</span><br></pre></td></tr></table></figure>

<ul>
<li><a target="_blank" rel="noopener" href="https://www.eofs.eu/_media/events/lad21/lad2021-lustre_2.15_and_beyond-dilger.pdf">lfs 2.15.0 roadmap features</a><ul>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-12815">Multiple TCP sockets performance</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-9121">UDSP</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-12125">Parallel rename within a directory</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-14792">Default DNE MDT space balance</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-14712">Improved ldiskfs “-o discard” efficiency</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-12043">Improve parallel client readahead</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-13798">Parallel large DIO</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-10983">Metadata Writeback Cache</a></li>
</ul>
</li>
</ul>
<h3 id="ALL"><a href="#ALL" class="headerlink" title="ALL"></a>ALL</h3><h3 id="collect-log"><a href="#collect-log" class="headerlink" title="collect log"></a>collect log</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">$ lctl  get_param  at_max</span><br><span class="line">at_max=600</span><br><span class="line">Maximum adaptive <span class="built_in">timeout</span> (<span class="keyword">in</span> seconds). The at_max parameter is an upper-limit on the service time estimate. If at_max is reached, an RPC request <span class="built_in">times</span> out.Setting at_max to 0 causes adaptive timeouts to be disabled and a fixed <span class="built_in">timeout</span> method to be used instead (see the section called “Setting Static Timeouts” Note If slow hardware causes the service estimate to increase beyond the default value of at_max, increase at_max to the maximum time you are willing to <span class="built_in">wait</span> <span class="keyword">for</span> an RPC completion.</span><br><span class="line">$ lctl  get_param  at_min</span><br><span class="line">at_min=50</span><br><span class="line">Minimum adaptive <span class="built_in">timeout</span> (<span class="keyword">in</span> seconds). The default value is 0. The at_min parameter is the minimum processing time that a server will report. Ideally, at_min should be <span class="built_in">set</span> to its default value. Clients base their timeouts on this value, but they <span class="keyword">do</span> not use this value directly.If, <span class="keyword">for</span> unknown reasons (usually due to temporary network outages), the adaptive <span class="built_in">timeout</span> value is too short and clients time out their RPCs, you can increase the at_min value to compensate <span class="keyword">for</span> this.</span><br><span class="line">$ lctl get_param at_extra</span><br><span class="line">at_extra=30</span><br><span class="line"><span class="comment">#Incremental amount of time that a server requests with each early reply (in seconds). The server does not know how much time the RPC will take, so it asks for a fixed value. The default is 30, which provides a balance between sending too many early replies for the same RPC and overestimating the actual completion time.When a server finds a queued request about to time out and needs to send an early reply out, the server adds the at_extra value. If the time expires, the lfs server drops the request, and the client enters recovery status and reconnects to restore the connection to normal status.If you see multiple early replies for the same RPC asking for 30-second increases, change the at_extra value to a larger number to cut down on early replies sent and, therefore, network load.</span></span><br><span class="line"></span><br><span class="line">$ lctl  get_param  <span class="built_in">timeout</span></span><br><span class="line"><span class="built_in">timeout</span>=300</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n debug</span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck mmap page dentry cache malloc quota dlmtrace reada vfstrace rpctrace&quot;</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck cache reada quota&quot;</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck&quot;</span></span><br><span class="line"></span><br><span class="line">mds $ lctl set_param debug=+rpctrace</span><br><span class="line">mds $ lctl dk &gt; dk</span><br><span class="line">00000100:00100000:3.0:1621645680.953924:0:5370:0:(service.c:2089:ptlrpc_server_handle_request()) Handling RPC pname:cluuid+ref:pid:xid:nid:opc ll_mgs_0002:781f6010-0ac2-a476-4567-56bb7b70d013+9:86470:x1700165046065472:12345-<span class="variable">$client_IP</span>@tcp:400</span><br><span class="line"></span><br><span class="line">client $ lctl set_param debug=+rpctrace</span><br><span class="line">client $ lctl dk &gt; dk</span><br><span class="line">00000100:00100000:15.0:1621645914.531574:0:77490:0:(client.c:1682:ptlrpc_send_new_req()) Sending RPC pname:cluuid:pid:xid:nid:opc vim:8c1a0181-62fb-78ee-8651-31b8edfd4244:77490:1700165046080640:<span class="variable">$MDS_IP</span>@tcp:101</span><br><span class="line"></span><br><span class="line"><span class="comment">#default</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck&quot;</span></span><br><span class="line">trace   Function entry/exit markers</span><br><span class="line">dlmtrace        Distributed locking-related information</span><br><span class="line">inode   </span><br><span class="line">super   </span><br><span class="line">malloc  Memory allocation or free information</span><br><span class="line">cache   Cache-related information</span><br><span class="line">info    Non-critical general information</span><br><span class="line">dentry  kernel namespace cache handling</span><br><span class="line">mmap    Memory-mapped IO interface</span><br><span class="line">page    Page cache and bulk data transfers</span><br><span class="line">info    Miscellaneous informational messages</span><br><span class="line">net     LNet network related debugging</span><br><span class="line">console Significant system events, printed to console</span><br><span class="line">warning Significant but non-fatal exceptions, printed to console</span><br><span class="line">error   Critical error messages, printed to console</span><br><span class="line">neterror        Significant LNet error messages</span><br><span class="line">emerg   Fatal system errors, printed to console</span><br><span class="line">config  Configuration and setup, enabled by default</span><br><span class="line">ha      Failover and recovery-related information, enabled by default</span><br><span class="line">hsm     Hierarchical space management/tiering</span><br><span class="line">ioctl   IOCTL-related information, enabled by default</span><br><span class="line">layout  File layout handling (PFL, FLR, DoM)</span><br><span class="line">lfsck   Filesystem consistency checking, enabled by default</span><br><span class="line">other   Miscellaneious other debug messages</span><br><span class="line">quota   Space accounting and management</span><br><span class="line">reada   Client readahead management</span><br><span class="line">rpctrace        Remote request/reply tracing and debugging</span><br><span class="line">sec     Security, Kerberos, Shared Secret Key handling</span><br><span class="line">snapshot        Filesystem snapshot management</span><br><span class="line">vfstrace        Kernel VFS interface operations</span><br><span class="line"></span><br><span class="line">$ lctl set_param debug_mb=500</span><br><span class="line">$ <span class="built_in">mkfifo</span> -m 777 /tmp/lfs.log; lctl debug_daemon start /tmp/lfs.log; <span class="built_in">tail</span> -f /tmp/lfs.log | strings</span><br><span class="line">or</span><br><span class="line"><span class="comment">## 500M</span></span><br><span class="line">$ lctl debug_daemon start /tmp/lfs.log 500</span><br><span class="line"></span><br><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lfs /mnt</span><br><span class="line"><span class="comment"># it &#x27;ll create trace.dat</span></span><br><span class="line">$ trace-cmd report</span><br><span class="line"></span><br><span class="line">$ lctl get_param debug_mb</span><br><span class="line">debug_mb=41</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.dump_namespaces</span><br><span class="line"><span class="comment">#no output</span></span><br><span class="line">$ lctl set_param ldlm.dump_namespaces <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.*.dump_page_cache | grep -cEi <span class="string">&#x27;lockd|dirty|writeback&#x27;</span></span><br><span class="line"></span><br><span class="line">$ strings dk</span><br><span class="line">$ lctl <span class="built_in">df</span> &lt;input file&gt; &lt;output file&gt;</span><br><span class="line"></span><br><span class="line">lfs rpm package install will not overvide the /lib/modules/$(<span class="built_in">uname</span> -r)/extra</span><br><span class="line">$ <span class="built_in">rm</span> -rf /lib/modules/$(<span class="built_in">uname</span> -r)/extra/; <span class="built_in">mkdir</span> /lib/modules/$(<span class="built_in">uname</span> -r)/extra</span><br><span class="line"></span><br><span class="line"><span class="comment">#PATH DEBUG_PATH</span></span><br><span class="line">$ lctl get_param debug_path</span><br><span class="line">$ lctl debug_list types</span><br><span class="line">debug_path=/tmp/lfs-log</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable(set to 1) bulk pages dump upon error on Client</span></span><br><span class="line">Client $ lctl get_param osc.*osc-[^mM]*.checksum_dump</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.checksum_dump=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable(set to 1) bulk pages dump upon error on OSS</span></span><br><span class="line">Oss$ lctl get_param obdfilter.*-OST*.checksum_dump</span><br><span class="line">obdfilter.fsname-OST0000.checksum_dump=0</span><br><span class="line"></span><br><span class="line"><span class="comment">## mds and oss</span></span><br><span class="line">can1=$(do_facet mds1 <span class="string">&quot;<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_canceld.stats&quot;</span> |awk <span class="string">&#x27;/ldlm_cancel/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">blk1=$(<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_cbd.stats |awk <span class="string">&#x27;/ldlm_bl_callback/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_mkdir -i0 -c1 <span class="variable">$DIR</span>/<span class="variable">$tdir</span>/d1</span><br><span class="line"></span><br><span class="line">can2=$(do_facet mds1 <span class="string">&quot;<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_canceld.stats&quot;</span> |awk <span class="string">&#x27;/ldlm_cancel/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">blk2=$(<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_cbd.stats | awk <span class="string">&#x27;/ldlm_bl_callback/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">[ <span class="variable">$can1</span> -eq <span class="variable">$can2</span> ] || error $((can2-can1)) <span class="string">&quot;cancel RPC occured.&quot;</span></span><br><span class="line">[ <span class="variable">$blk1</span> -eq <span class="variable">$blk2</span> ] || error $((blk2-blk1)) <span class="string">&quot;blocking RPC occured.&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="list-all-parameters"><a href="#list-all-parameters" class="headerlink" title="list all parameters"></a>list all parameters</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ lctl list_param -R <span class="string">&#x27;*&#x27;</span></span><br><span class="line">$ lctl list_param osc.*.*</span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> $(<span class="built_in">ls</span> /proc/fs/lfs); <span class="keyword">do</span> lctl get_param <span class="variable">$&#123;i&#125;</span>.*.*; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h3><ul>
<li><p>ban the client</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#x27;192.168.1.1@tcp1&#x27; &gt; /proc/fs/lfs/obdfilter/fsname-OST0000/evict_client</span><br><span class="line"></span><br><span class="line"># MDS</span><br><span class="line">/proc/fs/lfs/mdt/lfs1-MDT0000/evict_client</span><br><span class="line">/proc/fs/lfs/mgs/MGS/evict_client</span><br></pre></td></tr></table></figure>
</li>
<li><p>imperative recovery</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">#deactivate imperative recovery</span><br><span class="line">$ lctl set_param mgs.MGS.live.testfs=&quot;state=disabled&quot;</span><br><span class="line"></span><br><span class="line">#activate imperative recovery</span><br><span class="line">$ lctl set_param mgs.MGS.live.testfs=&quot;state=full&quot;</span><br><span class="line"></span><br><span class="line">$ lctl get_param mgs.MGS.live.testfs6</span><br><span class="line">mgs.MGS.live.testfs6=</span><br><span class="line">fsname: testfs6</span><br><span class="line">flags: 0x20     gen: 52</span><br><span class="line">testfs6-MDT0000</span><br><span class="line">testfs6-MDT0001</span><br><span class="line">testfs6-MDT0002</span><br><span class="line">testfs6-MDT0003</span><br><span class="line">testfs6-OST0000</span><br><span class="line">testfs6-OST0001</span><br><span class="line">testfs6-OST0002</span><br><span class="line">testfs6-OST0003</span><br><span class="line"></span><br><span class="line">Secure RPC Config Rules:</span><br><span class="line"></span><br><span class="line">imperative_recovery_state:</span><br><span class="line">    state: full</span><br><span class="line">    nonir_clients: 0</span><br><span class="line">    nidtbl_version: 22</span><br><span class="line">    notify_duration_total: 0.007111353</span><br><span class="line">    notify_duation_max: 0.001130805</span><br><span class="line">    notify_count: 20</span><br></pre></td></tr></table></figure>
</li>
<li><p>limits on ldlm memmory use on servers</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ldlm.lock_limit_mb (<span class="keyword">in</span> megabytes) - hard <span class="built_in">limit</span>   </span><br><span class="line"></span><br><span class="line">ldlm.lock_reclaim_threshold_mb - start to ask clients to release locks   </span><br><span class="line"></span><br><span class="line">Used to be 100*NUM_CPUS per client namespace by default   </span><br><span class="line">ldlm.<span class="variable">$NAMESPACE</span>.lru_size control   </span><br><span class="line">Setting that to 0 (new default) enables “lru resize”    </span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.lock_limit_mb</span><br><span class="line">ldlm.lock_limit_mb=77313</span><br></pre></td></tr></table></figure>

<ul>
<li><p>No root squash</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mds $ lctl set_param $fsname.mdt.root_squash=108:108</span><br><span class="line">or</span><br><span class="line">mds $ lctl set_param mdt.$&#123;fsname&#125;-MDT0000.root_squash=108:108</span><br><span class="line">mds $ lctl set_param mdt.$&#123;fsname&#125;-MDT0000.nosquash_nids=&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line"></span><br><span class="line">error: set_param: param_path &#x27;$fsname/mdt/root_squash&#x27;: No such file or directory</span><br><span class="line">mds $ lctl conf_param $fsname.mdt.root_squash=108:108</span><br><span class="line">mds $ lctl conf_param $fsname.mdt.nosquash_nids=&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line">mds $ cat /proc/fs/lf/mdt/$FNAME-MDT0000/nosquash_nids</span><br><span class="line">ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp</span><br><span class="line"></span><br><span class="line">client $ lctl set_param llite.$FNAME-*.nosquash_nids=&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>show open files</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># del last_recvd in mdt and pass the recovery, mdt and clients mount by no_recov</span><br><span class="line">$ cat /proc/fs/lfs/mdt/*/exports/*/open_files</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>show all clients</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lshowmount -e</span><br></pre></td></tr></table></figure>
</li>
<li><p>increase performance PERFORMANCE</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param -P timeout=300</span><br><span class="line">$ lctl set_param timeout=300</span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param -P osc.*.checksums=0</span><br><span class="line">OSS $ lctl get_param -n ost.*.ost_io.timeouts</span><br><span class="line"></span><br><span class="line">#disable xattr cache</span><br><span class="line">client $ lctl set_param llite.*.xattr_cache=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param  llite.*.fast_read</span><br><span class="line">llite.fsname-ffff8dab2cce8800.fast_read=1</span><br><span class="line"></span><br><span class="line">#set the client for increase the metadata ops</span><br><span class="line">Otherwise, the file attributes will be dropped from the client cache if the file has not been accessed before the LDLM lock timeout. The timeout is stored via lctl get_param ldlm.namespaces.*mdc*.lru_max_age</span><br><span class="line"></span><br><span class="line">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span><br><span class="line">client $ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=600000</span><br><span class="line">client $ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=3900000</span><br><span class="line">client $ lctl get_param ldlm.namespaces.*.lru_size ldlm.namespaces.*.lru_max_age</span><br><span class="line"></span><br><span class="line">#LRU</span><br><span class="line">MDS$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=400</span><br><span class="line">ldlm.namespaces.MGS.lru_size=400</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lru_size=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lru_size=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lru_size=400</span><br><span class="line"></span><br><span class="line">OSS$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=400</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.lru_size=400</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.lru_size=0</span><br><span class="line"></span><br><span class="line">client $ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=2400</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.lru_size=0</span><br><span class="line"></span><br><span class="line"># The lru_size parameter is used to control the number of client-side locks in the LRU cached locks queue</span><br><span class="line">client $ lctl get_param ldlm.namespaces.*mdc-*.lru_size #default dynamic</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=0</span><br><span class="line"></span><br><span class="line">#disable</span><br><span class="line">client$ lctl set_param ldlm.namespaces.*osc*.lru_size=5000</span><br><span class="line">#The total number of locks available is a function of the server RAM. The default limit is 50 locks/1 MB of RAM. If memory pressure is too high, the LRU size is shrunk. The number of locks on the server is limited tonum_osts_per_oss * num_clients * lru_size as followsa</span><br><span class="line"></span><br><span class="line">To determine the number of locks being granted with dynamic LRU resizing, run:</span><br><span class="line">client$ lctl get_param ldlm.namespaces.*.pool.limit</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.limit=195699</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.pool.limit=183850</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.*.pool.limit</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.pool.limit=183850</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.pool.limit=1</span><br><span class="line"></span><br><span class="line">MDS $ lctl get_param ldlm.namespaces.*.pool.limit</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span><br><span class="line">ldlm.namespaces.MGS.pool.limit=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.pool.limit=1</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.pool.limit=1</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.pool.limit=195699</span><br><span class="line"></span><br><span class="line">The lru_max_age parameter is used to control the age of client-side locks in the LRU cached locks queue. This limits how long unused locks are cached on the client, and avoids idle clients from holding locks for an excessive time, which reduces memory usage on both the client and server, as well as reducing work during server recovery.</span><br><span class="line">Client$ lctl set_param ldlm.namespaces.*MDT*.lru_max_age=900s</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_max_age=900s</span><br><span class="line">Client$ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_max_age=900000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#modify client not change mds and oss</span><br><span class="line">MDS $ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lru_max_age=3900000</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.lru_max_age=3900000</span><br><span class="line"></span><br><span class="line">MDS $ lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.MGS.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.pool.lock_volume_factor=1</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.pool.lock_volume_factor=1</span><br><span class="line"></span><br><span class="line">Client $  lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.pool.lock_volume_factor=1</span><br><span class="line"></span><br><span class="line">Only client$ lctl get_param ldlm.namespaces.*-MDT0000-mdc-*.lock_unused_count;</span><br><span class="line">Only client$ lctl get_param ldlm.namespaces.*-MDT0000-mdc-*.pool.recalc_period</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lock_unused_count=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.recalc_period=10</span><br><span class="line"></span><br><span class="line">#readahead</span><br><span class="line"># lfs rpc</span><br><span class="line">## 2.12.6 default</span><br><span class="line">$ lctl get_param -n osc.*OST0000-osc-[^mM]*.max_rpcs_in_flight</span><br><span class="line">64</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdc.*.max_rpcs_in_flight</span><br><span class="line">mdc.test0-MDT0000-mdc-ffff95067b45a000.max_rpcs_in_flight=8</span><br><span class="line">$ lctl set_param mdc.*.max_rpcs_in_flight=16</span><br><span class="line">mdc.test0-MDT0000-mdc-ffff95067b45a000.max_rpcs_in_flight=16</span><br><span class="line">$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line">#In lfs 2.13 client;</span><br><span class="line">#direct IO could reach high 120k, but the buffered IO only 4k. there are read</span><br><span class="line">#When I disable</span><br><span class="line">$ lctl set_param llite.*.read_ahead_async_file_threshold_mb=0</span><br><span class="line">echo 0 &gt; /sys/fs/lfs/llite/lfs-ffff9455c0a4b800/read_ahead_async_file_threshold_mb</span><br><span class="line">#I could got the high IOPS too.</span><br><span class="line"></span><br><span class="line">## 2.12.X readahead setting</span><br><span class="line">$ ls -1 /sys/kernel/debug/lfs/llite/lfs-ffff92fab9235800</span><br><span class="line">dump_page_cache</span><br><span class="line">extents_stats</span><br><span class="line">extents_stats_per_process</span><br><span class="line">max_cached_mb</span><br><span class="line">max_read_ahead_mb</span><br><span class="line">max_read_ahead_per_file_mb</span><br><span class="line">max_read_ahead_whole_mb</span><br><span class="line">nosquash_nids</span><br><span class="line">offset_stats</span><br><span class="line">read_ahead_stats</span><br><span class="line">root_squash</span><br><span class="line">sbi_flags</span><br><span class="line">site</span><br><span class="line">statahead_stats</span><br><span class="line">stats</span><br><span class="line">unstable_stats</span><br><span class="line"></span><br><span class="line">#readahead</span><br><span class="line">client $ lctl set_param llite.*.read_ahead_stats=c llite.testfs-ffff8daf56159000.read_ahead_stats=c</span><br><span class="line">client $ lctl get_param llite.*.*read*</span><br><span class="line">client $ lctl get_param llite.$&#123;fsname&#125;*.*read*</span><br><span class="line">llite.testfs-ffff8dcc59a15800.fast_read=1</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_async_active=12</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_async_file_threshold_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_range_kb=1024</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_whole_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_stats=</span><br><span class="line">snapshot_time             1642131829.087651284 secs.nsecs</span><br><span class="line">hits                      9999533 samples [pages]</span><br><span class="line">misses                    838588 samples [pages]  ---&gt; too high, readahead invalid</span><br><span class="line">readpage not consecutive  1293010 samples [pages] ---&gt; too high, readahead invalid</span><br><span class="line">zero size window          2837081 samples [pages] ---&gt; too high, readahead invalid</span><br><span class="line">failed to fast read       592502 samples [pages]</span><br><span class="line"></span><br><span class="line"># client max read ahead size</span><br><span class="line">cat /sys/kernel/debug/lfs/llite/lfs-ffff91b225941800/max_read_ahead_mb</span><br><span class="line">64</span><br><span class="line"></span><br><span class="line">$ cat /proc/fs/lfs/osc/fsname-*/state</span><br><span class="line">$ grep -Ri current_state  /proc/fs/lfs/osc/fsname-*/state</span><br><span class="line">current_state: FULL</span><br><span class="line">state_history:</span><br><span class="line"> - [ 1616015179, CONNECTING ]</span><br><span class="line"> - [ 1616015179, IDLE ]</span><br><span class="line"> - [ 1616018598, CONNECTING ]</span><br><span class="line"> - [ 1616018598, FULL ]</span><br><span class="line"> - [ 1616019092, CONNECTING ]</span><br><span class="line"> - [ 1616019092, IDLE ]</span><br><span class="line"> - [ 1616019285, CONNECTING ]</span><br><span class="line"> - [ 1616019285, FULL ]</span><br><span class="line"> - [ 1616021199, CONNECTING ]</span><br><span class="line"> - [ 1616021199, IDLE ]</span><br><span class="line"> - [ 1616023761, CONNECTING ]</span><br><span class="line"> - [ 1616023761, FULL ]</span><br><span class="line"> - [ 1616023908, CONNECTING ]</span><br><span class="line"> - [ 1616023908, IDLE ]</span><br><span class="line"> - [ 1616147184, CONNECTING ]</span><br><span class="line"> - [ 1616147184, FULL ]</span><br><span class="line"></span><br><span class="line">$ grep generation /proc/fs/lfs/osc/*-OST0000*/import</span><br><span class="line">       generation: 11</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">client $ lctl get_param osc.fsname-OST0000*.import</span><br><span class="line">client $ lctl get_param osc.fsname-OST0000*.state</span><br><span class="line"></span><br><span class="line">#set idle timeout</span><br><span class="line">$ lctl set_param osc.*.idle_timeout=20</span><br><span class="line"></span><br><span class="line">#optimze the metadata</span><br><span class="line">#Many system commands, such as ls -l, du, and find, traverse a directory sequentially. To make these commands run efficiently, the directory statahead can be enabled to improve the performance of directory traversal</span><br><span class="line"># Controls the maximum number of file attributes (metadata) that will be prefetched by the statahead thread. By default, statahead is enabled and statahead_max is 32 files, The maximum statahead_max is 8192 files</span><br><span class="line">$ lctl get_param llite.*.statahead_max</span><br><span class="line">llite.fsname-ffff8dab2cce8800.statahead_max=32</span><br><span class="line">$ lctl set_param llite.*.statahead_max=128</span><br><span class="line">$ lctl set_param -P llite.*.statahead_max=128</span><br><span class="line"></span><br><span class="line">The directory statahead thread will also prefetch the file size/block attributes from the OSTs, so that all file attributes are available on the client when requested by an application. This is controlled by the asynchronous glimpse lock (AGL) setting. The AGL behaviour can be disabled by setting:</span><br><span class="line">$ lctl set_param llite.*.statahead_agl=0</span><br><span class="line"></span><br><span class="line">#default enable</span><br><span class="line">$ lctl set_param llite.*.statahead_agl=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n llite.*.statahead_stats</span><br><span class="line">statahead total: 4</span><br><span class="line">statahead wrong: 0 ## Does it increase, monitor</span><br><span class="line">agl total: 4</span><br><span class="line">#A read-only interface that provides current statahead and AGL statistics, such as how many times statahead/AGL has been triggered since the last mount, how many statahead/AGL failures have occurred due to an incorrect prediction or other causes.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line">8</span><br><span class="line">$ echo 16 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line"># oss, lfs is extended to support RPCs up to 16MB in size</span><br><span class="line"># Lfs is extended to support RPCs up to 16MB in size. By enabling a larger RPC size, fewer RPCs will be required to transfer the same amount of data between clients and servers. With a larger RPC size, the OSS can submit more data to the underlying disks at once, therefore it can produce larger disk I/Os to fully utilize the increasing bandwidth of disks.</span><br><span class="line">#At client connection time, clients will negotiate with servers what the maximum RPC size it is possible to use, but the client can always send RPCs smaller than this maximum.</span><br><span class="line">#The parameter brw_size is used on the OST to tell the client the maximum (preferred) IO size. All clients that talk to this target should never send an RPC greater than this size. Clients can individually set a smaller RPC size limit via the osc.*.max_pages_per_rpc tunable.</span><br><span class="line">#The smallest brw_size that can be set for ZFS OSTs is the recordsize of that dataset. This ensures that the client can always write a full ZFS file block if it has enough dirty data, and does not otherwise force it to do read- modify-write operations for every RPC.</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.test0-OST*.brw_size</span><br><span class="line">obdfilter.test0-OST0000.brw_size=1</span><br><span class="line"></span><br><span class="line">$ lctl set_param obdfilter.*.brw_size=16 #16M</span><br><span class="line"></span><br><span class="line"># The parameter brw_size is used on the OST to tell the client the maximum (preferred) IO size. All clients that talk to this target should never send an RPC greater than this size. Clients can individually set a smaller RPC size limit via the osc.*.max_pages_per_rpc tunable.</span><br><span class="line"># The smallest brw_size that can be set for ZFS OSTs is the recordsize of that dataset. This ensures that the client can always write a full ZFS file block if it has enough dirty data, and does not otherwise force it to do read- modify-write operations for every RPC.</span><br><span class="line">$ lctl set_param obdfilter.fsname-OST*.brw_size=16</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Get all client info from mds</span><br><span class="line">$ cat /proc/fs/lfs/nodemap/default/exports</span><br><span class="line">&#123; nid: 192.168.11.1@tcp, uuid: b19f57a2-d206-41ea-0c1e-e13650c4de6d &#125;</span><br><span class="line">....</span><br><span class="line">ost/OSS/ost/timeouts</span><br><span class="line">   service : cur  50  worst  92 (at 1616674042, 8947s ago)   3  18   1   3</span><br><span class="line">   service : cur  50  worst  50 (at 1616674036, 8953s ago)   1   1   1   1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##client</span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc osc.*.max_rpcs_in_flight osc.*.max_dirty_mb llite.*.max_read_ahead_mb</span><br><span class="line"># In order to enable a larger RPC size, brw_size must be changed to an IO size value up to 16MB. To temporarily change brw_size, the following command should be run on the OSS:</span><br><span class="line">oss# lctl set_param obdfilter.fsname-OST*.brw_size=16</span><br><span class="line"></span><br><span class="line">To persistently change brw_size, the following command should be run:</span><br><span class="line">oss# lctl set_param -P obdfilter.fsname-OST*.brw_size=16</span><br><span class="line">When a client connects to an OST target, it will fetch brw_size from the target and pick the maximum value of brw_size and its local setting for max_pages_per_rpc as the actual RPC size. Therefore, the max_pages_per_rpc on the client side would have to be set to 16M, or 4096 if the PAGESIZE is 4KB, to enable a 16MB RPC. To temporarily make the change, the following command should be run on the client to setmax_pages_per_rpc:</span><br><span class="line">client$ lctl set_param osc.fsname-OST*.max_pages_per_rpc=16M</span><br><span class="line">client$ lctl set_param -P obdfilter.fsname-OST*.osc.max_pages_per_rpc=16M</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client$ lctl set_param osc.*.max_pages_per_rpc=256 osc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=256 osc.*.grant_shrink=0</span><br><span class="line">client$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.max_pages_per_rpc=256</span><br><span class="line"></span><br><span class="line"># cancel_lru_locks</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*osc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n osc.*.rpc_stats=0</span><br><span class="line">$ dd if=/dev/zero of=/lfs/test bs=1M count=10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### example for single OST</span><br><span class="line">$ lctl get_param -n &#x27;osc.*.rpc_stats&#x27; | sed -n &#x27;/pages per rpc/,/^$/p&#x27;</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                       0   0   0   |          0   0   0</span><br><span class="line">2:                       0   0   0   |          0   0   0</span><br><span class="line">4:                       0   0   0   |          0   0   0</span><br><span class="line">8:                       0   0   0   |          0   0   0</span><br><span class="line">16:                      0   0   0   |          0   0   0</span><br><span class="line">32:                      0   0   0   |          0   0   0</span><br><span class="line">64:                      0   0   0   |          0   0   0</span><br><span class="line">128:                     0   0   0   |          0   0   0</span><br><span class="line">256:                     0   0   0   |         10 100 100  ## why 256 ? in my test env, the brw_size=1 * 1048576 / 4096 PAGE_SIZE = 256</span><br><span class="line">                         |                       |</span><br><span class="line">                         |                      ---------- means dd bs=1M count=10 , 10 x read rpcs</span><br><span class="line">                         ---------------------------------write rpcs</span><br><span class="line"></span><br><span class="line">http://wiki.lfs.org/Lfs_Resiliency:_Understanding_Lfs_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lfs_for_Resiliency</span><br><span class="line">lctl</span><br><span class="line">lctl &gt; network tcp1</span><br><span class="line">lctl &gt; peer_list</span><br><span class="line">12345-xx.xx.xx.xx@tcp [0]0.0.0.0-&gt;0.0.0.0:0 #0</span><br><span class="line"></span><br><span class="line"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span><br><span class="line"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span><br><span class="line"># Minimum lock enqueue time (in seconds). The default is 100. The time it takes to enqueue a lock, ldlm_enqueue, is the maximum of the measured enqueue estimate (influenced by at_min and at_max parameters), multiplied by a weighting factor and the value of ldlm_enqueue_min.lfs Distributed Lock Manager (LDLM) lock enqueues have a dedicated minimum value for ldlm_enqueue_min. Lock enqueue timeouts increase as the measured enqueue times increase (similar to adaptive timeouts).</span><br><span class="line"></span><br><span class="line">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span><br><span class="line">#at_min The 40 second value factors into our calculation for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lfs servers is also 40 seconds</span><br><span class="line">#Adaptive Timeouts: In a Lfs file system servers keep track of the time it takes for RPCs to be completed</span><br><span class="line">#The quiescent_time in this formula is to account for the time it takes all Lfs clients to reestablish connections with all Lfs targets following an HSN quiesce. We&#x27;ve experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lfs clients, the number of Lfs targets, the number of Lfs file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"># readonly mount</span><br><span class="line">$ mount.lfs $zpool /ost0 -o rdonly_dev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Dynamic Peer Discovery (&quot;Discovery&quot; for short) is the process by which a node can discover the network interfaces it can reach a peer on without being pre-configured. This involves sending a ping to the peer. The ping response carries a flag bit to indicate that the peer is multi-rail capable. If it is the node then pushes its own network interface information to the peer. This protocol distributes the network interface information to both nodes and subsequently the nodes can excercise the peer network interfaces as well as its own, as described in further detail in this section. Discovery can be enabled, disabled or in verification mode. If it is in verification mode, then it will cross reference the discovered peer NIDs with the configured NIDs and complain if there is a discrepancy, but will continue to use the configured NIDs</span><br><span class="line"></span><br><span class="line">#Peer Credits</span><br><span class="line">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/lnet/peers</span><br><span class="line">$ cat /sys/kernel/debug/lnet/peers</span><br><span class="line">nid                      refs state  last   max   rtr   min    tx   min queue</span><br><span class="line">xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0</span><br><span class="line">tx is the number of peer credits currently available for this peer</span><br><span class="line"></span><br><span class="line">#lnet network Interface Credits</span><br><span class="line">$ cat /proc/sys/lnet/nis</span><br><span class="line">nid                      status alive refs peer  rtr   max    tx   min</span><br><span class="line">xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796</span><br><span class="line">max is total available (i.e. value of ko2iblnd credits)</span><br><span class="line">tx is the number currently available, Negative number indicates number of messages awaiting a credit</span><br><span class="line">min is the low water mark</span><br><span class="line"></span><br><span class="line">#Lctl conn_list-List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span><br><span class="line">$ lctl --net tcp conn_list</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/ksocklnd/parameters/*</span><br><span class="line">echo 512 &gt; /sys/module/ksocklnd/parameters/credits # or more</span><br><span class="line"># the number of concurrent sends (to all peers), defaults:64, set in server and client</span><br><span class="line"></span><br><span class="line">echo 240 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line">## default 180</span><br><span class="line"></span><br><span class="line">## not test</span><br><span class="line">echo 256 &gt; /sys/module/ksocklnd/parameters/peer_buffer_credits</span><br><span class="line">#default: 0; peer_buffer_credits=256 # per-peer router buffer credits</span><br><span class="line"># concurrent_sends=256 - send work-queue sizing # not test</span><br><span class="line"></span><br><span class="line">echo 32 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line">## the number of concurrent sends to a single peer, #default:8</span><br><span class="line"></span><br><span class="line">echo 70 &gt; /sys/module/ksocklnd/parameters/sock_timeout</span><br><span class="line">## default: 50 sec</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/lnet/parameters/*</span><br><span class="line">echo 15 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line">##default: 5  Acceptor&#x27;s timeout (seconds)</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span><br><span class="line"></span><br><span class="line">echo 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br><span class="line">##default: 127 Acceptor&#x27;s listen backlog</span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line">## Acceptor&#x27;s listen backlog, the number of the connections the server instance can buffer in the wait queue.</span><br><span class="line"></span><br><span class="line">echo 6 &gt; /sys/module/lnet/parameters/lnet_retry_count</span><br><span class="line">## default: 3 lnet_retry_count:Maximum number of times to retry transmitting a message</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /sys/module/lnet/parameters/use_tcp_bonding</span><br><span class="line">## default: 1  use_tcp_bonding:Set to 1 to use socklnd bonding. 0 to use Multi-Rail</span><br><span class="line"></span><br><span class="line">#default lfs.conf</span><br><span class="line">options lnet networks=tcp(enp129s0f1)</span><br><span class="line">options lnet accept_backlog=1024</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">options lnet lnet_retry_count=6</span><br><span class="line">options lnet lnet_transaction_timeout=120</span><br><span class="line">options ksocklnd credits=512</span><br><span class="line">options ksocklnd peer_credits=16</span><br><span class="line">options ptlrpc at_max=320</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=240</span><br><span class="line"></span><br><span class="line">http://lists.lustre.org/pipermail/lustre-discuss-lustre.org/2019-December/016813.html</span><br><span class="line">$ lnetctl net add --net tcp1 --if eno2  –peer-timeout 180 –peer-credits 128 –credits 1024 -peer_buffer_credits 0</span><br><span class="line">lctl set_param obdfilter.lfsbv-*.brw_size=4</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=1024</span><br><span class="line">lctl set_param osc.*.max_rpcs_in_flight=256</span><br><span class="line">lctl set_param osc.*.max_dirty_mb=2048</span><br><span class="line">or</span><br><span class="line">lctl set_param obdfilter.lfsbv-*.brw_size=16</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=4096</span><br><span class="line">lctl set_param osc.*.max_rpcs_in_flight=256</span><br><span class="line">lctl set_param osc.*.max_dirty_mb=8092</span><br><span class="line"></span><br><span class="line">$ cat /etc/modprobe.d/ksocklnd.conf</span><br><span class="line">options ksocklnd sock_timeout=100 credits=2560 peer_credits=63 enable_irq_affinity=0 concurrent_sends=63 fmr_pool_size=1280 pmr_pool_size=1280 fmr_flush_trigger=1024 nscheds=10  tx_buffer_size=1073741824 rx_buffer_size=1073741824</span><br><span class="line"></span><br><span class="line"># The maximum number of pages that will be sent in a single RPC request to the OST</span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">$ lctl set_param osc.*.max_pages_per_rpc=1024 # 1024 = 1024*4KB =4MB per RPC</span><br><span class="line">#Max RPCS in flight between OSC and OST</span><br><span class="line">$ lctl set_param -P $FNAME.osc.max_pages_per_rpc=1024</span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=128  mdc.*.max_rpcs_in_flight=128 osc.*.max_dirty_mb=1024 llite.*.max_read_ahead_mb=1024 osc.*.grant_shrink=0  osc.*.brw_size=16 osc.*.checksums=0</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*.max_rpcs_in_flight=64;</span><br><span class="line"># Max number of 4K pages per RPC</span><br><span class="line"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span><br><span class="line"></span><br><span class="line">256 = 1MB per RPC</span><br><span class="line"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span><br><span class="line">1024*4KB/1024(KB to MB)*64*2=512</span><br><span class="line">$ lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"># Maximum MBs of dirty data that can be written and queued on a client</span><br><span class="line">## Got the current dirty bytes</span><br><span class="line">$ lctl get_param osc.*.cur_dirty_bytes</span><br><span class="line">##  reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line"></span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4/1024*64*2=128</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128</span><br><span class="line"></span><br><span class="line">set the write cache</span><br><span class="line">512*4/1024*128*2=512</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=512; lctl set_param osc.*.max_rpcs_in_flight=128;lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"></span><br><span class="line">$ modinfo mdt | grep max_mod_rpcs_per_client</span><br><span class="line">parm:           max_mod_rpcs_per_client:maximum number of modify RPCs in flight allowed per client (uint)</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>DNE not auto balance</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">$ lfs mkdir -c stripe_count ./dirs ## stripe dirs in MDT DNE, loading balancing in diff MDT</span><br><span class="line">$ lfs mkdir -i mdt_index ./dirs</span><br><span class="line">$ lctl set_param fsname.mdt.enable_remote_dir=1</span><br><span class="line">$ lctl conf_param fsname.mdt.enable_remote_dir_gid=-1</span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir mdt.*.enable_remote_dir_gid</span><br><span class="line"></span><br><span class="line">$ mkdir /mdtest&#123;0..5&#125; # for 6 mdt</span><br><span class="line">clinet $ for i in &#123;0..5&#125;</span><br><span class="line">do</span><br><span class="line">  lfs mkdir -i $i /mdtest/$I</span><br><span class="line">done</span><br><span class="line">#how to test DNE feature in mdtest</span><br><span class="line">#add &quot;-u&quot; and &quot; -d /mdtest/0/@/mdtest/1/@/mdtest/2/@/mdtest/3/@/mdtest/4/@/mdtest/5/&quot;</span><br><span class="line"></span><br><span class="line">##DNE under 2.14</span><br><span class="line">$ lfs setdirstripe -c $mdt -i -1 $OUTDIR</span><br><span class="line">$ lfs setdirstripe -D -c $mdt -i -1 $OUTDIR</span><br><span class="line">$ lfs setstripe -c $OSTCOUNT --pool capacity $OUTDIR</span><br><span class="line">$ lfs setstripe -L mdt -E 64K -E -1 $OUTDIR (DoM testing)</span><br><span class="line"></span><br><span class="line">### https://www.opensfs.org/wp-content/uploads/Evaluation-of-DoM-SNE-scaling_Simmons_revised051821.pdf</span><br><span class="line">$ lctl set_param mdt.*.enable_dir_auto_split=1</span><br><span class="line">$ lctl set param mdt.*.dir_split_delta=1</span><br><span class="line">$ lctl set param mdt.*.dir_split_count=15000</span><br><span class="line"></span><br><span class="line"># modify service node</span><br><span class="line"># --erase-params</span><br><span class="line">$ dev=/dev/nvme0n1p1; tunefs.lustre --erase-params $dev; tunefs.lustre --mgsnode=192.168.0.224@tcp --servicenode=&quot;192.168.0.225@tcp:192.168.0.226@tcp&quot; --writeconf $dev; mount.lustre $dev /lustre/fs5/ -o abort_recovery</span><br><span class="line"></span><br><span class="line"># show info</span><br><span class="line">$ tunefs.lfs /dev/sdb1</span><br><span class="line">$ tune2fs -O mmp /dev/block_device # Enable MMP on ldiskfs</span><br><span class="line">$ tune2fs -O ^mmp /dev/block_device # disable MMP on ldiskfs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lctl get_param mdt.lfs-MDT0000.recovery_status</span><br><span class="line"></span><br><span class="line">$ lfs getstripe -m ./stripe/test.2</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">$ lfs getstripe -m ./stripe/test.1</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">$ lfs mkdir -c 8 ./your_dir</span><br><span class="line"></span><br><span class="line">$ lfs find --mdt-index 0 ./stripe</span><br><span class="line">./stripe/test.2</span><br><span class="line">./stripe/test.8</span><br><span class="line">./stripe/test.6</span><br><span class="line">./stripe/test.4</span><br><span class="line">./stripe/test.0</span><br><span class="line"></span><br><span class="line">$ lfs find --mdt-index 1 ./stripe</span><br><span class="line">./stripe</span><br><span class="line">./stripe/test.1</span><br><span class="line">./stripe/test.5</span><br><span class="line">./stripe/test.7</span><br><span class="line">./stripe/test.3</span><br><span class="line">./stripe/test.9</span><br></pre></td></tr></table></figure>
</li>
<li><p>read only</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param mdt.fs-MDT0000.readonly=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>show the recovery status</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param mdt.testfs-MDT0000.recovery_status</span><br><span class="line">mdt.testsfs-MDT0000.recovery_status=</span><br><span class="line">status: COMPLETE</span><br><span class="line">recovery_start: 47</span><br><span class="line">recovery_duration: 23</span><br><span class="line">completed_clients: 2/2</span><br><span class="line">replayed_requests: 0</span><br><span class="line">last_transno: 5673970243</span><br><span class="line">VBR: DISABLED</span><br><span class="line">IR: DISABLED</span><br></pre></td></tr></table></figure>
</li>
<li><p>Read cache</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#not in MDS /sys and /proc</span><br><span class="line">MDS $ lctl set_param -P osc.*.grant_shrink=0</span><br><span class="line">MDS $ lctl set_param -P osc.*.read_ahead_async_file_threshold_mb=0</span><br><span class="line">MDS $ lctl set_param -P osc.*.max_read_ahead_mb=64 ## IOPS and throughput balance</span><br><span class="line">#disable the maxinum cached file size on the OST</span><br><span class="line">#readcache_max_filesize - Controls the maximum size of a file that both the read cache and writethrough cache will try to keep in memory. Files larger than readcache_max_filesize will not be kept in cache for either reads or writes</span><br><span class="line">OSS $ lctl set_param obdfilter.*.readcache_max_filesize=32M</span><br><span class="line"></span><br><span class="line">#disable the maxinum cached file size on the OST</span><br><span class="line">OSS $ lctl set_param obdfilter.&#123;OST_name&#125;.readcache_max_filesize=-1</span><br><span class="line">OSS $ lctl get_param obdfilter.*.readcache_max_filesize</span><br><span class="line"></span><br><span class="line">#could not set -1, 0 worked</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.readcache_max_filesize</span><br><span class="line">$ lctl set_param osd-ldiskfs.*.readcache_max_filesize=0 # -1 out of range</span><br><span class="line"></span><br><span class="line">#disable read cache on all the OSTs of an OSS</span><br><span class="line">$ lctl set_param osd-ldiskfs.*.read_cache_enable=0</span><br><span class="line"></span><br><span class="line">#re-enable read cache</span><br><span class="line">$ lctl set_param osd-ldiskfs.&#123;OST_name&#125;.read_cache_enable=1</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br><span class="line"></span><br><span class="line">#disable writethrough_cache</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable=0 osd-ldiskfs.*.writethrough_cache_enable=0</span><br><span class="line"></span><br><span class="line"># re-enable</span><br><span class="line">$ lctl set_param osd-ldiskfs.*.read_cache_enable=1 osd-ldiskfs.*.writethrough_cache_enable=1</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable osd-ldiskfs.*.writethrough_cache_enable=1</span><br></pre></td></tr></table></figure>
<p>writethrough_cache_enable - Controls whether data sent to the OSS as a write request is kept in the read cache and available for later reads, or if it is discarded from cache when the write is completed. By default, the writethrough cache is enabled (writethrough_cache_enable&#x3D;1)</p>
</li>
</ul>
<p>If the writethrough cache is disabled (writethrough_cache_enabled&#x3D;0), the OSS discards the data after the write request from the client is completed. For subsequent read requests, or partial-page write requests, the OSS must re-read the data from disk    </p>
<ul>
<li><p>How many max_dirty_mb set ?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 256(max_pages_per_rpc) x 4KB = 1MB per RPC</span><br><span class="line"># 1024(max_pages_per_rpc) x 4KB = 4MB per RPC</span><br><span class="line"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span><br><span class="line"># 1024*4KB/1024(KB to MB)*64(max_rpcs_in_flight)*2=512</span><br></pre></td></tr></table></figure>
</li>
<li><p>disable debug</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=64  mdc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=512 llite.*.max_read_ahead_mb=1024 osc.*.grant_shrink=0 subsystem_debug=0 debug=0</span><br><span class="line"><span class="comment"># reduec max_read_ahead_mb=64~128 for iops</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>restricting the number of locks kept on the client (10000 locks, 20 minutes age), for the many metadata</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lru_max_age is the client LUR lock age</span></span><br><span class="line"><span class="comment">#could not set by -P</span></span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=1200000 <span class="comment">## default lru_max_age=3900000(65 mins)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>read cache</p>
<ul>
<li><p>readcache_max_filesize - Controls the maximum size of a file that both the read cache and writethrough cache will try to keep in memory. Files larger than readcache_max_filesize will not be kept in cache for either reads or writes.</p>
</li>
<li><p>max_read_ahead_per_file_mb could not large than max_read_ahead_mb</p>
</li>
<li><p>This is the global limit for all files and cannot be larger than 1&#x2F;2 of the client RAM. To disable readahead, setmax_read_ahead_mb&#x3D;0</p>
</li>
</ul>
</li>
<li><p>lock_timeouts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.*.*.lock_timeouts</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.MGS.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-OST0001-osc-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lock_timeouts=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>osp status</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.*.active</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.active=1</span><br><span class="line">osp.fsname-OST0001-osc-MDT0000.active=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>mdt info</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osd-zfs.fsname-MDT0000.mntdev</span><br><span class="line">osd-zfs.fsname-MDT0000.mntdev=test_mdt/test_mdt0</span><br><span class="line"></span><br><span class="line">$ lctl get_param mgs.MGS.mntdev</span><br><span class="line">mgs.MGS.mntdev=test_mdt/test_mdt0</span><br><span class="line"></span><br><span class="line">$ lctl barrier_stat fsname</span><br><span class="line">state: init</span><br><span class="line">timeout: 0 seconds</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_dir_migration</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_remote_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_striped_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_dir_migration</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_remote_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_striped_dir</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
</li>
<li><p>The global write barriers</p>
<ul>
<li>Snapshots are non-atomic across multiple MDTs and OSTs, which means that if there is activity on the file system while a snapshot is being taken, there may be user-visible namespace inconsistencies with files created or destroyed in the interval between the MDT and OST snapshots. In order to create a consistent snapshot of the file system, we are able to set a global write barrier, or “freeze” the system. Once set, all metadata modifications will be blocked until the write barrier is actively removed (“thawed”) or expired. The user can set a timeout parameter on a global barrier or the barrier can be explicitly removed. The default timeout period is 30 seconds.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lctl barrier_freeze $FSNAME 30</span><br><span class="line">$ lctl barrier_thaw $FSNAME</span><br><span class="line">$ lctl barrier_rescan $FSNAME</span><br><span class="line">0 of 1 MDT(s) in the filesystem $FSNAME are inactive</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>The threshold at which the allocation method switches from round-robin to weighted is set in this file. The default is to switch to the weighted algorithm when any two OSTs are out of balance by more than 17 percent.</p>
<ul>
<li>The weighting priority used by the weighted allocator can be adjusted in this file. Increasing the value of qos_prio_free puts more weighting on the amount of free space available on each OST and less on how stripes are distributed across OSTs. The default value is 91 percent weighting for free space rebalancing and 9 percent for OST balancing. When the free space priority is set to 100, weighting is based entirely on free space and location is no longer used by the striping algorithm<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param  lod.*.qos_prio_free</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_prio_free=91%</span><br><span class="line">$ lctl get_param  lod.*.qos_threshold_rr</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_threshold_rr=17%</span><br><span class="line">lod.*.qos_threshold_rr -</span><br><span class="line">lod.*.qos_prio_free -</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>The weighting priority used by the weighted allocator can be adjusted in this file. Increasing the value of qos_prio_free puts more weighting on the amount of free space available on each OST and less on how stripes are distributed across OSTs. The default value is 91 percent weighting for free space rebalancing and 9 percent for OST balancing. When the free space priority is set to 100, weighting is based entirely on free space and location is no longer used by the striping algorithm   </p>
</li>
<li><p>max create count</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.*.max_create_count</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.max_create_count=20000</span><br><span class="line"></span><br><span class="line"># disable the OST # default value 20000</span><br><span class="line">MDS $ lctl set_param osp.$FNAME-OST00XX-osc-MDT*.max_create_count=0</span><br><span class="line">MDS $ lctl set_param osp.$FNAME-OST00XX*.max_create_count=0</span><br><span class="line"></span><br><span class="line">#degrade will only prefer to skip the OST</span><br><span class="line">OSS $ lctl set_param obdfilter.$FNAME-OST0000.degraded=1</span><br><span class="line"></span><br><span class="line">#disable pre-create</span><br><span class="line">OSS $ lctl set_param obdfilter.$FNAME-OST0000*.no_precreate=1</span><br><span class="line"></span><br><span class="line">#get the Target</span><br><span class="line">                                                     ------ block dev</span><br><span class="line">                                                     |</span><br><span class="line">OSS $ lctl get_param obdfilter.$(tunefs.lustre /dev/md1 2&gt;/dev/null | grep -i Target | uniq | awk &#x27;&#123;print $2&#125;&#x27;).degraded</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>With lfs 2.9 and later, the MDS should be set to only disable file creation on that OST by setting max_create_count to zero</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param osp.osc_name.max_create_count=0</span><br><span class="line"></span><br><span class="line">$ $ lctl get_param osp.fsname-OST0000-osc-MDT0000.active</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.active=1</span><br><span class="line"></span><br><span class="line">#to deactivate the OSC on the MDS node(s) use:</span><br><span class="line">$ lctl set_param osp.fsname-OST0000-osc-MDT0000.active=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>dmo lock</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param -n mdt.*.dom_lock</span><br><span class="line">always</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n lod.*.dom_stripesize</span><br><span class="line">1048576</span><br></pre></td></tr></table></figure>
</li>
<li><p>reserved_mb_high</p>
<ul>
<li>the high watermark used to start object allocation if available space is more than this. The default is 0.2% of total OST size</li>
</ul>
</li>
<li><p>reserved_mb_low</p>
<ul>
<li>The low watermark used to stop object allocation if available space is less than this. The default is 0.1% of total OST size<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.*.reserved_mb_high</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.reserved_mb_high=255329</span><br><span class="line">$ lctl get_param osp.*.reserved_mb_low</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.reserved_mb_low=127664</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>force_sync</p>
<ul>
<li>commits and closes the current open journal transaction. This function also has a flag called force_sync<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param osp.*.force_sync=1</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.force_sync=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param  osp.*MDT*.sync_changes</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.sync_changes=0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>max_nolock_bytes</p>
<ul>
<li>Server-side locking set only for requests less than the blocks set in themax_nolock_bytes parameter. If this tunable is set to zero (0), it disables server-side locking for read&#x2F;write requests<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.max_nolock_bytes</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.MGS.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.max_nolock_bytes=0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>contention_seconds</p>
<ul>
<li>The resource keeps itself in a contended state as set in the parameter<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.contention_seconds</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contention_seconds=2</span><br><span class="line">ldlm.namespaces.MGS.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.contention_seconds=2</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.contention_seconds=2</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>contended_locks </p>
<ul>
<li>If the number of lock conflicts in the scan of granted and waiting queues at contended_locks is exceeded, the resource is considered to be contended.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.contended_locks</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contended_locks=32</span><br><span class="line">ldlm.namespaces.MGS.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.contended_locks=32</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.contended_locks=32</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.lock_reclaim_threshold_mb</span><br><span class="line">ldlm.lock_reclaim_threshold_mb=51542</span><br></pre></td></tr></table></figure>

<h4 id="monitor"><a href="#monitor" class="headerlink" title="monitor"></a>monitor</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br></pre></td><td class="code"><pre><span class="line">#you could replace stats to * to get more info, lctl get_param mds.MDS.mdt.*</span><br><span class="line">$ lctl get_param mdt.*MDT*.exports.*@*.stats</span><br><span class="line"></span><br><span class="line">#https://wiki.lustre.org/images/8/88/Lustre-Metrics-New-Techniques-for-Monitoring_Nolin_Wagner.pdf</span><br><span class="line"></span><br><span class="line">#Monitor all MDS</span><br><span class="line">$ lctl get_param mdt.*-MDT0000.md_stats osd-*.*MDT*.filesfree osd-*.*MDT*.filestotal  osd-*.*MDT*.kbytesfree  osd-*.*MDT*.kbytestotal</span><br><span class="line">mdt.fsname-MDT0000.md_stats=</span><br><span class="line">snapshot_time             1619838866.314148577 secs.nsecs</span><br><span class="line">open                      599 samples [reqs]</span><br><span class="line">close                     137 samples [reqs] 1 1 137</span><br><span class="line">mknod                     23 samples [reqs] 1 1 23 (min, max, sum)</span><br><span class="line">unlink                    9 samples [reqs]</span><br><span class="line">mkdir                     3 samples [reqs]</span><br><span class="line">rename                    6 samples [reqs]</span><br><span class="line">getattr                   87 samples [reqs]</span><br><span class="line">setattr                   17 samples [reqs]</span><br><span class="line">getxattr                  25 samples [reqs]</span><br><span class="line">setxattr                  11 samples [reqs]</span><br><span class="line">statfs                    25 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">samedir_rename            3 samples [reqs]</span><br><span class="line">crossdir_rename           3 samples [reqs]</span><br><span class="line"></span><br><span class="line">$ lctl get_param osd-*.*MDT*.filesfree</span><br><span class="line">osd-zfs.testfs-MDT0000.filesfree=797139104</span><br><span class="line">$ lctl get_param osd-*.*MDT*.filestotal</span><br><span class="line">osd-zfs.testfs-MDT0000.filestotal=811462460</span><br><span class="line">$ lctl get_param osd-*.*MDT*.kbytesfree</span><br><span class="line">osd-zfs.testfs-MDT0000.kbytesfree=3188556416</span><br><span class="line">$ lctl get_param osd-*.*MDT*.kbytestotal</span><br><span class="line">osd-zfs.testfs-MDT0000.kbytestotal=3196029568</span><br><span class="line"></span><br><span class="line">$ lctl set_param mdt.*.md_stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl list_param mdt.*.rename_stats</span><br><span class="line">mdt.fsname-MDT0000.rename_stats</span><br><span class="line">$ lctl get_param mdt.*.rename_stats</span><br><span class="line">mdt.fsname-MDT0000.rename_stats=</span><br><span class="line">rename_stats:</span><br><span class="line">- snapshot_time:  1619838907.404363357</span><br><span class="line">- same_dir</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line">- crossdir_src</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line">- crossdir_tgt</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line"></span><br><span class="line">$ lctl get_param mds.MDS.mdt.stats</span><br><span class="line">mds.MDS.mdt.stats=</span><br><span class="line">snapshot_time             1619836421.293308463 secs.nsecs</span><br><span class="line">req_waittime              13090 samples [usec] 5 994 1420115 197067923 (max, min, sum, sum of squares)</span><br><span class="line">req_qdepth                13090 samples [reqs] 0 1 1 1</span><br><span class="line">req_active                13090 samples [reqs] 1 2 13094 13102</span><br><span class="line">req_timeout               13090 samples [sec] 50 50 654500 32725000</span><br><span class="line">reqbuf_avail              26152 samples [bufs] 63 64 1673657 107109575</span><br><span class="line">ldlm_ibits_enqueue        519 samples [reqs] 1 1 519 519</span><br><span class="line">mds_reint_rename          2 samples [reqs] 1 1 2 2</span><br><span class="line">mds_reint_open            162 samples [reqs] 1 1 162 162</span><br><span class="line">mds_getattr               2 samples [usec] 65 75 140 9850</span><br><span class="line">mds_getattr_lock          3 samples [usec] 44 192 296 42400</span><br><span class="line">mds_connect               5 samples [usec] 17 10421 10865 108662423</span><br><span class="line">mds_disconnect            1 samples [usec] 210 210 210 44100</span><br><span class="line">mds_get_root              2 samples [usec] 16 41 57 1937</span><br><span class="line">mds_statfs                22 samples [usec] 27 203 1414 122592</span><br><span class="line">obd_ping                  12530 samples [usec] 4 565 439527 19537985</span><br><span class="line"></span><br><span class="line">$ lctl get_param mds.MDS.mdt_setattr.stats</span><br><span class="line">mds.MDS.mdt_setattr.stats=</span><br><span class="line">snapshot_time             1619836714.197733538 secs.nsecs</span><br><span class="line"></span><br><span class="line"># Metadata readdir service</span><br><span class="line">$ lctl get_param mds.MDS.mdt_readpage.stats</span><br><span class="line">mds.MDS.mdt_readpage.stats=</span><br><span class="line">snapshot_time             1619794903.241168757 secs.nsecs</span><br><span class="line">req_waittime              13 samples [usec] 10 86 509 25623</span><br><span class="line">req_qdepth                13 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                13 samples [reqs] 1 1 13 13</span><br><span class="line">req_timeout               13 samples [sec] 50 50 650 32500</span><br><span class="line">reqbuf_avail              32 samples [bufs] 63 64 2042 130310</span><br><span class="line">mds_close                 7 samples [usec] 21 153 578 60484</span><br><span class="line">mds_readpage              6 samples [usec] 270 498 2423 1031057</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*0000-osc-*.stats</span><br><span class="line">osc.fsname-OST0000-osc-MDT0000.stats=</span><br><span class="line">snapshot_time             1619794678.828478520 secs.nsecs</span><br><span class="line">req_waittime              18644 samples [usec] 237 8040 18490448 19195339776</span><br><span class="line">req_active                18644 samples [reqs] 1 2 18645 18647</span><br><span class="line">ost_create                2 samples [usec] 237 685 922 525394</span><br><span class="line">ost_get_info              1 samples [usec] 1951 1951 1951 3806401</span><br><span class="line">ost_connect               1 samples [usec] 1074 1074 1074 1153476</span><br><span class="line">ost_statfs                18640 samples [usec] 306 8040 18486501 19189854505</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619836833.236763001 secs.nsecs</span><br><span class="line">req_waittime              20 samples [usec] 17 683 2564 723094</span><br><span class="line">req_qdepth                20 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                20 samples [reqs] 1 1 20 20</span><br><span class="line">req_timeout               20 samples [sec] 50 50 1000 50000</span><br><span class="line">reqbuf_avail              53 samples [bufs] 63 64 3388 216580</span><br><span class="line">ldlm_cancel               20 samples [usec] 5 224 1432 162534</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_cbd.stats</span><br><span class="line">ldlm.services.ldlm_cbd.stats=</span><br><span class="line">snapshot_time             1619836850.112362316 secs.nsecs</span><br><span class="line">req_waittime              5 samples [usec] 25 189 549 80843</span><br><span class="line">req_qdepth                5 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                5 samples [reqs] 1 1 5 5</span><br><span class="line">req_timeout               5 samples [sec] 50 50 250 12500</span><br><span class="line">reqbuf_avail              13 samples [bufs] 1 1 13 13</span><br><span class="line">ldlm_bl_callback          5 samples [usec] 14 67 146 6134</span><br><span class="line"></span><br><span class="line">$ lctl --device MGS llog_print $&#123;fsname&#125;-MDT0000</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;target: $FSNAME-MDT&quot;</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;connect_flags&quot; | grep disp_stripe</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;import flags&quot; ### compare with under export flags</span><br><span class="line">$ lct get_param -n mgc.*.uuid</span><br><span class="line"></span><br><span class="line">$ lctl --device MGS llog_print $fsname-MDT0000     # show all</span><br><span class="line">$ lctl --device MGS llog_print $fsname-MDT0000  $star_index $end_index</span><br><span class="line">$ lctl --device MGS llog_cancel $fsname-MDT0000 $wrong_id ## delete wrong</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">$ cat /proc/fs/lfs/mgc/MGC192.168.0.1@tcp1/uuid</span><br><span class="line"></span><br><span class="line">$ lctl get_param -N mgs.MGS.exports.*</span><br><span class="line">mgs.MGS.exports.$client_ip@tcp1</span><br><span class="line"></span><br><span class="line"># 72 bytes is the minimum space required to store striping</span><br><span class="line"># information for a file striped across one OST:</span><br><span class="line"># (sizeof(struct lov_user_md_v3) +</span><br><span class="line">#  sizeof(struct lov_user_ost_data_v1))</span><br><span class="line"># not work in 2.12.6 ?</span><br><span class="line">$ lctl set_param -n llite.*.default_easize 72</span><br><span class="line"></span><br><span class="line"># 0 means Disable O_APPEND striping, verify it works</span><br><span class="line">#</span><br><span class="line">$ lctl get_param mdd.*.append_stripe_count</span><br><span class="line">mdd.lfs-MDT0000.append_stripe_count=1</span><br><span class="line">$ lctl set_param mdd.*.append_stripe_count=2</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdd.*.append_pool</span><br><span class="line">mdd.lfs-MDT0000.append_pool=</span><br><span class="line">$ lctl set_param mdd.*.append_pool=&#x27;none&#x27;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>FAIL_MDS_LOV_PREP_CREATE</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#define OBD_FAIL_MDS_LOV_PREP_CREATE 0x141</span><br><span class="line">$ lctl set_param fail_loc=0x80000141</span><br><span class="line">#define OBD_FAIL_MDS_READLINK_EPROTO     0x143</span><br><span class="line">touch $DIR/$tdir/$tfile || true</span><br><span class="line"></span><br><span class="line">$ lctl set_param fail_loc=0x80000143</span><br><span class="line">ls -l /lfs/$foo &amp;&amp; echo &quot;no error&quot;</span><br><span class="line"></span><br><span class="line">#define OBD_FAIL_OSD_LMA_INCOMPAT 0x194</span><br><span class="line">$ lctl set_param fail_loc=0x194</span><br><span class="line">ls -l $wdir/$tfile &amp;&amp; echo &quot;no error&quot;</span><br><span class="line"></span><br><span class="line">#define OBD_FAIL_LDLM_ENQUEUE_OLD_EXPORT 0x30e</span><br><span class="line">touch $DIR/f74a</span><br><span class="line">lctl set_param fail_loc=0x8000030e</span><br><span class="line">ls $DIR/f74a</span><br><span class="line">lctl set_param fail_loc=0</span><br><span class="line"></span><br><span class="line">#define OBD_FAIL_OSC_CHECKSUM_RECEIVE       0x408</span><br><span class="line">$ lctl set_param fail_loc=0x80000408</span><br><span class="line">$ dd if=$DIR/$tfile of=/dev/null bs=1M || error &quot;dd read error: $?&quot;</span><br><span class="line">$ lctl set_param fail_loc=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>stripesize</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param -n lod.*.stripesize</span><br><span class="line">1048576</span><br></pre></td></tr></table></figure>
</li>
<li><p>pool</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#Add pool</span><br><span class="line">MDS $ lctl pool_new &lt;fsname&gt;.&lt;poolname&gt;</span><br><span class="line">MDS $ lctl pool_add $FSNAME.pool1 OST[0-10/2]</span><br><span class="line">MDS $ lctl pool_list $FSNAME</span><br><span class="line">MDS $ lctl setstripe -p $FSNAME.pool1 /lfs/test</span><br></pre></td></tr></table></figure>
</li>
<li><p>atime_diff</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># OSTs by default only hold a transient atime that is updated when clients do read requests. Permanent atime is written to the MDT when the file is closed. However, on-disk atime is only updated if it is more than 60 seconds old</span><br><span class="line">In lfs 2.14, it is possible to set the OSTs to persistently store atime with each object, in order to get more accurate persistent atime updates for files that are open for a long time via the similarly-named obdfilter.*.atime_diff parameter.</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdd.*MDT0000*.atime_diff</span><br><span class="line">60</span><br></pre></td></tr></table></figure>
</li>
<li><p>prealloc id</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.*.prealloc_last_id</span><br><span class="line">osp.lfs-OST0000-osc-MDT0000.prealloc_last_id=121337627</span><br><span class="line">osp.lfs-OST0001-osc-MDT0000.prealloc_last_id=122220489</span><br><span class="line">osp.lfs-OST0002-osc-MDT0000.prealloc_last_id=62659128</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.*.prealloc_next_id</span><br><span class="line">osp.lfs-OST0000-osc-MDT0000.prealloc_next_id=121337612</span><br><span class="line">osp.lfs-OST0001-osc-MDT0000.prealloc_next_id=122220456</span><br><span class="line">osp.lfs-OST0002-osc-MDT0000.prealloc_next_id=62659083</span><br></pre></td></tr></table></figure>
</li>
<li><p>enable_remote_dir_gid</p>
<ul>
<li>With lfs software version 2.8, a new tunable is available to allow users with a specific group ID to create and delete remote and striped directories. This tunable is enable_remote_dir_gid. For example, setting this parameter to the ‘wheel’ or ‘admin’ group ID allows users with that GID to create and delete remote and striped directories. Setting this parameter to -1 on MDT0000 to permanently allow any non-root users create and delete remote and striped directories. On the MGS execute the following command<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid</span><br><span class="line">mdt.fsname-MDT0000.enable_remote_dir_gid=0</span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid=-1</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid</span><br><span class="line">mdt.lfs-MDT0000.enable_remote_dir_gid=0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>lfsck</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ lctl lfsck_start -M $(facet_svc mds1) -A -C -t namespace</span><br><span class="line">$ lctl lfsck_start --device $(facet_svc mds1) -A -C -t namespace</span><br><span class="line">$ lctl get_param mdd.*.lfsck_namespace&quot;</span><br><span class="line">mdd.lfs-MDT0000.lfsck_namespace=</span><br><span class="line">name: lfsck_namespace</span><br><span class="line">magic: 0xa06249ff</span><br><span class="line">version: 2</span><br><span class="line">status: init</span><br><span class="line">flags:</span><br><span class="line">param:</span><br><span class="line">last_completed_time: N/A</span><br><span class="line">time_since_last_completed: N/A</span><br><span class="line">latest_start_time: N/A</span><br><span class="line">time_since_latest_start: N/A</span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>prioritizes free space</p>
<ul>
<li>This setting controls how much lfs prioritizes free space (versus location) in allocation. The higher this number, the more lfs takes empty space on an OST into consideration for its allocation. When set to 100%, lfs uses ONLY empty space as the deciding factor for writes. Remember, this setting is only taken into consideration when lfs believes the OSTs to be imbalanced If you have set qos_threshold_rr to 100, this setting will have no effect.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param  lo[vd].*-mdtlov.qos_prio_free</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_prio_free=91%</span><br><span class="line">lov.fsname-MDT0000-mdtlov.qos_prio_free=91%</span><br><span class="line"></span><br><span class="line">$ lctl get_param *.*MDT0000-mdtlov.qos_threshold_rr</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_threshold_rr=17% <span class="comment">## set 100 means forces lfs to round-robin because it believes the OSTs are balanced</span></span><br><span class="line">lov.fsname-MDT0000-mdtlov.qos_threshold_rr=17%</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="OSS"><a href="#OSS" class="headerlink" title="OSS"></a>OSS</h3><ul>
<li><p>io.timeouts</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#in the over loading storage, increase the value</span></span><br><span class="line">$ lctl get_param -n ost.*.ost_io.timeouts</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.&#123;state,timeouts&#125;</span><br><span class="line">$ lctl get_param at_* <span class="built_in">timeout</span></span><br><span class="line">$ lctl get_param llite.*<span class="variable">$FNAME</span>*.stats</span><br></pre></td></tr></table></figure>
</li>
<li><p>grant info</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.testfs-MDT0000*.pool.*</span><br></pre></td></tr></table></figure>
</li>
<li><p>filefree</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param obdfilter.*.filesfree</span><br><span class="line">obdfilter.fsname-OST0000.filesfree=4183295296</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n osd-*.*OST0000.kbytesfree</span><br></pre></td></tr></table></figure>
</li>
<li><p><a target="_blank" rel="noopener" href="https://wiki.lustre.org/images/9/96/SDSC-Data-Oasis-GEn-II_Wagner.pdf">numa</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#example</span></span><br><span class="line">options lnet networks=<span class="string">&quot;tcp(bond0)&quot;</span></span><br><span class="line">libcfs.conf</span><br><span class="line">options libcfs cpu_pattern=<span class="string">&quot;0[2-7] 1[8-15]&quot;</span></span><br><span class="line">options ost oss_io_cpts=<span class="string">&quot;[0,1]&quot;</span> oss_cpts=<span class="string">&quot;[1]&quot;</span></span><br><span class="line">ksocklnd.conf</span><br><span class="line">options ksocklnd nscheds=6 peer_credits=48 credits=1024</span><br><span class="line"></span><br><span class="line"><span class="comment">#not set</span></span><br><span class="line">$ <span class="built_in">cat</span> /sys/module/ost/parameters/oss_cpts /sys/module/ost/parameters/oss_io_cpts /sys/module/ptlrpc/parameters/ldlm_cpts /sys/module/ptlrpc/parameters/ptlrpcd_cpts /sys/module/ptlrpc/parameters/ptlrpcd_per_cpt_max<span class="string">&quot;</span></span><br><span class="line"><span class="string">(null)</span></span><br><span class="line"><span class="string">(null)</span></span><br><span class="line"><span class="string">(null)</span></span><br><span class="line"><span class="string">(null)</span></span><br><span class="line"><span class="string">0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#no locality between the OST thread handling an RPC and the OST storage,  because the request has to be handled by a specific OST device regardless of which network interface the request arrives on</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>contended_locks</p>
<ul>
<li>If the number of lock conflicts in the scan of granted and waiting queues at contended_locks is exceeded, the resource is considered to be contended.</li>
</ul>
</li>
<li><p>contention_seconds</p>
<ul>
<li>The resource keeps itself in a contended state as set in the parameter.</li>
</ul>
</li>
<li><p>max_nolock_bytes</p>
<ul>
<li>Server-side locking set only for requests less than the blocks set in the max_nolock_bytes parameter. If this tunable is set to zero (0), it disables server-side locking for read&#x2F;write requests.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.max_nolock_bytes</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.max_nolock_bytes=0</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contention_seconds</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contention_seconds=2</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.contention_seconds=2</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contended_locks</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contended_locks=32</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.contended_locks=32</span><br><span class="line"></span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*.max_nolock_bytes=2000000</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*.max_nolock_bytes=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.lock_reclaim_threshold_mb</span><br><span class="line">ldlm.lock_reclaim_threshold_mb=736</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Read and print the last_rcvd file from a device</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">#display client information</span><br><span class="line">$ lr_reader -c /dev/sdh</span><br><span class="line">last_rcvd:</span><br><span class="line">uuid: fsms-MDT0000_UUID</span><br><span class="line"> feature_compat: 0x8</span><br><span class="line"> feature_incompat: 0x61c</span><br><span class="line"> feature_rocompat: 0x1</span><br><span class="line"> last_transaction: 4294967298</span><br><span class="line"> target_index: 0</span><br><span class="line"> mount_count: 1</span><br><span class="line"> client_area_start: 8192</span><br><span class="line"> client_area_size: 128</span><br><span class="line"> 79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br><span class="line"> generation: 2</span><br><span class="line"> last_transaction: 0</span><br><span class="line"> last_xid: 0</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line">#display reply data information</span><br><span class="line">$ lr_reader -r /dev/sdh</span><br><span class="line">...</span><br><span class="line">reply_data:</span><br><span class="line"> 0:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736549</span><br><span class="line"> last_xid: 1511845291497772</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"> 1:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736566</span><br><span class="line"> last_xid: 1511845291498048</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"></span><br><span class="line">$ cat /proc/fs/ldiskfs/dm-xx/options</span><br><span class="line">rw</span><br><span class="line">barrier</span><br><span class="line">no_mbcache</span><br><span class="line">user_xattr</span><br><span class="line">acl</span><br><span class="line">resuid=0</span><br><span class="line">resgid=0</span><br><span class="line">errors=remount-ro</span><br><span class="line">commit=5</span><br><span class="line">min_batch_time=0</span><br><span class="line">max_batch_time=15000</span><br><span class="line">stripe=0</span><br><span class="line">data=ordered</span><br><span class="line">inode_readahead_blks=32</span><br><span class="line">init_itable=10</span><br><span class="line">max_dir_size_kb=0</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param obdfilter.*.stats obdfilter.*OST*.kbytesfree ldlm.namespaces.filter-*.pool.granted</span><br><span class="line">obdfilter.fsname-OST0000.stats=</span><br><span class="line">snapshot_time             1619839622.706200894 secs.nsecs</span><br><span class="line">read_bytes                130 samples [bytes] 4096 1048576 109461504</span><br><span class="line">write_bytes               115 samples [bytes] 444 1048576 105471940</span><br><span class="line">setattr                   10 samples [reqs]</span><br><span class="line">punch                     4 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">destroy                   9 samples [reqs]</span><br><span class="line">create                    3 samples [reqs]</span><br><span class="line">statfs                    27512 samples [reqs]</span><br><span class="line">get_info                  5 samples [reqs]</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.namespaces.filter-*.pool.granted</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0001_UUID.pool.granted=34</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0003_UUID.pool.granted=24</span><br><span class="line">$ lctl get_param ldlm.namespaces.filter-*.pool.grant_rate</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0001_UUID.pool.grant_rate=83</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0003_UUID.pool.grant_rate=54</span><br><span class="line">$ lctl get_param ldlm.namespaces.filter-*.pool.cancel_rate</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0001_UUID.pool.cancel_rate=70</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0003_UUID.pool.cancel_rate=61</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lctl set_param obdfilter.*.stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.*OST*.exports.*@*.stats</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.*.exports.*.stats</span><br><span class="line">obdfilter.fsname-OST0000.exports.192.168.0.238@tcp.stats=</span><br><span class="line">snapshot_time             1619839674.573369178 secs.nsecs</span><br><span class="line">setattr                   1 samples [reqs]</span><br><span class="line">destroy                   9 samples [reqs]</span><br><span class="line">create                    3 samples [reqs]</span><br><span class="line">statfs                    27498 samples [reqs]</span><br><span class="line">get_info                  1 samples [reqs]</span><br><span class="line">obdfilter.fsname-OST0000.exports.192.168.0.233@tcp.stats=</span><br><span class="line">snapshot_time             1619839674.573451068 secs.nsecs</span><br><span class="line">read_bytes                130 samples [bytes] 4096 1048576 109461504</span><br><span class="line">write_bytes               115 samples [bytes] 444 1048576 105471940</span><br><span class="line">setattr                   9 samples [reqs]</span><br><span class="line">punch                     4 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">statfs                    25 samples [reqs]</span><br><span class="line">get_info                  4 samples [reqs]</span><br><span class="line">$ lctl set_param obdfilter.*.exports.*.stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param ost.OSS.ost.stats</span><br><span class="line">ost.OSS.ost.stats=</span><br><span class="line">snapshot_time             1619837407.840675602 secs.nsecs</span><br><span class="line">req_waittime              1786 samples [usec] 18 309 235121 35937999</span><br><span class="line">req_qdepth                1786 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                1786 samples [reqs] 1 2 1788 1792</span><br><span class="line">req_timeout               1786 samples [sec] 50 50 89300 4465000</span><br><span class="line">reqbuf_avail              5344 samples [bufs] 61 64 338434 21434154</span><br><span class="line">ldlm_glimpse_enqueue      5 samples [reqs] 1 1 5 5</span><br><span class="line">ldlm_extent_enqueue       3 samples [reqs] 1 1 3 3</span><br><span class="line">ost_create                2 samples [usec] 49 528 577 281185</span><br><span class="line">ost_get_info              1 samples [usec] 1498 1498 1498 2244004</span><br><span class="line">ost_connect               3 samples [usec] 669 1574 3159 3764093</span><br><span class="line">ost_disconnect            1 samples [usec] 305 305 305 93025</span><br><span class="line">obd_ping                  1771 samples [usec] 12 126 78662 3931506</span><br><span class="line"></span><br><span class="line">$ lctl get_param ost.OSS.ost_create.stats</span><br><span class="line">ost.OSS.ost_create.stats=</span><br><span class="line">snapshot_time             1619837439.899464538 secs.nsecs</span><br><span class="line">req_waittime              27072 samples [usec] 23 523 3523516 532341790</span><br><span class="line">req_qdepth                27072 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                27072 samples [reqs] 1 1 27072 27072</span><br><span class="line">req_timeout               27072 samples [sec] 50 50 1353600 67680000</span><br><span class="line">reqbuf_avail              54351 samples [bufs] 63 64 3424493 215767379</span><br><span class="line">ost_statfs                27072 samples [usec] 21 252 1600400 103030806</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619837467.579931100 secs.nsecs</span><br><span class="line">req_waittime              3 samples [usec] 100 171 423 62345</span><br><span class="line">req_qdepth                3 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                3 samples [reqs] 1 1 3 3</span><br><span class="line">req_timeout               3 samples [sec] 50 50 150 7500</span><br><span class="line">reqbuf_avail              9 samples [bufs] 64 64 576 36864</span><br><span class="line">ldlm_cancel               3 samples [usec] 49 121 248 23126</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619837467.579931100 secs.nsecs</span><br><span class="line">req_waittime              3 samples [usec] 100 171 423 62345</span><br><span class="line">req_qdepth                3 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                3 samples [reqs] 1 1 3 3</span><br><span class="line">req_timeout               3 samples [sec] 50 50 150 7500</span><br><span class="line">reqbuf_avail              9 samples [bufs] 64 64 576 36864</span><br><span class="line">ldlm_cancel               3 samples [usec] 49 121 248 23126</span><br><span class="line">$ lctl get_param ldlm.services.ldlm_cbd.stats</span><br><span class="line">ldlm.services.ldlm_cbd.stats=</span><br><span class="line">snapshot_time             1619837479.839607820 secs.nsecs</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param osc.testfs-OST0001*.*max*   /  lctl get_param ldlm.namespaces.*.*max*</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_parallel_ast=1024</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.max_parallel_ast=1024</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.max_parallel_ast=1024</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param osc.testfs-OST0001*.*grant*  / lctl get_param ldlm.namespaces.*.pool.*grant*</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.granted=0</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.grant_plan=4276</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.grant_speed=-426</span><br><span class="line">ldlm.namespaces.filter-testfs1-OST0000_UUID.pool.granted=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.granted=0</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.testfs1-MDT0000-*.*.*</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.cancel_rate=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.granted=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.limit=1</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.recalc_period=10</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.server_lock_volume=0</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.state=</span><br><span class="line">LDLM pool state (ldlm-pool-testfs1-MDT0000-lwp-OST0000-1):</span><br><span class="line">  SLV: 0</span><br><span class="line">  CLV: 0</span><br><span class="line">  LVF: 1</span><br><span class="line">  GR:  0</span><br><span class="line">  CR:  0</span><br><span class="line">  GS:  0</span><br><span class="line">  G:   0</span><br><span class="line">  L:   1</span><br><span class="line">ldlm.namespaces.testfs1-MDT0000-lwp-OST0000.pool.stats=</span><br><span class="line">snapshot_time             1647398330.677429043 secs.nsecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param obdfilter.*.brw_stats / lctl get_param osd-zfs.*.brw_stats</span><br><span class="line"></span><br><span class="line">obdfilter.testfs-OST0000.brw_stats=</span><br><span class="line">snapshot_time:         1593407004.726097825 (secs.nsecs)</span><br><span class="line"></span><br><span class="line">                           read      |     write</span><br><span class="line">pages per bulk r/w     rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">1:                 6006159   2   2   | 1239974   1   1</span><br><span class="line">2:                45845506  16  18   | 1809852   2   4</span><br><span class="line">4:                 1952464   0  18   | 53814   0   4</span><br><span class="line">8:                33533746  11  30   | 832520   1   5</span><br><span class="line">16:               12859649   4  35   | 91425   0   5</span><br><span class="line">32:                 620131   0  35   | 81660   0   5</span><br><span class="line">64:               38856531  13  49   | 307195   0   5</span><br><span class="line">128:               1697223   0  49   | 223084   0   6</span><br><span class="line">256:             142793845  50 100   | 69219436  93 100</span><br><span class="line"></span><br><span class="line">                           read      |     write</span><br><span class="line">discontiguous pages    rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">0:               284165254 100 100   | 1244480   1   1</span><br><span class="line">1:                       0   0 100   | 1810011   2   4</span><br><span class="line">2:                       0   0 100   | 14961   0   4</span><br><span class="line">3:                       0   0 100   | 38853   0   4</span><br><span class="line">4:                       0   0 100   | 10409   0   4</span><br><span class="line">5:                       0   0 100   | 23853   0   4</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ost.OSS.ost_io.req_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#show it in the lnetctl</span><br><span class="line">$  lnetctl stats show</span><br><span class="line">statistics:</span><br><span class="line">...</span><br><span class="line">    resend_count: 0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.$FSNAME*.checksums</span><br><span class="line"></span><br><span class="line">#All OST available bytes</span><br><span class="line">$ lctl get_param lov.zfsz2-clilov-*.kbytesavail</span><br><span class="line">lov.zfsz2-clilov-ffff8dab3afe6000.kbytesavail=139702738176</span><br><span class="line"></span><br><span class="line">$ lctl get_param lov.zfsz2-clilov-*.kbytestotal</span><br><span class="line">lov.zfsz2-clilov-ffff8dab3afe6000.kbytestotal=1038674946560</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.rpc_stats</span><br><span class="line">snapshot_time:         1619688798.673860765 (secs.nsecs)</span><br><span class="line">read RPCs in flight:  0</span><br><span class="line">write RPCs in flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending read pages:   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                       0   0   0   |          0   0   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">rpcs in flight        rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">0:                       0   0   0   |          0   0   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">offset                rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">0:                       0   0   0   |          0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl lov_getconfig /mnt</span><br><span class="line">default_stripe_count: 1</span><br><span class="line">default_stripe_size: 1048576</span><br><span class="line">default_stripe_offset: 18446744073709551615</span><br><span class="line">default_stripe_pattern: 1</span><br><span class="line">obd_count: 15</span><br><span class="line">OBDS:   obdidx          obdgen           obduuid</span><br><span class="line">             0               1           lfs-OST0000_UUID</span><br><span class="line">             1               1           lfs-OST0001_UUID</span><br><span class="line">             2               1           lfs-OST0002_UUID</span><br><span class="line">             3               1           lfs-OST0003_UUID</span><br><span class="line">             4               1           lfs-OST0004_UUID</span><br><span class="line">             5               1           lfs-OST0005_UUID</span><br><span class="line">             6               1           lfs-OST0006_UUID</span><br><span class="line">             7               1           lfs-OST0007_UUID</span><br><span class="line">             8               1           lfs-OST0008_UUID</span><br><span class="line">             9               1           lfs-OST0009_UUID</span><br><span class="line">            10               1           lfs-OST000a_UUID</span><br><span class="line">            11               1           lfs-OST000b_UUID</span><br><span class="line">            12               1           lfs-OST000c_UUID</span><br><span class="line">            13               1           lfs-OST000d_UUID</span><br><span class="line">            14               1           lfs-OST000e_UUID</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000-osc-[^mM]*.cur_grant_bytes</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab37559000.cur_grant_bytes=2097152</span><br><span class="line">osc.zfsz2-OST0000-osc-ffff8dab3afe6000.cur_grant_bytes=3407872</span><br><span class="line"></span><br><span class="line">#rpc info</span><br><span class="line">$ lctl get_param -n osc.fsname-OST0000-osc*.import  | grep &quot;target:&quot;</span><br><span class="line">    target: fsname-OST0000_UUID</span><br><span class="line">obdfilter_name=fsname-OST0000_UUID</span><br><span class="line"></span><br><span class="line">$ cat /proc/fs/lfs/osc/lfs-OST0000-osc-ffff88103a993000/import</span><br><span class="line"></span><br><span class="line">#default ost io threads, default is 0</span><br><span class="line">$ lctl get_param ost.OSS.ost_io.threads_max</span><br><span class="line">ost.OSS.ost_io.threads_max=128</span><br><span class="line">$ lctl get_param ost.OSS.ost_io.threads_started</span><br><span class="line">ost.OSS.ost_io.threads_started=21</span><br><span class="line"></span><br><span class="line"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size=1048576</span><br></pre></td></tr></table></figure>

<h4 id="format"><a href="#format" class="headerlink" title="format"></a><a target="_blank" rel="noopener" href="https://www.aglt2.org/wiki/AGLT2/ReFormatOST">format</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mdt</span></span><br><span class="line">FSNAME=lfs</span><br><span class="line">MDS1NID=1.1.1.1@tcp</span><br><span class="line">MDS2NID=2.2.2.2@tcp</span><br><span class="line"></span><br><span class="line">mkfs.lfs --mgs --backfstype=zfs --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> mdt_0/mgt_0</span><br><span class="line">mkfs.lfs --mdt --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span> --backfstype=zfs --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> --index=0 mdt_0/mdt_0</span><br><span class="line"></span><br><span class="line"><span class="comment">## add parameters</span></span><br><span class="line">mkfs.lfs --mkfsoptions=“-E stride=32,stripe_width=256” --ost -mgsnode=192.168.0.22@tcp /dev/sda1</span><br><span class="line"></span><br><span class="line"><span class="comment"># ost</span></span><br><span class="line">FSNAME=lfs</span><br><span class="line">MDS1NID=1.1.1.1@tcp</span><br><span class="line">MDS2NID=2.2.2.2@tcp</span><br><span class="line">OSS1NID=3.3.3.3@tcp</span><br><span class="line">OSS2NID=4.4.4.4@tcp</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..6&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> mkfs.lfs --reformat --backfstype=zfs --ost  --index=<span class="variable">$i</span>  --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;OSS1NID&#125;</span> --servicenode=<span class="variable">$&#123;OSS2NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span>  ost_<span class="variable">$i</span>/ost_<span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">omconfig storage controller action=createvdisk controller=1 size=max raid=r5 pdisk=0:1:5,0:1:6,0:1:7,0:1:8,0:1:9,0:1:10,0:1:11,0:1:12,0:1:13 stripesize=128kb readpolicy=ra writepolicy=wb name=ost22</span><br><span class="line">mkfs.lustre --ost --mgsnode=10.10.1.140@tcp0 --fsname=umt3 --reformat --index=11 --mkfsoptions=<span class="string">&quot;-i 2000000&quot;</span> --reformat --mountfsoptions=<span class="string">&quot;errors=remount-ro,extents,mballoc,stripe=256&quot;</span> /dev/sde</span><br><span class="line">tune2fs -O uninit_bg -m 1 -U cadf431a-6b03-4dd1-acc7-b6a3a0cbb69c /dev/sde</span><br><span class="line"></span><br><span class="line">omconfig storage controller action=createvdisk controller=1 size=max raid=r5 pdisk=1:2:0,1:2:1,1:2:2,1:2:3,1:2:4 stripesize=256kb readpolicy=ra writepolicy=wb name=ost31</span><br><span class="line">mkfs.lustre --ost --mgsnode=10.10.1.140@tcp0 --fsname=umt3 --reformat --index=32  --mkfsoptions=<span class="string">&quot;-i 1000000&quot;</span> --reformat --mountfsoptions=<span class="string">&quot;errors=remount-ro,extents,mballoc,stripe=256&quot;</span> /dev/sdf</span><br><span class="line">tune2fs -O uninit_bg -m 1 -U ffc8fa63-e7d7-470f-9691-72b56839a6b3 /dev/sdf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the issue</span></span><br><span class="line">During the work after an OST failed, and the files were all drained, the <span class="string">&quot;magic files&quot;</span> (LAST_ID, etc) were <span class="keyword">in</span> trouble. In particular, LAST_ID was not available. So, <span class="keyword">in</span> order to bring the OST back up after reformatting, I had to find a way to recreate this file.</span><br><span class="line"></span><br><span class="line">[root@lmd02 ~]<span class="comment"># lctl get_param osc.*.prealloc_next_id</span></span><br><span class="line">...</span><br><span class="line">osc.umt3-OST0025-osc.prealloc_next_id=6778336</span><br><span class="line">An alternate value is the prealloc_list_id, <span class="built_in">which</span> is larger, but considering the OST</span><br><span class="line">was already completely drained, Andreas Dilger has suggested to use the next_id value</span><br><span class="line">prealloc_last_id is 6778369</span><br><span class="line"></span><br><span class="line">See these URL</span><br><span class="line"></span><br><span class="line">http://wiki.lustre.org/manual/LustreManual20_HTML/LustreTroubleshooting.html</span><br><span class="line">https://groups.google.com/forum/<span class="comment">#!topic/lustre-discuss-list/NcDiutUirDg</span></span><br><span class="line"></span><br><span class="line">So, we have to get this value into place as LAST_ID.  </span><br><span class="line"></span><br><span class="line">     As an aside, we will need to be sure we are talking the correct index at all <span class="built_in">times</span>:</span><br><span class="line">     OK, so, using the directions</span><br><span class="line"></span><br><span class="line">     [root@umdist01 tmp]<span class="comment"># od -Ax -td4 last_rcvd |less</span></span><br><span class="line">     ...</span><br><span class="line">     000080           0           0           0          37</span><br><span class="line"></span><br><span class="line">     This matches with the <span class="string">&quot;lfs df&quot;</span> output, decimal 37, and is the index used <span class="keyword">in</span> the mkfs.lustre run.</span><br><span class="line"></span><br><span class="line">-----------------------------------------------------</span><br><span class="line">Data found on the Internet indicates, <span class="keyword">if</span> we have the mds/mdt offline, and mounted ldiskfs,</span><br><span class="line">we can <span class="keyword">do</span> the following to find the value to use <span class="keyword">in</span> LAST_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract last allocated object for all OSTs</span></span><br><span class="line">mds<span class="comment"># debugfs -c -R &quot;dump lov_objids /tmp/lo&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cut out the last allocated object for this OST index</span></span><br><span class="line">mds<span class="comment"># dd if=/tmp/lo of=/tmp/LAST_ID bs=8 skip=$&#123;OST index NN&#125; count=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># verify value is the right one (LAST_ID = next_id - 1)</span></span><br><span class="line">mds<span class="comment"># lctl get_param osc.*OST00NN.prealloc_next_id  # NN is OST index</span></span><br><span class="line">mds<span class="comment"># od -td8 /tmp/LAST_ID</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get OST filesystem ready for this value and copy it in place</span></span><br><span class="line">ossN<span class="comment"># mount -t ldiskfs /dev/&#123;ostdev&#125; /mnt/tmp</span></span><br><span class="line">ossN<span class="comment"># mkdir -p /mnt/tmp/O/0</span></span><br><span class="line">mds<span class="comment"># scp /tmp/LAST_ID ossN:/mnt/tmp/O/0/LAST_ID</span></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line">Instead, we note that we can start with ANY LAST_ID file, and edit it to create the one desired.</span><br><span class="line">Convert binary to text</span><br><span class="line">xxd /tmp/LAST_ID /tmp/LAST_ID.asc</span><br><span class="line"></span><br><span class="line">Fix it</span><br><span class="line">vi /tmp/LAST_ID.asc</span><br><span class="line">For example, 6513958 decimal is 0x636526, and appears <span class="keyword">in</span> the LAST_ID.asc file like so:</span><br><span class="line">0000000: 2665 6300 0000 0000                      &amp;ec.....</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Convert to binary</span><br><span class="line">xxd -r /tmp/LAST_ID.asc /tmp/LAST_ID.new</span><br><span class="line"></span><br><span class="line">Verify</span><br><span class="line"><span class="built_in">od</span> -Ax -td8 /tmp/LAST_ID.new</span><br><span class="line">copy it to the real LAST_ID</span><br><span class="line"></span><br><span class="line">So, how <span class="keyword">do</span> we <span class="keyword">do</span> the edit? Use a calculator to convert the <span class="string">&quot;prealloc_next_id&quot;</span> above to hex, <span class="keyword">for</span> example:</span><br><span class="line">[root@umdist01 ~]<span class="comment"># echo &quot;obase=16; 6778336&quot; | bc</span></span><br><span class="line">676DE0</span><br><span class="line">When you edit the .asc file above, this appears <span class="keyword">in</span> byte-order with the most significant of the 32 bits at the highest address. Don<span class="string">&#x27;t ask me which Endianism this is. So, when you edit the file, it goes in like so</span></span><br><span class="line"><span class="string">e06d 6700 0000 0000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">LAST_ID</span></span><br><span class="line"><span class="string">last_rcvd</span></span><br><span class="line"><span class="string">mountdata</span></span><br><span class="line"><span class="string">umt3-OST000b (for example, depending on the OST)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">last_rcvd -- Will be re-created during mount, or old one can be used. However, see below, if the LAST_ID has to be created from scratch, do not copy this file back.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">mountdata -- Copy created during run of mkfs.lustre can be used. EXCEPT for us it can&#x27;</span>t. Must use instead the mountdata copied out previously. Not sure why.</span><br><span class="line"></span><br><span class="line">umt3-OST000b -- would be recreated with a --writeconf, but it may also be created automatically during mount <span class="keyword">if</span> missing (it is an OST-<span class="built_in">local</span> copy of the MGS file of the same name so the OST can mount even <span class="keyword">if</span> the MGS is offline).</span><br></pre></td></tr></table></figure>

<h4 id="change-ipaddr"><a href="#change-ipaddr" class="headerlink" title="change ipaddr"></a>change ipaddr</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tunefs.lfs --erase-params --mgsnode=<span class="variable">$new_mgs_ip</span>@tcp --servicenode=<span class="variable">$new_service_ip</span>@tcp0 --writeconf /dev/md4</span><br></pre></td></tr></table></figure>

<h4 id="Enable-large-dir-feature-enable-on-the-2-15-X"><a href="#Enable-large-dir-feature-enable-on-the-2-15-X" class="headerlink" title="Enable large_dir feature, enable on the 2.15.X"></a>Enable large_dir feature, enable on the 2.15.X</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ tune2fs -O large_dir /dev/nvme0n1p1</span><br><span class="line">$ dumpe2fs -h /dev/nvme0n1p1 | grep feat</span><br><span class="line">$ dumpe2fs 1.45.2.wc1 (27-May-2019)</span><br><span class="line">Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery mmp flex_bg ea_inode dirdata large_dir sparse_super large_file huge_file uninit_bg dir_nlink quota</span><br><span class="line">Journal features:         journal_incompat_revoke</span><br></pre></td></tr></table></figure>

<h4 id="EC"><a href="#EC" class="headerlink" title="EC"></a>EC</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ lctl --device <span class="variable">$&#123;obdfilter_name&#125;</span>_osc cleanup</span><br><span class="line">$ lctl --device <span class="variable">$&#123;obdfilter_name&#125;</span>_osc detach</span><br><span class="line">$ lctl attach echo_client ec ec_uuid</span><br><span class="line">$ lctl --device ec create 1 | awk <span class="string">&#x27;/object id/ &#123;print $6&#125;&#x27;</span></span><br><span class="line"><span class="comment">## Get the id</span></span><br><span class="line">$ lctl --device ec getattr <span class="variable">$id</span></span><br><span class="line">$ lctl --device ec getattr <span class="variable">$id</span></span><br><span class="line">$ lctl --device ec</span><br><span class="line">$ lctl --device ec destroy <span class="variable">$id</span> 1</span><br><span class="line">$ lctl --device ec cleanup</span><br><span class="line">$ lctl --device ec detach</span><br><span class="line"></span><br><span class="line">$ lfs setstripe -E eof -c 4 -E eof -L erasure_code -ec_data_count 4 -ec_parity_count 2 file</span><br><span class="line">$ lfs mirror resync file</span><br></pre></td></tr></table></figure>

<h3 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">client $ lctl get_param -n llite.*.client_type</span><br><span class="line">local client</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.fsname-OST0000-osc*.resend_count</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.resend_count=10</span><br></pre></td></tr></table></figure>

<h4 id="monitor-1"><a href="#monitor-1" class="headerlink" title="monitor"></a>monitor</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br></pre></td><td class="code"><pre><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">osc.testfs1-OST0000-osc-ffff9a2736998000.rpc_stats=</span><br><span class="line">snapshot_time:         1647398762.213066047 (secs.nsecs)</span><br><span class="line">read RPCs in flight:  0</span><br><span class="line">write RPCs in flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending read pages:   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                      41   0   0   |         41   1   1</span><br><span class="line">2:                       0   0   0   |         10   0   1</span><br><span class="line">4:                       7   0   1   |         18   0   1</span><br><span class="line">8:                       1   0   1   |         48   1   3</span><br><span class="line">16:                      3   0   1   |         40   1   4</span><br><span class="line">32:                     10   0   1   |         31   0   5</span><br><span class="line">64:                      3   0   1   |         24   0   5</span><br><span class="line">128:                    17   0   1   |         70   1   7</span><br><span class="line">256:                  4279  98 100   |       3343  92 100</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.read_ahead_stats</span><br><span class="line">llite.ltfs1-ffff9a2736998000.read_ahead_stats=</span><br><span class="line">snapshot_time             1647398871.147899109 secs.nsecs</span><br><span class="line">hits                      857597 samples [pages]</span><br><span class="line">misses                    664 samples [pages]</span><br><span class="line">failed grab_cache_page    10319 samples [pages]</span><br><span class="line">read but discarded        82760 samples [pages]</span><br><span class="line">zero size window          468754 samples [pages]</span><br><span class="line">read-ahead to EOF         242 samples [pages]</span><br><span class="line">hit max r-a issue         965 samples [pages]</span><br><span class="line">failed to reach end       11560 samples [pages]</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.*read*</span><br><span class="line">llite.ltfs1-ffff9a2736998000.fast_read=1</span><br><span class="line">llite.ltfs1-ffff9a2736998000.max_read_ahead_mb=64</span><br><span class="line">llite.ltfs1-ffff9a2736998000.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.ltfs1-ffff9a2736998000.max_read_ahead_whole_mb=64</span><br><span class="line">llite.ltfs1-ffff9a2736998000.read_ahead_stats=</span><br><span class="line">snapshot_time             1647398887.020011954 secs.nsecs</span><br><span class="line">hits                      857597 samples [pages]</span><br><span class="line">misses                    664 samples [pages]</span><br><span class="line">failed grab_cache_page    10319 samples [pages]</span><br><span class="line">read but discarded        82760 samples [pages]</span><br><span class="line">zero size window          468754 samples [pages]</span><br><span class="line">read-ahead to EOF         242 samples [pages]</span><br><span class="line">hit max r-a issue         965 samples [pages]</span><br><span class="line">failed to reach end       11560 samples [pages]</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*.stats</span><br><span class="line"></span><br><span class="line">osc.testfs1-OST0000-osc-ffff9a2736998000.stats=</span><br><span class="line">snapshot_time             1647398966.425013406 secs.nsecs</span><br><span class="line">req_waittime              10027 samples [usec] 23 5567567 76549428 162550984799808</span><br><span class="line">req_active                10027 samples [reqs] 1 11 36957 214049</span><br><span class="line">ldlm_glimpse_enqueue      796 samples [reqs] 1 1 796 796</span><br><span class="line">ldlm_extent_enqueue       397 samples [reqs] 1 1 397 397</span><br><span class="line">read_bytes                4361 samples [bytes] 0 1048576 3514739172 3636771225513784</span><br><span class="line">write_bytes               3625 samples [bytes] 190 1048576 3514628763 3636770234184707</span><br><span class="line">ost_setattr               435 samples [usec] 40 2063486 2273570 4258904750384</span><br><span class="line">ost_read                  4361 samples [usec] 74 134260 26437092 465273858862</span><br><span class="line">ost_write                 3625 samples [usec] 69 5567567 47628317 157826643363867</span><br><span class="line">ost_connect               1 samples [usec] 3793 3793 3793 14386849</span><br><span class="line">ldlm_cancel               402 samples [usec] 51 5246 86898 115121580</span><br><span class="line">obd_ping                  10 samples [usec] 82 202 1243 168281</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdc.*MDT0000*.blocksize</span><br><span class="line">4096</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.active</span><br><span class="line">mdc.fsname-MDT0000-mdc-ffff8dab2cce8800.active=1</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.state</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.stats | grep -Ei &#x27;ost_read|ldlm_glimpse&#x27;</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.timeouts</span><br><span class="line">mdc.fsname-MDT0000-mdc-ffff8dab2cce8800.timeouts=</span><br><span class="line">last reply : 1619838998, 0s ago</span><br><span class="line">network    : cur  50  worst  50 (at 1619704491, 134507s ago)   1   1   1   1</span><br><span class="line">portal 12  : cur  50  worst  50 (at 1619704491, 134507s ago)  50  50  50  50</span><br><span class="line">portal 17  : cur  50  worst  50 (at 1619708441, 130557s ago)  50  50   0  50</span><br><span class="line">portal 23  : cur  50  worst  50 (at 1619769781, 69217s ago)  50  50  50   0</span><br><span class="line">portal 30  : cur  50  worst  50 (at 1619769796, 69202s ago)  50   0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.state</span><br><span class="line">$ lctl get_param osc.*OST0000*.stats</span><br><span class="line">$ lctl get_param osc.*OST0000*.timeouts</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.timeouts=</span><br><span class="line">last reply : 1619838981, 36s ago</span><br><span class="line">network    : cur  50  worst  50 (at 1619704492, 134525s ago)   1   1   1   1</span><br><span class="line">portal 28  : cur  50  worst  50 (at 1619704491, 134526s ago)  50  50  50  50</span><br><span class="line">portal 7   : cur  50  worst  50 (at 1619704492, 134525s ago)  50  50   0   0</span><br><span class="line">portal 6   : cur  50  worst  50 (at 1619769796, 69221s ago)  50   0   0  50</span><br><span class="line">portal 17  : cur  50  worst  50 (at 1619770886, 68131s ago)  50   0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.unstable_stats</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.unstable_stats=</span><br><span class="line">unstable_pages:                    0</span><br><span class="line">unstable_mb:                       0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.blocksize</span><br><span class="line">1048576</span><br><span class="line"></span><br><span class="line"># Client-Based I/O Extent Size Survey</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">disabled</span><br><span class="line"> write anything to this file to activate, then &#x27;0&#x27; or &#x27;disable&#x27; to deactivate</span><br><span class="line"></span><br><span class="line"># Monitor client all</span><br><span class="line">client $ lctl get_param llite.*.stats</span><br><span class="line">clinet $ lctl get_param llite.*.read_ahead_stat</span><br><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">osc.testfs1-OST0000-osc-ffff9a2736998000.rpc_stats=</span><br><span class="line">snapshot_time:         1647398762.213066047 (secs.nsecs)</span><br><span class="line">read RPCs in flight:  0</span><br><span class="line">write RPCs in flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending read pages:   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                      41   0   0   |         41   1   1</span><br><span class="line">2:                       0   0   0   |         10   0   1</span><br><span class="line">4:                       7   0   1   |         18   0   1</span><br><span class="line">8:                       1   0   1   |         48   1   3</span><br><span class="line">16:                      3   0   1   |         40   1   4</span><br><span class="line">32:                     10   0   1   |         31   0   5</span><br><span class="line">64:                      3   0   1   |         24   0   5</span><br><span class="line">128:                    17   0   1   |         70   1   7</span><br><span class="line">256:                  4279  98 100   |       3343  92 100</span><br><span class="line"></span><br><span class="line">#The file can be cleared and enabled by issuing the following command:</span><br><span class="line">$ lctl set_param llite.testfs-*.extents_stats=1</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838099.035887547 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838110.284058497 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line"></span><br><span class="line">$ dd if=/dev/urandom of=test_5 bs=4k count=100</span><br><span class="line">100+0 records in</span><br><span class="line">100+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.0048456 s, 84.5 MB/s</span><br><span class="line"></span><br><span class="line">$ ls -l test_5</span><br><span class="line">-rw-r--r-- 1 root root 409600 May  1 11:02 test_5</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838165.554498620 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100  100  100</span><br><span class="line">$ dd if=test_5 of=test_6 bs=16k count=25</span><br><span class="line">25+0 records in</span><br><span class="line">25+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.00201437 s, 203 MB/s</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838225.522203239 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100   80   80</span><br><span class="line">   8K -   16K :              0    0    0  |              0    0   80</span><br><span class="line">  16K -   32K :              0    0    0  |             25   20  100</span><br><span class="line">$ dd if=test_5 of=test_7 bs=16k count=25 iflag=direct</span><br><span class="line">25+0 records in</span><br><span class="line">25+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.010213 s, 40.1 MB/s</span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838404.667431860 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100   66   66</span><br><span class="line">   8K -   16K :              0    0    0  |              0    0   66</span><br><span class="line">  16K -   32K :             25  100  100  |             50   33  100</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*0000-osc-*.stats</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.stats=</span><br><span class="line">snapshot_time             1619795318.676450713 secs.nsecs</span><br><span class="line">req_waittime              1321 samples [usec] 282 81340 2770704 62526834430</span><br><span class="line">req_active                1321 samples [reqs] 1 27 2605 27949</span><br><span class="line">read_bytes                10 samples [bytes] 1048576 1048576 10485760 10995116277760</span><br><span class="line">write_bytes               85 samples [bytes] 32768 1048576 87162880 91210072981504</span><br><span class="line">ost_read                  10 samples [usec] 5584 15595 111076 1326778432</span><br><span class="line">ost_write                 85 samples [usec] 3899 81340 1826891 60616504085</span><br><span class="line">ost_connect               1 samples [usec] 2156 2156 2156 4648336</span><br><span class="line">ost_statfs                16 samples [usec] 282 753 8835 5208451</span><br><span class="line">ldlm_cancel               3 samples [usec] 547 1141 2390 2093894</span><br><span class="line">obd_ping                  1198 samples [usec] 319 1351 815234 569338326</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdc.*.import</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;state: FULL&quot;</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;connect_flags&quot;</span><br><span class="line">or</span><br><span class="line">$ lctl get_param  mdc.*.connect_flags | grep early_lock_cancel</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n llite.*.sbi_flags</span><br><span class="line">checksum            acl       lru_resize lazy_statfs 64bit_hash agl verbose layout xattr_cache fast_read file_secctx</span><br><span class="line">checksum user_xattr acl flock lru_resize lazy_statfs 64bit_hash agl verbose layout xattr_cache fast_read file_secctx</span><br><span class="line"></span><br><span class="line">$ lfs setstripe -S 65536 /lfs</span><br><span class="line"></span><br><span class="line">#Flush all of the metadata client (mdc) locks on this node</span><br><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.grant_shrink_interval</span><br><span class="line">osc.lfs-OST0000-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line">osc.lfs-OST0001-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line">osc.lfs-OST0002-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line"></span><br><span class="line">#This example reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.cur_grant_bytes=3407872</span><br><span class="line"></span><br><span class="line">#calculator the max_cur_granted</span><br><span class="line">undirty 34209792 + grant_chunk 3407872 = max_cur_granted 37617664</span><br><span class="line"></span><br><span class="line">#grant_chunk</span><br><span class="line">$  lctl get_param osc.*.import  | grep -Ei &#x27;max_brw_size|grant_extent_tax&#x27;</span><br><span class="line">       max_brw_size: 1048576</span><br><span class="line">       grant_extent_tax: 655360</span><br><span class="line">grant_chunk = $(((1048576+655360)*2)) = 3407872 bytes</span><br><span class="line"></span><br><span class="line">#undirty</span><br><span class="line">nrpages:256 rpc_in_flight:8</span><br><span class="line">rpc_in_flight++</span><br><span class="line">nrpegs 256 x  rpc_in_flight 9 = nrpages 2304</span><br><span class="line">max_dirty_mb:32 MB</span><br><span class="line">MB to pages = dirty_max_pages: 8192</span><br><span class="line">if dirty_max_pages 8192 &gt; nrpages 2304; nrpages = dirty_max_pages:8192</span><br><span class="line">nrpages 8192 x 4096 = undirty 33554432 bytes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lctl grant_max_extent_size 1073741824</span><br><span class="line">lctl grant_max_extent_size = max_extent_size/4096 to pages:262144</span><br><span class="line">(nrpages 8192 + max_extent_pages 262144 -1) / max_extent_pages 262144 = nrextents = 1</span><br><span class="line">grant_extent_tax: 655360</span><br><span class="line">undirty 34209792 bytes = undirty bytes: 33554432 + nrextents 1 * grant_extent_tax (bytes):655360</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*.grant_shrink</span><br><span class="line">client $ lctl set_param osc.*.grant_shrink=0</span><br><span class="line">#replace</span><br><span class="line">$ bpftrace -e &#x27;k:osc_should_shrink_grant &#123; override(0); &#125;&#x27; --unsafe</span><br><span class="line"></span><br><span class="line">### clear cache info</span><br><span class="line">client $ lctl set_param osc.*-osc*.rpc_stats 0</span><br><span class="line">client $ lctl set_param llite.*.read_ahead_stats 0</span><br><span class="line">client $ lctl set_param -n llite.*.max_cached_mb 128 ## 128MB</span><br><span class="line">#randomly read 1000 of 32K chunks from file large than 1G size</span><br><span class="line">find /fsname -type f -size +1G | head -n 128 | while read line; do dd if=$line of=/dev/null bs=128K count=1000 skip=$(shuf -i 2000-65000 -n 1) &amp; done</span><br><span class="line"></span><br><span class="line">#check status</span><br><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.max_cached_mb</span><br><span class="line">llite.fsname-ffff8dab2cce8800.max_cached_mb=</span><br><span class="line">users: 16</span><br><span class="line">max_cached_mb: 128  ##128MB=32768 pages</span><br><span class="line">used_mb: 128</span><br><span class="line">unused_mb: 0</span><br><span class="line">reclaim_count: 36</span><br><span class="line"></span><br><span class="line">#128M</span><br><span class="line">client lctl set_param llite.*.read_ahead_stats=c</span><br><span class="line">llite.testfs-ffff8daf56159000.read_ahead_stats=c</span><br><span class="line"></span><br><span class="line">llite.testfs-ffff8dcc59a15800.fast_read=1</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_async_active=12</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_async_file_threshold_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_range_kb=1024</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_whole_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_stats=</span><br><span class="line">snapshot_time             1642131829.087651284 secs.nsecs</span><br><span class="line">hits                      2887533 samples [pages]</span><br><span class="line">misses                    627508 samples [pages] &lt;--------------------</span><br><span class="line">readpage not consecutive  1255012 samples [pages]                    |</span><br><span class="line">zero size window          2887886 samples [pages]                    |</span><br><span class="line">failed to fast read       627508 samples [pages]                     |</span><br><span class="line">                                                                     |</span><br><span class="line">lctl get_param llite.testfs*.*read*                                  |</span><br><span class="line">llite.testfs-ffff8db8208be800.fast_read=1                            |</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_mb=64                   |-------------dff misses samples in the same case</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_per_file_mb=64          |</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_whole_mb=64             |</span><br><span class="line">llite.testfs-ffff8db8208be800.read_ahead_stats=                      |</span><br><span class="line">snapshot_time             1642130844.639513080 secs.nsecs            |</span><br><span class="line">hits                      19433183 samples [pages]                   |</span><br><span class="line">misses                    5 samples [pages] &lt;-------------------------</span><br><span class="line">readpage not consecutive  4 samples [pages]</span><br><span class="line">zero size window          12 samples [pages]</span><br><span class="line"></span><br><span class="line">failed to reach end       76012 samples [pages]</span><br><span class="line"></span><br><span class="line">Hits</span><br><span class="line">Misses</span><br><span class="line">Readpage not consecutive</span><br><span class="line">Miss inside window</span><br><span class="line">Failed grab_cache_page</span><br><span class="line">Failed lock match</span><br><span class="line">Read but discarded</span><br><span class="line">Zero length file</span><br><span class="line">Zero size window</span><br><span class="line">Read-ahead to EOF</span><br><span class="line">Hit max r-a issue</span><br><span class="line">Wrong page from</span><br><span class="line">grab_cache_page</span><br><span class="line"></span><br><span class="line">$ size=1; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=2; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=4; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=8; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=16; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">36</span><br><span class="line">if size 4 * PAGE_SIZE 4096 = 32768 &lt; rsize 131072(means dd bs=128k); means Small 16384 read IO 36</span><br><span class="line"></span><br><span class="line">1K 2K 4K ... 256 K</span><br><span class="line"></span><br><span class="line">## for example, just print first line</span><br><span class="line">$ size=1; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2; exit&#125;&#x27;</span><br><span class="line">209</span><br><span class="line">the size=1 * PAGE_SIZE 4096 = 4096 &lt; rsize 131072 (means dd bs=128k); means Small 4096 read IO 209</span><br><span class="line">$ size=8; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2; exit&#125;&#x27;</span><br><span class="line">14</span><br><span class="line">the size=8 * PAGE_SIZE 4096 = 32768 &lt; rsize 131072 (means dd bs=128k); means Small 32768 read IO 14</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*-osc*.rpc_stats 0</span><br><span class="line">$ lctl set_param llite.*.read_ahead_stats 0</span><br><span class="line">$ lctl set_param -n llite.*.max_cached_mb 64 #default 64</span><br><span class="line"></span><br><span class="line">#########################-test- monitor tools##################</span><br><span class="line"># setstripe</span><br><span class="line">$ lfs setstripe -S 4000M -c 50 /mnt/striped</span><br><span class="line"></span><br><span class="line">#Monitor</span><br><span class="line">$ lctl get_param  osc.*OST0000*.stats</span><br><span class="line">osc.$FSNAME-OST0000-osc-ffff8dab37559000.stats=</span><br><span class="line">snapshot_time             1619689024.314392020 secs.nsecs</span><br><span class="line">req_waittime              128 samples [usec] 138 2599 61091 35024171</span><br><span class="line">req_active                128 samples [reqs] 1 1 128 128</span><br><span class="line">ost_connect               1 samples [usec] 2599 2599 2599 6754801</span><br><span class="line">ost_statfs                3 samples [usec] 459 512 1433 686269</span><br><span class="line">obd_ping                  124 samples [usec] 138 575 57059 27583101</span><br><span class="line">$ lctl get_param mdc.lfs*.stats</span><br><span class="line">mdc.lfs-MDT0000-mdc-ffff8dab3afe6000.stats=</span><br><span class="line">snapshot_time             1619689557.206806185 secs.nsecs</span><br><span class="line">req_waittime              112 samples [usec] 71 1525 18376 7048052</span><br><span class="line">req_active                112 samples [reqs] 1 1 112 112</span><br><span class="line">mds_getattr               1 samples [usec] 86 86 86 7396</span><br><span class="line">mds_connect               1 samples [usec] 734 734 734 538756</span><br><span class="line">mds_get_root              1 samples [usec] 101 101 101 10201</span><br><span class="line">mds_statfs                3 samples [usec] 71 145 293 31995</span><br><span class="line">ldlm_cancel               1 samples [usec] 92 92 92 8464</span><br><span class="line">obd_ping                  103 samples [usec] 93 1525 16863 6429731</span><br><span class="line"></span><br><span class="line">OSS $ llobdstat ost_name 2</span><br><span class="line">Read: 9.56529e+13, Write: 2.76168e+13, create/destroy: 1832/171649, stat: 1.1192e+07, punch: 52249</span><br><span class="line">[NOTE: cx: create, dx: destroy, st: statfs, pu: punch ]</span><br><span class="line"></span><br><span class="line">Timestamp   Read-delta  ReadRate  Write-delta  WriteRate</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">1637629257    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:13</span><br><span class="line">1637629259    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:14</span><br><span class="line">1637629261   16.00MB    8.00MB/s     0.00MB    0.00MB/s st:14</span><br><span class="line">1637629263    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:12</span><br><span class="line">1637629265    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:16</span><br><span class="line">1637629267    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:16</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OSS $ llstat -i 1 ost</span><br><span class="line">MDS $ llstat -i 1 mds</span><br><span class="line"></span><br><span class="line">napshot_time             1637629207.425948584</span><br><span class="line">req_waittime              33338215</span><br><span class="line">req_qdepth                33338215</span><br><span class="line">req_active                33338215</span><br><span class="line">req_timeout               33338215</span><br><span class="line">reqbuf_avail              67080504</span><br><span class="line">ldlm_glimpse_enqueue      13497741</span><br><span class="line">ldlm_extent_enqueue       787370</span><br><span class="line">ost_setattr               388196</span><br><span class="line">ost_create                6643</span><br><span class="line">ost_destroy               690443</span><br><span class="line">ost_get_info              3191</span><br><span class="line">ost_connect               1721355</span><br><span class="line">ost_disconnect            1714914</span><br><span class="line">ost_sync                  79</span><br><span class="line">ost_set_info              5901</span><br><span class="line">ost_quotactl              2214221</span><br><span class="line">obd_ping                  12308161</span><br><span class="line">/proc/fs/lfs/ost/OSS/ost/stats @ 1637629208.426168645</span><br><span class="line">Name                      Cur.Count  Cur.Rate   #Events   Unit           last        min          avg        max    stddev</span><br><span class="line">req_waittime              6          6          33338221  [usec]          178          4       683.36    1846733  10378.79</span><br><span class="line">req_qdepth                6          6          33338221  [reqs]            0          0         0.02       2958      3.50</span><br><span class="line">req_active                6          6          33338221  [reqs]            8          1         2.20         79      7.54</span><br><span class="line">req_timeout               6          6          33338221  [sec]           300         50        50.01         92      0.52</span><br><span class="line">reqbuf_avail              12         12         67080516  [bufs]          739          0        62.16        117      2.89</span><br><span class="line">ldlm_glimpse_enqueue      2          2          13497743  [reqs]            2          1         1.00          1      0.00</span><br><span class="line">ldlm_extent_enqueue       0          0          787370    [reqs]            0          1         1.00          1      0.00</span><br><span class="line">ost_setattr               0          0          388196    [usec]            0          2      9493.19   78450702 241902.62</span><br><span class="line">ost_create                0          0          6643      [usec]            0     150442    994329.00   34318893 1112213.09</span><br><span class="line">ost_destroy               0          0          690443    [usec]            0         74     29416.49   21468510 104767.69</span><br><span class="line">ost_get_info              0          0          3191      [usec]            0          9      4283.79     447398  21558.91</span><br><span class="line">ost_connect               0          0          1721355   [usec]            0          7      3217.33   20717452  29239.94</span><br><span class="line">ost_disconnect            0          0          1714914   [usec]            0         34      5274.31   13105121  33557.39</span><br><span class="line">ost_sync                  0          0          79        [usec]            0          9    555830.46    1691862 476579.36</span><br><span class="line">ost_set_info              0          0          5901      [usec]            0          9        18.16         65      4.85</span><br><span class="line">ost_quotactl              0          0          2214221   [usec]            0          8       798.77    7068501  10658.14</span><br><span class="line">obd_ping                  4          4          12308165  [usec]           39          3         9.30        304      3.42</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li><p>stripe</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#stripe info</span><br><span class="line">$ lctl get_param  lov.fsname-clilov-\*.stripe*</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripecount=1</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripeoffset=-1</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripesize=1048576</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripetype=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>cancel_lru_locks</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param -n ldlm.namespaces.*osc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lock_unused_count</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lock_unused_count=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffffa05245a8c800.lock_unused_count=488</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffffa05245a8c800.lock_unused_count=1</span><br><span class="line">ldlm.namespaces.fsname-OST0001-osc-ffffa05245a8c800.lock_unused_count=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.namespaces.*mdc-*.lru_size</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=1</span><br><span class="line">$ lctl get_param ldlm.namespaces.*osc-*.lru_size</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.lru_size=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>readahead<br>disable readahead and set stripe count &#x3D; 1, compare the read performance, if not aligned, the result will slow than disabled readahead result.  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#disable the client read ahead</span></span><br><span class="line">$ lctl set_param llite.*.max_read_ahead_mb=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span></span><br><span class="line">options ptlrpc ldlm_num_threads=16</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="complie-client"><a href="#complie-client" class="headerlink" title="complie client"></a>complie client</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">#Mellanox driver rebuild</span><br><span class="line">$ export PATH=/usr/lib64/openmpi/bin/:$PATH</span><br><span class="line"></span><br><span class="line">#if you has the openmpi error when you rpmbuild, find the lfs.spec and delete all openmpi info</span><br><span class="line">$ rpmbuild -bb -v --without servers lfs.spec</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm</span><br><span class="line">$ rpmbuild  --rebuild --without servers --with lnet-dlc  lfs-2.10.3-1.src.rpm</span><br><span class="line">$ rpmbuild --rebuild --without servers --without lfs-tests lfs-2.10.3-1.src.rpm</span><br><span class="line">$ rpmbuild --define &#x27;kversion 2.6.32-220.4.1.el6.x86_64&#x27; --define &#x27;kdir /usr/src/kernels/2.6.32-220.4.1.el6.x86_64/&#x27; --rebuild lfs-client-2.1.0-2.6.32_131.6.1.el6.x86_64_g9d71fe8.src.rpm</span><br><span class="line"></span><br><span class="line">#rebuild with o2ib</span><br><span class="line">#After you have install Mellanox EFOD and you need re-compile the lustre clinet</span><br><span class="line"></span><br><span class="line">rebuild show &quot;If you still want to build Lustre for your OFED I/B stack, you need to install its devel headers RPM.&quot;</span><br><span class="line">$ ./configure --disable-server --with-o2ib=/usr/src/ofa_kernel/default #here is MOFED-devel path</span><br><span class="line"></span><br><span class="line">$ EXTRA_LNET_INCLUDE=&quot;-I/usr/src/ofa_kernel/default/include/ -include /usr/src/ofa_kernel/default/include/linux/compat-2.6.h&quot; ./configure --with-o2ib=/usr/src/ofa_kernel/default/</span><br><span class="line">$ EXTRA_LNET_INCLUDE=&quot;-I/usr/src/ofa_kernel/default/include/ -include /usr/src/ofa_kernel/default/include/linux/compat-2.6.h&quot; make rpms</span><br><span class="line"></span><br><span class="line">IB:</span><br><span class="line">$ ./configure --with-o2ib=/usr/src/ofa_kernel/default</span><br><span class="line"></span><br><span class="line">OPA:</span><br><span class="line">$ ./configure --with-o2ib=yes</span><br><span class="line"></span><br><span class="line">$ make rpms</span><br><span class="line"></span><br><span class="line"># centos 7.7</span><br><span class="line"># /root/rpmbuild/BUILD/lfs-2.10.8/lnet/klnds/o2iblnd/o2iblnd.h:69:27: fatal error: linux/pci-dma.h: No such file or directory</span><br><span class="line"></span><br><span class="line"># copy the file from the old kernel</span><br><span class="line">copy /usr/src/kernels/$&#123;the old version kernel&#125;/include/linux/pci-dma.h /usr/src/kernels/$&#123;the centos 7.7 kernel&#125;/include/linux/pci-dma.h</span><br><span class="line"></span><br><span class="line"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span><br><span class="line">If you want it pass, I was clear /tmp/tmp.* rpmbuild not help I guess maybe the old config in some tmpfs path.</span><br><span class="line">after you reboot and re-export the env, the compile will be successful.</span><br><span class="line">#Is real the realease production ? `too stupid` bug. just waste my time to type these words.</span><br><span class="line">There is no test team in lfs develop team, All users was the test team except you are going to buy DDN.</span><br><span class="line"></span><br><span class="line"># from source</span><br><span class="line">$ ./configure --enable-client --disable-server --with-linux=/usr/src/kernels/$(uname -r) --with-linux-obj=/usr/src/kernels/$(uname -r);make rpms/deps</span><br><span class="line">$ ./configure --enable-client --disable-server --with-linux=/usr/src/kernels/linux-5.4.123 --with-linux-obj=/usr/src/kernels/linux-5.4.123;make rpms/deps</span><br><span class="line">## install in ubuntu 18.04</span><br><span class="line">$ apt install uuid-dev libblkid-dev dietlibc-dev</span><br><span class="line">$ apt install build-essential debhelper devscripts fakeroot kernel-wedge libudev-dev pciutils-dev</span><br><span class="line">$ apt install module-assistant libreadline-dev dpatch libsnmp-dev quilt</span><br><span class="line">$ apt install linux-headers-$(uname -r)</span><br><span class="line">$ cd $&#123;BUILDPATH&#125;/lfs-release</span><br><span class="line">$ git reset --hard &amp;&amp; git clean -dfx</span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ ./configure --disable-server --with-linux=/usr/src/linux-headers-4.15.0-64-generic</span><br><span class="line">$ make install</span><br><span class="line">$ rm -rf  /lib/modules/4.15.0-64-generic/kernel/drivers/staging/lfs/</span><br><span class="line">$ depmod -a</span><br></pre></td></tr></table></figure>
<h5 id="ubuntu-install-client"><a href="#ubuntu-install-client" class="headerlink" title="ubuntu install client"></a>ubuntu install client</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ apt install libncurses5-dev libncurses-dev bison flex gnupg libelf-dev gcc libssl-dev bc bzip2 build-essential udev kmod cpio  libfuse-dev libattr1-dev libblkid-dev uuid-dev devscripts fakeroot kernel-wedge libudev-dev libpython3-dev swig  gettext texinfo debhelper dh-exec update-notifier-common sg3-utils attr mpi-default-bin selinux-utils python2 libpython2-stdlib libsgutils2-2 libpython2.7-stdlib linux-headers-$(<span class="built_in">uname</span> -r) libkeyutils-dev libnl-3-dev pkg-config libhwloc-dev libnl-genl-3-dev libsnmp-dev dpatch</span><br><span class="line">$ ./configure --enable-client --disable-server;make deps</span><br><span class="line">$ <span class="built_in">cd</span> debs</span><br></pre></td></tr></table></figure>

<h4 id="get-the-status"><a href="#get-the-status" class="headerlink" title="get the status"></a>get the status</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">client $ lfs osts</span><br><span class="line">client $ cat /proc/fs/lfs/lov/$fsname-clilov-fffff882037467800/target_obd</span><br><span class="line">mds    $ cat /proc/fs/lfs/lov/$fsname-MDT0000-mdtlov/target_obd</span><br><span class="line">client $ lctl get_param osc.*-OST*.active</span><br><span class="line"></span><br><span class="line"># check object server status in my production env</span><br><span class="line">$ (lfs osts | awk -F &#x27;[ :_]+&#x27; &#x27;$0~/OST/&#123;print $2&#125;&#x27; | while read line; do grep &quot;FULL&quot; /proc/fs/lfs/osc/$&#123;line&#125;-*/state &gt;/dev/null 2&gt;&amp;1 || echo -e &quot;Got these bad OSTs:&quot; $&#123;RED&#125;$line$&#123;NC&#125; ; done) &amp;&amp; exit 0</span><br><span class="line"></span><br><span class="line"># show ost ip  addr</span><br><span class="line">$ lctl dl -t</span><br><span class="line"></span><br><span class="line"># list nids</span><br><span class="line">$ lctl lst_nids</span><br><span class="line">$ lctl which_nid $your_ipaddr@tcp</span><br><span class="line">$ lctl ping $your_ipaddr@tcp</span><br><span class="line">$ lnetctl peer show --nid $ipaddr</span><br><span class="line">$ lnetctl peer del --prim_nid $ipaddr --nid $ipaddr</span><br><span class="line"></span><br><span class="line"># mount namespace &quot;D&quot; and clear mds info from client</span><br><span class="line">client $ echo 0 &gt; cat /sys/fs/lfs/mdc/$FNAME-*/active</span><br><span class="line">client $ lctl set_param mdc.$FNAME-*.active=0 #better than degarde</span><br></pre></td></tr></table></figure>

<h3 id="Backup-and-recovery-lfs-ZFS-OST"><a href="#Backup-and-recovery-lfs-ZFS-OST" class="headerlink" title="Backup and recovery lfs ZFS OST"></a>Backup and recovery lfs ZFS OST</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">OSS $ umount /lfs/test_ost0</span><br><span class="line">OSS $ zfs set canmount=on test_ost0/test_ost0</span><br><span class="line">OSS $ zfs mount -a</span><br><span class="line">OSS $ ls /test_ost0/test_ost0</span><br><span class="line">CONFIGS         nodemap  oi.103  oi.110  oi.118  oi.125  oi.18  oi.25  oi.32  oi.4   oi.47  oi.54  oi.61  oi.69  oi.76  oi.83  oi.90  oi.98</span><br><span class="line">......</span><br><span class="line">OSS $ tar cvf /backup/test_ost0.tar --xattrs-include=&quot;trusted.*&quot; --sparse .</span><br><span class="line">./</span><br><span class="line">./fld</span><br><span class="line">./oi.46/</span><br><span class="line">./oi.31/</span><br><span class="line">./oi.114/</span><br><span class="line">./quota_slave/</span><br><span class="line">./quota_slave/0x20000-OST0000</span><br><span class="line">......</span><br><span class="line">./oi.92/</span><br><span class="line">./oi.76/</span><br><span class="line">./oi.98/</span><br><span class="line">tar: Exiting with failure status due to previous errors</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">OSS $ cd ~</span><br><span class="line">OSS $ umount /test_ost0/test_ost0/</span><br><span class="line">OSS $ mkfs.lfs --reformat --backfstype=zfs --ost  --index=0  --fsname=testfs1 --servicenode=$&#123;oss_ipaddr&#125;@tcp --mgsnode=$&#123;mds_ipaddr&#125;@tcp  test_ost0/test_ost0</span><br><span class="line">OSS $ zfs set canmount=on test_ost0/test_ost0 </span><br><span class="line">OSS $ zfs mount -a</span><br><span class="line">OSS $ cd /test_ost0/test_ost0</span><br><span class="line">OSS $ tar xvpf /backup/test_ost0.tar --xattrs-include=&quot;trusted.*&quot; --sparse</span><br><span class="line">OSS $ rm -rf oi.16* lfsck_* LFSCK ##zfs could not delete oi.16</span><br><span class="line">OSS $ rm -f CATALOGS </span><br><span class="line">OSS $ getfattr -n trusted.lma fld </span><br><span class="line"># file: fld</span><br><span class="line">trusted.lma=0sAAAAAAAAAAABAAAAAgAAAAMAAAAAAAAA</span><br><span class="line"></span><br><span class="line">OSS $ rm -f fld</span><br><span class="line">OSS $ umount /test_ost0/test_ost0</span><br><span class="line"></span><br><span class="line">writeconf all OST and MDT</span><br><span class="line">patch https://review.whamcloud.com/#/c/46723/</span><br><span class="line"></span><br><span class="line">OSS $ lfs_rmmod</span><br><span class="line">OSS $ mount.lfs test_ost0/test_ost0 /lfs/test_ost0</span><br></pre></td></tr></table></figure>

<h3 id="lfs-multiple-ethernet-port-for-diff-LAN"><a href="#lfs-multiple-ethernet-port-for-diff-LAN" class="headerlink" title="lfs multiple ethernet port for diff LAN"></a>lfs multiple ethernet port for diff LAN</h3><p>just modify lnet driver parameters in server and client</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=tcp0(eth0),tcp1(ens12) <span class="comment">## in server and client</span></span><br><span class="line"></span><br><span class="line">client $ mount.lfs 192.168.0.1@tcp0:/lfs /mnt</span><br><span class="line">client $ mount.lfs 10.0.0.1@tcp1:/lfs /mnt</span><br><span class="line"></span><br><span class="line">[Fri Mar 25 15:55:47 2022] lfs: lfs-OST0000: Connection restored to ed9931e3-3560-4e6c-0914-9e6e65e6bfc0 (at 10.0.0.1@tcp)</span><br><span class="line">[Fri Mar 25 16:00:08 2022] lfs: lfs-OST0000: Connection restored to ed9931e3-3560-4e6c-0914-9e6e65e6bfc0 (at 10.0.0.1@tcp)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="trace-the-kernel-sock"><a href="#trace-the-kernel-sock" class="headerlink" title="trace the kernel sock"></a>trace the kernel sock</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">./lnet/include/lnet/lib-types.h</span><br><span class="line">        <span class="type">int</span>                             ln_niinit_self;</span><br><span class="line">        <span class="comment">/* LNetNIInit/LNetNIFini counter */</span></span><br><span class="line">        <span class="type">int</span>                             ln_refcount;</span><br><span class="line">        <span class="comment">/* SHUTDOWN/RUNNING/STOPPING */</span></span><br><span class="line"></span><br><span class="line">$ lctl --net tcp del_peer $ipaddr</span><br><span class="line">lnet/peer.c</span><br><span class="line"> LNetPut</span><br><span class="line">  ksocknal_alloc_tx</span><br><span class="line">   ksocknal_launch_packet</span><br><span class="line"></span><br><span class="line">$ ss -tulpen</span><br><span class="line">udp    UNCONN     <span class="number">0</span>      <span class="number">0</span>                                                             [::]:<span class="number">111</span>                                                                       [::]:*                   users:((<span class="string">&quot;rpcbind&quot;</span>,pid=<span class="number">936</span>,fd=<span class="number">9</span>)) ino:<span class="number">16271</span> sk:ffffa0b3b54a0000 v6only:<span class="number">1</span> &lt;-&gt;</span><br><span class="line"></span><br><span class="line">tcp    LISTEN     <span class="number">0</span>      <span class="number">127</span>                                                              *:<span class="number">988</span>                                                                          *:*                   ino:<span class="number">389210</span> sk:ffffa0b43b7e2e80 &lt;-&gt;</span><br><span class="line"></span><br><span class="line">tcp    LISTEN     <span class="number">0</span>      <span class="number">128</span>                                                              *:<span class="number">111</span>                                                                          *:*                   users:((<span class="string">&quot;rpcbind&quot;</span>,pid=<span class="number">936</span>,fd=<span class="number">8</span>)) ino:<span class="number">16270</span> sk:ffffa0b3b5548000 &lt;-&gt;</span><br><span class="line"></span><br><span class="line">$ crash /usr/lib/debug/usr/lib/modules/<span class="number">3.10</span><span class="number">.0</span><span class="number">-1160.</span>el7.x86_64/vmlinux</span><br><span class="line"></span><br><span class="line">crash&gt; <span class="class"><span class="keyword">struct</span> <span class="title">sock</span>.__<span class="title">sk_common</span>.<span class="title">skc_num</span> <span class="title">ffffa0b43b7e2e80</span></span></span><br><span class="line"><span class="class">  __<span class="title">sk_common</span>.<span class="title">skc_num</span> =</span> <span class="number">988</span></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="class"><span class="keyword">struct</span> <span class="title">sock</span>.<span class="title">sk_rcvbuf</span> <span class="title">ffffa0b43b7e2e80</span></span></span><br><span class="line"><span class="class">  <span class="title">sk_rcvbuf</span> =</span> <span class="number">134217728</span></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="class"><span class="keyword">struct</span> <span class="title">sock</span>.<span class="title">sk_sndbuf</span> <span class="title">ffffa0b43b7e2e80</span></span></span><br><span class="line"><span class="class"><span class="title">sk_sndbuf</span> =</span> <span class="number">134217728</span></span><br><span class="line">net.ipv4.tcp_rmem = <span class="number">65536</span>       <span class="number">134217728</span>       <span class="number">268435456</span></span><br><span class="line">net.ipv4.tcp_wmem = <span class="number">65536</span>       <span class="number">134217728</span>       <span class="number">268435456</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> sock.sk_max_ack_backlog ffffa0b43b7e2e80</span><br><span class="line">  sk_max_ack_backlog = <span class="number">127</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.sk_max_ack_backlog ffffa0b3b5548000</span><br><span class="line">  sk_max_ack_backlog = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.__sk_common.skc_num ffffa0b3b54a0000</span><br><span class="line">  __sk_common.skc_num = <span class="number">111</span></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.sk_rcvbuf ffffa0b3b54a0000</span><br><span class="line">  sk_rcvbuf = <span class="number">212992</span></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.sk_rcvbuf ffffa0b3b54a0000</span><br><span class="line">  sk_rcvbuf = <span class="number">212992</span></span><br><span class="line">net.core.rmem_default = <span class="number">212992</span></span><br><span class="line">net.core.rmem_max = <span class="number">212992</span></span><br><span class="line">net.core.wmem_default = <span class="number">212992</span></span><br><span class="line">net.core.wmem_max = <span class="number">212992</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#looks like the ss show the struct sock address</span></span><br><span class="line"></span><br><span class="line"># /proc/net/tcp</span><br><span class="line">netstat -wtpeav</span><br></pre></td></tr></table></figure>

<h3 id="lfs-magic-num"><a href="#lfs-magic-num" class="headerlink" title="lfs magic num"></a>lfs magic num</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">magic: 0xa0629d03</span><br></pre></td></tr></table></figure>

<h3 id="rocev2-test"><a href="#rocev2-test" class="headerlink" title="rocev2 test"></a>rocev2 test</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=o2ib(ens1f1np1),tcp(ens1f0np0)</span><br><span class="line"></span><br><span class="line">options lnet networks=<span class="string">&quot;tcp1(eth1),tcp2(eth2),o2ib0(ib0)&quot;</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">options lnet ip2nets=<span class="string">&quot;tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]&quot;</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br></pre></td></tr></table></figure>

<h3 id="DOM"><a href="#DOM" class="headerlink" title="DOM"></a>DOM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">client $ lfs setstripe -E 1M -L mdt -E -1 -S 1M -c 1 /testfs/dom</span><br><span class="line">client $ lfs getstripe /testfs/dom/test.txt</span><br><span class="line">/testfs/dom/test.txt</span><br><span class="line">  lcm_layout_gen:    2</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   2</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   262144</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   262144</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">    lcme_id:             2</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          0</span><br><span class="line">    lcme_extent.e_start: 262144</span><br><span class="line">    lcme_extent.e_end:   EOF</span><br><span class="line">      lmm_stripe_count:  1</span><br><span class="line">      lmm_stripe_size:   10485760</span><br><span class="line">      lmm_pattern:       raid0</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: -1</span><br><span class="line"></span><br><span class="line">client $ lfs getstripe -I1 /testfs/dom/test.txt </span><br><span class="line">/testfs/dom/test.txt</span><br><span class="line">  lcm_layout_gen:    2</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   2</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   262144</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   262144</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">client $ lfs find -L mdt /testfs/dom/</span><br><span class="line"></span><br><span class="line"><span class="comment">#append</span></span><br><span class="line">client $  <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=test.txt bs=1M count=1024 oflag=append</span><br><span class="line">1024+0 records <span class="keyword">in</span></span><br><span class="line">1024+0 records out</span><br><span class="line">1073741824 bytes (1.1 GB) copied, 0.658995 s, 1.6 GB/s</span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">ls</span> -lhs test.txt </span><br><span class="line">291M -rw-r--r-- 1 root root 1.0G Sep 14 14:02 test.txt</span><br><span class="line"></span><br><span class="line">client $ lfs getstripe test.txt </span><br><span class="line">test.txt</span><br><span class="line">  lcm_layout_gen:    3</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   2</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   262144</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   262144</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">    lcme_id:             2</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 262144</span><br><span class="line">    lcme_extent.e_end:   EOF</span><br><span class="line">      lmm_stripe_count:  1</span><br><span class="line">      lmm_stripe_size:   10485760</span><br><span class="line">      lmm_pattern:       raid0</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 3</span><br><span class="line">      lmm_objects:</span><br><span class="line">      - 0: &#123; l_ost_idx: 3, l_fid: [0x100030000:0x376e8d:0x0] &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">## not append ,dd 100M file</span></span><br><span class="line">client $ <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=test1.txt bs=1M count=100</span><br><span class="line">100+0 records <span class="keyword">in</span></span><br><span class="line">100+0 records out</span><br><span class="line">104857600 bytes (105 MB) copied, 0.152089 s, 689 MB/s</span><br><span class="line"></span><br><span class="line">client $  lfs getstripe test1.txt </span><br><span class="line">test1.txt</span><br><span class="line">  lcm_layout_gen:    3</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   2</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   262144</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   262144</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">    lcme_id:             2</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 262144</span><br><span class="line">    lcme_extent.e_end:   EOF</span><br><span class="line">      lmm_stripe_count:  1</span><br><span class="line">      lmm_stripe_size:   10485760</span><br><span class="line">      lmm_pattern:       raid0</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line">      lmm_objects:</span><br><span class="line">      - 0: &#123; l_ost_idx: 0, l_fid: [0x100000000:0x376e8d:0x0] &#125;</span><br><span class="line"></span><br><span class="line">mds $ lctl get_param -n lod.*MDT0000*.dom_stripesize</span><br><span class="line">1048576</span><br><span class="line">mds $ lctl conf_param testfs-MDT0000.lod.dom_stripesize=128K</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#disable DOM</span></span><br><span class="line">mds $ lctl conf_param lustre-MDT0000.lod.dom_stripesize=0</span><br></pre></td></tr></table></figure>


<h4 id="lnet-health"><a href="#lnet-health" class="headerlink" title="lnet health"></a><a target="_blank" rel="noopener" href="http://wiki.lfsfs.cn/index.php?title=LNet%E5%81%A5%E5%BA%B7%E5%8A%9F%E8%83%BD%E7%94%A8%E6%88%B7%E6%89%8B%E5%86%8C">lnet health</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config</span></span><br><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lfs 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show -v</span><br><span class="line">lnetctl peer show -v</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line">// To <span class="built_in">export</span> the current configuration to a YAML file</span><br><span class="line">lnetctl <span class="built_in">export</span> FILE.yaml</span><br><span class="line">lnetctl <span class="built_in">export</span> &gt; FILE.yaml</span><br><span class="line">// To import the configuration from a YAML file</span><br><span class="line">lnetctl import FILE.yaml</span><br><span class="line">lnetctl import &lt; FILE.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># the more value, the long recovery time, the default is disabled means lnet_health_sensitivity and lnet_retry_count set to 0</span></span><br><span class="line">$ lnetctl <span class="built_in">set</span> health_sensitivity: sensitivity to failure</span><br><span class="line">        0 - turn off health evaluation</span><br><span class="line">        &gt;0 - sensitivity value not more than 1000</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">set</span> recovery_interval: interval to ping unhealthy interfaces</span><br><span class="line">        &gt;0 - <span class="built_in">timeout</span> <span class="keyword">in</span> seconds</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">set</span> retry_count: number of retries</span><br><span class="line">        0 - turn of retries</span><br><span class="line">        &gt;0 - number of retries</span><br><span class="line"></span><br><span class="line"><span class="comment">#Important</span></span><br><span class="line">$ lnetctl <span class="built_in">set</span> transaction_timeout: Message/Response <span class="built_in">timeout</span></span><br><span class="line">        &gt;0 - <span class="built_in">timeout</span> <span class="keyword">in</span> seconds</span><br><span class="line"></span><br><span class="line">lnet_lnd_timeout = lnet_transaction_timeout / retry_count</span><br><span class="line"></span><br><span class="line"><span class="comment">#dump all config</span></span><br><span class="line">$ lnetctl global show</span><br><span class="line">global:</span><br><span class="line">    numa_range: 0</span><br><span class="line">    max_intf: 200</span><br><span class="line">    discovery: 1</span><br><span class="line">    drop_asym_route: 0</span><br><span class="line">    retry_count: 2</span><br><span class="line">    transaction_timeout: 50</span><br><span class="line">    health_sensitivity: 100</span><br><span class="line">    recovery_interval: 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#show local health</span></span><br><span class="line">$ lnetctl net show -v 4</span><br><span class="line"></span><br><span class="line"><span class="comment">#show remote health</span></span><br><span class="line">$ lnetctl peer show -v 4</span><br><span class="line"></span><br><span class="line"><span class="comment">#show status</span></span><br><span class="line">$ lnetctl stats show</span><br><span class="line"></span><br><span class="line"><span class="comment"># mark the route status, set 0 means disable</span></span><br><span class="line">options lnet auto_down=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># avoid_asym_router_failure, avoid push data to the block route cause data loss</span></span><br><span class="line">options lnet avoid_asym_router_failure=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping check the active route time interval, default: 60s</span></span><br><span class="line">options lnet live_router_check_interval=50</span><br><span class="line"></span><br><span class="line"><span class="comment"># dead_router_check_interval, default:60s</span></span><br><span class="line">options lnet dead_router_check_interval=50</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping timeout</span></span><br><span class="line">options lnet router_ping_timeout=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the route status from</span></span><br><span class="line">$ <span class="built_in">cat</span> /sys/kernel/debug/lnet/stats</span><br><span class="line">0 23 0 49912 49912 0 0 18468672 27625832 0 0</span><br><span class="line"></span><br><span class="line">$ lnetctl stats show</span><br><span class="line">statistics:</span><br><span class="line">    msgs_alloc: 0</span><br><span class="line">    msgs_max: 23</span><br><span class="line">    rst_alloc: 0</span><br><span class="line">    errors: 0</span><br><span class="line">    send_count: 49912</span><br><span class="line">    resend_count: 0</span><br><span class="line">    response_timeout_count: 0</span><br><span class="line">    local_interrupt_count: 0</span><br><span class="line">    local_dropped_count: 0</span><br><span class="line">    local_aborted_count: 0</span><br><span class="line">    local_no_route_count: 0</span><br><span class="line">    local_timeout_count: 0</span><br><span class="line">    local_error_count: 0</span><br><span class="line">    remote_dropped_count: 0</span><br><span class="line">    remote_error_count: 0</span><br><span class="line">    remote_timeout_count: 0</span><br><span class="line">    network_timeout_count: 0</span><br><span class="line">    recv_count: 49912</span><br><span class="line">    route_count: 0</span><br><span class="line">    drop_count: 0</span><br><span class="line">    send_length: 18468672</span><br><span class="line">    recv_length: 27625832</span><br><span class="line">    route_length: 0</span><br><span class="line">    drop_length: 0</span><br><span class="line"></span><br><span class="line">Servers:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib1(ib0)&quot;</span> routes=<span class="string">&quot;o2ib2 10.10.0.20@o2ib1&quot;</span></span><br><span class="line">Routers:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib1(ib0),o2ib2(ib1)&quot;</span> <span class="string">&quot;forwarding=enabled&quot;</span></span><br><span class="line">Clients:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib2(ib0)&quot;</span> routes=<span class="string">&quot;o2ib1 10.20.0.29@o2ib2&quot;</span></span><br><span class="line"></span><br><span class="line">Servers:</span><br><span class="line">lnetctl net add --net o2ib1 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl route add --net o2ib2 --gateway 10.10.0.20@o2ib1</span><br><span class="line">lnetctl peer add --nid 10.10.0.20@o2ib1,10.10.0.21@o2ib1</span><br><span class="line"></span><br><span class="line">Routers:</span><br><span class="line">lnetctl net add --net o2ib1 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl net add --net o2ib2 --<span class="keyword">if</span> ib2,ib3</span><br><span class="line">lnetctl peer add --nid 10.10.0.1@o2ib1,10.10.0.2@o2ib1</span><br><span class="line">lnetctl peer add --nid 10.20.0.1@o2ib2,10.20.0.2@o2ib2</span><br><span class="line">lnetctl <span class="built_in">set</span> routing 1</span><br><span class="line"></span><br><span class="line">Clients:</span><br><span class="line">lnetctl net add --net o2ib2 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl route add --net o2ib1 --gateway 10.20.0.29@o2ib2</span><br><span class="line">lnetctl peer add --nid 10.20.0.29@o2ib2,10.20.0.30@o2ib2</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">export</span> FILE.yaml</span><br><span class="line">$ lnetctl <span class="built_in">export</span> &gt; FILE.yaml</span><br><span class="line">$ lnetctl import FILE.yaml</span><br><span class="line">$ lnetctl import &lt; FILE.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="OPA-setting"><a href="#OPA-setting" class="headerlink" title="OPA setting"></a>OPA setting</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ko2iblnd-opa ko2iblnd</span><br><span class="line">options ko2iblnd-opa peer_credits=128 peer_credits_hiw=64 credits=1024 concurrent_sends=256 ntx=2048 map_on_demand=32 fmr_pool_size=2048 fmr_flush_trigger=512 fmr_cache=1</span><br><span class="line"></span><br><span class="line">//The default value is 8, /sys/kernel/debug/lnet/peers, LNet使用peer_credits和network_interface_credits通过网络发送块大小固定为1MB的数据。peer_credits参数管理可以同时发送到单个对等节点的并行</span><br><span class="line">数量</span><br><span class="line">ko2iblnd-opa peer_credits=128</span><br><span class="line"></span><br><span class="line">增加peer_credits的数量并不一定能获得良好的性能，因为在大型的配置中，增加数量会导致网络过载并增加OFED堆栈的内存利用率。可调参数network interface credits(credits)能够限制并行发送到单个网络的数</span><br><span class="line">量，并通过proc/sys/lnet/nis接口进行监控。可以通过特定lfs网络驱动程序(LND)的模块参数来增加network interface credits的数量:</span><br><span class="line">// The default value is 64 and is shared across all the CPU partitions (CPTs).</span><br><span class="line">ko2iblnd-opa credits=1024</span><br><span class="line"></span><br><span class="line">// The default value is 0</span><br><span class="line">ko2iblnd-opa map_on_demand=32</span><br></pre></td></tr></table></figure>


<h4 id="llmount"><a href="#llmount" class="headerlink" title="llmount"></a>llmount</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">https://wiki.whamcloud.com/display/PUB/Testing+a+Lustre+filesystem</span><br><span class="line"></span><br><span class="line">$ /usr/lib64/lfs/tests/llmount.sh</span><br><span class="line"></span><br><span class="line">$ export NAME=testfs                    # used by test-framework.sh to find the configuration file</span><br><span class="line">$ cat lustre/tests/cfg/$NAME.sh</span><br><span class="line">FSNAME=testfs</span><br><span class="line">MDSDEVBASE=/dev/vg_testfs/lvmdt</span><br><span class="line">OSTDEVBASE=/dev/vg_testfs/lvost</span><br><span class="line">OSTCOUNT=$&#123;OSTCOUNT:-5&#125;</span><br><span class="line">MODOPTS_LIBCFS=&quot;libcfs_panic_on_lbug=0&quot;</span><br><span class="line">FAIL_ON_ERROR=$&#123;FAIL_ON_ERROR:-true&#125;</span><br><span class="line">export SHARED_DIRECTORY=&quot;/tmp&quot;          # /tmp is shared for all services on the test node</span><br><span class="line">. $LUSTRE/tests/cfg/local.sh            # source all of the other configuration defaults</span><br><span class="line">unset OSTSIZE                           # use the size of the lvost devices, not a fixed size</span><br><span class="line">unset MDSSIZE                           # use the size of the lvmdt devices, not a fixed size</span><br></pre></td></tr></table></figure>

<h4 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP mdt testfs0-MDT0000 testfs0-MDT0000_UUID 10</span><br><span class="line">mds $ lctl dl | grep &quot;UP mdt&quot; | awk &#x27;&#123;print $1&#125;&#x27;</span><br><span class="line">8</span><br><span class="line">mds $ lctl --device 8 abort_recovery</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line"></span><br><span class="line">mount.lfs xxx xxx -o abort_recov</span><br></pre></td></tr></table></figure>

<h4 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h4><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setstripe -c 1  -i 4 /lfs/dir1</span><br><span class="line">$ copy /lfs/old_dir1/file1 /lfs/dir1</span><br><span class="line">$ <span class="built_in">md5sum</span> /lfs/old_dir1/file1 /lfs/dir1/file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># dont &#x27;t use lfs migrate, it &#x27;s too dangerous, it will cause data loss</span></span><br><span class="line"><span class="comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span></span><br><span class="line"><span class="comment">## lfs migrate -c 1  -i 4 filepath</span></span><br></pre></td></tr></table></figure>

<h4 id="Job-status"><a href="#Job-status" class="headerlink" title="Job status"></a>Job status</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment"># Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If there isn&#x27;t any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, <span class="built_in">sum</span>:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, <span class="built_in">sum</span>:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">mknod</span>:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">link</span>:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">unlink</span>:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">mkdir</span>:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"><span class="comment"># Clear stats for job &quot;dd.0&quot; on lfs-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup interval (seconds)</span></span><br><span class="line">lctl set_param -P testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">lctl set_param  testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">mds $  <span class="built_in">cat</span> /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval</span><br></pre></td></tr></table></figure>

<h4 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[client]# lfs fid2path /mnt      [0x200000400:0x1:0x0]</span><br><span class="line">                                       |         |   |</span><br><span class="line">                                       |         |   -- version</span><br><span class="line">                                       |         ---- object id</span><br><span class="line">                                       ----------Sequence</span><br><span class="line">[client]# lfs path2fid /mnt</span><br><span class="line">[0x200000007:0x1:0x0]</span><br></pre></td></tr></table></figure>


<h4 id="trace-with-debugfs"><a href="#trace-with-debugfs" class="headerlink" title="trace with debugfs"></a>trace with debugfs</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">$ dd if=RHEL5.5_x86_64.iso of=rhel5-1 bs=1M count=1 oflag=sync</span><br><span class="line"></span><br><span class="line">$ cat $(blktrace.log)</span><br><span class="line">  8,32   0     2591  1720.406537446 14972  Q   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2592  1720.406539742 14972  G   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2593  1720.406542136 14972  P   N [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2594  1720.406543690 14972  I   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line"></span><br><span class="line">$ line=19144704</span><br><span class="line">$ debugfs -c -R &quot;icheck $(($line/8))&quot; /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">/dev/sdc: catastrophic mode - not reading inode or group bitmaps</span><br><span class="line">Block   Inode number</span><br><span class="line">2393088 2623</span><br><span class="line">$ debugfs -R &quot;ncheck 2623&quot; /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">Inode   Pathname</span><br><span class="line">2623    /O/0/d13/2669</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lfs getstripe rhel5-1</span><br><span class="line">rhel5-1</span><br><span class="line">lmm_stripe_count:   1</span><br><span class="line">lmm_stripe_size:    1048576</span><br><span class="line">lmm_pattern:        1</span><br><span class="line">lmm_layout_gen:     0</span><br><span class="line">lmm_stripe_offset:  3</span><br><span class="line">lmm_pool:           OST0003</span><br><span class="line">        obdidx           objid           objid           group</span><br><span class="line">             3            2669          0xa6d                0</span><br><span class="line"></span><br><span class="line">$ debugfs -c -R &quot;stat &lt;2623&gt;&quot; /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">/dev/sdc: catastrophic mode - not reading inode or group bitmaps</span><br><span class="line">Inode: 2623   Type: regular    Mode:  0666   Flags: 0x80000</span><br><span class="line">Generation: 2790813459    Version: 0x00000005:00002bf8</span><br><span class="line">User:     0   Group:     0   Size: 1048576</span><br><span class="line">File ACL: 0    Directory ACL: 0</span><br><span class="line">Links: 1   Blockcount: 2048</span><br><span class="line">Fragment:  Address: 0    Number: 0    Size: 0</span><br><span class="line"> ctime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016</span><br><span class="line"> atime: 0x00000000:00000000 -- Thu Jan  1 08:00:00 1970</span><br><span class="line"> mtime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016</span><br><span class="line">crtime: 0x57148057:dfa2ff44 -- Mon Apr 18 14:36:07 2016</span><br><span class="line">Size of extra inode fields: 28</span><br><span class="line">Extended attributes stored in inode body:</span><br><span class="line">  lma = &quot;08 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 6d 0a 00 00 00 00 00 00 &quot; (24)</span><br><span class="line">  lma: fid=[0x100000000:0xa6d:0x0] compat=8 incompat=0</span><br><span class="line">  fid = &quot;d2 0b 00 00 02 00 00 00 40 00 00 00 00 00 00 00 &quot; (16)</span><br><span class="line">  fid: parent=[0x200000bd2:0x40:0x0] stripe=0</span><br><span class="line">EXTENTS:</span><br><span class="line">(0-255):2393088-2393343</span><br><span class="line"></span><br><span class="line">$ lfs fid2path /opt/ [0x200000bd2:0x40:0x0]</span><br><span class="line">/opt/OST0003/rhel5-1</span><br></pre></td></tr></table></figure>

<h4 id="llog-reader"><a href="#llog-reader" class="headerlink" title="llog_reader"></a>llog_reader</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ mount -t ldiskfs /dev/sda /mnt/mgs</span><br><span class="line">$ llog_reader /mnt/mgs/CONFIGS/tfs-client</span><br><span class="line"></span><br><span class="line">$ debugfs -c -R <span class="string">&#x27;dump CONFIGS/tfs-client /tmp/tfs-client&#x27;</span> /dev/sda</span><br><span class="line">$ llog_reader /tmp/tfs-client</span><br></pre></td></tr></table></figure>

<h3 id="2-12-8-client-performance-degraded-in-the-lustre-2-15-0-server"><a href="#2-12-8-client-performance-degraded-in-the-lustre-2-15-0-server" class="headerlink" title="2.12.8 client performance degraded in the lustre 2.15.0 server"></a>2.12.8 client performance degraded in the lustre 2.15.0 server</h3><p>2.15.0 OSS server time not sync<br>2.12.8_6_g5457c37 the 12 cores physical server</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ time <span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..65535&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> $(openssl rand -hex 16) &gt;&gt;  $(openssl rand -hex 8)_<span class="variable">$i</span> ; <span class="keyword">done</span></span><br><span class="line">real    40m53.549s</span><br></pre></td></tr></table></figure>

<p>2.15.0 dual core, KVM</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$time</span> <span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..65535&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> $(openssl rand -hex 16) &gt;&gt;  $(openssl rand -hex 8)_<span class="variable">$i</span> ; <span class="keyword">done</span></span><br><span class="line">real    11m20.142s</span><br></pre></td></tr></table></figure>

<h4 id="lfs-zfs-direct-IO-support"><a href="#lfs-zfs-direct-IO-support" class="headerlink" title="lfs zfs direct IO support"></a><a target="_blank" rel="noopener" href="https://lfs-discuss.lfs.narkive.com/S7kbvnG2/lfs-on-zfs-pooer-direct-i-o-performance">lfs zfs direct IO support</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">John, with newer Lfs clients it is possible <span class="keyword">for</span> multiple threads to submit non-overlapping writes concurrently (also not conflicting within a single page), see LU-1669 <span class="keyword">for</span> details.</span><br><span class="line"></span><br><span class="line">Even so, O_DIRECT writes need to be synchronous to disk on the OSS, as Patrick reports, because <span class="keyword">if</span> the OSS fails before the write is on disk there is no cached copy of the data on the client that can be used to resend the RPC.</span><br><span class="line"></span><br><span class="line">The problem is that the ZFS OSD has very long transaction commit <span class="built_in">times</span> <span class="keyword">for</span> synchronous writes because it does not yet have support <span class="keyword">for</span> the ZIL. Using buffered writes, or having very large O_DIRECT writes (e.g. 40MB or larger) and large RPCs (4MB, or up to 16MB <span class="keyword">in</span> 2.9.0) to amortize the <span class="built_in">sync</span> overhead may be beneficial <span class="keyword">if</span> you really want to use O_DIRECT.</span><br></pre></td></tr></table></figure>

<h3 id="Not-wait-the-zfs-sync"><a href="#Not-wait-the-zfs-sync" class="headerlink" title="Not wait the zfs sync"></a>Not wait the zfs sync</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line">or</span><br><span class="line">lctl set_param osd-zfs.*.osd_object_sync_delay_us=0</span><br><span class="line">or</span><br><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/osd_zfs/parameters/osd_object_sync_delay_us</span><br><span class="line"></span><br><span class="line"><span class="comment">## to default</span></span><br><span class="line"><span class="built_in">echo</span> -1 &gt; /sys/module/osd_zfs/parameters/osd_object_sync_delay_us</span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance until ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lfs to block waiting on a TXG to <span class="built_in">sync</span></span><br></pre></td></tr></table></figure>

<h3 id="errors"><a href="#errors" class="headerlink" title="errors"></a>errors</h3><h4 id="network"><a href="#network" class="headerlink" title="network"></a>network</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client_bulk_callback() event <span class="built_in">type</span> 2, status -5, desc ffff</span><br></pre></td></tr></table></figure>

<h3 id="Build-lfs-MASTER-with-zfs-under-almalinux-8-7"><a href="#Build-lfs-MASTER-with-zfs-under-almalinux-8-7" class="headerlink" title="Build lfs MASTER with zfs under almalinux 8.7"></a><a target="_blank" rel="noopener" href="https://wiki.whamcloud.com/pages/viewpage.action?pageId=154144662">Build lfs MASTER with zfs under almalinux 8.7</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line">$ yum remove kernel-selftests-internal-4.18.0-372.9.1.el8_lustre.x86_64 kernel-debuginfo-common-x86_64-4.18.0-372.9.1.el8_lustre.x86_64 kernel-devel-4.18.0-348.2.1.el8_lustre.x86_64 kernel-debuginfo-4.18.0-372.9.1.el8_lustre.x86_64 kernel-4.18.0-348.2.1.el8_lustre.x86_64 kernel-modules-internal-4.18.0-372.9.1.el8_lustre.x86_64 kernel-modules-internal-4.18.0-348.2.1.el8_lustre.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment"># install el8.7 kernel</span></span><br><span class="line">$ rpm -Uvh  --force kernel-headers-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-devel-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-core-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-modules-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-tools-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-core-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-modules-extra-4.18.0-425.10.1.el8_7.x86_64.rpm kernel-tools-libs-4.18.0-425.10.1.el8_7.x86_64.rpm</span><br><span class="line"></span><br><span class="line">$ dnf remove $(rpm -qa | grep lustre)</span><br><span class="line">$ dnf remove libss-devel-1.46.2.wc5-0.el8.x86_64</span><br><span class="line">$ rpm -Uhv --force e2fsprogs-libs-1.45.6-5.el8.x86_64.rpm libcom_err-1.45.6-5.el8.x86_64.rpm ../../../../devel/x86_64/os/Packages/e2fsprogs-static-1.45.6-5.el8.x86_64.rpm e2fsprogs-libs-1.45.6-5.el8.x86_64.rpm e2fsprogs-1.45.6-5.el8.x86_64.rpm libss-1.45.6-5.el8.x86_64.rpm libss-1.45.6-5.el8.x86_64.rpm e2fsprogs-devel-1.45.6-5.el8.x86_64.rpm libcom_err-1.45.6-5.el8.x86_64.rpm libcom_err-devel-1.45.6-5.el8.x86_64.rpm ../../../debug/x86_64/Packages/e2fsprogs-debugsource-1.45.6-5.el8.x86_64.rpm libss-1.45.6-5.el8.x86_64.rpm</span><br><span class="line"></span><br><span class="line">$ dnf install -y  audit-libs-devel binutils-devel elfutils-devel kabi-dw ncurses-devel newt-devel numactl-devel openssl-devel pciutils-devel perl perl-devel python2 python3-docutils xmlto xz-devel elfutils-libelf-devel libcap-devel libcap-ng-devel llvm-toolset libyaml libyaml-devel kernel-rpm-macros kernel-abi-whitelists uuid libuuid-devel libblkid libblkid-devel libtirpc-devel libtirpc libaio-devel libattr-devel   libffi-devel libudev-devel ncompress python3-cffi python3-devel python3-packaging  libmount libmount-devel make cmake automake gdb gcc libyaml-devel e2fsprogs e2fsprogs-devel e2fsprogs-libs libcom_err libcom_err-devel libss libss-devel libnl3-devel bpftool dwarves java-devel  libbabeltrace-devel libmnl-devel libbpf-devel python3-sphinx</span><br><span class="line"></span><br><span class="line">$ git <span class="built_in">clone</span> git://git.whamcloud.com/fs/lustre-release.git</span><br><span class="line">$ <span class="built_in">cd</span> lustre-release</span><br><span class="line"><span class="comment">#if you want switch the version</span></span><br><span class="line"><span class="comment">#$ git tag</span></span><br><span class="line"><span class="comment">#$ git checkout v2_15_2</span></span><br><span class="line"><span class="comment">#here is the default version</span></span><br><span class="line">$ git describe --tags</span><br><span class="line">v2_15_54</span><br><span class="line"></span><br><span class="line">$ sh ./autogen.sh</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HOME</span>; <span class="built_in">mkdir</span> -p kernel/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125;; <span class="built_in">cd</span> kernel; <span class="built_in">echo</span> <span class="string">&#x27;%_topdir %(echo $HOME)/kernel/rpmbuild&#x27;</span> &gt; ~/.rpmmacros</span><br><span class="line">$ wget https://repo.almalinux.org/vault/8.7/BaseOS/Source/Packages/kernel-4.18.0-425.10.1.el8_7.src.rpm</span><br><span class="line"></span><br><span class="line">$ rpm -ivh kernel-4.18.0-425.10.1.el8_7.src.rpm</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel/rpmbuild</span><br><span class="line">$ rpmbuild -bp --target=`<span class="built_in">uname</span> -m` ./SPECS/kernel.spec</span><br><span class="line">......</span><br><span class="line">Processed config files are <span class="keyword">in</span> /root/kernel/rpmbuild/BUILD/kernel-4.18.0-425.10.1.el8_7/linux-4.18.0-425.10.1.el8.x86_64/configs</span><br><span class="line">+ <span class="built_in">cd</span> ..</span><br><span class="line">+ find . <span class="string">&#x27;(&#x27;</span> -name <span class="string">&#x27;*.orig&#x27;</span> -o -name <span class="string">&#x27;*~&#x27;</span> <span class="string">&#x27;)&#x27;</span> -<span class="built_in">exec</span> <span class="built_in">rm</span> -f <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;;&#x27;</span></span><br><span class="line">+ find . -name .gitignore -<span class="built_in">exec</span> <span class="built_in">rm</span> -f <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;;&#x27;</span></span><br><span class="line">+ <span class="built_in">cd</span> ..</span><br><span class="line">+ <span class="built_in">exit</span> 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/lustre-release/lustre/kernel_patches/series</span><br><span class="line">$ <span class="built_in">rm</span> -f <span class="variable">$HOME</span>/lustre-kernel-x86_64-lustre.patch</span><br><span class="line">$ <span class="keyword">for</span> patch <span class="keyword">in</span> $(&lt;<span class="string">&quot;4.18-rhel8.7.series&quot;</span>); </span><br><span class="line">  <span class="keyword">do</span> </span><br><span class="line">     patch_file=<span class="string">&quot;<span class="variable">$HOME</span>/lustre-release/lustre/kernel_patches/patches/<span class="variable">$&#123;patch&#125;</span>&quot;</span>; </span><br><span class="line">     <span class="built_in">cat</span> <span class="string">&quot;<span class="variable">$&#123;patch_file&#125;</span>&quot;</span> &gt;&gt; <span class="string">&quot;<span class="variable">$HOME</span>/lustre-kernel-x86_64-lustre.patch&quot;</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> <span class="variable">$HOME</span>/lustre-kernel-x86_64-lustre.patch</span><br><span class="line">...</span><br><span class="line">Index: linux-4.18.0-240.22.1.el8_3/block/bio-integrity.c</span><br><span class="line">===================================================================</span><br><span class="line">--- linux-4.18.0-240.22.1.el8_3.orig/block/bio-integrity.c</span><br><span class="line">+++ linux-4.18.0-240.22.1.el8_3/block/bio-integrity.c</span><br><span class="line">@@ -39,7 +39,7 @@ void blk_flush_integrity(void)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cp</span> ~/lustre-kernel-x86_64-lustre.patch ~/kernel/rpmbuild/SOURCES/patch-4.18.0-lustre.patch</span><br><span class="line"></span><br><span class="line">$ sed -i.inst -e <span class="string">&#x27;/^    find $RPM_BUILD_ROOT\/lib\/modules\/$KernelVer/a\</span></span><br><span class="line"><span class="string">    cp -a fs/ext4/* $RPM_BUILD_ROOT/lib/modules/$KernelVer/build/fs/ext4\</span></span><br><span class="line"><span class="string">    rm -f $RPM_BUILD_ROOT/lib/modules/$KernelVer/build/fs/ext4/ext4-inode-test*&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/^# empty final patch to facilitate testing of kernel patches/i\</span></span><br><span class="line"><span class="string">Patch99995: patch-%&#123;version&#125;-lustre.patch&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/^ApplyOptionalPatch linux-kernel-test.patch/i\</span></span><br><span class="line"><span class="string">ApplyOptionalPatch patch-%&#123;version&#125;-lustre.patch&#x27;</span> \</span><br><span class="line">~/kernel/rpmbuild/SPECS/kernel.spec</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;# x86_64&#x27;</span> &gt; ~/kernel/rpmbuild/SOURCES/kernel-4.18.0-x86_64.config</span><br><span class="line"><span class="comment"># miss match with 8.7 config</span></span><br><span class="line"><span class="comment">#$ cat ~/lustre-release/lustre/kernel_patches/kernel_configs/kernel-4.18.0-4.18-rhel8-x86_64.config &gt;&gt; ~/kernel/rpmbuild/SOURCES/kernel-4.18.0-x86_64.config</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /boot/config-4.18.0-425.10.1.el8_7.x86_64 &gt;&gt; ~/kernel/rpmbuild/SOURCES/kernel-4.18.0-x86_64.config</span><br><span class="line">Find the line with <span class="string">&#x27;# IO Schedulers&#x27;</span> and insert following two lines below it: <span class="comment">#Enabled in the default</span></span><br><span class="line">CONFIG_IOSCHED_DEADLINE=y</span><br><span class="line">CONFIG_DEFAULT_IOSCHED=<span class="string">&quot;deadline&quot;</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel/rpmbuild</span><br><span class="line">$ buildid=<span class="string">&quot;_lustre&quot;</span></span><br><span class="line">$ rpmbuild -ba --with firmware --target x86_64 --with baseonly \</span><br><span class="line">           --without kabichk --define <span class="string">&quot;buildid <span class="variable">$&#123;buildid&#125;</span>&quot;</span> \</span><br><span class="line">           ~/kernel/rpmbuild/SPECS/kernel.spec</span><br><span class="line"></span><br><span class="line">....</span><br><span class="line">Checking <span class="keyword">for</span> unpackaged file(s): /usr/lib/rpm/check-files /root/kernel/rpmbuild/BUILDROOT/kernel-4.18.0-425.10.1.el8_lustre.x86_64</span><br><span class="line">Wrote: /root/kernel/rpmbuild/SRPMS/kernel-4.18.0-425.10.1.el8_lustre.src.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/noarch/kernel-doc-4.18.0-425.10.1.el8_lustre.noarch.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-headers-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-debuginfo-common-x86_64-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-core-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-devel-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-modules-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-modules-extra-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-modules-internal-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-debuginfo-4.18.0-425.10.1.el8_lustre.x86_64.rpm</span><br><span class="line">Executing(%clean): /bin/sh -e /var/tmp/rpm-tmp.gTAWhY</span><br><span class="line">+ <span class="built_in">umask</span> 022</span><br><span class="line">+ <span class="built_in">cd</span> /root/kernel/rpmbuild/BUILD</span><br><span class="line">+ <span class="built_in">cd</span> kernel-4.18.0-425.10.1.el8_7</span><br><span class="line">+ <span class="built_in">rm</span> -rf /root/kernel/rpmbuild/BUILDROOT/kernel-4.18.0-425.10.1.el8_lustre.x86_64</span><br><span class="line">+ <span class="built_in">exit</span> 0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">mkdir</span> -p /root/2.15.2/src</span><br><span class="line">$ <span class="built_in">mv</span> /root/kernel/rpmbuild/SRPMS/kernel-4.18.0-425.10.1.el8_lustre.src.rpm /root/2.15.2/src</span><br><span class="line">$ <span class="built_in">mv</span> /root/kernel/rpmbuild/RPMS/x86_64/*rpm /root/kernel/rpmbuild/RPMS/noarch/*rpm /root/2.15.2/</span><br><span class="line"></span><br><span class="line"><span class="comment"># if mellanox</span></span><br><span class="line">$ <span class="built_in">cd</span> MLNX_OFED_LINUX-5.9-0.5.6.0-rhel8.7-x86_64</span><br><span class="line">$ ./mlnxofedinstall --add-kernel-support</span><br><span class="line"></span><br><span class="line"><span class="comment">#build zfs,downloda zfs-2.1.5.tar.gz</span></span><br><span class="line">$ dnf install gcc make autoconf automake libtool rpm-build libtirpc-devel libblkid-devel libuuid-devel libudev-devel openssl-devel zlib-devel libaio-devel libattr-devel elfutils-libelf-devel kernel-devel-$(<span class="built_in">uname</span> -r) python3 python3-devel python3-setuptools python3-cffi libffi-devel git ncompress libcurl-devel python3-packaging dkms</span><br><span class="line"></span><br><span class="line">$ zfs zfs-2.1.5</span><br><span class="line"><span class="comment">#$ git checkout master</span></span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ ./configure; make -s -j $(grep -c MHz /proc/cpuinfo); make rpms</span><br><span class="line"></span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzpool5-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libnvpair3-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libuutil3-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzfs5-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzfs5-devel-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-test-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/noarch/zfs-dracut-2.1.5-1.el8.noarch.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/noarch/python3-pyzfs-2.1.5-1.el8.noarch.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-debugsource-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzpool5-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libnvpair3-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libuutil3-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzfs5-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-test-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#add ldiskfs</span></span><br><span class="line">--with-ldiskfsprogs</span><br><span class="line"></span><br><span class="line"><span class="comment">#build lustre with mellanox</span></span><br><span class="line">$ ./configure --enable-quota --with-linux=/root/kernel/rpmbuild/BUILD/kernel-4.18.0-425.10.1.el8_7/linux-4.18.0-425.10.1.el8_lustre.x86_64 --enable-server --with-zfs=/root/zfs/zfs-2.1.5 --with-o2ib=/usr/src/ofa_kernel/default</span><br><span class="line"></span><br><span class="line">or by defualt o2ib </span><br><span class="line">$ dnf install librdmacm-debuginfo rdma-core-debuginfo rdma-core-debugsource rdma-core-debugsource</span><br><span class="line"></span><br><span class="line">$ ./configure --enable-quota --with-linux=/root/kernel/rpmbuild/BUILD/kernel-4.18.0-425.10.1.el8_7/linux-4.18.0-425.10.1.el8_lustre.x86_64 --enable-server --with-zfs=/root/zfs/zfs-2.1.5 --with-o2ib=/usr/src/debug/rdma-core-41.0-1.el8.x86_64</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">config.status: executing libtool commands</span><br><span class="line"></span><br><span class="line">CC:            gcc</span><br><span class="line">LD:            /usr/bin/ld -m elf_x86_64</span><br><span class="line">CPPFLAGS:      -include /root/lustre-release/undef.h -include /root/lustre-release/config.h -I/root/lustre-release/lnet/include/uapi -I/root/lustre-release/lustre/include/uapi -I/root/lustre-release/libcfs/include -I/root/lustre-release/lnet/utils/ -I/root/lustre-release/lustre/include </span><br><span class="line">CFLAGS:        -g -O2 -Wall -Werror</span><br><span class="line">EXTRA_KCFLAGS: -include /root/lustre-release/undef.h -include /root/lustre-release/config.h  -g -I/root/lustre-release/libcfs/include -I/root/lustre-release/libcfs/include/libcfs -I/root/lustre-release/lnet/include/uapi -I/root/lustre-release/lnet/include -I/root/lustre-release/lustre/include/uapi -I/root/lustre-release/lustre/include -Wno-format-truncation -Wno-stringop-truncation -Wno-stringop-overflow</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;make&#x27;</span> to build Lustre.</span><br><span class="line"></span><br><span class="line">$ make rpms</span><br><span class="line">......</span><br><span class="line">Checking <span class="keyword">for</span> unpackaged file(s): /usr/lib/rpm/check-files /tmp/rpmbuild-lustre-root-UgvtYxFm/BUILDROOT/lustre-2.15.2-1.el8.x86_64</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-osd-ldiskfs-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-osd-ldiskfs-mount-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-osd-zfs-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-osd-zfs-mount-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-resource-agents-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-devel-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-tests-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-tests-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-iokit-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-debugsource-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-osd-ldiskfs-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-osd-ldiskfs-mount-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-osd-zfs-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-osd-zfs-mount-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/lustre-tests-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lustre-root-UgvtYxFm/RPMS/x86_64/kmod-lustre-tests-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Executing(%clean): /bin/sh -e /tmp/rpmbuild-lustre-root-UgvtYxFm/TMP/rpm-tmp.2EyaTj</span><br><span class="line"></span><br><span class="line">first time mount show input/output error when mellanox mount the broadcom RocEv2 or broadcom mount the mellanox</span><br><span class="line">each server mkfs as a lustre server</span><br><span class="line">mount twice sucess</span><br><span class="line"></span><br><span class="line">LustreError: 166-1: MGC192.168.0.16@o2ib2: Connection to MGS (at 192.168.0.16@o2ib2) was lost; <span class="keyword">in</span> progress operations using this service will fail</span><br><span class="line">LustreError: 15c-8: MGC192.168.0.16@o2ib2: Confguration from <span class="built_in">log</span> testfs6-client failed from MGS -5. Communication error between node &amp; MGS, a bad configuration, or other errors. See syslog <span class="keyword">for</span> more info</span><br><span class="line"></span><br><span class="line">bnxt_en 0000:8a:00.0: QPLIB: FP: CQ Processed Req wr_id[39] = 0xff39ee65e0331739 with status 0x3</span><br><span class="line">Lustre: Mounted lustre4-client</span><br><span class="line">Lustre: MGS: Client d1a8ba05-ce20-4632-8409-13306103f0d9 (at 192.168.0.17@o2ib2) reconnecting</span><br><span class="line">Lustre: MGS: haven<span class="string">&#x27;t heard from client d1a8ba05-ce20-4632-8409-13306103f0d9 (at 192.168.0.17@o2ib2) in 227 seconds. I think it&#x27;</span>s dead, and I am evicting it. exp 00000000eb5ecb96, cur 1676287681 expire 1676287531 last 1676287454</span><br></pre></td></tr></table></figure>

<h4 id="kernel-modules"><a href="#kernel-modules" class="headerlink" title="kernel modules"></a>kernel modules</h4><ul>
<li>&#x2F;sys&#x2F;module&#x2F;osd_zfs&#x2F;parameters&#x2F;osd_object_sync_delay_us</li>
</ul>
<h4 id="idmapped-mounts"><a href="#idmapped-mounts" class="headerlink" title="idmapped mounts"></a>idmapped mounts</h4><p><a target="_blank" rel="noopener" href="https://lwn.net/Articles/838916/">1</a><br><a target="_blank" rel="noopener" href="https://lpc.events/event/11/contributions/1086/attachments/926/1826/christian_brauner_idmapped_mounts.pdf">2</a><br><a target="_blank" rel="noopener" href="https://github.com/containerd/containerd/pull/4734">3</a><br><a target="_blank" rel="noopener" href="https://github.com/containers/podman/issues/10374">4</a></p>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#NET"><span class="toc-number">1.</span> <span class="toc-text">NET</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#UDSP"><span class="toc-number">1.1.</span> <span class="toc-text">UDSP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#project-quota"><span class="toc-number">1.2.</span> <span class="toc-text">project quota</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#User-group-quota"><span class="toc-number">2.</span> <span class="toc-text">User group quota</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#re-writeconf"><span class="toc-number">2.1.</span> <span class="toc-text">re-writeconf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#changelog"><span class="toc-number">2.2.</span> <span class="toc-text">changelog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kdump"><span class="toc-number">2.3.</span> <span class="toc-text">kdump</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><span class="toc-number"></span> <span class="toc-text">1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><span class="toc-number"></span> <span class="toc-text">1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#reaadonly-mount"><span class="toc-number">0.1.</span> <span class="toc-text">reaadonly mount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DNE"><span class="toc-number">0.2.</span> <span class="toc-text">DNE</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ALL"><span class="toc-number">1.</span> <span class="toc-text">ALL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collect-log"><span class="toc-number">2.</span> <span class="toc-text">collect log</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#list-all-parameters"><span class="toc-number">2.1.</span> <span class="toc-text">list all parameters</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDS"><span class="toc-number">3.</span> <span class="toc-text">MDS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor"><span class="toc-number">3.1.</span> <span class="toc-text">monitor</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OSS"><span class="toc-number">4.</span> <span class="toc-text">OSS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Monitor"><span class="toc-number">4.1.</span> <span class="toc-text">Monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#format"><span class="toc-number">4.2.</span> <span class="toc-text">format</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#change-ipaddr"><span class="toc-number">4.3.</span> <span class="toc-text">change ipaddr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Enable-large-dir-feature-enable-on-the-2-15-X"><span class="toc-number">4.4.</span> <span class="toc-text">Enable large_dir feature, enable on the 2.15.X</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EC"><span class="toc-number">4.5.</span> <span class="toc-text">EC</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client"><span class="toc-number">5.</span> <span class="toc-text">Client</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor-1"><span class="toc-number">5.1.</span> <span class="toc-text">monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#complie-client"><span class="toc-number">5.2.</span> <span class="toc-text">complie client</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ubuntu-install-client"><span class="toc-number">5.2.1.</span> <span class="toc-text">ubuntu install client</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#get-the-status"><span class="toc-number">5.3.</span> <span class="toc-text">get the status</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backup-and-recovery-lfs-ZFS-OST"><span class="toc-number">6.</span> <span class="toc-text">Backup and recovery lfs ZFS OST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-multiple-ethernet-port-for-diff-LAN"><span class="toc-number">7.</span> <span class="toc-text">lfs multiple ethernet port for diff LAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#trace-the-kernel-sock"><span class="toc-number">8.</span> <span class="toc-text">trace the kernel sock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-magic-num"><span class="toc-number">9.</span> <span class="toc-text">lfs magic num</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rocev2-test"><span class="toc-number">10.</span> <span class="toc-text">rocev2 test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DOM"><span class="toc-number">11.</span> <span class="toc-text">DOM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lnet-health"><span class="toc-number">11.1.</span> <span class="toc-text">lnet health</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OPA-setting"><span class="toc-number">11.2.</span> <span class="toc-text">OPA setting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llmount"><span class="toc-number">11.3.</span> <span class="toc-text">llmount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Skip-recovery"><span class="toc-number">11.4.</span> <span class="toc-text">Skip recovery</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-migarate"><span class="toc-number">11.5.</span> <span class="toc-text">lfs migarate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Job-status"><span class="toc-number">11.6.</span> <span class="toc-text">Job status</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-fid-and-path"><span class="toc-number">11.7.</span> <span class="toc-text">lfs fid and path</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#trace-with-debugfs"><span class="toc-number">11.8.</span> <span class="toc-text">trace with debugfs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llog-reader"><span class="toc-number">11.9.</span> <span class="toc-text">llog_reader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-12-8-client-performance-degraded-in-the-lustre-2-15-0-server"><span class="toc-number">12.</span> <span class="toc-text">2.12.8 client performance degraded in the lustre 2.15.0 server</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-zfs-direct-IO-support"><span class="toc-number">12.1.</span> <span class="toc-text">lfs zfs direct IO support</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Not-wait-the-zfs-sync"><span class="toc-number">13.</span> <span class="toc-text">Not wait the zfs sync</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#errors"><span class="toc-number">14.</span> <span class="toc-text">errors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#network"><span class="toc-number">14.1.</span> <span class="toc-text">network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Build-lfs-MASTER-with-zfs-under-almalinux-8-7"><span class="toc-number">15.</span> <span class="toc-text">Build lfs MASTER with zfs under almalinux 8.7</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#kernel-modules"><span class="toc-number">15.1.</span> <span class="toc-text">kernel modules</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#idmapped-mounts"><span class="toc-number">15.2.</span> <span class="toc-text">idmapped mounts</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2019/12/29/lfs_cmd/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2019/12/29/lfs_cmd/&text=lfs command"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2019/12/29/lfs_cmd/&is_video=false&description=lfs command"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=lfs command&body=Check out this article: http://example.com/2019/12/29/lfs_cmd/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2019/12/29/lfs_cmd/&name=lfs command&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2019/12/29/lfs_cmd/&t=lfs command"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2023
    John Doe
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = '67e8c052/67e8c052.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'blog-comments';
      var utterances_theme = 'github-dark';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
