<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="lfs Maintenance The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per seco">
<meta property="og:type" content="article">
<meta property="og:title" content="lfs command">
<meta property="og:url" content="http://example.com/2019/12/29/lfs_cmd/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="lfs Maintenance The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per seco">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2019-12-29T07:41:16.000Z">
<meta property="article:modified_time" content="2024-09-15T06:09:19.496Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="fs">
<meta property="article:tag" content="filesys">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>lfs command</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2020/03/03/crash/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2019/11/26/xfs/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2019/12/29/lfs_cmd/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2019/12/29/lfs_cmd/&text=lfs command"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2019/12/29/lfs_cmd/&is_video=false&description=lfs command"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=lfs command&body=Check out this article: http://example.com/2019/12/29/lfs_cmd/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2019/12/29/lfs_cmd/&name=lfs command&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2019/12/29/lfs_cmd/&t=lfs command"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-Maintenance"><span class="toc-number">1.</span> <span class="toc-text">lfs Maintenance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reinstall-not-working"><span class="toc-number">2.</span> <span class="toc-text">reinstall not working</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-bugs-in-test"><span class="toc-number">3.</span> <span class="toc-text">lfs bugs in test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#print-OST"><span class="toc-number">4.</span> <span class="toc-text">print OST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hot-file"><span class="toc-number">5.</span> <span class="toc-text">[hot file]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NET"><span class="toc-number">6.</span> <span class="toc-text">NET</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#UDSP"><span class="toc-number">6.1.</span> <span class="toc-text">UDSP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lustre-test"><span class="toc-number">6.2.</span> <span class="toc-text">lustre test</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#project-quota"><span class="toc-number">6.3.</span> <span class="toc-text">project quota</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#User-group-quota"><span class="toc-number">7.</span> <span class="toc-text">User group quota</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#re-writeconf"><span class="toc-number">7.1.</span> <span class="toc-text">re-writeconf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#changelog"><span class="toc-number">7.2.</span> <span class="toc-text">changelog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kdump"><span class="toc-number">7.3.</span> <span class="toc-text">kdump</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><span class="toc-number"></span> <span class="toc-text">1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><span class="toc-number"></span> <span class="toc-text">1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#reaadonly-mount"><span class="toc-number">0.1.</span> <span class="toc-text">reaadonly mount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DNE"><span class="toc-number">0.2.</span> <span class="toc-text">DNE</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ALL"><span class="toc-number">1.</span> <span class="toc-text">ALL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collect-log"><span class="toc-number">2.</span> <span class="toc-text">collect log</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#list-all-parameters"><span class="toc-number">2.1.</span> <span class="toc-text">list all parameters</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDS"><span class="toc-number">3.</span> <span class="toc-text">MDS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor"><span class="toc-number">3.1.</span> <span class="toc-text">monitor</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OSS"><span class="toc-number">4.</span> <span class="toc-text">OSS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Monitor"><span class="toc-number">4.1.</span> <span class="toc-text">Monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#format"><span class="toc-number">4.2.</span> <span class="toc-text">format</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#change-ipaddr"><span class="toc-number">4.3.</span> <span class="toc-text">change ipaddr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EC"><span class="toc-number">4.4.</span> <span class="toc-text">EC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PFL-Progressive-file-layouts"><span class="toc-number">4.5.</span> <span class="toc-text">PFL Progressive file layouts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client-CLIENT"><span class="toc-number">5.</span> <span class="toc-text">Client CLIENT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor-1"><span class="toc-number">5.1.</span> <span class="toc-text">monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#complie-client"><span class="toc-number">5.2.</span> <span class="toc-text">complie client</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ubuntu-install-client"><span class="toc-number">5.2.1.</span> <span class="toc-text">ubuntu install client</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#get-the-status"><span class="toc-number">5.3.</span> <span class="toc-text">get the status</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backup-and-recovery-lfs-ZFS-OST"><span class="toc-number">6.</span> <span class="toc-text">Backup and recovery lfs ZFS OST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-multiple-ethernet-port-for-diff-LAN"><span class="toc-number">7.</span> <span class="toc-text">lfs multiple ethernet port for diff LAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#trace-the-kernel-sock"><span class="toc-number">8.</span> <span class="toc-text">trace the kernel sock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-magic-num"><span class="toc-number">9.</span> <span class="toc-text">lfs magic num</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rocev2-test"><span class="toc-number">10.</span> <span class="toc-text">rocev2 test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DOM"><span class="toc-number">11.</span> <span class="toc-text">DOM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lnet-health"><span class="toc-number">11.1.</span> <span class="toc-text">lnet health</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OPA-setting"><span class="toc-number">11.2.</span> <span class="toc-text">OPA setting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llmount"><span class="toc-number">11.3.</span> <span class="toc-text">llmount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Skip-recovery"><span class="toc-number">11.4.</span> <span class="toc-text">Skip recovery</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-migarate"><span class="toc-number">11.5.</span> <span class="toc-text">lfs migarate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Job-status"><span class="toc-number">11.6.</span> <span class="toc-text">Job status</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-fid-and-path"><span class="toc-number">11.7.</span> <span class="toc-text">lfs fid and path</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#trace-with-debugfs"><span class="toc-number">11.8.</span> <span class="toc-text">trace with debugfs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llog-reader"><span class="toc-number">11.9.</span> <span class="toc-text">llog_reader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-12-8-client-performance-degraded-in-the-lfs-2-15-0-server"><span class="toc-number">12.</span> <span class="toc-text">2.12.8 client performance degraded in the lfs 2.15.0 server</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-zfs-direct-IO-support"><span class="toc-number">12.1.</span> <span class="toc-text">lfs zfs direct IO support</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Not-wait-the-zfs-sync"><span class="toc-number">13.</span> <span class="toc-text">Not wait the zfs sync</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#errors"><span class="toc-number">14.</span> <span class="toc-text">errors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#network"><span class="toc-number">14.1.</span> <span class="toc-text">network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Build-lfs-MASTER-with-zfs-under-almalinux-8-7"><span class="toc-number">15.</span> <span class="toc-text">Build lfs MASTER with zfs under almalinux 8.7</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#kernel-modules"><span class="toc-number">15.1.</span> <span class="toc-text">kernel modules</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#idmapped-mounts"><span class="toc-number">15.2.</span> <span class="toc-text">idmapped mounts</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#debug"><span class="toc-number">15.3.</span> <span class="toc-text">debug</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#show-scrub-status"><span class="toc-number">15.4.</span> <span class="toc-text">show scrub status</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-extended-attributes"><span class="toc-number">16.</span> <span class="toc-text">lfs extended attributes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dell-lustre-tuning-with-OPF"><span class="toc-number">17.</span> <span class="toc-text">Dell lustre tuning with OPF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lustre-word"><span class="toc-number">18.</span> <span class="toc-text">Lustre word</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        lfs command
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">John Doe</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-12-29T07:41:16.000Z" itemprop="datePublished">2019-12-29</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Storage/">Storage</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/filesys/" rel="tag">filesys</a>, <a class="tag-link-link" href="/tags/fs/" rel="tag">fs</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h3 id="lfs-Maintenance"><a href="#lfs-Maintenance" class="headerlink" title="lfs Maintenance"></a>lfs Maintenance</h3><ul>
<li><a target="_blank" rel="noopener" href="https://hpcf.umbc.edu/general-productivity/lfs-best-practices/">The lfs server can only handle about 15,000 remote procedure calls (RPCs, inter-process communications that allow the client to cause a procedure to be executed on the server) per second. Contention slows the performance of your applications and weakens the overall health of the lfs filesystem</a><ul>
<li>Avoid Using ls -l</li>
<li>Avoid Having a Large Number of Files in a Single Directory</li>
<li>Avoid Accessing Small Files</li>
<li>Avoid Repetitive “stat” Operations</li>
<li>Avoid Having Multiple Processes Open the Same File(s) at the Same Time</li>
<li>Avoid Repetitive Open&#x2F;Close Operations</li>
</ul>
</li>
</ul>
<h3 id="reinstall-not-working"><a href="#reinstall-not-working" class="headerlink" title="reinstall not working"></a>reinstall not working</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ find /usr /var /etc/ -<span class="built_in">type</span> d | grep 4.18.0-553.5.1 <span class="comment">#&lt;---kernel version</span></span><br><span class="line"><span class="comment">#Deleting in preparation for a symlink</span></span><br><span class="line"></span><br><span class="line">$ modprobe ib_cm</span><br><span class="line">$ modinfo ib_cm  <span class="comment">#is it the MOFED or the original version of the kernel</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#in another cause , it you install 553.8.1.el8_10_lustre.x86_64 and 553.8.1.el8_10.x86_64, there is a kernel version same with lustre, MOFED install will be wrong, remove the same kernel</span></span><br></pre></td></tr></table></figure>
<p>remove all installation directory<br>because some of kernel module link not removed cause create link failed in reinstall    </p>
<h3 id="lfs-bugs-in-test"><a href="#lfs-bugs-in-test" class="headerlink" title="lfs bugs in test"></a>lfs bugs in test</h3><ul>
<li>2.15.3 <ul>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-17229">can ‘t skip the tgt recovery</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-16408">can ‘t skip the tgt recovery</a></li>
</ul>
</li>
</ul>
<h3 id="print-OST"><a href="#print-OST" class="headerlink" title="print OST"></a><a target="_blank" rel="noopener" href="https://www.scc.kit.edu/scc/docs/Lustre/kit_lad15_20150922.pdf">print OST</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> OST$(<span class="built_in">printf</span> %04X <span class="variable">$i</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get object IDs for file name:</span></span><br><span class="line">[user@client]$ lfs getstripe myfile</span><br><span class="line">lmm_stripe_count: 2</span><br><span class="line"> obdidx objid objid group</span><br><span class="line"> 0 71666856 0x4458ca8 0  &lt;-----------71666856 = 0x4458ca8</span><br><span class="line"> 2 72574780 0x453673c 0</span><br><span class="line"></span><br><span class="line">[user@client]$ <span class="built_in">stat</span> --format=<span class="string">&quot;%u %g&quot;</span> myfile</span><br><span class="line">8972 12345</span><br><span class="line"></span><br><span class="line">[root@OST0]<span class="comment"># statcmd=&quot;stat \</span></span><br><span class="line"> /O/0/d$((<span class="number">71666856</span> % <span class="number">32</span>))/71666856<span class="string">&quot;   &lt;--------------71666856 = 0x4458ca8</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[root@OST0]# debugfs -c -R &quot;</span><span class="variable">$statcmd</span><span class="string">&quot; \</span></span><br><span class="line"><span class="string"> /dev/mapper/ost_pfs2wor2_0</span></span><br><span class="line"><span class="string">User: 8972 Group: 12345 Size: 5</span></span><br><span class="line"><span class="string"> fid: parent=[0x200018a62:0x8a4d:0x0] stripe=0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[user@client]$ lfs path2fid myfile</span></span><br><span class="line"><span class="string">[0x200018a62:0x8a4d:0x0]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[root@client]# lfs fid2path pfs2wor2 [0x200018a62:0x8a4d:0x0] </span></span><br><span class="line"><span class="string">&lt;path_to_myfile&gt;/myfile</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ ls -i test_file</span></span><br><span class="line"><span class="string">144116513912193025 test_file</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ stat -c %i test_file</span></span><br><span class="line"><span class="string">144116513912193025</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ printf &quot;</span>%<span class="comment">#x\n&quot; $(stat -c %i test_file)</span></span><br><span class="line">0x2000134b2010001</span><br><span class="line"></span><br><span class="line">$ lfs fid2path /tfs12 [0x2000134b2:0x010001:0x0]</span><br><span class="line">/lutre/test/test_file</span><br></pre></td></tr></table></figure>

<h3 id="hot-file"><a href="#hot-file" class="headerlink" title="[hot file]"></a>[hot file]</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param debug=+rpctrace</span><br><span class="line"></span><br><span class="line">root     30566  0.1  0.0      0     0 ?        D     2022 1115:24 [ll_ost_io01_005]</span><br><span class="line">root     30775  0.1  0.0      0     0 ?        D     2022 1111:28 [ll_ost_io01_006]</span><br><span class="line">root     31300  0.1  0.0      0     0 ?        D     2022 1119:45 [ll_ost_io01_007]</span><br><span class="line">root     33170  0.0  0.0      0     0 ?        D     2022 128:45 [ll_ost00_034]</span><br><span class="line">root     33187  0.0  0.0      0     0 ?        D     2022 138:51 [ll_ost00_050]</span><br><span class="line">root     37776  0.1  0.0      0     0 ?        D     2022 1126:04 [ll_ost_io01_008]</span><br><span class="line">root     37778  0.1  0.0      0     0 ?        D     2022 1122:12 [ll_ost_io01_009]</span><br><span class="line">root     37781  0.1  0.0      0     0 ?        D     2022 1112:57 [ll_ost_io01_010]</span><br><span class="line">root     37784  0.1  0.0      0     0 ?        D     2022 1100:38 [ll_ost_io01_013]</span><br><span class="line">root     37785  0.1  0.0      0     0 ?        D     2022 1114:08 [ll_ost_io01_014]</span><br><span class="line">root     37793  0.1  0.0      0     0 ?        D     2022 1127:13 [ll_ost_io01_016]</span><br><span class="line">root     37795  0.1  0.0      0     0 ?        D     2022 1118:31 [ll_ost_io01_017]</span><br><span class="line">root     38259  0.0  0.0      0     0 ?        D     2022 145:08 [ll_ost01_029]</span><br><span class="line">root     38262  0.0  0.0      0     0 ?        D     2022 142:34 [ll_ost01_032]</span><br><span class="line">root     38289  0.0  0.0      0     0 ?        D     2022 144:00 [ll_ost01_050]</span><br><span class="line">root     38293  0.0  0.0      0     0 ?        D     2022 138:26 [ll_ost01_054]</span><br><span class="line">root     38398  0.1  0.0      0     0 ?        D     2022 1108:31 [ll_ost_io01_022]</span><br><span class="line">root     38399  0.1  0.0      0     0 ?        D     2022 1102:46 [ll_ost_io01_023]</span><br><span class="line">root     38401  0.1  0.0      0     0 ?        D     2022 1121:16 [ll_ost_io01_025]</span><br><span class="line">root     38404  0.1  0.0      0     0 ?        D     2022 1121:55 [ll_ost_io01_027]</span><br><span class="line">root     38407  0.1  0.0      0     0 ?        D     2022 1114:40 [ll_ost_io01_030]</span><br><span class="line">root     38411  0.1  0.0      0     0 ?        D     2022 1140:46 [ll_ost_io01_034]</span><br><span class="line">root     38451  0.1  0.0      0     0 ?        D     2022 1155:12 [ll_ost_io01_036]</span><br><span class="line">root     38456  0.1  0.0      0     0 ?        D     2022 1130:14 [ll_ost_io01_039]</span><br><span class="line">root     38459  0.1  0.0      0     0 ?        D     2022 1107:47 [ll_ost_io01_040]</span><br><span class="line">root     38460  0.1  0.0      0     0 ?        D     2022 1135:06 [ll_ost_io01_041]</span><br><span class="line">root     38466  0.1  0.0      0     0 ?        D     2022 1129:21 [ll_ost_io01_046]</span><br><span class="line">root     38468  0.1  0.0      0     0 ?        D     2022 1119:59 [ll_ost_io01_048]</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/38262/stack</span><br><span class="line">[&lt;ffffffffc1488945&gt;] ptlrpc_wait_event+0x345/0x360 [ptlrpc]</span><br><span class="line">[&lt;ffffffffc148f0c2&gt;] ptlrpc_main+0xa02/0x1470 [ptlrpc]</span><br><span class="line">[&lt;ffffffff87ac5c21&gt;] kthread+0xd1/0xe0</span><br><span class="line">[&lt;ffffffff88193df7&gt;] ret_from_fork_nospec_end+0x0/0x39</span><br><span class="line">[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/38451/stack </span><br><span class="line">[&lt;ffffffffc084c085&gt;] wait_transaction_locked+0x85/0xd0 [jbd2]</span><br><span class="line">[&lt;ffffffffc084c378&gt;] add_transaction_credits+0x278/0x310 [jbd2]</span><br><span class="line">[&lt;ffffffffc084c601&gt;] start_this_handle+0x1a1/0x430 [jbd2]</span><br><span class="line">[&lt;ffffffffc084cab3&gt;] jbd2__journal_start+0xf3/0x1f0 [jbd2]</span><br><span class="line">[&lt;ffffffffc0ee8459&gt;] __ldiskfs_journal_start_sb+0x69/0xe0 [ldiskfs]</span><br><span class="line">[&lt;ffffffffc12ede6e&gt;] osd_trans_start+0x20e/0x4e0 [osd_ldiskfs]</span><br><span class="line">[&lt;ffffffffc1726425&gt;] ofd_trans_start+0x75/0xf0 [ofd]</span><br><span class="line">[&lt;ffffffffc172d701&gt;] ofd_commitrw_write+0xa31/0x1db0 [ofd]</span><br><span class="line">[&lt;ffffffffc1731c8c&gt;] ofd_commitrw+0x47c/0xa50 [ofd]</span><br><span class="line">[&lt;ffffffffc14e0c2c&gt;] obd_commitrw+0x9c/0x370 [ptlrpc]</span><br><span class="line">[&lt;ffffffffc14e50d2&gt;] tgt_brw_write+0xf02/0x1ad0 [ptlrpc]</span><br><span class="line">[&lt;ffffffffc14e6f1a&gt;] tgt_request_handle+0xada/0x1570 [ptlrpc]</span><br><span class="line">[&lt;ffffffffc148b88b&gt;] ptlrpc_server_handle_request+0x24b/0xab0 [ptlrpc]</span><br><span class="line">[&lt;ffffffffc148f1f4&gt;] ptlrpc_main+0xb34/0x1470 [ptlrpc]</span><br><span class="line">[&lt;ffffffff87ac5c21&gt;] kthread+0xd1/0xe0</span><br><span class="line">[&lt;ffffffff88193df7&gt;] ret_from_fork_nospec_end+0x0/0x39</span><br><span class="line">[&lt;ffffffffffffffff&gt;] 0xffffffffffffffff</span><br><span class="line"></span><br><span class="line">$ grep lock dk | grep OST0049 -c</span><br><span class="line">609</span><br><span class="line">$ grep lock dk | grep OST0048 -c</span><br><span class="line">714 &lt;------the highest random IOPS, at least this one is the hot file, random write 8 Bytes</span><br><span class="line">$ grep lock dk | grep OST004b -c</span><br><span class="line">4350 &lt;----- hot but not busy , it <span class="string">&#x27;s the seq IO</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">analysis the client by perf trace with OSS blktrace IO model, not debug from lustre log   </span></span><br><span class="line"><span class="string">next from lustre log</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    71.837 ( 0.039 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    71.880 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202192440, whence: SET                     ) = 107202192440</span></span><br><span class="line"><span class="string">    71.894 ( 0.007 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    71.905 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202197640, whence: SET                     ) = 107202197640</span></span><br><span class="line"><span class="string">    71.914 ( 0.044 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    71.963 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202198320, whence: SET                     ) = 107202198320</span></span><br><span class="line"><span class="string">    71.975 ( 0.007 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    71.987 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202199560, whence: SET                     ) = 107202199560</span></span><br><span class="line"><span class="string">    71.999 ( 0.035 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.038 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202201784, whence: SET                     ) = 107202201784</span></span><br><span class="line"><span class="string">    72.047 ( 0.006 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.057 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202203816, whence: SET                     ) = 107202203816</span></span><br><span class="line"><span class="string">    72.069 ( 0.037 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.111 ( 0.005 ms): Test2/338984 lseek(fd: 5762, offset: 107202206784, whence: SET                     ) = 107202206784</span></span><br><span class="line"><span class="string">    72.124 ( 0.006 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.134 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202207488, whence: SET                     ) = 107202207488</span></span><br><span class="line"><span class="string">    72.143 ( 0.005 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.153 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202212656, whence: SET                     ) = 107202212656</span></span><br><span class="line"><span class="string">    72.162 ( 0.036 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.202 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202214920, whence: SET                     ) = 107202214920</span></span><br><span class="line"><span class="string">    72.215 ( 0.006 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.225 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202216352, whence: SET                     ) = 107202216352</span></span><br><span class="line"><span class="string">    72.234 ( 0.038 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.277 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202217936, whence: SET                     ) = 107202217936</span></span><br><span class="line"><span class="string">    72.287 ( 0.006 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.297 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202226120, whence: SET                     ) = 107202226120</span></span><br><span class="line"><span class="string">    72.306 ( 0.040 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.351 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202227080, whence: SET                     ) = 107202227080</span></span><br><span class="line"><span class="string">    72.360 ( 0.006 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.370 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202229400, whence: SET                     ) = 107202229400</span></span><br><span class="line"><span class="string">    72.379 ( 0.039 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.423 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202231984, whence: SET                     ) = 107202231984</span></span><br><span class="line"><span class="string">    72.434 ( 0.015 ms): Test2/338984 write(fd: 5762, buf: 0x2b8ab3c0a100, count: 8                         ) = 8</span></span><br><span class="line"><span class="string">    72.453 ( 0.004 ms): Test2/338984 lseek(fd: 5762, offset: 107202233688, whence: SET                     ) = 107202233688</span></span><br></pre></td></tr></table></figure>

<h3 id="NET"><a href="#NET" class="headerlink" title="NET"></a>NET</h3><h4 id="UDSP"><a href="#UDSP" class="headerlink" title="UDSP"></a><a target="_blank" rel="noopener" href="https://wiki.whamcloud.com/display/LNet/UDSP+Behaviour">UDSP</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lustre global</span></span><br><span class="line">struct ksock_nal_data ksocknal_data</span><br><span class="line"></span><br><span class="line"><span class="comment">#backup lnet config</span></span><br><span class="line">$ lnetctl <span class="built_in">export</span> --backup lnet.conf</span><br><span class="line"></span><br><span class="line"><span class="comment">#disable tcp, only rocev2</span></span><br><span class="line">$ lnetctl net del --net tcp</span><br><span class="line"><span class="comment">#import </span></span><br><span class="line">$ lnetctl import lnet.conf</span><br><span class="line"></span><br><span class="line">$ lnetctl net show</span><br><span class="line">net:</span><br><span class="line">    - net <span class="built_in">type</span>: lo</span><br><span class="line">      <span class="built_in">local</span> NI(s):</span><br><span class="line">        - nid: 0@lo</span><br><span class="line">          status: up</span><br><span class="line">    - net <span class="built_in">type</span>: tcp</span><br><span class="line">      <span class="built_in">local</span> NI(s):</span><br><span class="line">        - nid: 192.168.122.110@tcp</span><br><span class="line">          status: up</span><br><span class="line">          interfaces:</span><br><span class="line">              0: eth0</span><br><span class="line">        - nid: 192.168.122.253@tcp</span><br><span class="line">          status: up</span><br><span class="line">          interfaces:</span><br><span class="line">              0: eth1</span><br><span class="line">    - net <span class="built_in">type</span>: tcp1</span><br><span class="line">      <span class="built_in">local</span> NI(s):</span><br><span class="line">        - nid: 192.168.122.238@tcp1</span><br><span class="line">          status: up</span><br><span class="line">          interfaces:</span><br><span class="line">              0: eth2</span><br><span class="line"></span><br><span class="line"><span class="comment">#network order</span></span><br><span class="line"><span class="comment">#Prioritize one of PeerA NIDs on tcp</span></span><br><span class="line">$ lnetctl udsp add --src 192.168.122.110@tcp</span><br><span class="line"></span><br><span class="line"><span class="comment">#Prioritize tcp1 as source on PeerA</span></span><br><span class="line">$ lnetctl udsp add --src tcp1</span><br><span class="line"></span><br><span class="line">$ lnetctl udsp show</span><br><span class="line">    - idx: 0</span><br><span class="line"></span><br><span class="line">$ lnetctl udsp del 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#found a strage order, either format --mgsnode/--servicenode or lnet.conf loading kernel module, o2ib should be first &lt;---</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># configure o2ib to be preferred</span></span><br><span class="line">$ lnetctl udsp add --src o2ib --priority 0</span><br><span class="line"></span><br><span class="line">$ lnetctl udsp add --src 192.168.0.1@o2ib --priority 0</span><br><span class="line">$ lnetctl udsp add --dst 192.168.1.1@o2ib --priority 0</span><br><span class="line">$ lnetctl udsp add --src 192.168.0.1@o2ib --dst 192.168.1.1@o2ib</span><br><span class="line"></span><br><span class="line">$ lnetctl udsp add --dst 192.168.0.1@o2ib --rte 192.168.1.2@o2ib</span><br><span class="line">$ lnetctl <span class="built_in">export</span> --backup</span><br><span class="line"></span><br><span class="line">$ lnetctl net show -v 4 | grep put</span><br><span class="line">$ lnetctl peer show -v 4</span><br><span class="line"></span><br><span class="line">$ lnetctl net show -v | grep -E <span class="string">&#x27;nid|send_count|recv_count&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lnetctl net add --net tcp --<span class="keyword">if</span> eth0</span><br><span class="line">$ lnetctl net add --net kfi --<span class="keyword">if</span> cxi0</span><br><span class="line">$ lnetctl net add --net kfi --<span class="keyword">if</span> cxi1</span><br><span class="line">$ lnetctl udsp add --src kfi --priority 0</span><br><span class="line">$ lnetctl <span class="built_in">set</span> health_sensitivity 0</span><br><span class="line">$ lnetctl net <span class="built_in">set</span> --health 0 --nid 17@kfi</span><br><span class="line">$ lnetctl net <span class="built_in">set</span> --health 0 --nid 5@kfi</span><br><span class="line">$ lnetctl net show -v 4 | grep -E <span class="string">&#x27;nid|priority|health value&#x27;</span></span><br><span class="line">        - nid: 0@lo</span><br><span class="line">              net priority: -1</span><br><span class="line">              nid priority: -1</span><br><span class="line">              health value: 0</span><br><span class="line">        - nid: 10.53.129.76@o2ib</span><br><span class="line">              net priority: 0</span><br><span class="line">              nid priority: -1</span><br><span class="line">              health value: 1000</span><br><span class="line">        - nid: 10.53.129.76@tcp</span><br><span class="line">              net priority: -1</span><br><span class="line">              nid priority: -1</span><br><span class="line">              health value: 1000</span><br><span class="line"></span><br><span class="line">https://www.opensfs.org/wp-content/uploads/LUG2023-kfilndandUDSP.pdf</span><br></pre></td></tr></table></figure>

<h4 id="lustre-test"><a href="#lustre-test" class="headerlink" title="lustre test"></a>lustre test</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lst.sh -t A@tcp -f B@tcp -m <span class="built_in">read</span> -g servers</span><br></pre></td></tr></table></figure>

<h4 id="project-quota"><a href="#project-quota" class="headerlink" title="project quota"></a>project quota</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">ldiskfs mds$ tune2fs -O project /dev/md1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mds$ lctl conf_param  <span class="variable">$FSNAME</span>.quota.mdt=ugp</span><br><span class="line">mds$ lctl conf_param  testfs-MDT0000.*.quota.mdt=ugp</span><br><span class="line"></span><br><span class="line">mds$ lctl conf_param  <span class="variable">$FSNAME</span>.quota.ost=ugp</span><br><span class="line">mds$ lctl conf_param  testfs-MDT0000.*.quota.ost=ugp</span><br><span class="line"></span><br><span class="line">mds$ lctl get_param osd-*.*.quota_slave.info</span><br><span class="line">osd-ldiskfs.testfs-MDT0000.quota_slave.info=</span><br><span class="line">target name:    testfs-MDT0000</span><br><span class="line">pool ID:        0</span><br><span class="line"><span class="built_in">type</span>:           md</span><br><span class="line">quota enabled:  ugp</span><br><span class="line">conn to master: setup</span><br><span class="line">space acct:     ugp</span><br><span class="line">user uptodate:  glb[1],slv[1],reint[0]</span><br><span class="line">group uptodate: glb[1],slv[1],reint[0]</span><br><span class="line">project uptodate: glb[1],slv[1],reint[0]</span><br><span class="line"></span><br><span class="line">client $ mount.lfs xx.xx.xx.xx@tcp:/fsname /testfs -o localflock,user_xattr</span><br><span class="line">client $ <span class="built_in">mkdir</span> /testfs/prj3</span><br><span class="line">client $ chattr +P /testfs/prj3</span><br><span class="line">client $ chattr -p 3 /testfs/prj3</span><br><span class="line"><span class="comment">###client $ lfs quota -p 3 /testfs/prj3</span></span><br><span class="line">client $ lfs setquota -p 3 -b 10G -B 11G -i 10000 -I 11000 /testfs/prj3</span><br><span class="line">client $ <span class="built_in">chown</span> nfsnobody.nfsnobody /testfs/prj3</span><br><span class="line">client $ su - nfsnobody -s /bin/bash -c <span class="string">&#x27;dd if=/dev/zero of=/testfs/prj3/test3 bs=1M count=20000&#x27;</span></span><br><span class="line"><span class="built_in">dd</span>: error writing ‘/testfs/prj3/test3’: Disk quota exceeded</span><br><span class="line">301+0 records <span class="keyword">in</span></span><br><span class="line">300+0 records out</span><br><span class="line">314572800 bytes (315 MB) copied, 5.22756 s, 60.2 MB/s</span><br><span class="line"></span><br><span class="line">client $  lfs quota -p 3 -h /testfs</span><br><span class="line">Disk quotas <span class="keyword">for</span> prj 3 (pid 3):</span><br><span class="line">     Filesystem    used   quota   <span class="built_in">limit</span>   grace   files   quota   <span class="built_in">limit</span>   grace</span><br><span class="line">          /testfs  20.09G*    20G     21G    none       4   10000   11000       -</span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">mkdir</span> /testfs/prj3/test100</span><br><span class="line">client $ <span class="built_in">chown</span> games.games /testfs/prj3/test100</span><br><span class="line">client $ su - games -s /bin/bash -c <span class="string">&#x27;dd if=/dev/zero of=/testfs/prj3/test100/test0 bs=1M count=20000&#x27;</span></span><br><span class="line"><span class="built_in">dd</span>: error writing ‘/testfs/prj3/test100/test0’: Disk quota exceeded</span><br><span class="line">2+0 records <span class="keyword">in</span></span><br><span class="line">1+0 records out</span><br><span class="line">1445888 bytes (1.4 MB) copied, 0.0689412 s, 21.0 MB/s</span><br></pre></td></tr></table></figure>

<h3 id="User-group-quota"><a href="#User-group-quota" class="headerlink" title="User group quota"></a>User group quota</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## you can &#x27;t use lctl set_param, it &#x27;s not work</span></span><br><span class="line">mds $ lctl conf_param/set_param -P fsname.quota.ost|mdt=u|g|p|ugp|none</span><br><span class="line"></span><br><span class="line"><span class="comment">#enable</span></span><br><span class="line">mds $ lctl conf_param  <span class="variable">$FSNAME</span>.quota.ost=ugp</span><br><span class="line">mds $ lctl conf_param  <span class="variable">$FSNAME</span>.quota.mdt=ugp</span><br><span class="line"></span><br><span class="line">mds $ <span class="built_in">cat</span> /proc/fs/lfs/osd-ldiskfs/<span class="variable">$FSNAME</span>-MDT0000/quota_slave/info</span><br><span class="line">quota enabled:  <span class="string">&quot;ug&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#disable quota</span></span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FSNAME</span>.quota.ost=none</span><br><span class="line">mds $ lctl set_param -P <span class="variable">$FSNAME</span>.quota.mdt=none</span><br><span class="line"></span><br><span class="line"><span class="comment">### lctl set_param -P must reboot, no -P not work, if you don&#x27; t reboot ,you have too use conf_param</span></span><br><span class="line"></span><br><span class="line">client $ lfs setquota -u user1 -b 307200 -B 309200 -i 10000 -I 11000 /mnt/lfs</span><br><span class="line">client $ lfs setquota -g group1 -b 5120000 -B 5150000 -i 100000 -I 101000 /mnt/lfs</span><br><span class="line"></span><br><span class="line">client $ lfs quota -u user1 -v /mnt/lfs</span><br><span class="line">client $ lfs quota -t -p /mnt/lfs</span><br><span class="line">Block grace time: 1w; Inode grace time: 1w</span><br></pre></td></tr></table></figure>

<h4 id="re-writeconf"><a href="#re-writeconf" class="headerlink" title="re-writeconf"></a>re-writeconf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mds$ tunefs.lfs --writeconf /dev/sdx</span><br><span class="line">oss$ tunefs.lfs --writeconf /dev/ost0</span><br></pre></td></tr></table></figure>

<h4 id="changelog"><a href="#changelog" class="headerlink" title="changelog"></a>changelog</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">$ lctl --device $FSNAME-MDT0000 changelog_register</span><br><span class="line">$ lctl --device $FSNAME-MDT0001 changelog_register</span><br><span class="line"></span><br><span class="line">$ lctl set_param mdt.*.hsm_control=enabled</span><br><span class="line">$ lctl set_param -P  mdt.*.hsm_control=enabled</span><br><span class="line">$ lctl set_param mdt.*.hsm.active_request_timeout=10800</span><br><span class="line">$ lctl set_param mdt.*.hsm.max_requests=16</span><br><span class="line">$ lctl set_param _ mdt.*.hsm.max_requests=16</span><br><span class="line"></span><br><span class="line"># create user</span><br><span class="line">$ lctl --device $FSNAME-MDT0000 changelog_register</span><br><span class="line"># del user</span><br><span class="line">$ lctl --device $FSNAME-MDT0000 changelog_deregister cl1</span><br><span class="line"></span><br><span class="line">#show user</span><br><span class="line">$ lctl get_param mdd.*.changelog_users</span><br><span class="line"></span><br><span class="line"># Get the size</span><br><span class="line">$ lctl get_param mdd.*.changelog_users mdd.*.changelog_size</span><br><span class="line"></span><br><span class="line"># changelog mask</span><br><span class="line">$ lctl set_param mdd.$FSNAME-MDT*.changelog_mask=MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RNMFM RNMTO OPEN LYOUT TRUNC CLOSE IOCTL TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line">$ lctl get_param mdd.$FSNAME-MDT*.changelog_mask</span><br><span class="line">MARK CREAT MKDIR HLINK SLINK MKNOD UNLNK RMDIR RENME RNMTO OPEN LYOUT TRUNC SATTR XATTR HSM MTIME CTIME</span><br><span class="line"></span><br><span class="line">$ FSNAME=testfs; lctl set_param mdd.$FSNAME-MDT0000.changelog_mask=&quot;UNLNK RMDIR RENME RNMTO NOPEN&quot;</span><br><span class="line">mdd.testfs-MDT0000.changelog_mask=UNLNK RMDIR RENME RNMTO NOPEN</span><br><span class="line">$ lctl get_param mdd.$FSNAME-MDT0000.changelog_mask</span><br><span class="line">mdd.testfs-MDT0000.changelog_mask=MARK UNLNK RMDIR RENME RNMTO NOPEN</span><br><span class="line"></span><br><span class="line">#Audit just the changelog extention</span><br><span class="line">#To have a fully functional Changelogs-based audit facility, some additional Changelog record types must be enabled, to be able to record events such as OPEN, ATIME, GETXATTR and DENIED OPEN.</span><br><span class="line">mds# lctl set_param mdd.lfs-MDT0000.changelog_mask=ALL</span><br><span class="line">mdd.seb-MDT0000.changelog_mask=ALL</span><br><span class="line"></span><br><span class="line">To prevent nodes pertaining to a nodemap to generate Changelog entries</span><br><span class="line">$ lctl nodemap_modify --name nm1 --property audit_mode --value 0</span><br><span class="line">$ lctl set_param mdd.lfs-MDT0000.changelog_deniednext=120</span><br><span class="line">mdd.seb-MDT0000.changelog_deniednext=120</span><br><span class="line">$ lctl get_param mdd.lfs-MDT0000.changelog_deniednext</span><br><span class="line">mdd.seb-MDT0000.changelog_deniednext=120</span><br><span class="line"></span><br><span class="line"># Get the changelog</span><br><span class="line">$ lfs changelog $FSNAME-MDT0000 &gt; lfs-changelog</span><br><span class="line">$ fs changelog $fsname-MDT0000 [startrec [endrec]]</span><br><span class="line"></span><br><span class="line"># clear all</span><br><span class="line">$ lctl changelog_clear mdt_name userid endrec</span><br><span class="line"></span><br><span class="line">MARK    Internal recordkeeping</span><br><span class="line">CREAT   Regular file creation</span><br><span class="line">MKDIR   Directory creation</span><br><span class="line">HLINK   Hard link</span><br><span class="line">SLINK   Soft link</span><br><span class="line">MKNOD   Other file creation</span><br><span class="line">UNLNK   Regular file removal</span><br><span class="line">RMDIR   Directory removal</span><br><span class="line">RENME   Rename, original</span><br><span class="line">RNMTO   Rename, final</span><br><span class="line">OPEN    Open</span><br><span class="line">CLOSE   Close</span><br><span class="line">LYOUT   Layout change</span><br><span class="line">TRUNC   Regular file truncated</span><br><span class="line">SATTR   Attribute change</span><br><span class="line">XATTR   Extended attribute change (setxattr)</span><br><span class="line">HSM     HSM specific event</span><br><span class="line">MTIME   MTIME change</span><br><span class="line">CTIME   CTIME change</span><br><span class="line">ATIME   ATIME change</span><br><span class="line">MIGRT   Migration event</span><br><span class="line">FLRW    File Level Replication: file initially written</span><br><span class="line">RESYNC  File Level Replication: file re-synced</span><br><span class="line">GXATR   Extended attribute access (getxattr)</span><br><span class="line">NOPEN   Denied open</span><br><span class="line">````</span><br><span class="line"></span><br><span class="line">#### kdump</span><br></pre></td></tr></table></figure>

<h4 id="kdump"><a href="#kdump" class="headerlink" title="kdump"></a>kdump</h4><p>yum -y install kexec-tools<br>cat &#x2F;etc&#x2F;kdump.conf<br>nfs my.nfsserver.example.org:&#x2F;path&#x2F;to&#x2F;expor<br>core_collector makedumpfile -d 16 -c<br>#-c Compress dump data by each page<br>#core_collector makedumpfile -d 16 -c message_level 16</p>
<h1 id="1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><a href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages" class="headerlink" title="1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages"></a>1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</h1><h1 id="1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><a href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages" class="headerlink" title="1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages"></a>1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</h1><p>#ssh <a href="mailto:&#x75;&#x73;&#101;&#114;&#64;&#109;&#121;&#x2e;&#115;&#101;&#114;&#x76;&#101;&#114;&#46;&#x65;&#x78;&#x61;&#x6d;&#112;&#x6c;&#101;&#x2e;&#111;&#x72;&#x67;">&#x75;&#x73;&#101;&#114;&#64;&#109;&#121;&#x2e;&#115;&#101;&#114;&#x76;&#101;&#114;&#46;&#x65;&#x78;&#x61;&#x6d;&#112;&#x6c;&#101;&#x2e;&#111;&#x72;&#x67;</a>:&#x2F;dest&#x2F;path<br>#By default, uses ssh key at &#x2F;root&#x2F;.ssh&#x2F;kdump_id_rsa<br>#core_collector makedumpfile <options></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### Disable </span><br></pre></td></tr></table></figure>
<p>#Notify a device that user cl1 no longer needs records (up toand including 3)<br>$ lfs changelog_clear $FSNAME-MDT0000 cl1 3</p>
<p>#To stop changelogs, changelog_mask should be set to MARK only<br>$ lctl set_param mdd.$FSNAME-MDT0000.changelog_mask&#x3D;MARK<br>mdd.lfs-MDT0000.changelog_mask&#x3D;MARK</p>
<p>#or youcan set it -all<br>$ lctl set_param mdd.$FSNAME-MDT0000.changelog_mask&#x3D;-all</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### FSCK</span><br><span class="line">```bash</span><br><span class="line">Dec 29 14:11:32 mookie kernel: LDISKFS-fs error (device sdz): ldiskfs_lookup: unlinked inode 5384166 in dir #145170469</span><br><span class="line">Dec 29 14:11:32 mookie kernel: Remounting filesystem read-only</span><br></pre></td></tr></table></figure>

<ul>
<li>Flush the journal<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ umount /lfs</span><br><span class="line">$ mount -t ldiskfs /dev/sdx /lfs</span><br><span class="line">$ umount /lfs</span><br></pre></td></tr></table></figure></li>
</ul>
<ul>
<li><p>Ensure e2fsprogs version ,it ‘s not default linux version ,it ‘s lfs version</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa | grep e2fsprogs</span><br><span class="line">e2fsprogs-1.42.12.wc1-7.el6.x86_64</span><br><span class="line">e2fsprogs-libs-1.42.12.wc1-7.el6.x86_64</span><br></pre></td></tr></table></figure>
</li>
<li><p>Before fsck，make sure the mount point has been <font color=red>umount</font></p>
</li>
<li><p>Can check multiple MDT&#x2F;OSTs in parallel</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Check only mode</span><br><span class="line">$ e2fsck -fn /dev/sdx</span><br><span class="line"></span><br><span class="line"># Prudent mode</span><br><span class="line">$ e2fsck -fp /dev/sdx</span><br><span class="line"></span><br><span class="line"># Answer yes</span><br><span class="line">$ e2fsck -fy /dev/sdx</span><br></pre></td></tr></table></figure>


<h4 id="reaadonly-mount"><a href="#reaadonly-mount" class="headerlink" title="reaadonly mount"></a>reaadonly mount</h4><p>add “-o nosvc”  </p>
<h4 id="DNE"><a href="#DNE" class="headerlink" title="DNE"></a>DNE</h4><p>2.15.X could be auto balance</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ lfs <span class="built_in">mkdir</span> -i1 /mnt/lfs/example</span><br><span class="line">or</span><br><span class="line">$ lfs setdirstripe -i 1 /tfs12/test_mdt/test_mdt1</span><br><span class="line"></span><br><span class="line"><span class="comment">#lfs setdirstripe -D -c 1 -i -1 [--max-depth[-rr] &lt;levels&gt;] &lt;directory&gt;</span></span><br><span class="line"><span class="comment">#LU-13439, LU-13440</span></span><br><span class="line"></span><br><span class="line">$ lfs setdirstripe -c 2 -i 1 -H all_char /mnt/lfs/example</span><br><span class="line">$ lfs getdirstripe /mnt/lfs/example</span><br><span class="line">lmv_stripe_count: 2 lmv_stripe_offset: 1 lmv_hash_type: all_char</span><br><span class="line">mdtidx           FID[<span class="built_in">seq</span>:oid:ver]</span><br><span class="line">       1           [0x240000400:0x4:0x0]  </span><br><span class="line">       0           [0x200000401:0x4:0x0]</span><br><span class="line"></span><br><span class="line">$ lfs getdirstripe -O lustre-MDT0000_UUID -r /tfs12/test_mdt</span><br><span class="line">/tfs12/test_mdt/test_mdt0</span><br><span class="line">lmv_stripe_count: 0 lmv_stripe_offset: 0 lmv_hash_type: none</span><br><span class="line">/tfs12/test_mdt/test_mdt0/dir1</span><br><span class="line">lmv_stripe_count: 0 lmv_stripe_offset: 0 lmv_hash_type: none</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>disable DNE balance   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setdirstripe -D -c 1 -i 0 --max-inherit=1</span><br></pre></td></tr></table></figure>

<ul>
<li><a target="_blank" rel="noopener" href="https://www.eofs.eu/_media/events/lad21/lad2021-lfs_2.15_and_beyond-dilger.pdf">lfs 2.15.0 roadmap features</a><ul>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-12815">Multiple TCP sockets performance</a><ul>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-15136">conns_per_peer </a><ul>
<li>socklnd conns_per_peer set to 0 provides the optimal setting for the interface given its bandwidth. Make it default.</li>
</ul>
</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-9121">UDSP</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-12125">Parallel rename within a directory</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-14792">Default DNE MDT space balance</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-14712">Improved ldiskfs “-o discard” efficiency</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-12043">Improve parallel client readahead</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-10983">Metadata Writeback Cache</a></li>
<li><a target="_blank" rel="noopener" href="https://jira.whamcloud.com/browse/LU-13798">Parallel large DIO</a></li>
<li>DNE - Distributed Namespace</li>
<li>DoM - Data on Metadata</li>
<li>PFL - Progressive File Layout</li>
<li>File Level Replication - FLR</li>
<li>PCC - Persistent Client Cache<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br></pre></td><td class="code"><pre><span class="line">1x client</span><br><span class="line">options lnet networks=tcp2(enp129s0f0np0)</span><br><span class="line">options ko2iblnd conns_per_peer=4</span><br><span class="line">options ksocklnd conns_per_peer=4</span><br><span class="line">options lnet accept_backlog=1024</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">options lnet lnet_retry_count=6</span><br><span class="line">options lnet lnet_transaction_timeout=120</span><br><span class="line">options ksocklnd credits=512</span><br><span class="line">options ksocklnd peer_credits=16</span><br><span class="line">options ptlrpc at_max=320</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=240</span><br><span class="line"></span><br><span class="line"><span class="comment">#impact the performance(set in the client)</span></span><br><span class="line">options ko2iblnd peer_credits=8 &lt;------<span class="keyword">in</span> the rocev2 and lfs 2.15.3 client, increase it will cause sequence write(1MiB) amplification, randwrite 4KiB is OK, 4 clients, 64x fio each client</span><br><span class="line">options ksocklnd peer_credits=8 &lt;------tcp is ok, could be increased</span><br><span class="line">osc.*.max_rpcs_in_flight &lt;---the higher the lower ops ??? make sure</span><br><span class="line"></span><br><span class="line">options lnet networks=o2ib(ens2np0),tcp(ens2np0) &lt;------ <span class="keyword">if</span> you tcp(ens2np0),o2ib(ens2np0) cause bug, not show everytime. you <span class="string">&#x27;d better let o2ib in the first</span></span><br><span class="line"><span class="string">                                                         eg: client could not be mount, if you switch them, mount be sucessful</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">1x server</span></span><br><span class="line"><span class="string">$ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=64  mdc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=512 llite.*.max_read_ahead_mb=0 osc.*.grant_shrink=0  mdc.*.max_mod_rpcs_in_flight=50</span></span><br><span class="line"><span class="string">options lnet networks=tcp(ens1f0np0),o2ib(ens1f0np0),o2ib2(ens1f1np1),tcp2(ens1f1np1)</span></span><br><span class="line"><span class="string">options lnet accept_backlog=1024</span></span><br><span class="line"><span class="string">options lnet accept_timeout=15</span></span><br><span class="line"><span class="string">options lnet lnet_retry_count=6</span></span><br><span class="line"><span class="string">options lnet lnet_transaction_timeout=120</span></span><br><span class="line"><span class="string">options ptlrpc at_max=320 at_min=50 ldlm_enqueue_min=240</span></span><br><span class="line"><span class="string">options ko2iblnd nscheds=6 peer_credits=128 credits=1024 conns_per_peer=4</span></span><br><span class="line"><span class="string">options ksocklnd nscheds=6 peer_credits=128 credits=1024 conns_per_peer=4</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[global]</span></span><br><span class="line"><span class="string">rw=read</span></span><br><span class="line"><span class="string">ioengine=libaio</span></span><br><span class="line"><span class="string">iodepth=64</span></span><br><span class="line"><span class="string">direct=1</span></span><br><span class="line"><span class="string">fallocate=none</span></span><br><span class="line"><span class="string">size=100G</span></span><br><span class="line"><span class="string">numjobs=1</span></span><br><span class="line"><span class="string">group_reporting</span></span><br><span class="line"><span class="string">time_based</span></span><br><span class="line"><span class="string">runtime=3600</span></span><br><span class="line"><span class="string">bssplit=1M/100</span></span><br><span class="line"><span class="string">[test]</span></span><br><span class="line"><span class="string">directory=/tfs6/test100/stripe_dir</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">test: (groupid=0, jobs=1): err= 0: pid=12850: Mon Mar 27 13:53:59 2023</span></span><br><span class="line"><span class="string">  read: IOPS=10.0k, BW=10.7GiB/s (11.5GB/s)(7246GiB/674746msec)</span></span><br><span class="line"><span class="string">    slat (usec): min=58, max=3599, avg=66.83, stdev= 6.55</span></span><br><span class="line"><span class="string">    clat (usec): min=1380, max=11781, avg=5751.98, stdev=342.56</span></span><br><span class="line"><span class="string">     lat (usec): min=1443, max=12049, avg=5818.94, stdev=342.46</span></span><br><span class="line"><span class="string">    clat percentiles (usec):</span></span><br><span class="line"><span class="string">     |  1.00th=[ 5014],  5.00th=[ 5211], 10.00th=[ 5342], 20.00th=[ 5473],</span></span><br><span class="line"><span class="string">     | 30.00th=[ 5538], 40.00th=[ 5669], 50.00th=[ 5735], 60.00th=[ 5800],</span></span><br><span class="line"><span class="string">     | 70.00th=[ 5932], 80.00th=[ 5997], 90.00th=[ 6194], 95.00th=[ 6325],</span></span><br><span class="line"><span class="string">     | 99.00th=[ 6587], 99.50th=[ 6718], 99.90th=[ 6980], 99.95th=[ 7439],</span></span><br><span class="line"><span class="string">     | 99.99th=[ 9372]</span></span><br><span class="line"><span class="string">   bw (  MiB/s): min=10845, max=11032, per=100.00%, avg=11008.27, stdev= 7.48, samples=1347</span></span><br><span class="line"><span class="string">   iops        : min=10845, max=11032, avg=11008.27, stdev= 7.49, samples=1347</span></span><br><span class="line"><span class="string">  lat (msec)   : 2=0.01%, 4=0.02%, 10=99.97%, 20=0.01%</span></span><br><span class="line"><span class="string">  cpu          : usr=1.41%, sys=73.72%, ctx=1934986, majf=0, minf=3248</span></span><br><span class="line"><span class="string">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span></span><br><span class="line"><span class="string">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span></span><br><span class="line"><span class="string">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span></span><br><span class="line"><span class="string">     issued rwts: total=7420356,0,0,0 short=0,0,0,0 dropped=0,0,0,0</span></span><br><span class="line"><span class="string">     latency   : target=0, window=0, percentile=100.00%, depth=64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Run status group 0 (all jobs):</span></span><br><span class="line"><span class="string">   READ: bw=10.7GiB/s (11.5GB/s), 10.7GiB/s-10.7GiB/s (11.5GB/s-11.5GB/s), io=7246GiB (7781GB), run=674746-674746msec</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">OSS 4x NVME SSD</span></span><br><span class="line"><span class="string">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span></span><br><span class="line"><span class="string">           0.00    0.00    5.92    0.00    0.00   94.08</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Device            r/s     w/s     rMB/s     wMB/s   rrqm/s   wrqm/s  %rrqm  %wrqm r_await w_await aqu-sz rareq-sz wareq-sz  svctm  %util</span></span><br><span class="line"><span class="string">nvme3n1       3368.00    0.00   2750.00      0.00     0.00     0.00   0.00   0.00    0.25    0.00   0.83   836.10     0.00   0.30 100.00</span></span><br><span class="line"><span class="string">nvme4n1       3338.00    0.00   2750.00      0.00     0.00     0.00   0.00   0.00    0.25    0.00   0.84   843.62     0.00   0.30 100.00</span></span><br><span class="line"><span class="string">nvme1n1       21999.00    0.00   2749.88      0.00     0.00     0.00   0.00   0.00    0.49    0.00  10.67   128.00     0.00   0.05 100.00</span></span><br><span class="line"><span class="string">nvme2n1       22000.00    0.00   2750.00      0.00     0.00     0.00   0.00   0.00    0.48    0.00  10.57   128.00     0.00   0.05 100.00</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&lt;-------here is the tcp------&gt;</span></span><br><span class="line"><span class="string">[global]</span></span><br><span class="line"><span class="string">rw=write</span></span><br><span class="line"><span class="string">ioengine=libaio</span></span><br><span class="line"><span class="string">iodepth=64</span></span><br><span class="line"><span class="string">direct=1</span></span><br><span class="line"><span class="string">fallocate=none</span></span><br><span class="line"><span class="string">size=100G</span></span><br><span class="line"><span class="string">numjobs=1</span></span><br><span class="line"><span class="string">group_reporting</span></span><br><span class="line"><span class="string">time_based</span></span><br><span class="line"><span class="string">runtime=3600</span></span><br><span class="line"><span class="string">bssplit=1M/100</span></span><br><span class="line"><span class="string">[rw]</span></span><br><span class="line"><span class="string">directory=/tfs6/stripe</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">fio-3.19</span></span><br><span class="line"><span class="string">Starting 1 process</span></span><br><span class="line"><span class="string">^Cbs: 1 (f=1): [W(1)][1.1%][w=10.2GiB/s][w=10.5k IOPS][eta 59m:20s]</span></span><br><span class="line"><span class="string">rw: (groupid=0, jobs=1): err= 0: pid=73838: Tue Apr 25 17:12:28 2023</span></span><br><span class="line"><span class="string">  write: IOPS=10.3k, BW=10.0GiB/s (10.8GB/s)(404GiB/40176msec); 0 zone resets</span></span><br><span class="line"><span class="string">    slat (usec): min=50, max=529, avg=82.21, stdev=18.92</span></span><br><span class="line"><span class="string">    clat (usec): min=673, max=45095, avg=6137.29, stdev=3555.68</span></span><br><span class="line"><span class="string">     lat (usec): min=731, max=45198, avg=6219.62, stdev=3556.00</span></span><br><span class="line"><span class="string">    clat percentiles (usec):</span></span><br><span class="line"><span class="string">     |  1.00th=[ 1860],  5.00th=[ 2507], 10.00th=[ 2933], 20.00th=[ 3687],</span></span><br><span class="line"><span class="string">     | 30.00th=[ 4359], 40.00th=[ 4948], 50.00th=[ 5538], 60.00th=[ 5997],</span></span><br><span class="line"><span class="string">     | 70.00th=[ 6587], 80.00th=[ 7570], 90.00th=[ 9634], 95.00th=[12256],</span></span><br><span class="line"><span class="string">     | 99.00th=[20841], 99.50th=[25297], 99.90th=[33817], 99.95th=[36439],</span></span><br><span class="line"><span class="string">     | 99.99th=[40109]</span></span><br><span class="line"><span class="string">   bw (  MiB/s): min= 7250, max=10734, per=100.00%, avg=10297.66, stdev=601.64, samples=80</span></span><br><span class="line"><span class="string">   iops        : min= 7250, max=10734, avg=10297.65, stdev=601.63, samples=80</span></span><br><span class="line"><span class="string">  lat (usec)   : 750=0.01%, 1000=0.03%</span></span><br><span class="line"><span class="string">  lat (msec)   : 2=1.50%, 4=22.96%, 10=66.38%, 20=7.97%, 50=1.15%</span></span><br><span class="line"><span class="string">  cpu          : usr=18.98%, sys=62.36%, ctx=28950, majf=0, minf=5819</span></span><br><span class="line"><span class="string">  IO depths    : 1=0.1%, 2=0.1%, 4=0.1%, 8=0.1%, 16=0.1%, 32=0.1%, &gt;=64=100.0%</span></span><br><span class="line"><span class="string">     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, &gt;=64=0.0%</span></span><br><span class="line"><span class="string">     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.1%, &gt;=64=0.0%</span></span><br><span class="line"><span class="string">     issued rwts: total=0,413341,0,0 short=0,0,0,0 dropped=0,0,0,0</span></span><br><span class="line"><span class="string">     latency   : target=0, window=0, percentile=100.00%, depth=64</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Run status group 0 (all jobs):</span></span><br><span class="line"><span class="string">  WRITE: bw=10.0GiB/s (10.8GB/s), 10.0GiB/s-10.0GiB/s (10.8GB/s-10.8GB/s), io=404GiB (433GB), run=40176-40176msec</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#&lt;--------CPU--------&gt;&lt;----------Disks-----------&gt;&lt;----------Network----------&gt;&lt;-------TCP--------&gt;</span></span><br><span class="line"><span class="string">#cpu sys inter  ctxsw KBRead  Reads KBWrit Writes   KBIn  PktIn  KBOut  PktOut   IP  Tcp  Udp Icmp </span></span><br><span class="line"><span class="string">  31  31  300K 244898      0      0 10506K  53159 10665K  1260K  11441   71236    0    0    0    0 </span></span><br><span class="line"><span class="string">  31  31  300K 239304      0      0 10582K  53222 10561K  1247K  11723   76586    0    0    0    0 </span></span><br><span class="line"><span class="string">  31  31  302K 247787      0      0 10572K  53397 10608K  1253K  11492   72876    0    0    0    0 </span></span><br><span class="line"><span class="string">  28  28  276K 226763      0      0  9737K  49180 10591K  1252K  11673   75595    0    0    0    0 </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">ior-3.3.3/bin/ior -w -r -t 256M -b 256G -o ./iorfile --posix.odirect</span></span><br><span class="line"><span class="string">[test-7f53:72832] mca_base_component_repository_open: unable to open mca_btl_usnic: libefa.so.1: cannot open shared object file: No such file or directory (ignored)</span></span><br><span class="line"><span class="string">[test-7f53:72832] mca_base_component_repository_open: unable to open mca_btl_ofi: libefa.so.1: cannot open shared object file: No such file or directory (ignored)</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">No OpenFabrics connection schemes reported that they were able to be</span></span><br><span class="line"><span class="string">used on a specific port.  As such, the openib BTL (OpenFabrics</span></span><br><span class="line"><span class="string">support) will be disabled for this port.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Local host:           test-7f53</span></span><br><span class="line"><span class="string">  Local device:         mlx5_0</span></span><br><span class="line"><span class="string">  Local port:           1</span></span><br><span class="line"><span class="string">  CPCs attempted:       rdmacm, udcm</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">[test-7f53:72832] mca_base_component_repository_open: unable to open mca_mtl_ofi: libefa.so.1: cannot open shared object file: No such file or directory (ignored)</span></span><br><span class="line"><span class="string">IOR-3.3.0: MPI Coordinated Test of Parallel I/O</span></span><br><span class="line"><span class="string">Began               : Tue Apr 25 16:59:37 2023</span></span><br><span class="line"><span class="string">Command line        : /opt/lib/el8/ior-3.3.3/bin/ior -w -r -t 256M -b 256G -o ./iorfile --posix.odirect</span></span><br><span class="line"><span class="string">Machine             : Linux test-7f53</span></span><br><span class="line"><span class="string">TestID              : 0</span></span><br><span class="line"><span class="string">StartTime           : Tue Apr 25 16:59:37 2023</span></span><br><span class="line"><span class="string">Path                : /tfs6/stripe</span></span><br><span class="line"><span class="string">FS                  : 8.4 TiB   Used FS: 0.0%   Inodes: 33.9 Mi   Used Inodes: 0.0%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Options: </span></span><br><span class="line"><span class="string">api                 : POSIX</span></span><br><span class="line"><span class="string">apiVersion          : </span></span><br><span class="line"><span class="string">test filename       : ./iorfile</span></span><br><span class="line"><span class="string">access              : single-shared-file</span></span><br><span class="line"><span class="string">type                : independent</span></span><br><span class="line"><span class="string">segments            : 1</span></span><br><span class="line"><span class="string">ordering in a file  : sequential</span></span><br><span class="line"><span class="string">ordering inter file : no tasks offsets</span></span><br><span class="line"><span class="string">nodes               : 1</span></span><br><span class="line"><span class="string">tasks               : 1</span></span><br><span class="line"><span class="string">clients per node    : 1</span></span><br><span class="line"><span class="string">repetitions         : 1</span></span><br><span class="line"><span class="string">xfersize            : 256 MiB</span></span><br><span class="line"><span class="string">blocksize           : 256 GiB</span></span><br><span class="line"><span class="string">aggregate filesize  : 256 GiB</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Results: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter</span></span><br><span class="line"><span class="string">------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----</span></span><br><span class="line"><span class="string">[test-7f53:72833] 1 more process has sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port</span></span><br><span class="line"><span class="string">[test-7f53:72833] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages</span></span><br><span class="line"><span class="string">write     7151       27.93      0.035798    268435456  262144     0.000385   36.66      0.000289   36.66      0   </span></span><br><span class="line"><span class="string">read      8678       33.90      0.029501    268435456  262144     0.000363   30.21      0.000016   30.21      0   </span></span><br><span class="line"><span class="string">remove    -          -          -           -          -          -          -          -          0.000565   0   </span></span><br><span class="line"><span class="string">Max Write: 7151.07 MiB/sec (7498.44 MB/sec)</span></span><br><span class="line"><span class="string">Max Read:  8677.59 MiB/sec (9099.11 MB/sec)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Summary of all tests:</span></span><br><span class="line"><span class="string">Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum</span></span><br><span class="line"><span class="string">write        7151.07    7151.07    7151.07       0.00      27.93      27.93      27.93       0.00   36.65803         NA            NA     0      1   1    1   0     0        1         0    0      1 274877906944 268435456  262144.0 POSIX      0</span></span><br><span class="line"><span class="string">read         8677.59    8677.59    8677.59       0.00      33.90      33.90      33.90       0.00   30.20931         NA            NA     0      1   1    1   0     0        1         0    0      1 274877906944 268435456  262144.0 POSIX      0</span></span><br><span class="line"><span class="string">Finished            : Tue Apr 25 17:00:44 2023</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">--------------------------------- 4x NVME merge to single md0 dev</span></span><br><span class="line"><span class="string">IOR-3.3.0: MPI Coordinated Test of Parallel I/O</span></span><br><span class="line"><span class="string">Began               : Wed Apr 26 09:02:09 2023</span></span><br><span class="line"><span class="string">Command line        : /opt/lib/el8/ior-3.3.3/bin/ior -w -r -t 256M -b 256G -o ./iorfile --posix.odirect</span></span><br><span class="line"><span class="string">Machine             : Linux test-7f53</span></span><br><span class="line"><span class="string">TestID              : 0</span></span><br><span class="line"><span class="string">StartTime           : Wed Apr 26 09:02:09 2023</span></span><br><span class="line"><span class="string">Path                : /tfs6/test100</span></span><br><span class="line"><span class="string">FS                  : 8.4 TiB   Used FS: 0.0%   Inodes: 17.0 Mi   Used Inodes: 0.0%</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Options: </span></span><br><span class="line"><span class="string">api                 : POSIX</span></span><br><span class="line"><span class="string">apiVersion          : </span></span><br><span class="line"><span class="string">test filename       : ./iorfile</span></span><br><span class="line"><span class="string">access              : single-shared-file</span></span><br><span class="line"><span class="string">type                : independent</span></span><br><span class="line"><span class="string">segments            : 1</span></span><br><span class="line"><span class="string">ordering in a file  : sequential</span></span><br><span class="line"><span class="string">ordering inter file : no tasks offsets</span></span><br><span class="line"><span class="string">nodes               : 1</span></span><br><span class="line"><span class="string">tasks               : 1</span></span><br><span class="line"><span class="string">clients per node    : 1</span></span><br><span class="line"><span class="string">repetitions         : 1</span></span><br><span class="line"><span class="string">xfersize            : 256 MiB</span></span><br><span class="line"><span class="string">blocksize           : 256 GiB</span></span><br><span class="line"><span class="string">aggregate filesize  : 256 GiB</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Results: </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">access    bw(MiB/s)  IOPS       Latency(s)  block(KiB) xfer(KiB)  open(s)    wr/rd(s)   close(s)   total(s)   iter</span></span><br><span class="line"><span class="string">------    ---------  ----       ----------  ---------- ---------  --------   --------   --------   --------   ----</span></span><br><span class="line"><span class="string">[test-7f53:112360] 1 more process has sent help message help-mpi-btl-openib-cpc-base.txt / no cpcs for port</span></span><br><span class="line"><span class="string">[test-7f53:112360] Set MCA parameter &quot;orte_base_help_aggregate&quot; to 0 to see all help / error messages</span></span><br><span class="line"><span class="string">write     4728       18.47      0.054148    268435456  262144     0.000274   55.45      0.000193   55.45      0   </span></span><br><span class="line"><span class="string">read      5373       20.99      0.047647    268435456  262144     0.000289   48.79      0.000014   48.79      0   </span></span><br><span class="line"><span class="string">remove    -          -          -           -          -          -          -          -          0.000220   0   </span></span><br><span class="line"><span class="string">Max Write: 4727.70 MiB/sec (4957.36 MB/sec)</span></span><br><span class="line"><span class="string">Max Read:  5372.77 MiB/sec (5633.76 MB/sec)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Summary of all tests:</span></span><br><span class="line"><span class="string">Operation   Max(MiB)   Min(MiB)  Mean(MiB)     StdDev   Max(OPs)   Min(OPs)  Mean(OPs)     StdDev    Mean(s) Stonewall(s) Stonewall(MiB) Test# #Tasks tPN reps fPP reord reordoff reordrand seed segcnt   blksiz    xsize aggs(MiB)   API RefNum</span></span><br><span class="line"><span class="string">write        4727.70    4727.70    4727.70       0.00      18.47      18.47      18.47       0.00   55.44849         NA            NA     0      1   1    1   0     0        1         0    0      1 274877906944 268435456  262144.0 POSIX      0</span></span><br><span class="line"><span class="string">read         5372.77    5372.77    5372.77       0.00      20.99      20.99      20.99       0.00   48.79124         NA            NA     0      1   1    1   0     0        1         0    0      1 274877906944 268435456  262144.0 POSIX      0</span></span><br><span class="line"><span class="string">Finished            : Wed Apr 26 09:03:54 2023</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">----how about 4x NVME SSD, 4x OST vs 4x NVME in single OST (mdadm raid0 128k) ?</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">randread  direct 4K raw-ost:3319MiB/s               md0: 2985MiB/s</span></span><br><span class="line"><span class="string">randwrite direct 4k raw-ost:2516MiB/s               md0: 2079MiB/s</span></span><br><span class="line"><span class="string">randrw    direct 4k raw-ost:799MiB/s x2=1598 MiB    md0: 527MiB/s x2 =1054 MiB </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">md0 could reach 10GB/s don &#x27;</span>t need any feauture to support</span><br><span class="line">md0 <span class="built_in">seq</span> <span class="built_in">read</span></span><br><span class="line"><span class="comment">#&lt;--------CPU--------&gt;&lt;----------Disks-----------&gt;&lt;----------Network----------&gt;&lt;-------TCP--------&gt;</span></span><br><span class="line"><span class="comment">#cpu sys inter  ctxsw KBRead  Reads KBWrit Writes   KBIn  PktIn  KBOut  PktOut   IP  Tcp  Udp Icmp </span></span><br><span class="line">  11  11  238K 108146 10102K  80819      0      0   7306  72681 10175K   1193K    0    0    0    0 </span><br><span class="line">  11  11  235K 107396 10083K  80665     12      3   7338  72951 10235K   1200K    0    0    0    0 </span><br><span class="line">  11  11  236K 107316 10100K  80801     52      4   7095  69703 10104K   1185K    0    0    0    0 </span><br><span class="line">  11  11  238K 108417 10107K  80861      0      0   7341  73187 10181K   1194K    0    0    0    0 </span><br><span class="line">  11  11  234K 107314 10110K  80885      0      0   7251  71843 10173K   1193K    0    0    0    0 </span><br><span class="line">  11  11  239K 108334 10133K  81065      0      0   7281  72193 10210K   1197K    0    0    0    0 </span><br><span class="line">  11  11  235K 106422 10107K  80862     12      3   7338  73050 10205K   1197K    0    0    0    0 </span><br><span class="line">  11  11  237K 108006 10110K  80881      4      1   7137  70025 10183K   1194K    0    0    0    0 </span><br><span class="line">  11  11  235K 107193 10104K  80834     12      3   7291  72458 10171K   1193K    0    0    0    0 </span><br><span class="line">  11  11  234K 107293 10085K  80681      0      0   7133  69922 10199K   1196K    0    0    0    0 </span><br><span class="line">  11  11  237K 107540 10113K  80907      0      0   7038  68597 10160K   1191K    0    0    0    0 </span><br><span class="line">  11  11  238K 107511 10102K  80819      8      2   7208  71146 10179K   1193K    0    0    0    0 </span><br><span class="line">  11  11  236K 108285 10082K  80663      8      2   7288  72384 10178K   1193K    0    0    0    0 </span><br><span class="line">  11  11  236K 107801 10076K  80612      0      0   7150  70336 10156K   1191K    0    0    0    0 </span><br><span class="line">  11  11  236K 106535 10073K  80588     12      3   7248  71879 10153K   1191K    0    0    0    0 </span><br><span class="line"></span><br><span class="line">md0 <span class="built_in">seq</span> write</span><br><span class="line"><span class="comment">#&lt;--------CPU--------&gt;&lt;----------Disks-----------&gt;&lt;----------Network----------&gt;&lt;-------TCP--------&gt;</span></span><br><span class="line"><span class="comment">#cpu sys inter  ctxsw KBRead  Reads KBWrit Writes   KBIn  PktIn  KBOut  PktOut   IP  Tcp  Udp Icmp </span></span><br><span class="line">  20  20  248K 120766      0      0  7543K  61515  7418K 869220   6988   79747    0    0    0    0 </span><br><span class="line">  19  19  240K 115283      0      0  7315K  59633  7484K 876905   7186   82570    0    0    0    0 </span><br><span class="line">  20  20  241K 115843      0      0  7453K  60858  7467K 874827   7282   84115    0    0    0    0 </span><br><span class="line">  19  19  240K 116794      0      0  7419K  60498  7464K 874324   6854   77448    0    0    0    0 </span><br><span class="line">  20  20  243K 118411      0      0  7423K  60534  7372K 863777   7074   81182    0    0    0    0 </span><br><span class="line">  20  20  245K 119545      0      0  7440K  60692 14874K  1742K  14119  161698    0    0    0    0 </span><br><span class="line">  19  19  237K 118460      0      0  7286K  59438  7406K 867531   7157   82349    0    0    0    0 </span><br><span class="line">  21  20  247K 119045      0      0  7524K  61368  7482K 876606   7302   84436    0    0    0    0 </span><br><span class="line">  20  20  238K 116510      0      0  7301K  59580  7425K 869633   7106   81484    0    0    0    0 </span><br><span class="line">  20  20  246K 120198      0      0  7485K  61070  7528K 881895   7095   80940    0    0    0    0 </span><br><span class="line">  19  19  237K 112979      0      0  7280K  59337  7589K 889111   7379   85268    0    0    0    0 </span><br><span class="line">  20  20  237K 112832      0      0  7409K  60442  7337K 859569   6688   75368    0    0    0    0 </span><br><span class="line">  20  20  241K 117617      0      0  7333K  59812  7348K 860973   6887   78464    0    0    0    0 </span><br><span class="line">  19  19  237K 113702      0      0  7371K  60104  7411K 868099   6927   78820    0    0    0    0 </span><br><span class="line">  20  20  243K 119458      0      0  7454K  60818  7323K 858030   6948   79541    0    0    0    0 </span><br><span class="line"></span><br><span class="line">md0 <span class="built_in">seq</span> rw</span><br><span class="line"><span class="comment">#&lt;--------CPU--------&gt;&lt;----------Disks-----------&gt;&lt;----------Network----------&gt;&lt;-------TCP--------&gt;</span></span><br><span class="line"><span class="comment">#cpu sys inter  ctxsw KBRead  Reads KBWrit Writes   KBIn  PktIn  KBOut  PktOut   IP  Tcp  Udp Icmp </span></span><br><span class="line">  13  13  190K 113152  3185K  25487  3283K  26994  6431K 806255  6420K  838522    0    0    0    0 </span><br><span class="line">  12  12  192K 113576  3228K  25830  3185K  26135  3354K 418906  3237K  424098    0    0    0    0 </span><br><span class="line">  12  12  191K 113695  3288K  26311  3179K  26123  3161K 398860  3306K  430287    0    0    0    0 </span><br><span class="line">  12  12  187K 111345  3166K  25331  3162K  25949  3240K 408109  3301K  430348    0    0    0    0 </span><br><span class="line">  12  12  185K 110210  3188K  25510  3035K  24939  3147K 393846  3174K  413846    0    0    0    0 </span><br><span class="line">  13  13  196K 114608  3201K  25609  3256K  26762  3076K 388658  3232K  420012    0    0    0    0 </span><br><span class="line">  13  13  193K 115317  3194K  25559  3295K  27072  3316K 415356  3209K  420804    0    0    0    0 </span><br><span class="line">  12  12  193K 114798  3186K  25493  3267K  26830  3263K 408976  3239K  424724    0    0    0    0 </span><br><span class="line">  12  12  194K 115218  3251K  26015  3256K  26761  3302K 414683  3287K  429594    0    0    0    0 </span><br><span class="line">  12  12  192K 114854  3218K  25746  3278K  26920  3266K 410316  3296K  430256    0    0    0    0 </span><br><span class="line">  13  13  194K 116664  3291K  26328  3279K  26964  3297K 412404  3214K  421243    0    0    0    0 </span><br><span class="line">  13  13  193K 114736  3285K  26280  3251K  26707  3320K 416683  3346K  435631    0    0    0    0 </span><br><span class="line">  12  12  192K 113834  3191K  25528  3290K  27054  3315K 417075  3258K  424564    0    0    0    0 </span><br><span class="line">  13  13  197K 118394  3292K  26343  3343K  27524  3325K 416851  3252K  425064    0    0    0    0 </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#CRAY IB setting</span></span><br><span class="line">ko2iblnd:<span class="built_in">timeout</span></span><br><span class="line">Default 10. The o2iblnd <span class="built_in">timeout</span> <span class="keyword">in</span> seconds. Cray/HPE recommends setting this to 10 seconds.</span><br><span class="line">ko2iblnd:peer_timeout</span><br><span class="line">Default 0. Number of seconds without aliveness news it takes to <span class="built_in">declare</span> a peer dead. Cray/HPE recommends setting this to 0.</span><br><span class="line">ko2iblnd:keepalive</span><br><span class="line">Default 30. Idle time <span class="keyword">in</span> seconds before sending a keepalive. Cray/HPE recommends setting this to 30.</span><br><span class="line">ko2iblnd:credits</span><br><span class="line">Default 2048. Number of concurrent sends allowed by o2iblnd. Shared by all CPU partitions (CPT). Cray/HPE recommends setting this to 2048.</span><br><span class="line">ko2iblnd:ntx</span><br><span class="line">Default 2048. Number of message descriptors allocated <span class="keyword">for</span> each pool. Cray/HPE recommends setting this to 2048.</span><br><span class="line">ko2iblnd:peer_credits</span><br><span class="line">Enter the value <span class="keyword">for</span> the ko2iblnd parameter peer_credits. This is the number of concurrent sends to a single peer. This value must be the same on all external login clients and the Lustre file system servers.</span><br><span class="line">ko2iblnd:peer_credits 16</span><br><span class="line">Enter the value <span class="keyword">for</span> the ko2iblnd parameter peer_credits. This is the number of concurrent sends to a single peer. This value must be the same on all external login clients and the Lustre file system servers.</span><br><span class="line">ko2iblnd:concurrent_sends 16</span><br><span class="line">Determines send work queue sizing. If this option is omitted, the default is calculated based on the values of peer_credits and map_on_demand. This value must be the same on the external login clients and the Lustre file system servers.</span><br><span class="line">ko2iblnd:map_on_demand</span><br><span class="line">Default 0. Controls the use of fast memory registration (FMR). Cray/HPE recommends setting this value to 0 <span class="keyword">for</span> InfiniBand HCAs.</span><br><span class="line">lnet:alive_router_check_interval</span><br><span class="line">Default 60. Number of seconds between alive (live or dead) router health checks. Recommendation: <span class="built_in">set</span> this value to 35 seconds. A value less than or equal to 0 disables pinging of alive (live or dead) routes.</span><br><span class="line">lnet:router_ping_timeout</span><br><span class="line">Default 10. Number of seconds to <span class="built_in">wait</span> <span class="keyword">for</span> the reply to a router health query. Cray/HPE recommends using the default value of 10 seconds.</span><br><span class="line">lnet:avoid_asym_router_failure</span><br><span class="line">Avoid asymmetrical router failures (0 to <span class="built_in">disable</span>; 1 to <span class="built_in">enable</span>).</span><br><span class="line">module_params:ptlrpc_at_max</span><br><span class="line">Default 400. Adaptive <span class="built_in">timeout</span> maximum <span class="keyword">in</span> seconds. Cray/HPE recommends setting this to 400 on a CLE client.</span><br><span class="line">module_params:ptlrpc_at_min</span><br><span class="line">Default 40. Adaptive <span class="built_in">timeout</span> minimum <span class="keyword">in</span> seconds. Cray/HPE recommends setting this to 40 on a CLE client.</span><br><span class="line">module_params:ptlrpc_ldlm_enqueue_min</span><br><span class="line">Default 260. Lock enqueue <span class="built_in">timeout</span> minimum <span class="keyword">in</span> seconds. This is the minimum amount of time a server will <span class="built_in">wait</span> to see traffic on a lock before it assumes a client is misbehaving and takes action to revoke the lock by evicting the client. Cray/HPE recommends setting this to 260 <span class="keyword">for</span> CLE clients.</span><br><span class="line"></span><br><span class="line">https://www.hpe.com/psnow/resources/ebooks/a00113867en_us_v2/Lustre_Server_Recommended_Tuning_Parameters_4.x.html</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p>PERFORMANCE    </p>
<h3 id="ALL"><a href="#ALL" class="headerlink" title="ALL"></a>ALL</h3><h3 id="collect-log"><a href="#collect-log" class="headerlink" title="collect log"></a>collect log</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br></pre></td><td class="code"><pre><span class="line">$ lctl  get_param  at_max</span><br><span class="line">at_max=600</span><br><span class="line">Maximum adaptive <span class="built_in">timeout</span> (<span class="keyword">in</span> seconds). The at_max parameter is an upper-limit on the service time estimate. If at_max is reached, an RPC request <span class="built_in">times</span> out.Setting at_max to 0 causes adaptive timeouts to be disabled and a fixed <span class="built_in">timeout</span> method to be used instead (see the section called “Setting Static Timeouts” Note If slow hardware causes the service estimate to increase beyond the default value of at_max, increase at_max to the maximum time you are willing to <span class="built_in">wait</span> <span class="keyword">for</span> an RPC completion.</span><br><span class="line">$ lctl  get_param  at_min</span><br><span class="line">at_min=50</span><br><span class="line">Minimum adaptive <span class="built_in">timeout</span> (<span class="keyword">in</span> seconds). The default value is 0. The at_min parameter is the minimum processing time that a server will report. Ideally, at_min should be <span class="built_in">set</span> to its default value. Clients base their timeouts on this value, but they <span class="keyword">do</span> not use this value directly.If, <span class="keyword">for</span> unknown reasons (usually due to temporary network outages), the adaptive <span class="built_in">timeout</span> value is too short and clients time out their RPCs, you can increase the at_min value to compensate <span class="keyword">for</span> this.</span><br><span class="line">$ lctl get_param at_extra</span><br><span class="line">at_extra=30</span><br><span class="line"><span class="comment">#Incremental amount of time that a server requests with each early reply (in seconds). The server does not know how much time the RPC will take, so it asks for a fixed value. The default is 30, which provides a balance between sending too many early replies for the same RPC and overestimating the actual completion time.When a server finds a queued request about to time out and needs to send an early reply out, the server adds the at_extra value. If the time expires, the lfs server drops the request, and the client enters recovery status and reconnects to restore the connection to normal status.If you see multiple early replies for the same RPC asking for 30-second increases, change the at_extra value to a larger number to cut down on early replies sent and, therefore, network load.</span></span><br><span class="line"></span><br><span class="line">$ lctl  get_param  <span class="built_in">timeout</span></span><br><span class="line"><span class="built_in">timeout</span>=300</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n debug</span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck mmap page dentry cache malloc quota dlmtrace reada vfstrace rpctrace&quot;</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck cache reada quota&quot;</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck rpctrace vfstrace dentry&quot;</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#-1</span></span><br><span class="line">$ lctl get_param -n debug</span><br><span class="line">trace inode super iotrace malloc cache info ioctl neterror net warning buffs other dentry nettrace page dlmtrace error emerg ha rpctrace vfstrace reada mmap config console quota sec lfsck hsm snapshot layout</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MDS <span class="built_in">log</span> : </span><br><span class="line"> Lustre: 11022:0:(service.c:1379:ptlrpc_at_send_early_reply()) @@@ Already past deadline (-3s), not sending early reply. Consider increasing at_early_margin (5)?  req@0000000041961346 x1799999232459392/t0(0) o103-&gt;14966544-5795-4b86-80ab-0b28d1f274d7@10.32.140.6@tcp:639/0 lens 3584/0 e 0 to 0 dl 1716800424 ref 2 fl New:/0/ffffffff rc 0/-1 job:<span class="string">&#x27;&#x27;</span></span><br><span class="line">11084:0:(service.c:2162:ptlrpc_server_handle_req_in()) @@@ Slow req_in handling 7s  req@00000000f36c36aa x1799967339969088/t0(0) o103-&gt;3a043633-cbcc-4ebc-8a16-5f3a0c60f3d0@10.32.140.8@tcp:0/0 lens 792/0 e 0 to 0 dl 0 ref 1 fl New:/0/ffffffff rc 0/-1 job:<span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">echo</span> 15 &gt; /sys/module/ptlrpc/parameters/at_early_margin</span><br><span class="line">at_early_margin - Time <span class="keyword">in</span> seconds of an advance queued request <span class="built_in">timeout</span> at <span class="built_in">which</span> the server sends a request to the client to extend the <span class="built_in">timeout</span> time. The default value is 5.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mds $ lctl set_param debug=+rpctrace</span><br><span class="line">mds $ lctl dk &gt; dk</span><br><span class="line">00000100:00100000:3.0:1621645680.953924:0:5370:0:(service.c:2089:ptlrpc_server_handle_request()) Handling RPC pname:cluuid+ref:pid:xid:nid:opc ll_mgs_0002:781f6010-0ac2-a476-4567-56bb7b70d013+9:86470:x1700165046065472:12345-<span class="variable">$client_IP</span>@tcp:400</span><br><span class="line"></span><br><span class="line">client $ lctl set_param debug=+rpctrace</span><br><span class="line">client $ lctl dk &gt; dk</span><br><span class="line">00000100:00100000:15.0:1621645914.531574:0:77490:0:(client.c:1682:ptlrpc_send_new_req()) Sending RPC pname:cluuid:pid:xid:nid:opc vim:8c1a0181-62fb-78ee-8651-31b8edfd4244:77490:1700165046080640:<span class="variable">$MDS_IP</span>@tcp:101</span><br><span class="line"></span><br><span class="line"><span class="comment">#default</span></span><br><span class="line">$ lctl set_param debug=<span class="string">&quot;ioctl neterror warning error emerg ha config console lfsck&quot;</span></span><br><span class="line">trace   Function entry/exit markers</span><br><span class="line">dlmtrace        Distributed locking-related information</span><br><span class="line">inode   </span><br><span class="line">super   </span><br><span class="line">malloc  Memory allocation or free information</span><br><span class="line">cache   Cache-related information</span><br><span class="line">info    Non-critical general information</span><br><span class="line">dentry  kernel namespace cache handling</span><br><span class="line">mmap    Memory-mapped IO interface</span><br><span class="line">page    Page cache and bulk data transfers</span><br><span class="line">info    Miscellaneous informational messages</span><br><span class="line">net     LNet network related debugging</span><br><span class="line">console Significant system events, printed to console</span><br><span class="line">warning Significant but non-fatal exceptions, printed to console</span><br><span class="line">error   Critical error messages, printed to console</span><br><span class="line">neterror        Significant LNet error messages</span><br><span class="line">emerg   Fatal system errors, printed to console</span><br><span class="line">config  Configuration and setup, enabled by default</span><br><span class="line">ha      Failover and recovery-related information, enabled by default</span><br><span class="line">hsm     Hierarchical space management/tiering</span><br><span class="line">ioctl   IOCTL-related information, enabled by default</span><br><span class="line">layout  File layout handling (PFL, FLR, DoM)</span><br><span class="line">lfsck   Filesystem consistency checking, enabled by default</span><br><span class="line">other   Miscellaneious other debug messages</span><br><span class="line">quota   Space accounting and management</span><br><span class="line">reada   Client readahead management</span><br><span class="line">rpctrace        Remote request/reply tracing and debugging</span><br><span class="line">sec     Security, Kerberos, Shared Secret Key handling</span><br><span class="line">snapshot        Filesystem snapshot management</span><br><span class="line">vfstrace        Kernel VFS interface operations</span><br><span class="line"></span><br><span class="line"><span class="comment">#collectl the log </span></span><br><span class="line">mds$ modprobe libcfs</span><br><span class="line">mds$ lctl set_param debug=-1 debug_mb=1024</span><br><span class="line">mds$ lctl set_param panic_on_lbug=0</span><br><span class="line">mds$ mount -t lustre /dev/md0 /mnt/mdt &amp;</span><br><span class="line">[<span class="built_in">wait</span> <span class="keyword">until</span> MDS is mounted]</span><br><span class="line">client$ [mount client, <span class="keyword">then</span> <span class="built_in">unlink</span> a file, presumably hitting an error]</span><br><span class="line">mds$ lctl dk | xz &gt; /tmp/debug.log.xz</span><br><span class="line"></span><br><span class="line">$ lctl set_param debug_mb=500</span><br><span class="line">$ <span class="built_in">mkfifo</span> -m 777 /tmp/lfs.log; lctl debug_daemon start /tmp/lfs.log; <span class="built_in">tail</span> -f /tmp/lfs.log | strings</span><br><span class="line">or</span><br><span class="line"><span class="comment">## 500M</span></span><br><span class="line">$ lctl debug_daemon start /tmp/lfs.log 500</span><br><span class="line"></span><br><span class="line">$ trace-cmd record -p <span class="keyword">function</span> mount <span class="variable">$ipaddr</span>@tcp:/lfs /mnt</span><br><span class="line"><span class="comment"># it &#x27;ll create trace.dat</span></span><br><span class="line">$ trace-cmd report</span><br><span class="line"></span><br><span class="line">$ lctl get_param debug_mb</span><br><span class="line">debug_mb=41</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.dump_namespaces</span><br><span class="line"><span class="comment">#no output</span></span><br><span class="line">$ lctl set_param ldlm.dump_namespaces <span class="string">&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.*.dump_page_cache | grep -cEi <span class="string">&#x27;lockd|dirty|writeback&#x27;</span></span><br><span class="line"></span><br><span class="line">$ strings dk</span><br><span class="line">$ lctl <span class="built_in">df</span> &lt;input file&gt; &lt;output file&gt;</span><br><span class="line"></span><br><span class="line">lfs rpm package install will not overvide the /lib/modules/$(<span class="built_in">uname</span> -r)/extra</span><br><span class="line">$ <span class="built_in">rm</span> -rf /lib/modules/$(<span class="built_in">uname</span> -r)/extra/; <span class="built_in">mkdir</span> /lib/modules/$(<span class="built_in">uname</span> -r)/extra</span><br><span class="line"></span><br><span class="line"><span class="comment">#PATH DEBUG_PATH</span></span><br><span class="line">$ lctl get_param debug_path</span><br><span class="line">$ lctl debug_list types</span><br><span class="line">debug_path=/tmp/lfs-log</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable(set to 1) bulk pages dump upon error on Client</span></span><br><span class="line">Client $ lctl get_param osc.*osc-[^mM]*.checksum_dump</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.checksum_dump=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable(set to 1) bulk pages dump upon error on OSS</span></span><br><span class="line">Oss$ lctl get_param obdfilter.*-OST*.checksum_dump</span><br><span class="line">obdfilter.fsname-OST0000.checksum_dump=0</span><br><span class="line"></span><br><span class="line"><span class="comment">## mds and oss</span></span><br><span class="line">can1=$(do_facet mds1 <span class="string">&quot;<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_canceld.stats&quot;</span> |awk <span class="string">&#x27;/ldlm_cancel/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">blk1=$(<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_cbd.stats |awk <span class="string">&#x27;/ldlm_bl_callback/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">test_mkdir -i0 -c1 <span class="variable">$DIR</span>/<span class="variable">$tdir</span>/d1</span><br><span class="line"></span><br><span class="line">can2=$(do_facet mds1 <span class="string">&quot;<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_canceld.stats&quot;</span> |awk <span class="string">&#x27;/ldlm_cancel/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">blk2=$(<span class="variable">$LCTL</span> get_param -n ldlm.services.ldlm_cbd.stats | awk <span class="string">&#x27;/ldlm_bl_callback/ &#123;print $2&#125;&#x27;</span>)</span><br><span class="line">[ <span class="variable">$can1</span> -eq <span class="variable">$can2</span> ] || error $((can2-can1)) <span class="string">&quot;cancel RPC occured.&quot;</span></span><br><span class="line">[ <span class="variable">$blk1</span> -eq <span class="variable">$blk2</span> ] || error $((blk2-blk1)) <span class="string">&quot;blocking RPC occured.&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="list-all-parameters"><a href="#list-all-parameters" class="headerlink" title="list all parameters"></a>list all parameters</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ lctl list_param -R <span class="string">&#x27;*&#x27;</span></span><br><span class="line">$ lctl list_param osc.*.*</span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> $(<span class="built_in">ls</span> /proc/fs/lfs); <span class="keyword">do</span> lctl get_param <span class="variable">$&#123;i&#125;</span>.*.*; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>

<h3 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h3><ul>
<li><p>disable OST in the mds</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Disabling write only. Read on clients possible.:</span></span><br><span class="line">mds$ lctl set_param osc.lustre-OST000d-osc.active=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#Disable read/write (disabling device permanently):</span></span><br><span class="line">mds$ lctl conf_param lustre-OST0003-osc-MDT0000.osc.active=0</span><br><span class="line">mds$ lctl conf_param lustre-OST0003-osc-MDT0001.osc.active=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>fstrim</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># fstrim - discard unused blocks on a mounted filesystem</span><br><span class="line">MDS $ fstrim -a</span><br><span class="line">OSS $ fstrim -a</span><br></pre></td></tr></table></figure>
</li>
<li><p><a target="_blank" rel="noopener" href="https://wiki.whamcloud.com/display/LNet/Useful+Lustre+commands">evicting client</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">MDS    $ lctl get_param -n mgc.*.uuid</span><br><span class="line">Client $ lctl get_param llite.*.uuid</span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param mdt.*.evict_client=&lt;client_uuid&gt;</span><br><span class="line">Lustre: 4405:0:(genops.c:1710:obd_export_evict_by_uuid()) tfs1-MDT0000: evicting 7e465de8-e9f9-4ced-a544-8ea0de1a5313 at adminstrative request</span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param mdt.&lt;target&gt;.evict_client=uuid:&lt;client_uuid&gt;<span class="string">&quot; or &quot;</span>...=nid:&lt;client_nid&gt;</span><br><span class="line">or</span><br><span class="line">MDS $ lctl set_param mdt.*evict_client=nid:&lt;nid&gt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>parallel rename</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#not enable under release version</span></span><br><span class="line">$ lctl get_param mdt.*.enable_parallel_rename_file</span><br><span class="line">$ lctl get_param mdt.*.enable_parallel_rename_dir</span><br></pre></td></tr></table></figure>
</li>
<li><p>ban the client</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># MDS</span><br><span class="line">/proc/fs/lfs/mdt/lfs1-MDT0000/evict_client #wait the tcp to release (SYN_SENT status)</span><br><span class="line">/proc/fs/lfs/mgs/MGS/evict_client</span><br><span class="line"></span><br><span class="line">$ echo 192.168.1.1@tcp1 &gt; /proc/fs/lfs/mgs/MGS/evict_client</span><br><span class="line">$ echo &#x27;192.168.1.1@tcp1&#x27; &gt; /proc/fs/lfs/obdfilter/fsname-OST0000/evict_client</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>imperative recovery</p>
</li>
<li><p>Imperative recovery</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">#deactivate imperative recovery</span><br><span class="line">$ lctl set_param mgs.MGS.live.testfs=&quot;state=disabled&quot;</span><br><span class="line"></span><br><span class="line">#activate imperative recovery</span><br><span class="line">$ lctl set_param mgs.MGS.live.testfs=&quot;state=full&quot;</span><br><span class="line"></span><br><span class="line">$ lctl get_param mgs.MGS.live.testfs6</span><br><span class="line">mgs.MGS.live.testfs6=</span><br><span class="line">fsname: testfs6</span><br><span class="line">flags: 0x20     gen: 52</span><br><span class="line">testfs6-MDT0000</span><br><span class="line">testfs6-MDT0001</span><br><span class="line">testfs6-MDT0002</span><br><span class="line">testfs6-MDT0003</span><br><span class="line">testfs6-OST0000</span><br><span class="line">testfs6-OST0001</span><br><span class="line">testfs6-OST0002</span><br><span class="line">testfs6-OST0003</span><br><span class="line"></span><br><span class="line">Secure RPC Config Rules:</span><br><span class="line"></span><br><span class="line">imperative_recovery_state:</span><br><span class="line">    state: full</span><br><span class="line">    nonir_clients: 0</span><br><span class="line">    nidtbl_version: 22</span><br><span class="line">    notify_duration_total: 0.007111353</span><br><span class="line">    notify_duation_max: 0.001130805</span><br><span class="line">    notify_count: 20</span><br><span class="line"></span><br><span class="line">#MDS OSS #show recovery status</span><br><span class="line">$ lctl get_param *.*.recovery_status</span><br><span class="line">obdfilter.myth-OST0004.recovery_status=</span><br><span class="line">status: COMPLETE</span><br><span class="line">recovery_start: 1594074852</span><br><span class="line">recovery_duration: 278</span><br><span class="line">completed_clients: 1/3</span><br><span class="line">replayed_requests: 0</span><br><span class="line">last_transno: 120259116149</span><br><span class="line"></span><br><span class="line">#MDS OSS skip tgt recovery</span><br><span class="line">$ lctl --device lfs-OST0004 abort_recovery</span><br><span class="line"></span><br><span class="line"># Recover a device</span><br><span class="line">$ lctl --device &lt;device number or name&gt; recover</span><br></pre></td></tr></table></figure>
</li>
<li><p>limits on ldlm memmory use on servers</p>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ldlm.lock_limit_mb (<span class="keyword">in</span> megabytes) - hard <span class="built_in">limit</span>   </span><br><span class="line"></span><br><span class="line">ldlm.lock_reclaim_threshold_mb - start to ask clients to release locks   </span><br><span class="line"></span><br><span class="line">Used to be 100*NUM_CPUS per client namespace by default   </span><br><span class="line">ldlm.<span class="variable">$NAMESPACE</span>.lru_size control   </span><br><span class="line">Setting that to 0 (new default) enables “lru resize”    </span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.lock_limit_mb</span><br><span class="line">ldlm.lock_limit_mb=77313</span><br></pre></td></tr></table></figure>

<ul>
<li><p>No root squash</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">mds $ lctl set_param $fsname.mdt.root_squash=108:108</span><br><span class="line">or</span><br><span class="line">mds $ lctl set_param mdt.$&#123;fsname&#125;-MDT0000.root_squash=108:108</span><br><span class="line">mds $ lctl set_param mdt.$&#123;fsname&#125;-MDT0000.nosquash_nids=&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line"></span><br><span class="line">error: set_param: param_path &#x27;$fsname/mdt/root_squash&#x27;: No such file or directory</span><br><span class="line">mds $ lctl conf_param $fsname.mdt.root_squash=108:108</span><br><span class="line">mds $ lctl conf_param $fsname.mdt.nosquash_nids=&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br><span class="line">mds $ cat /proc/fs/lf/mdt/$FSNAME-MDT0000/nosquash_nids</span><br><span class="line">ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp</span><br><span class="line"></span><br><span class="line">client $ lctl set_param llite.$FSNAME-*.nosquash_nids=&quot;ip.ip.ip.ip@tcp ip1.ip1.ip1.ip@tcp&quot;</span><br></pre></td></tr></table></figure>
</li>
<li><p>show open files</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># del last_recvd in mdt and pass the recovery, mdt and clients mount by no_recov</span><br><span class="line">$ cat /proc/fs/lfs/mdt/*/exports/*/open_files</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>show all clients</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lshowmount -e</span><br></pre></td></tr></table></figure>
</li>
<li><p>increase performance PERFORMANCE</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param -P timeout=300</span><br><span class="line">$ lctl set_param timeout=300</span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param -P osc.*.checksums=0</span><br><span class="line">OSS $ lctl get_param -n ost.*.ost_io.timeouts</span><br><span class="line"></span><br><span class="line">#disable xattr cache, robinhood client</span><br><span class="line">client $ lctl set_param llite.*.xattr_cache=0</span><br><span class="line"></span><br><span class="line">client $ lctl get_param  llite.*.fast_read</span><br><span class="line">llite.fsname-ffff8dab2cce8800.fast_read=1</span><br><span class="line"></span><br><span class="line">https://patchwork.kernel.org/project/lustre-devel/patch/1582838290-17243-53-git-send-email-jsimmons@infradead.org/</span><br><span class="line"># if the import is idle (no locks, no active RPCs, no non-PING reply for last osc_idle_timeout seconds), then pinger tries to disconnect asynchronously</span><br><span class="line"># lctl set_param osc.*.idle_timeout=N controls new feature:</span><br><span class="line"># N=0 - disable</span><br><span class="line"># N&gt;0 - seconds to idle before disconnect</span><br><span class="line"></span><br><span class="line">132da391-76c5-313e-bb11-6e2714ec47b6-&gt;lfs10-OST000e_UUID: not pinging (in recovery or recovery disabled: IDLE)</span><br><span class="line">import_set_state_nolock@ffff930d3683e800 lfs10-OST0000_UUID: changing import state from FULL to CONNECTING</span><br><span class="line">ptlrpc_disconnect_and_idle_import^@lfs10-OST0000-osc-ffff930d36847000: disconnect after 41s idle</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*.idle_timeout</span><br><span class="line">osc.lustre-OST0000-osc-ffff9f33f4b44800.idle_timeout=20</span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param -P osc.*.idle_timeout=0</span><br><span class="line">MDS $ lctl set_param -P osc.*.idle_timeout=7200</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#set the client for increase the metadata ops</span><br><span class="line">Otherwise, the file attributes will be dropped from the client cache if the file has not been accessed before the LDLM lock timeout. The timeout is stored via lctl get_param ldlm.namespaces.*mdc*.lru_max_age</span><br><span class="line"></span><br><span class="line">#restricting the number of locks kept on the client (10000 locks, 10 minutes age)</span><br><span class="line">client $ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=600000</span><br><span class="line">client $ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=3900000</span><br><span class="line">client $ lctl get_param ldlm.namespaces.*.lru_size ldlm.namespaces.*.lru_max_age</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">One of the troubles of version 2.13 ZFS has been Segfaulting binaries on Lustre. There seems to be no simple way out of the trouble except to wait for 2.14. By default max is set to 65 minutes in milliseconds, and we have found out that if the value is increased substantially, then the Segfaults are less frequent.</span><br><span class="line">client $ lctl set_param ldlm.namespaces.*.lru_max_age=3888000000</span><br><span class="line">client $ lctl set_param ldlm.namespaces.*mdt*.lru_size=clear</span><br><span class="line"></span><br><span class="line">#drop all the locks on the client</span><br><span class="line">client $ lctl set_param ldlm.namespaces.*.lru_size=clear</span><br><span class="line"></span><br><span class="line">#LRU</span><br><span class="line">MDS$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=400</span><br><span class="line">ldlm.namespaces.MGS.lru_size=400</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lru_size=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lru_size=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lru_size=400</span><br><span class="line"></span><br><span class="line">OSS$ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=400</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.lru_size=400</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.lru_size=0</span><br><span class="line"></span><br><span class="line">client $ lctl get_param ldlm.namespaces.*.lru_size</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lru_size=2400</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.lru_size=0</span><br><span class="line"></span><br><span class="line"># The lru_size parameter is used to control the number of client-side locks in the LRU cached locks queue</span><br><span class="line">client $ lctl get_param ldlm.namespaces.*mdc-*.lru_size #default dynamic</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=0</span><br><span class="line"></span><br><span class="line">#disable</span><br><span class="line">client$ lctl set_param ldlm.namespaces.*osc*.lru_size=5000</span><br><span class="line">#The total number of locks available is a function of the server RAM. The default limit is 50 locks/1 MB of RAM. If memory pressure is too high, the LRU size is shrunk. The number of locks on the server is limited tonum_osts_per_oss * num_clients * lru_size as followsa</span><br><span class="line"></span><br><span class="line">To determine the number of locks being granted with dynamic LRU resizing, run:</span><br><span class="line">client$ lctl get_param ldlm.namespaces.*.pool.limit</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.limit=195699</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.pool.limit=183850</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.*.pool.limit</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.pool.limit=183850</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.pool.limit=1</span><br><span class="line"></span><br><span class="line">MDS $ lctl get_param ldlm.namespaces.*.pool.limit</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.pool.limit=0</span><br><span class="line">ldlm.namespaces.MGS.pool.limit=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.pool.limit=1</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.pool.limit=1</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.pool.limit=195699</span><br><span class="line"></span><br><span class="line">The lru_max_age parameter is used to control the age of client-side locks in the LRU cached locks queue. This limits how long unused locks are cached on the client, and avoids idle clients from holding locks for an excessive time, which reduces memory usage on both the client and server, as well as reducing work during server recovery.</span><br><span class="line">Client$ lctl set_param ldlm.namespaces.*MDT*.lru_max_age=900s</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_max_age=900s</span><br><span class="line">Client$ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_max_age=900000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#modify client not change mds and oss</span><br><span class="line">MDS $ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lru_max_age=3900000</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.*MDT*.lru_max_age</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.lru_max_age=3900000</span><br><span class="line"></span><br><span class="line">MDS $ lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.MGS.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.pool.lock_volume_factor=1</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.pool.lock_volume_factor=1</span><br><span class="line"></span><br><span class="line">Client $  lctl get_param ldlm.namespaces.*.pool.lock_volume_factor</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.pool.lock_volume_factor=1</span><br><span class="line"></span><br><span class="line">Only client$ lctl get_param ldlm.namespaces.*-MDT0000-mdc-*.lock_unused_count;</span><br><span class="line">Only client$ lctl get_param ldlm.namespaces.*-MDT0000-mdc-*.pool.recalc_period</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lock_unused_count=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.pool.recalc_period=10</span><br><span class="line"></span><br><span class="line">#readahead</span><br><span class="line"># lfs rpc</span><br><span class="line">## 2.12.6 default</span><br><span class="line">$ lctl get_param -n osc.*OST0000-osc-[^mM]*.max_rpcs_in_flight</span><br><span class="line">64</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdc.*.max_rpcs_in_flight</span><br><span class="line">mdc.test0-MDT0000-mdc-ffff95067b45a000.max_rpcs_in_flight=8</span><br><span class="line">$ lctl set_param mdc.*.max_rpcs_in_flight=16</span><br><span class="line">mdc.test0-MDT0000-mdc-ffff95067b45a000.max_rpcs_in_flight=16</span><br><span class="line">$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line">#In lfs 2.13 client;</span><br><span class="line">#direct IO could reach high 120k, but the buffered IO only 4k. there are read</span><br><span class="line">#When I disable</span><br><span class="line">$ lctl set_param llite.*.read_ahead_async_file_threshold_mb=0</span><br><span class="line">echo 0 &gt; /sys/fs/lfs/llite/lfs-ffff9455c0a4b800/read_ahead_async_file_threshold_mb</span><br><span class="line">#I could got the high IOPS too.</span><br><span class="line"></span><br><span class="line">## 2.12.X readahead setting</span><br><span class="line">$ ls -1 /sys/kernel/debug/lfs/llite/lfs-ffff92fab9235800</span><br><span class="line">dump_page_cache</span><br><span class="line">extents_stats</span><br><span class="line">extents_stats_per_process</span><br><span class="line">max_cached_mb</span><br><span class="line">max_read_ahead_mb</span><br><span class="line">max_read_ahead_per_file_mb</span><br><span class="line">max_read_ahead_whole_mb</span><br><span class="line">nosquash_nids</span><br><span class="line">offset_stats</span><br><span class="line">read_ahead_stats</span><br><span class="line">root_squash</span><br><span class="line">sbi_flags</span><br><span class="line">site</span><br><span class="line">statahead_stats</span><br><span class="line">stats</span><br><span class="line">unstable_stats</span><br><span class="line"></span><br><span class="line">#readahead</span><br><span class="line">client $ lctl set_param llite.*.read_ahead_stats=c llite.testfs-ffff8daf56159000.read_ahead_stats=c</span><br><span class="line">client $ lctl get_param llite.*.*read*</span><br><span class="line">client $ lctl get_param llite.$&#123;fsname&#125;*.*read*</span><br><span class="line">llite.testfs-ffff8dcc59a15800.fast_read=1</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_async_active=12</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_async_file_threshold_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_range_kb=1024</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_whole_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_stats=</span><br><span class="line">snapshot_time             1642131829.087651284 secs.nsecs</span><br><span class="line">hits                      9999533 samples [pages]</span><br><span class="line">misses                    838588 samples [pages]  ---&gt; too high, readahead invalid</span><br><span class="line">readpage not consecutive  1293010 samples [pages] ---&gt; too high, readahead invalid</span><br><span class="line">zero size window          2837081 samples [pages] ---&gt; too high, readahead invalid</span><br><span class="line">failed to fast read       592502 samples [pages]</span><br><span class="line"></span><br><span class="line"># client max read ahead size</span><br><span class="line">cat /sys/kernel/debug/lfs/llite/lfs-ffff91b225941800/max_read_ahead_mb</span><br><span class="line">64</span><br><span class="line"></span><br><span class="line">$ cat /proc/fs/lfs/osc/fsname-*/state</span><br><span class="line">$ grep -Ri current_state  /proc/fs/lfs/osc/fsname-*/state</span><br><span class="line">current_state: FULL</span><br><span class="line">state_history:</span><br><span class="line"> - [ 1616015179, CONNECTING ]</span><br><span class="line"> - [ 1616015179, IDLE ]</span><br><span class="line"> - [ 1616018598, CONNECTING ]</span><br><span class="line"> - [ 1616018598, FULL ]</span><br><span class="line"> - [ 1616019092, CONNECTING ]</span><br><span class="line"> - [ 1616019092, IDLE ]</span><br><span class="line"> - [ 1616019285, CONNECTING ]</span><br><span class="line"> - [ 1616019285, FULL ]</span><br><span class="line"> - [ 1616021199, CONNECTING ]</span><br><span class="line"> - [ 1616021199, IDLE ]</span><br><span class="line"> - [ 1616023761, CONNECTING ]</span><br><span class="line"> - [ 1616023761, FULL ]</span><br><span class="line"> - [ 1616023908, CONNECTING ]</span><br><span class="line"> - [ 1616023908, IDLE ]</span><br><span class="line"> - [ 1616147184, CONNECTING ]</span><br><span class="line"> - [ 1616147184, FULL ]</span><br><span class="line"></span><br><span class="line">$ grep generation /proc/fs/lfs/osc/*-OST0000*/import</span><br><span class="line">       generation: 11</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">client $ lctl get_param osc.fsname-OST0000*.import</span><br><span class="line">client $ lctl get_param osc.fsname-OST0000*.state</span><br><span class="line"></span><br><span class="line">#set idle timeout</span><br><span class="line">$ lctl set_param osc.*.idle_timeout=20</span><br><span class="line"></span><br><span class="line">#optimze the metadata</span><br><span class="line">#Many system commands, such as ls -l, du, and find, traverse a directory sequentially. To make these commands run efficiently, the directory statahead can be enabled to improve the performance of directory traversal</span><br><span class="line"># Controls the maximum number of file attributes (metadata) that will be prefetched by the statahead thread. By default, statahead is enabled and statahead_max is 32 files, The maximum statahead_max is 8192 files</span><br><span class="line">$ lctl get_param llite.*.statahead_max</span><br><span class="line">llite.fsname-ffff8dab2cce8800.statahead_max=32</span><br><span class="line">$ lctl set_param llite.*.statahead_max=128</span><br><span class="line">$ lctl set_param -P llite.*.statahead_max=128</span><br><span class="line"></span><br><span class="line">The directory statahead thread will also prefetch the file size/block attributes from the OSTs, so that all file attributes are available on the client when requested by an application. This is controlled by the asynchronous glimpse lock (AGL) setting. The AGL behaviour can be disabled by setting:</span><br><span class="line">$ lctl set_param llite.*.statahead_agl=0</span><br><span class="line"></span><br><span class="line">#default enable</span><br><span class="line">$ lctl set_param llite.*.statahead_agl=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n llite.*.statahead_stats</span><br><span class="line">statahead total: 4</span><br><span class="line">statahead wrong: 0 ## Does it increase, monitor</span><br><span class="line">agl total: 4</span><br><span class="line">#A read-only interface that provides current statahead and AGL statistics, such as how many times statahead/AGL has been triggered since the last mount, how many statahead/AGL failures have occurred due to an incorrect prediction or other causes.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ cat /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line">8</span><br><span class="line">$ echo 16 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line"># oss, lfs is extended to support RPCs up to 16MB in size</span><br><span class="line"># Lfs is extended to support RPCs up to 16MB in size. By enabling a larger RPC size, fewer RPCs will be required to transfer the same amount of data between clients and servers. With a larger RPC size, the OSS can submit more data to the underlying disks at once, therefore it can produce larger disk I/Os to fully utilize the increasing bandwidth of disks.</span><br><span class="line">#At client connection time, clients will negotiate with servers what the maximum RPC size it is possible to use, but the client can always send RPCs smaller than this maximum.</span><br><span class="line">#The parameter brw_size is used on the OST to tell the client the maximum (preferred) IO size. All clients that talk to this target should never send an RPC greater than this size. Clients can individually set a smaller RPC size limit via the osc.*.max_pages_per_rpc tunable.</span><br><span class="line">#The smallest brw_size that can be set for ZFS OSTs is the recordsize of that dataset. This ensures that the client can always write a full ZFS file block if it has enough dirty data, and does not otherwise force it to do read- modify-write operations for every RPC.</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.test0-OST*.brw_size</span><br><span class="line">obdfilter.test0-OST0000.brw_size=1</span><br><span class="line"></span><br><span class="line">#Large Bulk IO (16MB RPC)</span><br><span class="line">OSS   $ lctl set_param obdfilter.*.brw_size=16 #16M</span><br><span class="line">client$ lctl set_param osc.fsname-OST*.max_pages_per_rpc=16M</span><br><span class="line">client$ lctl set_param -P obdfilter.fsname-OST*.osc.max_pages_per_rpc=16M</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># The parameter brw_size is used on the OST to tell the client the maximum (preferred) IO size. All clients that talk to this target should never send an RPC greater than this size. Clients can individually set a smaller RPC size limit via the osc.*.max_pages_per_rpc tunable.</span><br><span class="line"># The smallest brw_size that can be set for ZFS OSTs is the recordsize of that dataset. This ensures that the client can always write a full ZFS file block if it has enough dirty data, and does not otherwise force it to do read- modify-write operations for every RPC.</span><br><span class="line">$ lctl set_param obdfilter.fsname-OST*.brw_size=16</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Get all client info from mds</span><br><span class="line">$ cat /proc/fs/lfs/nodemap/default/exports</span><br><span class="line">&#123; nid: 192.168.11.1@tcp, uuid: b19f57a2-d206-41ea-0c1e-e13650c4de6d &#125;</span><br><span class="line">....</span><br><span class="line">ost/OSS/ost/timeouts</span><br><span class="line">   service : cur  50  worst  92 (at 1616674042, 8947s ago)   3  18   1   3</span><br><span class="line">   service : cur  50  worst  50 (at 1616674036, 8953s ago)   1   1   1   1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##client</span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc osc.*.max_rpcs_in_flight osc.*.max_dirty_mb llite.*.max_read_ahead_mb</span><br><span class="line"># In order to enable a larger RPC size, brw_size must be changed to an IO size value up to 16MB. To temporarily change brw_size, the following command should be run on the OSS:</span><br><span class="line">oss# lctl set_param obdfilter.fsname-OST*.brw_size=16</span><br><span class="line"></span><br><span class="line">To persistently change brw_size, the following command should be run:</span><br><span class="line">oss# lctl set_param -P obdfilter.fsname-OST*.brw_size=16</span><br><span class="line">When a client connects to an OST target, it will fetch brw_size from the target and pick the maximum value of brw_size and its local setting for max_pages_per_rpc as the actual RPC size. Therefore, the max_pages_per_rpc on the client side would have to be set to 16M, or 4096 if the PAGESIZE is 4KB, to enable a 16MB RPC. To temporarily make the change, the following command should be run on the client to setmax_pages_per_rpc:</span><br><span class="line">client$ lctl set_param osc.fsname-OST*.max_pages_per_rpc=16M</span><br><span class="line">client$ lctl set_param -P obdfilter.fsname-OST*.osc.max_pages_per_rpc=16M</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client$ lctl set_param osc.*.max_pages_per_rpc=256 osc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=256 osc.*.grant_shrink=0</span><br><span class="line">client$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.max_pages_per_rpc=256</span><br><span class="line"></span><br><span class="line"># cancel_lru_locks</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*osc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n osc.*.rpc_stats=0</span><br><span class="line">$ dd if=/dev/zero of=/lfs/test bs=1M count=10</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### example for single OST</span><br><span class="line">$ lctl get_param -n &#x27;osc.*.rpc_stats&#x27; | sed -n &#x27;/pages per rpc/,/^$/p&#x27;</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                       0   0   0   |          0   0   0</span><br><span class="line">2:                       0   0   0   |          0   0   0</span><br><span class="line">4:                       0   0   0   |          0   0   0</span><br><span class="line">8:                       0   0   0   |          0   0   0</span><br><span class="line">16:                      0   0   0   |          0   0   0</span><br><span class="line">32:                      0   0   0   |          0   0   0</span><br><span class="line">64:                      0   0   0   |          0   0   0</span><br><span class="line">128:                     0   0   0   |          0   0   0</span><br><span class="line">256:                     0   0   0   |         10 100 100  ## why 256 ? in my test env, the brw_size=1 * 1048576 / 4096 PAGE_SIZE = 256</span><br><span class="line">                         |                       |</span><br><span class="line">                         |                      ---------- means dd bs=1M count=10 , 10 x read rpcs</span><br><span class="line">                         ---------------------------------write rpcs</span><br><span class="line"></span><br><span class="line">http://wiki.lfs.org/Lfs_Resiliency:_Understanding_Lfs_Message_Loss_and_Tuning_for_Resiliency#Tuning_Lfs_for_Resiliency</span><br><span class="line">$ lctl --net tcp peer_list</span><br><span class="line">$ lctl --net tcp connection_list</span><br><span class="line">lctl</span><br><span class="line">lctl &gt; network tcp1</span><br><span class="line">lctl &gt; peer_list</span><br><span class="line">12345-xx.xx.xx.xx@tcp [0]0.0.0.0-&gt;0.0.0.0:0 #0</span><br><span class="line"></span><br><span class="line"># ldlm_enqueue_min = max(2*net_latency, net_latency + quiescent_time) +\\ 2*service_time</span><br><span class="line"># ldlm_enqueue_min = max(2*50, 50 + 140) + 2*50 = 50+140 + 100 = 290</span><br><span class="line"># Minimum lock enqueue time (in seconds). The default is 100. The time it takes to enqueue a lock, ldlm_enqueue, is the maximum of the measured enqueue estimate (influenced by at_min and at_max parameters), multiplied by a weighting factor and the value of ldlm_enqueue_min.lfs Distributed Lock Manager (LDLM) lock enqueues have a dedicated minimum value for ldlm_enqueue_min. Lock enqueue timeouts increase as the measured enqueue times increase (similar to adaptive timeouts).</span><br><span class="line"></span><br><span class="line">#at_max The largest potential RPC timeout that a client can set is 2*at_max. By lowering at_max from 600 to 400 seconds we reduce the worst case I/O delay from 1200 seconds, or 20 minutes, to 800 seconds or just over 13 minutes.</span><br><span class="line">#at_min The 40 second value factors into our calculaton for an appropriate LDLM timeout as discussed in section LDLM Timeouts. Our recommendation for Lfs servers is also 40 seconds</span><br><span class="line">#Adaptive Timeouts: In a Lfs file system servers keep track of the time it takes for RPCs to be completed</span><br><span class="line">#The quiescent_time in this formula is to account for the time it takes all Lfs clients to reestablish connections with all Lfs targets following an HSN quiesce. We&#x27;ve experimentally determined an average time to be approximately 140 seconds, but it is possible that this value may vary based on different factors such as the number of Lfs clients, the number of Lfs targets, the number of Lfs file systems mounted on each client, etc. Thus, given an at_min of 40 seconds</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">options ptlrpc ldlm_enqueue_min=250</span><br><span class="line"></span><br><span class="line"># readonly mount</span><br><span class="line">$ mount.lfs $zpool /ost0 -o rdonly_dev</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#Dynamic Peer Discovery (&quot;Discovery&quot; for short) is the process by which a node can discover the network interfaces it can reach a peer on without being pre-configured. This involves sending a ping to the peer. The ping response carries a flag bit to indicate that the peer is multi-rail capable. If it is the node then pushes its own network interface information to the peer. This protocol distributes the network interface information to both nodes and subsequently the nodes can excercise the peer network interfaces as well as its own, as described in further detail in this section. Discovery can be enabled, disabled or in verification mode. If it is in verification mode, then it will cross reference the discovered peer NIDs with the configured NIDs and complain if there is a discrepancy, but will continue to use the configured NIDs</span><br><span class="line"></span><br><span class="line">#Peer Credits</span><br><span class="line">#Governs the number of concurrent sends to a single peer, End-to-end flow control accomplished at higher layer. e.g. max_rpcs_in_flight</span><br><span class="line"></span><br><span class="line">$ cat /proc/sys/lnet/peers</span><br><span class="line">$ cat /sys/kernel/debug/lnet/peers</span><br><span class="line">nid                      refs state  last   max   rtr   min    tx   min queue</span><br><span class="line">xx.xx.xx.xx@o2ib            3    up    -1   126   126   126   126   110 0</span><br><span class="line">tx is the number of peer credits currently available for this peer</span><br><span class="line"></span><br><span class="line">#lnet network Interface Credits</span><br><span class="line">$ cat /proc/sys/lnet/nis</span><br><span class="line">nid                      status alive refs peer  rtr   max    tx   min</span><br><span class="line">xx.xx.xx.xx@o2ib              up    -1    9  126    0  2048  2048  1796</span><br><span class="line">max is total available (i.e. value of ko2iblnd credits)</span><br><span class="line">tx is the number currently available, Negative number indicates number of messages awaiting a credit</span><br><span class="line">min is the low water mark</span><br><span class="line"></span><br><span class="line">#IB Link-Level Credit</span><br><span class="line">#Every HCA port and Switch port implements a credit mechanism between the senders and the receivers on the link. It is implemented for each link direction, and for each Virtual Lane (VL) that is implemented by the hardware at each end of the physical link.</span><br><span class="line">#Multiple credit ‘state machines’ and separate buffers per VL contribute to having:</span><br><span class="line">#Traffic on one VL that will not interfere with traffic on another VL</span><br><span class="line">#Within a VL, traffic in one direction that will not interfere with traffic in the reverse direction</span><br><span class="line"></span><br><span class="line">#While a physical link is active, the sender and receiver on each direction of each VL are continually communicating, so the sender knows how much buffer space remains for that VL on the receiving side. The amount of available space is expressed in terms of ‘credits’, where a credit represents a certain number of bytes. If the receiving VL buffer becomes full, the sender stops sending data until more space (credit) becomes available. When a receiver reports that there are zero available credits, it begins applying ‘backpressure’ to the sender</span><br><span class="line"></span><br><span class="line">#Lctl conn_list-List active TCP connections, type (bulk/control), tx_buffer_size, rx_buffer_size</span><br><span class="line">$ lctl --net tcp conn_list</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/ksocklnd/parameters/*</span><br><span class="line">echo 512 &gt; /sys/module/ksocklnd/parameters/credits # or more</span><br><span class="line"># the number of concurrent sends (to all peers), defaults:64, set in server and client</span><br><span class="line"></span><br><span class="line">echo 240 &gt; /sys/module/ksocklnd/parameters/peer_timeout</span><br><span class="line">## default 180</span><br><span class="line"></span><br><span class="line">## not test</span><br><span class="line">echo 256 &gt; /sys/module/ksocklnd/parameters/peer_buffer_credits</span><br><span class="line">#default: 0; peer_buffer_credits=256 # per-peer router buffer credits</span><br><span class="line"># concurrent_sends=256 - send work-queue sizing # not test</span><br><span class="line"></span><br><span class="line">echo 32 &gt; /sys/module/ksocklnd/parameters/peer_credits</span><br><span class="line">## the number of concurrent sends to a single peer, #default:8</span><br><span class="line"></span><br><span class="line">echo 70 &gt; /sys/module/ksocklnd/parameters/sock_timeout</span><br><span class="line">## default: 50 sec</span><br><span class="line"></span><br><span class="line">chmod a+w /sys/module/lnet/parameters/*</span><br><span class="line">echo 15 &gt; /sys/module/lnet/parameters/accept_timeout</span><br><span class="line">##default: 5  Acceptor&#x27;s timeout (seconds)</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">## Specifies the number of seconds the server waits for data to arrive from the client. If data does not arrive before the timeout expires then the connection is closed. By setting it to less than the default 30 seconds, you can free up threads sooner. However, you may also disconnect users with slower connections.</span><br><span class="line"></span><br><span class="line">echo 2000 &gt; /sys/module/lnet/parameters/accept_backlog</span><br><span class="line">##default: 127 Acceptor&#x27;s listen backlog</span><br><span class="line">options lnet accept_backlog=2000</span><br><span class="line">## Acceptor&#x27;s listen backlog, the number of the connections the server instance can buffer in the wait queue.</span><br><span class="line"></span><br><span class="line">echo 6 &gt; /sys/module/lnet/parameters/lnet_retry_count</span><br><span class="line">## default: 3 lnet_retry_count:Maximum number of times to retry transmitting a message</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /sys/module/lnet/parameters/use_tcp_bonding</span><br><span class="line">## default: 1  use_tcp_bonding:Set to 1 to use socklnd bonding. 0 to use Multi-Rail</span><br><span class="line"></span><br><span class="line">#default lfs.conf</span><br><span class="line">options lnet networks=tcp(enp129s0f1)</span><br><span class="line">options lnet accept_backlog=1024</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">options lnet lnet_retry_count=6</span><br><span class="line">options lnet lnet_transaction_timeout=120</span><br><span class="line">options ksocklnd credits=512</span><br><span class="line">options ksocklnd peer_credits=16</span><br><span class="line">options ptlrpc at_max=320</span><br><span class="line">options ptlrpc at_min=50</span><br><span class="line">options ptlrpc ldlm_enqueue_min=240</span><br><span class="line"></span><br><span class="line">http://lists.lfs.org/pipermail/lfs-discuss-lfs.org/2019-December/016813.html</span><br><span class="line">$ lnetctl net add --net tcp1 --if eno2  –peer-timeout 180 –peer-credits 128 –credits 1024 -peer_buffer_credits 0</span><br><span class="line">lctl set_param obdfilter.lfsbv-*.brw_size=4</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=256 osc.*.max_dirty_mb=2048</span><br><span class="line">or</span><br><span class="line">lctl set_param obdfilter.lfsbv-*.brw_size=16</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=4096</span><br><span class="line">lctl set_param osc.*.max_rpcs_in_flight=256</span><br><span class="line">lctl set_param osc.*.max_dirty_mb=8092</span><br><span class="line"></span><br><span class="line">$ cat /etc/modprobe.d/ksocklnd.conf</span><br><span class="line">options ksocklnd sock_timeout=100 credits=2560 peer_credits=63 enable_irq_affinity=0 concurrent_sends=63 fmr_pool_size=1280 pmr_pool_size=1280 fmr_flush_trigger=1024 nscheds=10  tx_buffer_size=1073741824 rx_buffer_size=1073741824</span><br><span class="line"></span><br><span class="line"># The maximum number of pages that will be sent in a single RPC request to the OST</span><br><span class="line">$ lctl get_param osc.*.max_pages_per_rpc</span><br><span class="line">$ lctl set_param osc.*.max_pages_per_rpc=1024 # 1024 = 1024*4KB =4MB per RPC</span><br><span class="line">#Max RPCS in flight between OSC and OST</span><br><span class="line">$ lctl set_param -P $FSNAME.osc.max_pages_per_rpc=1024</span><br><span class="line"></span><br><span class="line">#MDS setting1 normal default, brw=1MiB enable debug and checksums, basic</span><br><span class="line">Client $ lctl get_param osc.*.max_pages_per_rpc osc.*.max_rpcs_in_flight  mdc.*.max_rpcs_in_flight osc.*.max_dirty_mb llite.*.max_read_ahead_mb osc.*.grant_shrink  mdc.*.max_mod_rpcs_in_flight</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9336c4643000.max_pages_per_rpc=1024</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9336c4643000.max_pages_per_rpc=1024</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9336c4643000.max_pages_per_rpc=1024</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9336c4643000.max_rpcs_in_flight=8</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9336c4643000.max_rpcs_in_flight=8</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9336c4643000.max_rpcs_in_flight=8</span><br><span class="line">mdc.tfs12-MDT0000-mdc-ffff9336c4643000.max_rpcs_in_flight=8</span><br><span class="line">mdc.tfs12-MDT0001-mdc-ffff9336c4643000.max_rpcs_in_flight=8</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9336c4643000.max_dirty_mb=2000</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9336c4643000.max_dirty_mb=2000</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9336c4643000.max_dirty_mb=2000</span><br><span class="line">llite.tfs12-ffff9336c4643000.max_read_ahead_mb=64</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9336c4643000.grant_shrink=1</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9336c4643000.grant_shrink=1</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9336c4643000.grant_shrink=1</span><br><span class="line">mdc.tfs12-MDT0000-mdc-ffff9336c4643000.max_mod_rpcs_in_flight=7</span><br><span class="line">mdc.tfs12-MDT0001-mdc-ffff9336c4643000.max_mod_rpcs_in_flight=7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=64  mdc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=512 llite.*.max_read_ahead_mb=128 osc.*.grant_shrink=0  mdc.*.max_mod_rpcs_in_flight=16</span><br><span class="line">#In 1.8.1, we introduced a feature called grant shrinking which forces idle clients to release grant space after some time.</span><br><span class="line">#lustre: ptlrpc: migrate pinger to 64 bit time</span><br><span class="line">#https://lore.kernel.org/lustre-devel/87o8z16t95.fsf@notabene.neil.brown.name/T/</span><br><span class="line"></span><br><span class="line">#default in client, 2.15.3 server and client</span><br><span class="line">client $ lctl get_param osc.*.max_pages_per_rpc osc.*.max_rpcs_in_flight mdc.*.max_rpcs_in_flight osc.*.max_dirty_mb llite.*.max_read_ahead_mb osc.*.grant_shrink  mdc.*.max_mod_rpcs_in_flight</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9633c5a2d800.max_pages_per_rpc=1024</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9633c5a2d800.max_pages_per_rpc=1024</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9633c5a2d800.max_pages_per_rpc=1024</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9633c5a2d800.max_rpcs_in_flight=8</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9633c5a2d800.max_rpcs_in_flight=8</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9633c5a2d800.max_rpcs_in_flight=8</span><br><span class="line">mdc.tfs12-MDT0000-mdc-ffff9633c5a2d800.max_rpcs_in_flight=8</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9633c5a2d800.max_dirty_mb=972</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9633c5a2d800.max_dirty_mb=972</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9633c5a2d800.max_dirty_mb=972</span><br><span class="line">llite.tfs12-ffff9633c5a2d800.max_read_ahead_mb=242</span><br><span class="line">osc.tfs12-OST0000-osc-ffff9633c5a2d800.grant_shrink=1</span><br><span class="line">osc.tfs12-OST0001-osc-ffff9633c5a2d800.grant_shrink=1</span><br><span class="line">osc.tfs12-OST0002-osc-ffff9633c5a2d800.grant_shrink=1</span><br><span class="line">mdc.tfs12-MDT0000-mdc-ffff9633c5a2d800.max_mod_rpcs_in_flight=7</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#client disable connection idle</span><br><span class="line">client $ lctl set_param osc.*.idle_timeout=0</span><br><span class="line"></span><br><span class="line">#MDS setting2 disable debug setting,  mdc.*.max_mod_rpcs_in_flight=50 cause BUG</span><br><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=64  mdc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=512 llite.*.max_read_ahead_mb=1024 osc.*.grant_shrink=0 subsystem_debug=0 debug=0 fail_loc=0x238 osc.*.idle_timeout=0</span><br><span class="line"># reduec max_read_ahead_mb=64~128 for iops</span><br><span class="line"></span><br><span class="line"># MDT need enough CPU for benchmark, 18 cores Gold 3rd Xeon or EPYC 3 AMD or more cores</span><br><span class="line"></span><br><span class="line">#MDS setting3 brw rpc 16MiB and disable checksums</span><br><span class="line">MDS $ lctl set_param -P osc.*.max_pages_per_rpc=1024 osc.*.max_rpcs_in_flight=64 mdc.*.max_rpcs_in_flight=64 osc.*.max_dirty_mb=1024 llite.*.max_read_ahead_mb=1024 osc.*.grant_shrink=0  osc.*.brw_size=16 osc.*.checksums=0  mdc.*.max_mod_rpcs_in_flight=50 osc.*.idle_timeout=0 </span><br><span class="line"></span><br><span class="line"># enable fake IO. Good for performance testing, working with checksums=0, casue lbug....</span><br><span class="line">$ lctl set_param fail_loc=0x238</span><br><span class="line"></span><br><span class="line">$ echo 64 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line"># after disable(enable_remote_rename=0) the remote rename function ,the </span><br><span class="line">lctl set_param mdt.*.enable_remote_rename=0</span><br><span class="line">   68.327 (70.277 ms): renameat2(olddfd: -100, oldname: 0x7ffc32099292, newdfd: -100, newname: 0x6c7940) = 0</span><br><span class="line">   138.609 ( 0.316 ms): lstat(filename: 0x3209931a, statbuf: 0x7ffc32005800                   ) = 0</span><br><span class="line">   138.928 ( 0.255 ms): lstat(filename: 0x6c7940, statbuf: 0x7ffc32005890                     ) = -1 ENOENT No such file or directory</span><br><span class="line">   139.186 (72.632 ms): renameat2(olddfd: -100, oldname: 0x7ffc3209931a, newdfd: -100, newname: 0x6c7940) = 0</span><br><span class="line">   211.822 ( 0.299 ms): lstat(filename: 0x320993a2, statbuf: 0x7ffc32005800                   ) = 0</span><br><span class="line">   212.123 ( 0.244 ms): lstat(filename: 0x6c7940, statbuf: 0x7ffc32005890                     ) = -1 ENOENT No such file or directory</span><br><span class="line">   212.370 (75.688 ms): renameat2(olddfd: -100, oldname: 0x7ffc320993a2, newdfd: -100, newname: 0x6c7940) = 0</span><br><span class="line">   288.064 ( 0.315 ms): lstat(filename: 0x3209942a, statbuf: 0x7ffc32005800                   ) = 0</span><br><span class="line">   288.381 ( 0.248 ms): lstat(filename: 0x6c7940, statbuf: 0x7ffc32005890                     ) = -1 ENOENT No such file or directory</span><br><span class="line">   288.632 (73.348 ms): renameat2(olddfd: -100, oldname: 0x7ffc3209942a, newdfd: -100, newname: 0x6c7940) = 0</span><br><span class="line">   361.984 ( 0.322 ms): lstat(filename: 0x320994b2, statbuf: 0x7ffc32005800                   ) = 0</span><br><span class="line">   362.308 ( 0.251 ms): lstat(filename: 0x6c7940, statbuf: 0x7ffc32005890                     ) = -1 ENOENT No such file or directory</span><br><span class="line">   362.562 (78.532 ms): renameat2(olddfd: -100, oldname: 0x7ffc320994b2, newdfd: -100, newname: 0x6c7940) = 0</span><br><span class="line">   441.098 ( 0.298 ms): lstat(filename: 0x3209953a, statbuf: 0x7ffc32005800                   ) = 0</span><br><span class="line">   441.399 ( 0.278 ms): lstat(filename: 0x6c7940, statbuf: 0x7ffc32005890                     ) = -1 ENOENT No such file or directory</span><br><span class="line">   441.679 (75.049 ms): renameat2(olddfd: -100, oldname: 0x7ffc3209953a, newdfd: -100, newname: 0x6c7940) = 0</span><br><span class="line"></span><br><span class="line">in the 2.15.4, after enable_remote_rename=0, the latency from 70+ to 20ms+ , mdt iostat throughput from 30MB/s increase to 200MB/s</span><br><span class="line"></span><br><span class="line">   189.469 (22.449 ms): renameat2(olddfd: -100, oldname: 0x7ffd720acde0, newdfd: -100, newname: 0x1d46940) = 0</span><br><span class="line">   211.925 ( 0.311 ms): lstat(filename: 0x720ace68, statbuf: 0x7ffd71ee07c0                   ) = 0</span><br><span class="line">   212.239 ( 0.245 ms): lstat(filename: 0x1d46940, statbuf: 0x7ffd71ee0850                    ) = -1 ENOENT No such file or directory</span><br><span class="line">   212.487 (22.872 ms): renameat2(olddfd: -100, oldname: 0x7ffd720ace68, newdfd: -100, newname: 0x1d46940) = 0</span><br><span class="line">   235.365 ( 0.327 ms): lstat(filename: 0x720acef0, statbuf: 0x7ffd71ee07c0                   ) = 0</span><br><span class="line">   235.695 ( 0.273 ms): lstat(filename: 0x1d46940, statbuf: 0x7ffd71ee0850                    ) = -1 ENOENT No such file or directory</span><br><span class="line">   235.971 (30.462 ms): renameat2(olddfd: -100, oldname: 0x7ffd720acef0, newdfd: -100, newname: 0x1d46940) = 0</span><br><span class="line">   266.439 ( 0.477 ms): lstat(filename: 0x720acf78, statbuf: 0x7ffd71ee07c0                   ) = 0</span><br><span class="line">   266.919 ( 0.364 ms): lstat(filename: 0x1d46940, statbuf: 0x7ffd71ee0850                    ) = -1 ENOENT No such file or directory</span><br><span class="line">   267.286 (24.106 ms): renameat2(olddfd: -100, oldname: 0x7ffd720acf78, newdfd: -100, newname: 0x1d46940) = 0</span><br><span class="line">   291.397 ( 0.392 ms): lstat(filename: 0x720ad000, statbuf: 0x7ffd71ee07c0                   ) = 0</span><br><span class="line">   291.792 ( 0.250 ms): lstat(filename: 0x1d46940, statbuf: 0x7ffd71ee0850                    ) = -1 ENOENT No such file or directory</span><br><span class="line">   292.045 (25.781 ms): renameat2(olddfd: -100, oldname: 0x7ffd720ad000, newdfd: -100, newname: 0x1d46940) = 0</span><br><span class="line">   317.832 ( 0.318 ms): lstat(filename: 0x720ad088, statbuf: 0x7ffd71ee07c0                   ) = 0</span><br><span class="line">   318.153 ( 0.269 ms): lstat(filename: 0x1d46940, statbuf: 0x7ffd71ee0850                    ) = -1 ENOENT No such file or directory</span><br><span class="line">   318.425 (31.965 ms): renameat2(olddfd: -100, oldname: 0x7ffd720ad088, newdfd: -100, newname: 0x1d46940) = 0</span><br><span class="line">   350.397 ( 0.385 ms): lstat(filename: 0x720ad110, statbuf: 0x7ffd71ee07c0                   ) = 0</span><br><span class="line">   350.785 ( 0.242 ms): lstat(filename: 0x1d46940, statbuf: 0x7ffd71ee0850                    ) = -1 ENOENT No such file or directory</span><br><span class="line">   351.030 (22.646 ms): renameat2(olddfd: -100, oldname: 0x7ffd720ad110, newdfd: -100, newname: 0x1d46940) = 0</span><br><span class="line"></span><br><span class="line">#MDT tasket CPU</span><br><span class="line">$ lscpu</span><br><span class="line">$ for i in $(ps aux |grep -E &#x27;sock|mdt|osp|nvme1n1&#x27; | awk &#x27;&#123;print $2&#125;&#x27;); do taskset -cp 6-17 $i; echo $i; done</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#2.15.4 #improve the throughput for the mdt, 3x CPU usage and throughput for MDT(test app: a lot of mv)</span><br><span class="line">$ echo 128 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*.max_rpcs_in_flight=64;</span><br><span class="line"># Max number of 4K pages per RPC</span><br><span class="line"># Increase for small IO or long fast network paths (high BDP), May want to decrease to preempt TCP congestion</span><br><span class="line"></span><br><span class="line">256 = 1MB per RPC</span><br><span class="line"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span><br><span class="line">1024*4KB/1024(KB to MB)*64*2=512</span><br><span class="line">$ lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"># Maximum MBs of dirty data that can be written and queued on a client</span><br><span class="line">## Got the current dirty bytes</span><br><span class="line">$ lctl get_param osc.*.cur_dirty_bytes</span><br><span class="line">##  reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line"></span><br><span class="line">Set per OST or each clients</span><br><span class="line">256*4/1024*64*2=128</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=256; lctl set_param osc.*.max_rpcs_in_flight=64;lctl set_param osc.*.max_dirty_mb=128</span><br><span class="line"></span><br><span class="line">set the write cache</span><br><span class="line">512*4/1024*128*2=512</span><br><span class="line">lctl set_param osc.*.max_pages_per_rpc=512; lctl set_param osc.*.max_rpcs_in_flight=128;lctl set_param osc.*.max_dirty_mb=512</span><br><span class="line"></span><br><span class="line">$ modinfo mdt | grep max_mod_rpcs_per_client</span><br><span class="line">parm:           max_mod_rpcs_per_client:maximum number of modify RPCs in flight allowed per client (uint)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">##another word docuemnt test in ARM arch</span><br><span class="line">#server</span><br><span class="line">obdfilter.*.brw_size=16</span><br><span class="line">obdfilter.*.precreate_batch=1024</span><br><span class="line">osp.*.max_rpcs_in_flight=128</span><br><span class="line">mdt.*.dom_lock=trylock</span><br><span class="line">debug=0</span><br><span class="line"></span><br><span class="line">#client</span><br><span class="line">llite.*.max_read_head_mb=2048</span><br><span class="line">llite.*.max_read_ahead_per_file_mb=32</span><br><span class="line">mdc.*.max_rpcs_in_flight=128</span><br><span class="line">osc.*.max_pages_per_rpc=4M</span><br><span class="line">osc.*.max_rpcs_in_flight=256</span><br><span class="line">osc.*.max_dirty_mb=2000</span><br><span class="line">ldlm.namespaces.*.lru_size=4000000</span><br><span class="line">ldlm.namespeces.*.lru_max_age=30000</span><br><span class="line">osc.*.checksums=0</span><br><span class="line">debug=0</span><br><span class="line"></span><br><span class="line">lfs setdirstripe -D -c -1 $workdir</span><br><span class="line">lfs setstripe -C $((osts*4)) $workdir/ior-hard</span><br><span class="line">lfs setripe -c -1 $workdir/ior-hard</span><br><span class="line"></span><br><span class="line">lfs setstripe -E 64k -L mdt $workdir/mdtest-easy_or_hard</span><br><span class="line"></span><br><span class="line">LU-17525</span><br><span class="line"></span><br><span class="line">buffer IO, write back throttle, nvme buffer, latency; echo 0 &gt; wbt_lat_usec ; /sys/devices/virtual/block/sda/queue/wbt_lat_usec</span><br></pre></td></tr></table></figure>
</li>
<li><p>DNE not auto balance the lfs vers &lt; 2.15</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"># DNE related issues can happen, and if  rename() is used by users, then it is pretty tough to disable remote_rename if already enabled for a long time. If however there is need to disable remote_rename  (this seems to open up an entire can of worms if enabled). To see the value:</span><br><span class="line">$ lctl get_param mdt.*.enable_remote_rename</span><br><span class="line">$ lctl set_param mdt.*.enable_remote_rename=0</span><br><span class="line"></span><br><span class="line">$ lfs mkdir -c stripe_count ./dirs ## stripe dirs in MDT DNE, loading balancing in diff MDT</span><br><span class="line">$ lfs mkdir -i mdt_index ./dirs</span><br><span class="line">$ lctl set_param fsname.mdt.enable_remote_dir=1</span><br><span class="line">$ lctl conf_param fsname.mdt.enable_remote_dir_gid=-1</span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir mdt.*.enable_remote_dir_gid</span><br><span class="line"></span><br><span class="line">$ mkdir /mdtest&#123;0..5&#125; # for 6 mdt</span><br><span class="line">clinet $ for i in &#123;0..5&#125;</span><br><span class="line">do</span><br><span class="line">  lfs mkdir -i $i /mdtest/$I</span><br><span class="line">done</span><br><span class="line">#how to test DNE feature in mdtest</span><br><span class="line">#add &quot;-u&quot; and &quot; -d /mdtest/0/@/mdtest/1/@/mdtest/2/@/mdtest/3/@/mdtest/4/@/mdtest/5/&quot;</span><br><span class="line"></span><br><span class="line">##DNE under 2.14</span><br><span class="line">$ lfs setdirstripe -c $mdt -i -1 $OUTDIR</span><br><span class="line">$ lfs setdirstripe -D -c $mdt -i -1 $OUTDIR</span><br><span class="line">$ lfs setstripe -c $OSTCOUNT --pool capacity $OUTDIR</span><br><span class="line">$ lfs setstripe -L mdt -E 64K -E -1 $OUTDIR (DoM testing)</span><br><span class="line"></span><br><span class="line">### https://www.opensfs.org/wp-content/uploads/Evaluation-of-DoM-SNE-scaling_Simmons_revised051821.pdf</span><br><span class="line">$ lctl set_param mdt.*.enable_dir_auto_split=1</span><br><span class="line">$ lctl set param mdt.*.dir_split_delta=1</span><br><span class="line">$ lctl set param mdt.*.dir_split_count=15000</span><br><span class="line"></span><br><span class="line"># modify service node</span><br><span class="line"># --erase-params</span><br><span class="line">$ dev=/dev/nvme0n1p1; tunefs.lfs --erase-params $dev; tunefs.lfs --mgsnode=192.168.0.224@tcp --servicenode=&quot;192.168.0.225@tcp:192.168.0.226@tcp&quot; --writeconf $dev; mount.lfs $dev /lfs/fs5/ -o abort_recovery</span><br><span class="line"></span><br><span class="line"># show info</span><br><span class="line">$ tunefs.lfs /dev/sdb1</span><br><span class="line">$ tune2fs -O mmp /dev/block_device # Enable MMP on ldiskfs</span><br><span class="line">$ tune2fs -O ^mmp /dev/block_device # disable MMP on ldiskfs</span><br><span class="line"></span><br><span class="line"># large dir</span><br><span class="line">ext4_dx_add_entry:2281: Directory (ino: 24641537) index full, reach max htree level :2</span><br><span class="line">EXT4-fs warning (device nvme1n1p4): ext4_dx_add_entry:2285: Large directory feature is not enabled on this filesystem</span><br><span class="line"></span><br><span class="line">#show flag</span><br><span class="line">$ tune2fs -l /dev/sda6 | grep features</span><br><span class="line">$ file -s /dev/sda6</span><br><span class="line">$ debugfs -R features /dev/sda6</span><br><span class="line"></span><br><span class="line">$ tune2fs -O large_dir /dev/nvme0n1p1</span><br><span class="line">$ dumpe2fs -h /dev/nvme0n1p1 | grep feat</span><br><span class="line">$ dumpe2fs 1.45.2.wc1 (27-May-2019)</span><br><span class="line">Filesystem features:      has_journal ext_attr resize_inode dir_index filetype needs_recovery mmp flex_bg ea_inode dirdata large_dir sparse_super large_file huge_file uninit_bg dir_nlink quota</span><br><span class="line">Journal features:         journal_incompat_revoke</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#FID-in-dirent, Dirdata, Additional filesystem metadata performance tuning required:</span><br><span class="line">$ tune2fs -O dirdata /dev/md0_mdt0_p2</span><br><span class="line">FID作为文件名称的一部分存储在父目录中。该功能通过减少磁盘I/O显著提高了ls命令执行的性能。</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdt.lfs-MDT0000.recovery_status</span><br><span class="line"></span><br><span class="line">$ lfs getstripe -m ./stripe/test.2</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">$ lfs getstripe -m ./stripe/test.1</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">$ lfs mkdir -c 8 ./your_dir</span><br><span class="line"></span><br><span class="line">$ lfs find --mdt-index 0 ./stripe</span><br><span class="line">./stripe/test.2</span><br><span class="line">./stripe/test.8</span><br><span class="line">./stripe/test.6</span><br><span class="line">./stripe/test.4</span><br><span class="line">./stripe/test.0</span><br><span class="line"></span><br><span class="line">$ lfs find --mdt-index 1 ./stripe</span><br><span class="line">./stripe</span><br><span class="line">./stripe/test.1</span><br><span class="line">./stripe/test.5</span><br><span class="line">./stripe/test.7</span><br><span class="line">./stripe/test.3</span><br><span class="line">./stripe/test.9</span><br></pre></td></tr></table></figure>
</li>
<li><p>read only</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param mdt.fs-MDT0000.readonly=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>show the recovery status</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param mdt.testfs-MDT0000.recovery_status</span><br><span class="line">mdt.testsfs-MDT0000.recovery_status=</span><br><span class="line">status: COMPLETE</span><br><span class="line">recovery_start: 47</span><br><span class="line">recovery_duration: 23</span><br><span class="line">completed_clients: 2/2</span><br><span class="line">replayed_requests: 0</span><br><span class="line">last_transno: 5673970243</span><br><span class="line">VBR: DISABLED</span><br><span class="line">IR: DISABLED</span><br></pre></td></tr></table></figure>
</li>
<li><p>Read cache</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#not in MDS /sys and /proc</span><br><span class="line">MDS $ lctl set_param -P osc.*.grant_shrink=0</span><br><span class="line">MDS $ lctl set_param -P osc.*.read_ahead_async_file_threshold_mb=0</span><br><span class="line">MDS $ lctl set_param -P osc.*.max_read_ahead_mb=64 ## IOPS and throughput balance</span><br><span class="line">#disable the maxinum cached file size on the OST</span><br><span class="line">#readcache_max_filesize - Controls the maximum size of a file that both the read cache and writethrough cache will try to keep in memory. Files larger than readcache_max_filesize will not be kept in cache for either reads or writes</span><br><span class="line">OSS $ lctl set_param obdfilter.*.readcache_max_filesize=32M</span><br><span class="line"></span><br><span class="line">#disable the maxinum cached file size on the OST</span><br><span class="line">OSS $ lctl set_param obdfilter.&#123;OST_name&#125;.readcache_max_filesize=-1</span><br><span class="line">OSS $ lctl get_param obdfilter.*.readcache_max_filesize</span><br><span class="line"></span><br><span class="line">#could not set -1, 0 worked</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.readcache_max_filesize</span><br><span class="line">$ lctl set_param osd-ldiskfs.*.readcache_max_filesize=0 # -1 out of range</span><br><span class="line"></span><br><span class="line">#disable read cache on all the OSTs of an OSS, nvme ssd defalut read_cache_enable=0</span><br><span class="line">$ lctl set_param osd-ldiskfs.*.read_cache_enable=0</span><br><span class="line"></span><br><span class="line">#re-enable read cache</span><br><span class="line">$ lctl set_param osd-ldiskfs.&#123;OST_name&#125;.read_cache_enable=1</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable</span><br><span class="line"></span><br><span class="line">#disable writethrough_cache, nvme default read_cache_enable=0 and writethrough_cache_enable=0</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable osd-ldiskfs.*.writethrough_cache_enable</span><br><span class="line"></span><br><span class="line"># re-enable</span><br><span class="line">$ lctl set_param osd-ldiskfs.*.read_cache_enable=1 osd-ldiskfs.*.writethrough_cache_enable=1</span><br><span class="line">$ lctl get_param osd-ldiskfs.*.read_cache_enable osd-ldiskfs.*.writethrough_cache_enable=1</span><br></pre></td></tr></table></figure>
<p>writethrough_cache_enable - Controls whether data sent to the OSS as a write request is kept in the read cache and available for later reads, or if it is discarded from cache when the write is completed. By default, the writethrough cache is enabled (writethrough_cache_enable&#x3D;1)</p>
</li>
</ul>
<p>If the writethrough cache is disabled (writethrough_cache_enabled&#x3D;0), the OSS discards the data after the write request from the client is completed. For subsequent read requests, or partial-page write requests, the OSS must re-read the data from disk    </p>
<ul>
<li><p>How many max_dirty_mb set ?</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 256(max_pages_per_rpc) x 4KB = 1MB per RPC</span><br><span class="line"># 1024(max_pages_per_rpc) x 4KB = 4MB per RPC</span><br><span class="line"># max_pages_per_rpc*4*max_rpcs_in_flight*2=max_dirty_mb</span><br><span class="line"># 1024*4KB/1024(KB to MB)*64(max_rpcs_in_flight)*2=512</span><br></pre></td></tr></table></figure>
</li>
<li><p>restricting the number of locks kept on the client (10000 locks, 20 minutes age), for the many metadata</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lru_max_age is the client LUR lock age</span></span><br><span class="line"><span class="comment">#could not set by -P</span></span><br><span class="line">$ lctl set_param ldlm.namespaces.*.lru_size=10000 ldlm.namespaces.*.lru_max_age=1200000 <span class="comment">## default lru_max_age=3900000(65 mins)</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>read cache</p>
<ul>
<li><p>readcache_max_filesize - Controls the maximum size of a file that both the read cache and writethrough cache will try to keep in memory. Files larger than readcache_max_filesize will not be kept in cache for either reads or writes.</p>
</li>
<li><p>max_read_ahead_per_file_mb could not large than max_read_ahead_mb</p>
</li>
<li><p>This is the global limit for all files and cannot be larger than 1&#x2F;2 of the client RAM. To disable readahead, setmax_read_ahead_mb&#x3D;0</p>
</li>
</ul>
</li>
<li><p>lock_timeouts</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.*.*.lock_timeouts</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.MGS.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.fsname-OST0001-osc-MDT0000.lock_timeouts=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.lock_timeouts=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>osp status</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.*.active</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.active=1</span><br><span class="line">osp.fsname-OST0001-osc-MDT0000.active=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>mdt info</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osd-zfs.fsname-MDT0000.mntdev</span><br><span class="line">osd-zfs.fsname-MDT0000.mntdev=test_mdt/test_mdt0</span><br><span class="line"></span><br><span class="line">$ lctl get_param mgs.MGS.mntdev</span><br><span class="line">mgs.MGS.mntdev=test_mdt/test_mdt0</span><br><span class="line"></span><br><span class="line">$ lctl barrier_stat fsname</span><br><span class="line">state: init</span><br><span class="line">timeout: 0 seconds</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_dir_migration</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_remote_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_striped_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_dir_migration</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_remote_dir</span><br><span class="line">1</span><br><span class="line">$ lctl get_param -n mdt.*MDT*.enable_striped_dir</span><br><span class="line">1</span><br></pre></td></tr></table></figure>
</li>
<li><p>The global write barriers</p>
<ul>
<li>Snapshots are non-atomic across multiple MDTs and OSTs, which means that if there is activity on the file system while a snapshot is being taken, there may be user-visible namespace inconsistencies with files created or destroyed in the interval between the MDT and OST snapshots. In order to create a consistent snapshot of the file system, we are able to set a global write barrier, or “freeze” the system. Once set, all metadata modifications will be blocked until the write barrier is actively removed (“thawed”) or expired. The user can set a timeout parameter on a global barrier or the barrier can be explicitly removed. The default timeout period is 30 seconds.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lctl barrier_freeze $FSNAME 30</span><br><span class="line">$ lctl barrier_thaw $FSNAME</span><br><span class="line">$ lctl barrier_rescan $FSNAME</span><br><span class="line">0 of 1 MDT(s) in the filesystem $FSNAME are inactive</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>The threshold at which the allocation method switches from round-robin to weighted is set in this file. The default is to switch to the weighted algorithm when any two OSTs are out of balance by more than 17 percent.</p>
<ul>
<li>The weighting priority used by the weighted allocator can be adjusted in this file. Increasing the value of qos_prio_free puts more weighting on the amount of free space available on each OST and less on how stripes are distributed across OSTs. The default value is 91 percent weighting for free space rebalancing and 9 percent for OST balancing. When the free space priority is set to 100, weighting is based entirely on free space and location is no longer used by the striping algorithm<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param  lod.*.qos_prio_free</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_prio_free=91%</span><br><span class="line">$ lctl get_param  lod.*.qos_threshold_rr</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_threshold_rr=17%</span><br><span class="line">lod.*.qos_threshold_rr -</span><br><span class="line">lod.*.qos_prio_free -</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>The weighting priority used by the weighted allocator can be adjusted in this file. Increasing the value of qos_prio_free puts more weighting on the amount of free space available on each OST and less on how stripes are distributed across OSTs. The default value is 91 percent weighting for free space rebalancing and 9 percent for OST balancing. When the free space priority is set to 100, weighting is based entirely on free space and location is no longer used by the striping algorithm   </p>
</li>
<li><p>max create count</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.*.max_create_count</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.max_create_count=20000</span><br><span class="line"></span><br><span class="line"># disable the OST # default value 20000</span><br><span class="line">MDS $ lctl set_param osp.$FSNAME-OST0000-osc-MDT*.max_create_count=0</span><br><span class="line">MDS $ lctl set_param osp.$FSNAME-OST0000-*.max_create_count=0</span><br><span class="line"></span><br><span class="line">#degrade will only prefer to skip the OST</span><br><span class="line">OSS $ lctl set_param obdfilter.$FSNAME-OST0000.degraded=1</span><br><span class="line"></span><br><span class="line">#disable pre-create</span><br><span class="line">OSS $ lctl set_param obdfilter.$FSNAME-OST0000*.no_precreate=1</span><br><span class="line"></span><br><span class="line">#get the Target</span><br><span class="line">                                                     ------ block dev</span><br><span class="line">                                                     |</span><br><span class="line">OSS $ lctl get_param obdfilter.$(tunefs.lfs /dev/md1 2&gt;/dev/null | grep -i Target | uniq | awk &#x27;&#123;print $2&#125;&#x27;).degraded</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>With lfs 2.9 and later, the MDS should be set to only disable file creation on that OST by setting max_create_count to zero</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param osp.osc_name.max_create_count=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.fsname-OST0000-osc-MDT0000.active</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.active=1</span><br><span class="line"></span><br><span class="line">#to deactivate the OSC on the MDS node(s) use:</span><br><span class="line">$ lctl set_param osp.fsname-OST0000-osc-MDT0000.active=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>dmo lock</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param -n mdt.*.dom_lock</span><br><span class="line">always</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n lod.*.dom_stripesize</span><br><span class="line">1048576</span><br></pre></td></tr></table></figure>
</li>
<li><p>reserved_mb_high</p>
<ul>
<li>the high watermark used to start object allocation if available space is more than this. The default is 0.2% of total OST size</li>
</ul>
</li>
<li><p>reserved_mb_low</p>
<ul>
<li>The low watermark used to stop object allocation if available space is less than this. The default is 0.1% of total OST size<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.*.reserved_mb_high</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.reserved_mb_high=255329</span><br><span class="line">$ lctl get_param osp.*.reserved_mb_low</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.reserved_mb_low=127664</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>force_sync</p>
<ul>
<li>commits and closes the current open journal transaction. This function also has a flag called force_sync<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param osp.*.force_sync=1</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.force_sync=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param  osp.*MDT*.sync_changes</span><br><span class="line">osp.fsname-OST0000-osc-MDT0000.sync_changes=0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>max_nolock_bytes</p>
<ul>
<li>Server-side locking set only for requests less than the blocks set in themax_nolock_bytes parameter. If this tunable is set to zero (0), it disables server-side locking for read&#x2F;write requests<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.max_nolock_bytes</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.MGS.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.max_nolock_bytes=0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>contention_seconds</p>
<ul>
<li>The resource keeps itself in a contended state as set in the parameter<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.contention_seconds</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contention_seconds=2</span><br><span class="line">ldlm.namespaces.MGS.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.contention_seconds=2</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.contention_seconds=2</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>contended_locks </p>
<ul>
<li>If the number of lock conflicts in the scan of granted and waiting queues at contended_locks is exceeded, the resource is considered to be contended.<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.contended_locks</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contended_locks=32</span><br><span class="line">ldlm.namespaces.MGS.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-MDT0000.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-MDT0000.contended_locks=32</span><br><span class="line">ldlm.namespaces.mdt-fsname-MDT0000_UUID.contended_locks=32</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.lock_reclaim_threshold_mb</span><br><span class="line">ldlm.lock_reclaim_threshold_mb=51542</span><br></pre></td></tr></table></figure>

<h4 id="monitor"><a href="#monitor" class="headerlink" title="monitor"></a>monitor</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><span class="line">#you could replace stats to * to get more info, lctl get_param mds.MDS.mdt.*</span><br><span class="line">$ lctl get_param mdt.*MDT*.exports.*@*.stats</span><br><span class="line"></span><br><span class="line">#https://wiki.lfs.org/images/8/88/Lustre-Metrics-New-Techniques-for-Monitoring_Nolin_Wagner.pdf</span><br><span class="line"></span><br><span class="line">#Monitor all MDS</span><br><span class="line">$ lctl get_param mdt.*-MDT0000.md_stats osd-*.*MDT*.filesfree osd-*.*MDT*.filestotal  osd-*.*MDT*.kbytesfree  osd-*.*MDT*.kbytestotal</span><br><span class="line">mdt.fsname-MDT0000.md_stats=</span><br><span class="line">snapshot_time             1619838866.314148577 secs.nsecs</span><br><span class="line">open                      599 samples [reqs]</span><br><span class="line">close                     137 samples [reqs] 1 1 137</span><br><span class="line">mknod                     23 samples [reqs] 1 1 23 (min, max, sum)</span><br><span class="line">unlink                    9 samples [reqs]</span><br><span class="line">mkdir                     3 samples [reqs]</span><br><span class="line">rename                    6 samples [reqs]</span><br><span class="line">getattr                   87 samples [reqs]</span><br><span class="line">setattr                   17 samples [reqs]</span><br><span class="line">getxattr                  25 samples [reqs]</span><br><span class="line">setxattr                  11 samples [reqs]</span><br><span class="line">statfs                    25 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">samedir_rename            3 samples [reqs]</span><br><span class="line">crossdir_rename           3 samples [reqs]</span><br><span class="line"></span><br><span class="line">$ lctl get_param osd-*.*MDT*.filesfree</span><br><span class="line">osd-zfs.testfs-MDT0000.filesfree=797139104</span><br><span class="line">$ lctl get_param osd-*.*MDT*.filestotal</span><br><span class="line">osd-zfs.testfs-MDT0000.filestotal=811462460</span><br><span class="line">$ lctl get_param osd-*.*MDT*.kbytesfree</span><br><span class="line">osd-zfs.testfs-MDT0000.kbytesfree=3188556416</span><br><span class="line">$ lctl get_param osd-*.*MDT*.kbytestotal</span><br><span class="line">osd-zfs.testfs-MDT0000.kbytestotal=3196029568</span><br><span class="line"></span><br><span class="line">$ lctl set_param mdt.*.md_stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl list_param mdt.*.rename_stats</span><br><span class="line">mdt.fsname-MDT0000.rename_stats</span><br><span class="line">$ lctl get_param mdt.*.rename_stats</span><br><span class="line">mdt.fsname-MDT0000.rename_stats=</span><br><span class="line">rename_stats:</span><br><span class="line">- snapshot_time:  1619838907.404363357</span><br><span class="line">- same_dir</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line">- crossdir_src</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line">- crossdir_tgt</span><br><span class="line">      16KB: &#123; sample:   3, pct: 100, cum_pct: 100 &#125;</span><br><span class="line"></span><br><span class="line">$ lctl get_param mds.MDS.mdt.stats</span><br><span class="line">mds.MDS.mdt.stats=</span><br><span class="line">snapshot_time             1619836421.293308463 secs.nsecs</span><br><span class="line">req_waittime              13090 samples [usec] 5 994 1420115 197067923 (max, min, sum, sum of squares)</span><br><span class="line">req_qdepth                13090 samples [reqs] 0 1 1 1</span><br><span class="line">req_active                13090 samples [reqs] 1 2 13094 13102</span><br><span class="line">req_timeout               13090 samples [sec] 50 50 654500 32725000</span><br><span class="line">reqbuf_avail              26152 samples [bufs] 63 64 1673657 107109575</span><br><span class="line">ldlm_ibits_enqueue        519 samples [reqs] 1 1 519 519</span><br><span class="line">mds_reint_rename          2 samples [reqs] 1 1 2 2</span><br><span class="line">mds_reint_open            162 samples [reqs] 1 1 162 162</span><br><span class="line">mds_getattr               2 samples [usec] 65 75 140 9850</span><br><span class="line">mds_getattr_lock          3 samples [usec] 44 192 296 42400</span><br><span class="line">mds_connect               5 samples [usec] 17 10421 10865 108662423</span><br><span class="line">mds_disconnect            1 samples [usec] 210 210 210 44100</span><br><span class="line">mds_get_root              2 samples [usec] 16 41 57 1937</span><br><span class="line">mds_statfs                22 samples [usec] 27 203 1414 122592</span><br><span class="line">obd_ping                  12530 samples [usec] 4 565 439527 19537985</span><br><span class="line"></span><br><span class="line">$ lctl get_param mds.MDS.mdt_setattr.stats</span><br><span class="line">mds.MDS.mdt_setattr.stats=</span><br><span class="line">snapshot_time             1619836714.197733538 secs.nsecs</span><br><span class="line"></span><br><span class="line"># Metadata readdir service</span><br><span class="line">$ lctl get_param mds.MDS.mdt_readpage.stats</span><br><span class="line">mds.MDS.mdt_readpage.stats=</span><br><span class="line">snapshot_time             1619794903.241168757 secs.nsecs</span><br><span class="line">req_waittime              13 samples [usec] 10 86 509 25623</span><br><span class="line">req_qdepth                13 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                13 samples [reqs] 1 1 13 13</span><br><span class="line">req_timeout               13 samples [sec] 50 50 650 32500</span><br><span class="line">reqbuf_avail              32 samples [bufs] 63 64 2042 130310</span><br><span class="line">mds_close                 7 samples [usec] 21 153 578 60484</span><br><span class="line">mds_readpage              6 samples [usec] 270 498 2423 1031057</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*0000-osc-*.stats</span><br><span class="line">osc.fsname-OST0000-osc-MDT0000.stats=</span><br><span class="line">snapshot_time             1619794678.828478520 secs.nsecs</span><br><span class="line">req_waittime              18644 samples [usec] 237 8040 18490448 19195339776</span><br><span class="line">req_active                18644 samples [reqs] 1 2 18645 18647</span><br><span class="line">ost_create                2 samples [usec] 237 685 922 525394</span><br><span class="line">ost_get_info              1 samples [usec] 1951 1951 1951 3806401</span><br><span class="line">ost_connect               1 samples [usec] 1074 1074 1074 1153476</span><br><span class="line">ost_statfs                18640 samples [usec] 306 8040 18486501 19189854505</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619836833.236763001 secs.nsecs</span><br><span class="line">req_waittime              20 samples [usec] 17 683 2564 723094</span><br><span class="line">req_qdepth                20 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                20 samples [reqs] 1 1 20 20</span><br><span class="line">req_timeout               20 samples [sec] 50 50 1000 50000</span><br><span class="line">reqbuf_avail              53 samples [bufs] 63 64 3388 216580</span><br><span class="line">ldlm_cancel               20 samples [usec] 5 224 1432 162534</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_cbd.stats</span><br><span class="line">ldlm.services.ldlm_cbd.stats=</span><br><span class="line">snapshot_time             1619836850.112362316 secs.nsecs</span><br><span class="line">req_waittime              5 samples [usec] 25 189 549 80843</span><br><span class="line">req_qdepth                5 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                5 samples [reqs] 1 1 5 5</span><br><span class="line">req_timeout               5 samples [sec] 50 50 250 12500</span><br><span class="line">reqbuf_avail              13 samples [bufs] 1 1 13 13</span><br><span class="line">ldlm_bl_callback          5 samples [usec] 14 67 146 6134</span><br><span class="line"></span><br><span class="line">$ lctl --device MGS llog_print $&#123;fsname&#125;-MDT0000</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;target: $FSNAME-MDT&quot;</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;connect_flags&quot; | grep disp_stripe</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;import flags&quot; ### compare with under export flags</span><br><span class="line"></span><br><span class="line">MDS    $ lctl get_param -n mgc.*.uuid</span><br><span class="line">Client $ lctl get_param llite.*.uuid</span><br><span class="line"></span><br><span class="line">$ lctl --device MGS llog_print $fsname-MDT0000     # show all</span><br><span class="line">$ lctl --device MGS llog_print $fsname-MDT0000  $star_index $end_index</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># on the MGS: get the llog</span><br><span class="line">$ lctl --device MGS llog_print $fsname-client</span><br><span class="line">$ lctl --device MGS llog_print $fsname-OSTxxxx</span><br><span class="line">$ lctl --device MGS llog_print $fsnmae-MDTxxxx</span><br><span class="line"></span><br><span class="line">## ldiskfs cause some data not integrity, it &#x27;s very useful</span><br><span class="line">$ lctl --device MGS llog_cancel $fsname-MDT0000 $wrong_id ## delete wrong</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">$ cat /proc/fs/lfs/mgc/MGC192.168.0.1@tcp1/uuid</span><br><span class="line"></span><br><span class="line">$ lctl get_param -N mgs.MGS.exports.*</span><br><span class="line">mgs.MGS.exports.$client_ip@tcp1</span><br><span class="line"></span><br><span class="line"># 72 bytes is the minimum space required to store striping</span><br><span class="line"># information for a file striped across one OST:</span><br><span class="line"># (sizeof(struct lov_user_md_v3) +</span><br><span class="line">#  sizeof(struct lov_user_ost_data_v1))</span><br><span class="line"># not work in 2.12.6 ?</span><br><span class="line">$ lctl set_param -n llite.*.default_easize 72</span><br><span class="line"></span><br><span class="line"># 0 means Disable O_APPEND striping, verify it works</span><br><span class="line">#</span><br><span class="line">$ lctl get_param mdd.*.append_stripe_count</span><br><span class="line">mdd.lfs-MDT0000.append_stripe_count=1</span><br><span class="line">$ lctl set_param mdd.*.append_stripe_count=2</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdd.*.append_pool</span><br><span class="line">mdd.lfs-MDT0000.append_pool=</span><br><span class="line">$ lctl set_param mdd.*.append_pool=&#x27;none&#x27;</span><br></pre></td></tr></table></figure>

<ul>
<li><p>FAIL_MDS_LOV_PREP_CREATE</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#define OBD_FAIL_MDS_LOV_PREP_CREATE 0x141</span><br><span class="line">$ lctl set_param fail_loc=0x80000141</span><br><span class="line">#define OBD_FAIL_MDS_READLINK_EPROTO     0x143</span><br><span class="line">touch $DIR/$tdir/$tfile || true</span><br><span class="line"></span><br><span class="line">$ lctl set_param fail_loc=0x80000143</span><br><span class="line">ls -l /lfs/$foo &amp;&amp; echo &quot;no error&quot;</span><br><span class="line"></span><br><span class="line">#define OBD_FAIL_OSD_LMA_INCOMPAT 0x194</span><br><span class="line">$ lctl set_param fail_loc=0x194</span><br><span class="line">ls -l $wdir/$tfile &amp;&amp; echo &quot;no error&quot;</span><br><span class="line"></span><br><span class="line">#define OBD_FAIL_LDLM_ENQUEUE_OLD_EXPORT 0x30e</span><br><span class="line">touch $DIR/f74a</span><br><span class="line">lctl set_param fail_loc=0x8000030e</span><br><span class="line">ls $DIR/f74a</span><br><span class="line">lctl set_param fail_loc=0</span><br><span class="line"></span><br><span class="line">#define OBD_FAIL_OSC_CHECKSUM_RECEIVE       0x408</span><br><span class="line">$ lctl set_param fail_loc=0x80000408</span><br><span class="line">$ dd if=$DIR/$tfile of=/dev/null bs=1M || error &quot;dd read error: $?&quot;</span><br><span class="line">$ lctl set_param fail_loc=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>stripesize</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param -n lod.*.stripesize</span><br><span class="line">1048576</span><br></pre></td></tr></table></figure>
</li>
<li><p>pool POOL</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#Add pool</span><br><span class="line">MDS $ lctl pool_new &lt;fsname&gt;.&lt;poolname&gt;</span><br><span class="line">MDS $ lctl pool_add $FSNAME.pool1 OST[0-10/2]</span><br><span class="line">MDS $ lctl pool_list $FSNAME</span><br><span class="line">Client $ lfs setstripe -p $FSNAME.pool1 /lfs/test</span><br></pre></td></tr></table></figure>
</li>
<li><p>atime_diff</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># OSTs by default only hold a transient atime that is updated when clients do read requests. Permanent atime is written to the MDT when the file is closed. However, on-disk atime is only updated if it is more than 60 seconds old</span><br><span class="line">In lfs 2.14, it is possible to set the OSTs to persistently store atime with each object, in order to get more accurate persistent atime updates for files that are open for a long time via the similarly-named obdfilter.*.atime_diff parameter.</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdd.*MDT0000*.atime_diff</span><br><span class="line">60</span><br></pre></td></tr></table></figure>
</li>
<li><p>prealloc id</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param osp.lfs-OST*-osc-MDT0000/prealloc_status</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.*.prealloc_last_id</span><br><span class="line">osp.lfs-OST0000-osc-MDT0000.prealloc_last_id=121337627</span><br><span class="line">osp.lfs-OST0001-osc-MDT0000.prealloc_last_id=122220489</span><br><span class="line">osp.lfs-OST0002-osc-MDT0000.prealloc_last_id=62659128</span><br><span class="line"></span><br><span class="line">$ lctl get_param osp.*.prealloc_next_id</span><br><span class="line">osp.lfs-OST0000-osc-MDT0000.prealloc_next_id=121337612</span><br><span class="line">osp.lfs-OST0001-osc-MDT0000.prealloc_next_id=122220456</span><br><span class="line">osp.lfs-OST0002-osc-MDT0000.prealloc_next_id=62659083</span><br></pre></td></tr></table></figure>
</li>
<li><p>enable_remote_dir_gid</p>
<ul>
<li>With lfs software version 2.8, a new tunable is available to allow users with a specific group ID to create and delete remote and striped directories. This tunable is enable_remote_dir_gid. For example, setting this parameter to the ‘wheel’ or ‘admin’ group ID allows users with that GID to create and delete remote and striped directories. Setting this parameter to -1 on MDT0000 to permanently allow any non-root users create and delete remote and striped directories. On the MGS execute the following command<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid</span><br><span class="line">mdt.fsname-MDT0000.enable_remote_dir_gid=0</span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid=-1</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdt.*.enable_remote_dir_gid</span><br><span class="line">mdt.lfs-MDT0000.enable_remote_dir_gid=0</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>lfsck</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param -n osd-ldiskfs.lustre-OST0001.oi_scrub</span><br><span class="line"></span><br><span class="line">$ lctl lfsck_start -M $(facet_svc mds1) -A -C -t namespace</span><br><span class="line">$ lctl lfsck_start --device $(facet_svc mds1) -A -C -t namespace</span><br><span class="line">$ lctl get_param mdd.*.lfsck_namespace&quot;</span><br><span class="line">mdd.lfs-MDT0000.lfsck_namespace=</span><br><span class="line">name: lfsck_namespace</span><br><span class="line">magic: 0xa06249ff</span><br><span class="line">version: 2</span><br><span class="line">status: init</span><br><span class="line">flags:</span><br><span class="line">param:</span><br><span class="line">last_completed_time: N/A</span><br><span class="line">time_since_last_completed: N/A</span><br><span class="line">latest_start_time: N/A</span><br><span class="line">time_since_latest_start: N/A</span><br><span class="line">...</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p>prioritizes free space</p>
<ul>
<li>This setting controls how much lfs prioritizes free space (versus location) in allocation. The higher this number, the more lfs takes empty space on an OST into consideration for its allocation. When set to 100%, lfs uses ONLY empty space as the deciding factor for writes. Remember, this setting is only taken into consideration when lfs believes the OSTs to be imbalanced If you have set qos_threshold_rr to 100, this setting will have no effect.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param  lo[vd].*-mdtlov.qos_prio_free</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_prio_free=91%</span><br><span class="line">lov.fsname-MDT0000-mdtlov.qos_prio_free=91%</span><br><span class="line"></span><br><span class="line">$ lctl get_param *.*MDT0000-mdtlov.qos_threshold_rr</span><br><span class="line">lod.fsname-MDT0000-mdtlov.qos_threshold_rr=17% <span class="comment">## set 100 means forces lfs to round-robin because it believes the OSTs are balanced</span></span><br><span class="line">lov.fsname-MDT0000-mdtlov.qos_threshold_rr=17%</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="OSS"><a href="#OSS" class="headerlink" title="OSS"></a>OSS</h3><ul>
<li><p>io.timeouts</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#in the over loading storage, increase the value</span></span><br><span class="line">$ lctl get_param -n ost.*.ost_io.timeouts</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.&#123;state,timeouts&#125;</span><br><span class="line">$ lctl get_param at_* <span class="built_in">timeout</span></span><br><span class="line">$ lctl get_param llite.*<span class="variable">$FSNAME</span>*.stats</span><br></pre></td></tr></table></figure>
</li>
<li><p>grant info</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.testfs-MDT0000*.pool.*</span><br></pre></td></tr></table></figure>
</li>
<li><p>filefree</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param obdfilter.*.filesfree</span><br><span class="line">obdfilter.fsname-OST0000.filesfree=4183295296</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n osd-*.*OST0000.kbytesfree</span><br></pre></td></tr></table></figure>
</li>
<li><p><a target="_blank" rel="noopener" href="https://wiki.lfs.org/images/9/96/SDSC-Data-Oasis-GEn-II_Wagner.pdf">numa</a><br><a target="_blank" rel="noopener" href="https://github.com/DDNStorage/lfs_manual_markdown/blob/master/04.03-Tuning%20a%20Lustre%20File%20System.md#cpu-partition-string-patterns">cpu partition</a><br><a target="_blank" rel="noopener" href="https://github.com/gmelikov/lfs/commit/3eb7a1dfc3e7401ebcc45ccb116ed673607fd27f">LU-11454 ptlrpc: Make CPU binding switchable</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br></pre></td><td class="code"><pre><span class="line">2.15.2</span><br><span class="line">$ lctl get_param cpu_partition_table</span><br><span class="line">cpu_partition_table=</span><br><span class="line">0	: 0 1 2 3 4 5 6 7 8 9 10 11 24 25 26 27 28 29 30 31 32 33 34 35</span><br><span class="line">1	: 12 13 14 15 16 17 18 19 20 21 22 23 36 37 38 39 40 41 42 43 44 45 46 47</span><br><span class="line"></span><br><span class="line"><span class="comment">#example</span></span><br><span class="line">options lnet networks=<span class="string">&quot;tcp(bond0)&quot;</span></span><br><span class="line">o2ib0(ib0)[0,1] will ensure that all messages <span class="keyword">for</span> o2ib0 will be handled by LND threads executing onCPT0 and CPT1</span><br><span class="line">tcp1(eth0)[0]. Messages <span class="keyword">for</span> tcp1 are handled by threads on CPT0</span><br><span class="line"></span><br><span class="line"><span class="comment">#Network interface (NI) credits are shared across all CPU partitions (CPT). For example, if a machine has four CPTs and the number of NI credits is 512, then each partition has 128 credits. If a large number of CPTs exist on the system, LNet checks and validates the NI credits for each CPT to ensure each CPT has a workable number of credits. For example, if a machine has 16 CPTs and the number of NI credits is 256, then each partition only has 16 credits. 16 NI credits is low and could negatively impact performance. As a result, LNet automatically adjusts the credits to 8* peer_credits( peer_credits is 8 by default), so each partition has 64 credits.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#The value of cpu_npartitions must be an integer between 1 and the number of online CPUs</span></span><br><span class="line"><span class="comment">#by default there was a single CPT if the online CPU core count was four or fewer, and additional CPTs would be created depending on the number of CPU cores, typically with 4-8 cores per CPT</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Setting cpu_npartitions=1 will disable most of the SMP Node Affinity functionality.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#If cpu_pattern=N is used, then there will be one CPT for each NUMA node in the system, with each CPT mapping all of the CPU cores for that NUMA node</span></span><br><span class="line"></span><br><span class="line">libcfs.conf</span><br><span class="line">options libcfs cpu_pattern=<span class="string">&quot;0[10-11] 1[19-20]&quot;</span></span><br><span class="line">or</span><br><span class="line">options libcfs cpu_pattern=<span class="string">&quot;0[2,4,6] 1[3,5,7] #custom lustre cpu group</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">options ost oss_io_cpts=&quot;</span>[0,1]<span class="string">&quot; oss_cpts=&quot;</span>[1]<span class="string">&quot;</span></span><br><span class="line"><span class="string">ksocklnd.conf</span></span><br><span class="line"><span class="string">options ksocklnd nscheds=6 peer_credits=128 credits=1024</span></span><br><span class="line"><span class="string">options ko2iblnd nscheds=6 peer_credits=128 credits=1024</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#The Network Request Scheduler (NRS) allows the administrator to influence the order in which RPCs are handled at servers</span></span><br><span class="line"><span class="string">$ lctl get_param ost.OSS.ost_io.nrs_policies</span></span><br><span class="line"><span class="string">$ lctl set_param ldlm.services.ldlm_cbd.nrs_policies=crrn</span></span><br><span class="line"><span class="string">ldlm.services.ldlm_cbd.nrs_policies=crrn</span></span><br><span class="line"><span class="string">$ lctl set_param &#123;service&#125;.nrs_policies=&quot;</span>policy_name reg|hp<span class="string">&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ lctl set_param ost.OSS.ost_io.nrs_policies=&quot;</span>trr reg<span class="string">&quot;</span></span><br><span class="line"><span class="string">ost.OSS.ost_io.nrs_policies=&quot;</span>trr reg<span class="string">&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#not set</span></span><br><span class="line"><span class="string">$ cat /sys/module/ost/parameters/oss_cpts /sys/module/ost/parameters/oss_io_cpts /sys/module/ptlrpc/parameters/ldlm_cpts /sys/module/ptlrpc/parameters/ptlrpcd_cpts /sys/module/ptlrpc/parameters/ptlrpcd_per_cpt_max&quot;</span></span><br><span class="line">(null)</span><br><span class="line">(null)</span><br><span class="line">(null)</span><br><span class="line">(null)</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">mds_num_cpts=[EXPRESSION] binds the default MDS service threads to CPTs defined by EXPRESSION. For example mds_num_cpts=[0-3] will <span class="built_in">bind</span> the MDS service threads to CPT[0,1,2,3].</span><br><span class="line">mds_rdpg_num_cpts=[EXPRESSION] binds the <span class="built_in">read</span> page service threads to CPTs defined by EXPRESSION. The <span class="built_in">read</span> page service handles file close and readdir requests. For example mds_rdpg_num_cpts=[4] will <span class="built_in">bind</span> the <span class="built_in">read</span> page threads to CPT4.</span><br><span class="line">mds_attr_num_cpts=[EXPRESSION] binds the setattr service threads to CPTs defined by EXPRESSION.</span><br><span class="line"></span><br><span class="line">NUMA node0 CPU(s):   0-7</span><br><span class="line">NUMA node1 CPU(s):   8-15</span><br><span class="line">NUMA node2 CPU(s):   16-23</span><br><span class="line">NUMA node3 CPU(s):   24-31</span><br><span class="line"></span><br><span class="line">31:00.0 Serial Attached SCSI controller: Broadcom / LSI Fusion-MPT 12GSAS/PCIe Secure SAS38xx</span><br><span class="line">find /sys | grep 0000:31:00.0 | grep numa</span><br><span class="line">/sys/devices/pci0000:30/0000:30:02.0/0000:31:00.0/numa_node</span><br><span class="line">$ <span class="built_in">cat</span> /sys/devices/pci0000:30/0000:30:02.0/0000:31:00.0/numa_node</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> /sys/class/net/ens3f0/device/numa_node </span><br><span class="line">2</span><br><span class="line"></span><br><span class="line"><span class="comment">#test in OSS node</span></span><br><span class="line">options libcfs cpu_npartitions=0</span><br><span class="line">options libcfs cpu_pattern=<span class="string">&quot;0[0-7] 1[24-31] 2[16-23]&quot;</span> <span class="comment">#custom lustre cpu group</span></span><br><span class="line">options lnet networks=tcp(ens3f0)[1],o2ib(ens3f0)[1]</span><br><span class="line">options lnet accept_backlog=1024</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">options lnet lnet_retry_count=6</span><br><span class="line">options lnet lnet_transaction_timeout=120</span><br><span class="line"><span class="comment">#options mdt mds_num_cpts=[0] mds_rdpg_num_cpts=[0]</span></span><br><span class="line">options ost oss_cpts=[0] oss_io_cpts=[0]</span><br><span class="line">options ptlrpc at_max=320 at_min=50 ldlm_enqueue_min=240 ptlrpcd_cpts=[2] ldlm_cpts=[2]</span><br><span class="line">options ksocklnd nscheds=6 peer_credits=16 credits=1024 conns_per_peer=4</span><br><span class="line">options ko2iblnd nscheds=6 peer_credits=16 credits=1024 conns_per_peer=4</span><br><span class="line"></span><br><span class="line">options libcfs cpu_npartitions=0</span><br><span class="line">options libcfs cpu_pattern=<span class="string">&quot;0[0-7] 1[24-31] 2[16-23]&quot;</span></span><br><span class="line"><span class="comment">#options lnet networks=tcp(ens3f0np0)[0],o2ib(ens6f0np0)[1]</span></span><br><span class="line">options lnet networks=tcp(ens3f0np0)[0]</span><br><span class="line">options lnet accept_backlog=1024</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">options lnet lnet_retry_count=6</span><br><span class="line">options lnet lnet_transaction_timeout=120</span><br><span class="line">options mdt mds_num_cpts=[1] mds_rdpg_num_cpts=[0]</span><br><span class="line">options ptlrpc at_max=320 at_min=50 ldlm_enqueue_min=240 ptlrpcd_cpts=[2] ldlm_cpts=[2]</span><br><span class="line"></span><br><span class="line">%Cpu0  :  0.0 us, 22.1 sy,  0.0 ni, 73.8 <span class="built_in">id</span>,  0.0 wa,  1.3 hi,  2.7 si,  0.0 st</span><br><span class="line">%Cpu1  :  0.0 us, 22.5 sy,  0.0 ni, 75.5 <span class="built_in">id</span>,  0.0 wa,  1.3 hi,  0.7 si,  0.0 st</span><br><span class="line">%Cpu2  :  0.0 us, 20.3 sy,  0.0 ni, 73.2 <span class="built_in">id</span>,  0.0 wa,  1.7 hi,  4.7 si,  0.0 st</span><br><span class="line">%Cpu3  :  0.0 us, 21.1 sy,  0.0 ni, 75.9 <span class="built_in">id</span>,  0.0 wa,  1.4 hi,  1.7 si,  0.0 st</span><br><span class="line">%Cpu4  :  0.0 us, 22.3 sy,  0.0 ni, 75.3 <span class="built_in">id</span>,  0.0 wa,  1.4 hi,  1.0 si,  0.0 st &lt;------------<span class="built_in">test</span> worked, mds config</span><br><span class="line">%Cpu5  :  0.0 us, 20.9 sy,  0.0 ni, 73.0 <span class="built_in">id</span>,  0.0 wa,  1.4 hi,  4.7 si,  0.0 st</span><br><span class="line">%Cpu6  :  0.0 us, 20.7 sy,  0.0 ni, 74.8 <span class="built_in">id</span>,  0.0 wa,  1.4 hi,  3.1 si,  0.0 st</span><br><span class="line">%Cpu7  :  0.0 us, 21.8 sy,  0.0 ni, 72.5 <span class="built_in">id</span>,  0.0 wa,  1.3 hi,  4.4 si,  0.0 st</span><br><span class="line">%Cpu8  :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu9  :  3.0 us,  4.0 sy,  0.0 ni, 92.7 <span class="built_in">id</span>,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu10 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu11 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu12 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu13 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu14 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu15 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu16 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu17 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu18 :  0.0 us,  0.0 sy,  0.0 ni, 98.7 <span class="built_in">id</span>,  0.0 wa,  0.3 hi,  1.0 si,  0.0 st</span><br><span class="line">%Cpu19 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu20 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu21 :  0.0 us,  0.0 sy,  0.0 ni, 99.7 <span class="built_in">id</span>,  0.0 wa,  0.3 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu22 :  0.0 us,  0.3 sy,  0.0 ni, 99.7 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu23 :  0.0 us,  0.0 sy,  0.0 ni,100.0 <span class="built_in">id</span>,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu24 :  0.0 us, 31.5 sy,  0.0 ni, 67.4 <span class="built_in">id</span>,  0.0 wa,  1.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu25 :  0.0 us, 31.3 sy,  0.0 ni, 67.0 <span class="built_in">id</span>,  0.3 wa,  1.0 hi,  0.3 si,  0.0 st</span><br><span class="line">%Cpu26 :  0.0 us, 31.2 sy,  0.0 ni, 66.8 <span class="built_in">id</span>,  0.3 wa,  1.3 hi,  0.3 si,  0.0 st</span><br><span class="line">%Cpu27 :  0.0 us, 31.6 sy,  0.0 ni, 67.0 <span class="built_in">id</span>,  0.0 wa,  1.0 hi,  0.3 si,  0.0 st</span><br><span class="line">%Cpu28 :  0.0 us, 31.2 sy,  0.0 ni, 67.1 <span class="built_in">id</span>,  0.3 wa,  1.3 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu29 :  0.0 us, 31.6 sy,  0.0 ni, 67.3 <span class="built_in">id</span>,  0.0 wa,  1.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu30 :  0.0 us, 31.3 sy,  0.0 ni, 67.3 <span class="built_in">id</span>,  0.3 wa,  1.0 hi,  0.0 si,  0.0 st</span><br><span class="line">%Cpu31 :  0.0 us, 30.9 sy,  0.0 ni, 67.4 <span class="built_in">id</span>,  0.3 wa,  1.0 hi,  0.3 si,  0.0 st</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#In some cases, we would like network traffic to remain local to a single CPU to help keep the processor cache warm and minimize the impact of context switches. </span></span><br><span class="line">options ksocklnd enable_irq_affinity=1</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">options libcfs cpu_npartitions=0</span><br><span class="line">options libcfs cpu_pattern=<span class="string">&quot;0[0-11] 1[12-23]&quot;</span></span><br><span class="line">options lnet networks=tcp(ens1f0np0)[0],o2ib(ens1f0np0)[1]</span><br><span class="line">https://github.com/DDNStorage/lfs_manual_markdown/blob/master/04.03-Tuning%20a%20Lustre%20File%20System.md<span class="comment">#binding-network-interface-against-cpu-partitions</span></span><br><span class="line">For example, o2ib0(ib0)[0,1] will ensure that all messages <span class="keyword">for</span> o2ib0 will be handled by LND threads executing onCPT0 and CPT1. An additional example might be: tcp1(eth0)[0]. Messages <span class="keyword">for</span> tcp1 are handled by threads onCPT0.</span><br><span class="line"></span><br><span class="line">options lnet accept_backlog=1024</span><br><span class="line">options lnet accept_timeout=15</span><br><span class="line">options lnet lnet_retry_count=6</span><br><span class="line">options lnet lnet_transaction_timeout=120</span><br><span class="line">options mdt mds_num_cpts=[0] mds_rdpg_num_cpts=[1] &lt;--not suggest multiple group, easy to crash</span><br><span class="line">options ost oss_cpts=[0] oss_io_cpts=[1]</span><br><span class="line">options ptlrpc at_max=320 at_min=50 ldlm_enqueue_min=240</span><br><span class="line">options ptlrpc at_max=320 at_min=50 ldlm_enqueue_min=240 ptlrpcd_cpts=[0] ldlm_cpts=[1]</span><br><span class="line">options ksocklnd nscheds=6 peer_credits=128 credits=1024 conns_per_peer=2</span><br><span class="line">options ko2iblnd nscheds=6 peer_credits=128 credits=1024 conns_per_peer=2</span><br><span class="line">https://github.com/DDNStorage/lfs_manual_markdown/blob/master/04.03-Tuning%20a%20Lustre%20File%20System.md</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#https://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;options ptlrpc ptlrpcd_per_cpt_max=32&quot;</span> &gt;&gt; /etc/modprobe.d/lfs.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;options ksocklnd credits=2560&quot;</span> &gt;&gt; /etc/modprobe.d/lfs.conf</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;options ko2iblnd conns_per_peer=2&quot;</span> &gt;&gt; /etc/modprobe.d/lfs.conf</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;options ksocklnd conns_per_peer=2&quot;</span> &gt;&gt; /etc/modprobe.d/lfs.conf</span><br><span class="line"></span><br><span class="line">The MDC max_mod_rpcs_in_flight parameter defines the maximum number of file system modifying RPCs that can be sent <span class="keyword">in</span> parallel by a client to a MDT target</span><br><span class="line">MDS $ lctl set_param -P  mdc.*.max_mod_rpcs_in_flight=32</span><br><span class="line"></span><br><span class="line">client $ lctl set_param osc.*OST*.max_rpcs_in_flight=32 mdc.*.max_rpcs_in_flight=64 mdc.*.max_mod_rpcs_in_flight=50</span><br><span class="line"></span><br><span class="line"><span class="comment">#no locality between the OST thread handling an RPC and the OST storage,  because the request has to be handled by a specific OST device regardless of which network interface the request arrives on</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>contended_locks</p>
<ul>
<li>If the number of lock conflicts in the scan of granted and waiting queues at contended_locks is exceeded, the resource is considered to be contended.</li>
</ul>
</li>
<li><p>contention_seconds</p>
<ul>
<li>The resource keeps itself in a contended state as set in the parameter.</li>
</ul>
</li>
<li><p>max_nolock_bytes</p>
<ul>
<li>Server-side locking set only for requests less than the blocks set in the max_nolock_bytes parameter. If this tunable is set to zero (0), it disables server-side locking for read&#x2F;write requests.<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param ldlm.namespaces.*.max_nolock_bytes</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.max_nolock_bytes=0</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contention_seconds</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contention_seconds=2</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.contention_seconds=2</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.contention_seconds=2</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.contended_locks</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.contended_locks=32</span><br><span class="line">ldlm.namespaces.filter-fsname-OST0000_UUID.contended_locks=32</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-lwp-OST0000.contended_locks=32</span><br><span class="line"></span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*.max_nolock_bytes=2000000</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*.max_nolock_bytes=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.lock_reclaim_threshold_mb</span><br><span class="line">ldlm.lock_reclaim_threshold_mb=736</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Read and print the last_rcvd file from a device</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">#display client information</span><br><span class="line">$ lr_reader -c /dev/sdh</span><br><span class="line">last_rcvd:</span><br><span class="line">uuid: fsms-MDT0000_UUID</span><br><span class="line"> feature_compat: 0x8</span><br><span class="line"> feature_incompat: 0x61c</span><br><span class="line"> feature_rocompat: 0x1</span><br><span class="line"> last_transaction: 4294967298</span><br><span class="line"> target_index: 0</span><br><span class="line"> mount_count: 1</span><br><span class="line"> client_area_start: 8192</span><br><span class="line"> client_area_size: 128</span><br><span class="line"> 79136f3b-7d85-e265-37aa-dbb40ec5a30c:</span><br><span class="line"> generation: 2</span><br><span class="line"> last_transaction: 0</span><br><span class="line"> last_xid: 0</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line">#display reply data information</span><br><span class="line">$ lr_reader -r /dev/sdh</span><br><span class="line">...</span><br><span class="line">reply_data:</span><br><span class="line"> 0:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736549</span><br><span class="line"> last_xid: 1511845291497772</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"> 1:</span><br><span class="line"> client_generation: 2</span><br><span class="line"> last_transaction: 4426736566</span><br><span class="line"> last_xid: 1511845291498048</span><br><span class="line"> last_result: 0</span><br><span class="line"> last_data: 0</span><br><span class="line"></span><br><span class="line">$ cat /proc/fs/ldiskfs/dm-xx/options</span><br><span class="line">rw</span><br><span class="line">barrier</span><br><span class="line">no_mbcache</span><br><span class="line">user_xattr</span><br><span class="line">acl</span><br><span class="line">resuid=0</span><br><span class="line">resgid=0</span><br><span class="line">errors=remount-ro</span><br><span class="line">commit=5</span><br><span class="line">min_batch_time=0</span><br><span class="line">max_batch_time=15000</span><br><span class="line">stripe=0</span><br><span class="line">data=ordered</span><br><span class="line">inode_readahead_blks=32</span><br><span class="line">init_itable=10</span><br><span class="line">max_dir_size_kb=0</span><br><span class="line"></span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param obdfilter.*.stats obdfilter.*OST*.kbytesfree ldlm.namespaces.filter-*.pool.granted</span><br><span class="line">obdfilter.fsname-OST0000.stats=</span><br><span class="line">snapshot_time             1619839622.706200894 secs.nsecs</span><br><span class="line">read_bytes                130 samples [bytes] 4096 1048576 109461504</span><br><span class="line">write_bytes               115 samples [bytes] 444 1048576 105471940</span><br><span class="line">setattr                   10 samples [reqs]</span><br><span class="line">punch                     4 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">destroy                   9 samples [reqs]</span><br><span class="line">create                    3 samples [reqs]</span><br><span class="line">statfs                    27512 samples [reqs]</span><br><span class="line">get_info                  5 samples [reqs]</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.namespaces.filter-*.pool.granted</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0001_UUID.pool.granted=34</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0003_UUID.pool.granted=24</span><br><span class="line">$ lctl get_param ldlm.namespaces.filter-*.pool.grant_rate</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0001_UUID.pool.grant_rate=83</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0003_UUID.pool.grant_rate=54</span><br><span class="line">$ lctl get_param ldlm.namespaces.filter-*.pool.cancel_rate</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0001_UUID.pool.cancel_rate=70</span><br><span class="line">ldlm.namespaces.filter-testfs-OST0003_UUID.pool.cancel_rate=61</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lctl set_param obdfilter.*.stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.*OST*.exports.*@*.stats</span><br><span class="line"></span><br><span class="line">$ lctl get_param obdfilter.*.exports.*.stats</span><br><span class="line">obdfilter.fsname-OST0000.exports.192.168.0.238@tcp.stats=</span><br><span class="line">snapshot_time             1619839674.573369178 secs.nsecs</span><br><span class="line">setattr                   1 samples [reqs]</span><br><span class="line">destroy                   9 samples [reqs]</span><br><span class="line">create                    3 samples [reqs]</span><br><span class="line">statfs                    27498 samples [reqs]</span><br><span class="line">get_info                  1 samples [reqs]</span><br><span class="line">obdfilter.fsname-OST0000.exports.192.168.0.233@tcp.stats=</span><br><span class="line">snapshot_time             1619839674.573451068 secs.nsecs</span><br><span class="line">read_bytes                130 samples [bytes] 4096 1048576 109461504</span><br><span class="line">write_bytes               115 samples [bytes] 444 1048576 105471940</span><br><span class="line">setattr                   9 samples [reqs]</span><br><span class="line">punch                     4 samples [reqs]</span><br><span class="line">sync                      4 samples [reqs]</span><br><span class="line">statfs                    25 samples [reqs]</span><br><span class="line">get_info                  4 samples [reqs]</span><br><span class="line">$ lctl set_param obdfilter.*.exports.*.stats=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param ost.OSS.ost.stats</span><br><span class="line">ost.OSS.ost.stats=</span><br><span class="line">snapshot_time             1619837407.840675602 secs.nsecs</span><br><span class="line">req_waittime              1786 samples [usec] 18 309 235121 35937999</span><br><span class="line">req_qdepth                1786 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                1786 samples [reqs] 1 2 1788 1792</span><br><span class="line">req_timeout               1786 samples [sec] 50 50 89300 4465000</span><br><span class="line">reqbuf_avail              5344 samples [bufs] 61 64 338434 21434154</span><br><span class="line">ldlm_glimpse_enqueue      5 samples [reqs] 1 1 5 5</span><br><span class="line">ldlm_extent_enqueue       3 samples [reqs] 1 1 3 3</span><br><span class="line">ost_create                2 samples [usec] 49 528 577 281185</span><br><span class="line">ost_get_info              1 samples [usec] 1498 1498 1498 2244004</span><br><span class="line">ost_connect               3 samples [usec] 669 1574 3159 3764093</span><br><span class="line">ost_disconnect            1 samples [usec] 305 305 305 93025</span><br><span class="line">obd_ping                  1771 samples [usec] 12 126 78662 3931506</span><br><span class="line"></span><br><span class="line">$ lctl get_param ost.OSS.ost_create.stats</span><br><span class="line">ost.OSS.ost_create.stats=</span><br><span class="line">snapshot_time             1619837439.899464538 secs.nsecs</span><br><span class="line">req_waittime              27072 samples [usec] 23 523 3523516 532341790</span><br><span class="line">req_qdepth                27072 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                27072 samples [reqs] 1 1 27072 27072</span><br><span class="line">req_timeout               27072 samples [sec] 50 50 1353600 67680000</span><br><span class="line">reqbuf_avail              54351 samples [bufs] 63 64 3424493 215767379</span><br><span class="line">ost_statfs                27072 samples [usec] 21 252 1600400 103030806</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619837467.579931100 secs.nsecs</span><br><span class="line">req_waittime              3 samples [usec] 100 171 423 62345</span><br><span class="line">req_qdepth                3 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                3 samples [reqs] 1 1 3 3</span><br><span class="line">req_timeout               3 samples [sec] 50 50 150 7500</span><br><span class="line">reqbuf_avail              9 samples [bufs] 64 64 576 36864</span><br><span class="line">ldlm_cancel               3 samples [usec] 49 121 248 23126</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.services.ldlm_canceld.stats</span><br><span class="line">ldlm.services.ldlm_canceld.stats=</span><br><span class="line">snapshot_time             1619837467.579931100 secs.nsecs</span><br><span class="line">req_waittime              3 samples [usec] 100 171 423 62345</span><br><span class="line">req_qdepth                3 samples [reqs] 0 0 0 0</span><br><span class="line">req_active                3 samples [reqs] 1 1 3 3</span><br><span class="line">req_timeout               3 samples [sec] 50 50 150 7500</span><br><span class="line">reqbuf_avail              9 samples [bufs] 64 64 576 36864</span><br><span class="line">ldlm_cancel               3 samples [usec] 49 121 248 23126</span><br><span class="line">$ lctl get_param ldlm.services.ldlm_cbd.stats</span><br><span class="line">ldlm.services.ldlm_cbd.stats=</span><br><span class="line">snapshot_time             1619837479.839607820 secs.nsecs</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param osc.testfs-OST0001*.*max*   /  lctl get_param ldlm.namespaces.*.*max*</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.max_parallel_ast=1024</span><br><span class="line">ldlm.namespaces.filter-teslustre1-OST0000_UUID.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.filter-teslustre1-OST0000_UUID.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.filter-teslustre1-OST0000_UUID.max_parallel_ast=1024</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.lru_max_age=3900000</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.max_nolock_bytes=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.max_parallel_ast=1024</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param osc.testfs-OST0001*.*grant*  / lctl get_param ldlm.namespaces.*.pool.*grant*</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.MGC192.168.0.238@tcp.pool.granted=0</span><br><span class="line">ldlm.namespaces.filter-teslustre1-OST0000_UUID.pool.grant_plan=4276</span><br><span class="line">ldlm.namespaces.filter-teslustre1-OST0000_UUID.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.filter-teslustre1-OST0000_UUID.pool.grant_speed=-426</span><br><span class="line">ldlm.namespaces.filter-teslustre1-OST0000_UUID.pool.granted=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.granted=0</span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param ldlm.namespaces.teslustre1-MDT0000-*.*.*</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.cancel_rate=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.grant_plan=115635</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.grant_rate=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.grant_speed=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.granted=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.limit=1</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.lock_volume_factor=1</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.recalc_period=10</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.server_lock_volume=0</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.state=</span><br><span class="line">LDLM pool state (ldlm-pool-teslustre1-MDT0000-lwp-OST0000-1):</span><br><span class="line">  SLV: 0</span><br><span class="line">  CLV: 0</span><br><span class="line">  LVF: 1</span><br><span class="line">  GR:  0</span><br><span class="line">  CR:  0</span><br><span class="line">  GS:  0</span><br><span class="line">  G:   0</span><br><span class="line">  L:   1</span><br><span class="line">ldlm.namespaces.teslustre1-MDT0000-lwp-OST0000.pool.stats=</span><br><span class="line">snapshot_time             1647398330.677429043 secs.nsecs</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OSS $ lctl get_param obdfilter.*.brw_stats / lctl get_param osd-zfs.*.brw_stats</span><br><span class="line"></span><br><span class="line">obdfilter.testfs-OST0000.brw_stats=</span><br><span class="line">snapshot_time:         1593407004.726097825 (secs.nsecs)</span><br><span class="line"></span><br><span class="line">                           read      |     write</span><br><span class="line">pages per bulk r/w     rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">1:                 6006159   2   2   | 1239974   1   1</span><br><span class="line">2:                45845506  16  18   | 1809852   2   4</span><br><span class="line">4:                 1952464   0  18   | 53814   0   4</span><br><span class="line">8:                33533746  11  30   | 832520   1   5</span><br><span class="line">16:               12859649   4  35   | 91425   0   5</span><br><span class="line">32:                 620131   0  35   | 81660   0   5</span><br><span class="line">64:               38856531  13  49   | 307195   0   5</span><br><span class="line">128:               1697223   0  49   | 223084   0   6</span><br><span class="line">256:             142793845  50 100   | 69219436  93 100</span><br><span class="line"></span><br><span class="line">                           read      |     write</span><br><span class="line">discontiguous pages    rpcs  % cum % |  rpcs        % cum %</span><br><span class="line">0:               284165254 100 100   | 1244480   1   1</span><br><span class="line">1:                       0   0 100   | 1810011   2   4</span><br><span class="line">2:                       0   0 100   | 14961   0   4</span><br><span class="line">3:                       0   0 100   | 38853   0   4</span><br><span class="line">4:                       0   0 100   | 10409   0   4</span><br><span class="line">5:                       0   0 100   | 23853   0   4</span><br><span class="line"></span><br><span class="line">OSS $ lctl set_param ost.OSS.ost_io.req_history=100</span><br><span class="line">OSS $ lctl get_param ost.OSS.ost_io.req_history</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ modprobe -a lfs</span><br><span class="line">$ lfs_rmmod</span><br><span class="line">$ lnetctl lnet unconfigure </span><br><span class="line">#Note that rmmod assumes that if lnet is the only one, it is performing routing function and it will not be unloaded. To do this (error saying that the module is busy), you will need to explicitly unconfigure the lnet.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#show it in the lnetctl</span><br><span class="line"></span><br><span class="line">$  lnetctl stats show</span><br><span class="line">statistics:</span><br><span class="line">...</span><br><span class="line">    resend_count: 0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.$FSNAME*.checksums</span><br><span class="line"></span><br><span class="line">#All OST available bytes</span><br><span class="line">$ lctl get_param lov.zfsz2-clilov-*.kbytesavail</span><br><span class="line">lov.zfsz2-clilov-ffff8dab3afe6000.kbytesavail=139702738176</span><br><span class="line"></span><br><span class="line">$ lctl get_param lov.zfsz2-clilov-*.kbytestotal</span><br><span class="line">lov.zfsz2-clilov-ffff8dab3afe6000.kbytestotal=1038674946560</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.rpc_stats</span><br><span class="line">snapshot_time:         1619688798.673860765 (secs.nsecs)</span><br><span class="line">read RPCs in flight:  0</span><br><span class="line">write RPCs in flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending read pages:   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                       0   0   0   |          0   0   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">rpcs in flight        rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">0:                       0   0   0   |          0   0   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">offset                rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">0:                       0   0   0   |          0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl lov_getconfig /mnt</span><br><span class="line">default_stripe_count: 1</span><br><span class="line">default_stripe_size: 1048576</span><br><span class="line">default_stripe_offset: 18446744073709551615</span><br><span class="line">default_stripe_pattern: 1</span><br><span class="line">obd_count: 15</span><br><span class="line">OBDS:   obdidx          obdgen           obduuid</span><br><span class="line">             0               1           lfs-OST0000_UUID</span><br><span class="line">             1               1           lfs-OST0001_UUID</span><br><span class="line">             2               1           lfs-OST0002_UUID</span><br><span class="line">             3               1           lfs-OST0003_UUID</span><br><span class="line">             4               1           lfs-OST0004_UUID</span><br><span class="line">             5               1           lfs-OST0005_UUID</span><br><span class="line">             6               1           lfs-OST0006_UUID</span><br><span class="line">             7               1           lfs-OST0007_UUID</span><br><span class="line">             8               1           lfs-OST0008_UUID</span><br><span class="line">             9               1           lfs-OST0009_UUID</span><br><span class="line">            10               1           lfs-OST000a_UUID</span><br><span class="line">            11               1           lfs-OST000b_UUID</span><br><span class="line">            12               1           lfs-OST000c_UUID</span><br><span class="line">            13               1           lfs-OST000d_UUID</span><br><span class="line">            14               1           lfs-OST000e_UUID</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000-osc-[^mM]*.cur_grant_bytes</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab37559000.cur_grant_bytes=2097152</span><br><span class="line">osc.zfsz2-OST0000-osc-ffff8dab3afe6000.cur_grant_bytes=3407872</span><br><span class="line"></span><br><span class="line">#rpc info</span><br><span class="line">$ lctl get_param -n osc.fsname-OST0000-osc*.import  | grep &quot;target:&quot;</span><br><span class="line">    target: fsname-OST0000_UUID</span><br><span class="line">obdfilter_name=fsname-OST0000_UUID</span><br><span class="line"></span><br><span class="line">$ cat /proc/fs/lfs/osc/lfs-OST0000-osc-ffff88103a993000/import</span><br><span class="line"></span><br><span class="line">#default ost io threads, default is 0</span><br><span class="line">$ lctl get_param ost.OSS.ost_io.threads_max</span><br><span class="line">ost.OSS.ost_io.threads_max=128</span><br><span class="line">$ lctl get_param ost.OSS.ost_io.threads_started</span><br><span class="line">ost.OSS.ost_io.threads_started=21</span><br><span class="line"></span><br><span class="line"># osd_sync_destroy_max_size &quot;Maximum object size to use synchronous destroy</span><br><span class="line">options osd_zfs osd_sync_destroy_max_size=1048576</span><br></pre></td></tr></table></figure>

<h4 id="format"><a href="#format" class="headerlink" title="format"></a><a target="_blank" rel="noopener" href="https://www.aglt2.org/wiki/AGLT2/ReFormatOST">format</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#DNE format, host1 and host2 is a pair node</span></span><br><span class="line">host1 $ <span class="built_in">echo</span> mkfs.lustre --reformat --mgs --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS3NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS4NID&#125;</span> /dev/sda1</span><br><span class="line">host1 $ <span class="built_in">echo</span> mkfs.lustre --reformat --mdt --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS3NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS4NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS3NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS4NID&#125;</span> --mkfsoptions=\&quot;-i 16384\&quot; --index=0 /dev/sda2</span><br><span class="line"></span><br><span class="line"><span class="comment">#host3 and host4</span></span><br><span class="line">host3 $ <span class="built_in">echo</span> mkfs.lustre --reformat --mdt --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS3NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS4NID&#125;</span> --servicenode=<span class="variable">$&#123;OSS1NID&#125;</span> --servicenode=<span class="variable">$&#123;OSS2NID&#125;</span> --servicenode=<span class="variable">$&#123;OSS3NID&#125;</span> --servicenode=<span class="variable">$&#123;OSS4NID&#125;</span> --mkfsoptions=\&quot;-i 16384\&quot; --index=1 /dev/sda1</span><br><span class="line"></span><br><span class="line"><span class="comment"># mdt</span></span><br><span class="line">FSNAME=lfs</span><br><span class="line">MDS1NID=1.1.1.1@tcp</span><br><span class="line">MDS2NID=2.2.2.2@tcp</span><br><span class="line"></span><br><span class="line">mkfs.lfs --mgs --backfstype=zfs --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> mdt_0/mgt_0</span><br><span class="line">mkfs.lfs --mdt --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span> --backfstype=zfs --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;MDS1NID&#125;</span> --servicenode=<span class="variable">$&#123;MDS2NID&#125;</span> --index=0 mdt_0/mdt_0</span><br><span class="line"></span><br><span class="line"><span class="comment">#1 = enable lazy_init</span></span><br><span class="line">--mkfsoptions=<span class="string">&#x27;-i 1024&#x27;</span> <span class="comment">#inode, I test use -mkfsoptions=&#x27;-i 65536&#x27;</span></span><br><span class="line"></span><br><span class="line">--mkfsoptions=<span class="string">&quot;-E lazy_journal_init=1,lazy_itable_init=1&quot;</span></span><br><span class="line"></span><br><span class="line">--mountfsoptions=<span class="string">&quot;stripe=192&quot;</span></span><br><span class="line">--mountfsoptions=errors=remount-ro,user_xattr,localflock</span><br><span class="line"></span><br><span class="line">--mkfsoptions=<span class="string">&quot;-i bytes-per-inode&quot;</span></span><br><span class="line">Decreasing the inode ratio tunable bytes-per-inode will create more inodes <span class="keyword">for</span> a given MDT size, but will leave less space <span class="keyword">for</span> extra per-file metadata and is not recommended</span><br><span class="line">The inode ratio must always be strictly larger than the MDT inode size, <span class="built_in">which</span> is 1024 bytes by default. It is recommended to use an inode ratio at least 1024 bytes larger than the inode size to ensure the MDT does not run out of space. Increasing the inode ratio to at least hold the most common file size (e.g. 5120 or 66560 bytes <span class="keyword">if</span> 4KB or 64KB files are widely used) is recommended <span class="keyword">for</span> DoM</span><br><span class="line"></span><br><span class="line">https://github.com/DDNStorage/lustre_manual_markdown/blob/master/02.02-Determining%20Hardware%20Configuration%20Requirements%20and%20Formatting%20Options.md</span><br><span class="line">To <span class="built_in">set</span> the inode ratio, use the --mkfsoptions=<span class="string">&quot;-i *bytes-per-inode*&quot;</span> argument to mkfs.lustreto specify the expected average (mean) size of OST objects</span><br><span class="line">For example, to create an OST with an expected average object size of 8 MiB run</span><br><span class="line">[oss<span class="comment">#] mkfs.lustre --ost --mkfsoptions=&quot;-i $((8192 * 1024))&quot; ...</span></span><br><span class="line"></span><br><span class="line">--mkfsoptions=<span class="string">&quot;-I inode-size&quot;</span> option. Increasing the inode size will provide more space <span class="keyword">in</span> the inode <span class="keyword">for</span> a larger Lustre file layout, ACLs, user and system extended attributes, SELinux and other security labels, and other internal metadata. However, <span class="keyword">if</span> these features or other in-inode xattrs are not needed, the larger inode size will hurt metadata performance as 2x, 4x, or 8x as much data would be <span class="built_in">read</span> or written <span class="keyword">for</span> each MDT inode access.</span><br><span class="line"></span><br><span class="line">--mkfsoptions=<span class="string">&quot;recordsize=1024K -o compression=lz4 -o mountpoint=none&quot;</span></span><br><span class="line">--mkfsoptions=<span class="string">&quot;-E stride=4 -E stripe-width=2&quot;</span> --mkfsoptions=<span class="string">&quot;-E stripe=128 -E stride=32&quot;</span> </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## add parameters</span></span><br><span class="line">mkfs.lfs --mkfsoptions=“-E stride=32,stripe_width=256” --ost -mgsnode=192.168.0.22@tcp /dev/sda1</span><br><span class="line"></span><br><span class="line"><span class="comment"># ost</span></span><br><span class="line">FSNAME=lfs</span><br><span class="line">MDS1NID=1.1.1.1@tcp</span><br><span class="line">MDS2NID=2.2.2.2@tcp</span><br><span class="line">OSS1NID=3.3.3.3@tcp</span><br><span class="line">OSS2NID=4.4.4.4@tcp</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..6&#125;</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> mkfs.lfs --reformat --backfstype=zfs --ost  --index=<span class="variable">$i</span>  --fsname=<span class="variable">$&#123;FSNAME&#125;</span> --servicenode=<span class="variable">$&#123;OSS1NID&#125;</span> --servicenode=<span class="variable">$&#123;OSS2NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS1NID&#125;</span> --mgsnode=<span class="variable">$&#123;MDS2NID&#125;</span>  ost_<span class="variable">$i</span>/ost_<span class="variable">$i</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">omconfig storage controller action=createvdisk controller=1 size=max raid=r5 pdisk=0:1:5,0:1:6,0:1:7,0:1:8,0:1:9,0:1:10,0:1:11,0:1:12,0:1:13 stripesize=128kb readpolicy=ra writepolicy=wb name=ost22</span><br><span class="line">mkfs.lfs --ost --mgsnode=10.10.1.140@tcp0 --fsname=umt3 --reformat --index=11 --mkfsoptions=<span class="string">&quot;-i 2000000&quot;</span> --reformat --mountfsoptions=<span class="string">&quot;errors=remount-ro,extents,mballoc,stripe=256&quot;</span> /dev/sde</span><br><span class="line">tune2fs -O uninit_bg -m 1 -U cadf431a-6b03-4dd1-acc7-b6a3a0cbb69c /dev/sde</span><br><span class="line"></span><br><span class="line">omconfig storage controller action=createvdisk controller=1 size=max raid=r5 pdisk=1:2:0,1:2:1,1:2:2,1:2:3,1:2:4 stripesize=256kb readpolicy=ra writepolicy=wb name=ost31</span><br><span class="line">mkfs.lfs --ost --mgsnode=10.10.1.140@tcp0 --fsname=umt3 --reformat --index=32  --mkfsoptions=<span class="string">&quot;-i 1000000&quot;</span> --reformat --mountfsoptions=<span class="string">&quot;errors=remount-ro,extents,mballoc,stripe=256&quot;</span> /dev/sdf</span><br><span class="line">tune2fs -O uninit_bg -m 1 -U ffc8fa63-e7d7-470f-9691-72b56839a6b3 /dev/sdf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the issue</span></span><br><span class="line">During the work after an OST failed, and the files were all drained, the <span class="string">&quot;magic files&quot;</span> (LAST_ID, etc) were <span class="keyword">in</span> trouble. In particular, LAST_ID was not available. So, <span class="keyword">in</span> order to bring the OST back up after reformatting, I had to find a way to recreate this file.</span><br><span class="line"></span><br><span class="line">[root@lmd02 ~]<span class="comment"># lctl get_param osc.*.prealloc_next_id</span></span><br><span class="line">...</span><br><span class="line">osc.umt3-OST0025-osc.prealloc_next_id=6778336</span><br><span class="line">An alternate value is the prealloc_list_id, <span class="built_in">which</span> is larger, but considering the OST</span><br><span class="line">was already completely drained, Andreas Dilger has suggested to use the next_id value</span><br><span class="line">prealloc_last_id is 6778369</span><br><span class="line"></span><br><span class="line">See these URL</span><br><span class="line"></span><br><span class="line">http://wiki.lfs.org/manual/LustreManual20_HTML/LustreTroubleshooting.html</span><br><span class="line">https://groups.google.com/forum/<span class="comment">#!topic/lfs-discuss-list/NcDiutUirDg</span></span><br><span class="line"></span><br><span class="line">So, we have to get this value into place as LAST_ID.  </span><br><span class="line"></span><br><span class="line">     As an aside, we will need to be sure we are talking the correct index at all <span class="built_in">times</span>:</span><br><span class="line">     OK, so, using the directions</span><br><span class="line"></span><br><span class="line">     [root@umdist01 tmp]<span class="comment"># od -Ax -td4 last_rcvd |less</span></span><br><span class="line">     ...</span><br><span class="line">     000080           0           0           0          37</span><br><span class="line"></span><br><span class="line">     This matches with the <span class="string">&quot;lfs df&quot;</span> output, decimal 37, and is the index used <span class="keyword">in</span> the mkfs.lfs run.</span><br><span class="line"></span><br><span class="line">-----------------------------------------------------</span><br><span class="line">Data found on the Internet indicates, <span class="keyword">if</span> we have the mds/mdt offline, and mounted ldiskfs,</span><br><span class="line">we can <span class="keyword">do</span> the following to find the value to use <span class="keyword">in</span> LAST_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># extract last allocated object for all OSTs</span></span><br><span class="line">mds<span class="comment"># debugfs -c -R &quot;dump lov_objids /tmp/lo&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cut out the last allocated object for this OST index</span></span><br><span class="line">mds<span class="comment"># dd if=/tmp/lo of=/tmp/LAST_ID bs=8 skip=$&#123;OST index NN&#125; count=1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># verify value is the right one (LAST_ID = next_id - 1)</span></span><br><span class="line">mds<span class="comment"># lctl get_param osc.*OST00NN.prealloc_next_id  # NN is OST index</span></span><br><span class="line">mds<span class="comment"># od -td8 /tmp/LAST_ID</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># get OST filesystem ready for this value and copy it in place</span></span><br><span class="line">ossN<span class="comment"># mount -t ldiskfs /dev/&#123;ostdev&#125; /mnt/tmp</span></span><br><span class="line">ossN<span class="comment"># mkdir -p /mnt/tmp/O/0</span></span><br><span class="line">mds<span class="comment"># scp /tmp/LAST_ID ossN:/mnt/tmp/O/0/LAST_ID</span></span><br><span class="line"></span><br><span class="line">---------------------------------------------------------------------------------</span><br><span class="line">Instead, we note that we can start with ANY LAST_ID file, and edit it to create the one desired.</span><br><span class="line">Convert binary to text</span><br><span class="line">xxd /tmp/LAST_ID /tmp/LAST_ID.asc</span><br><span class="line"></span><br><span class="line">Fix it</span><br><span class="line">vi /tmp/LAST_ID.asc</span><br><span class="line">For example, 6513958 decimal is 0x636526, and appears <span class="keyword">in</span> the LAST_ID.asc file like so:</span><br><span class="line">0000000: 2665 6300 0000 0000                      &amp;ec.....</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Convert to binary</span><br><span class="line">xxd -r /tmp/LAST_ID.asc /tmp/LAST_ID.new</span><br><span class="line"></span><br><span class="line">Verify</span><br><span class="line"><span class="built_in">od</span> -Ax -td8 /tmp/LAST_ID.new</span><br><span class="line">copy it to the real LAST_ID</span><br><span class="line"></span><br><span class="line">So, how <span class="keyword">do</span> we <span class="keyword">do</span> the edit? Use a calculator to convert the <span class="string">&quot;prealloc_next_id&quot;</span> above to hex, <span class="keyword">for</span> example:</span><br><span class="line">[root@umdist01 ~]<span class="comment"># echo &quot;obase=16; 6778336&quot; | bc</span></span><br><span class="line">676DE0</span><br><span class="line">When you edit the .asc file above, this appears <span class="keyword">in</span> byte-order with the most significant of the 32 bits at the highest address. Don<span class="string">&#x27;t ask me which Endianism this is. So, when you edit the file, it goes in like so</span></span><br><span class="line"><span class="string">e06d 6700 0000 0000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">LAST_ID</span></span><br><span class="line"><span class="string">last_rcvd</span></span><br><span class="line"><span class="string">mountdata</span></span><br><span class="line"><span class="string">umt3-OST000b (for example, depending on the OST)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">last_rcvd -- Will be re-created during mount, or old one can be used. However, see below, if the LAST_ID has to be created from scratch, do not copy this file back.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">mountdata -- Copy created during run of mkfs.lfs can be used. EXCEPT for us it can&#x27;</span>t. Must use instead the mountdata copied out previously. Not sure why.</span><br><span class="line"></span><br><span class="line">umt3-OST000b -- would be recreated with a --writeconf, but it may also be created automatically during mount <span class="keyword">if</span> missing (it is an OST-<span class="built_in">local</span> copy of the MGS file of the same name so the OST can mount even <span class="keyword">if</span> the MGS is offline).</span><br></pre></td></tr></table></figure>

<h4 id="change-ipaddr"><a href="#change-ipaddr" class="headerlink" title="change ipaddr"></a>change ipaddr</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ tunefs.lfs --erase-params --mgsnode=<span class="variable">$new_mgs_ip</span>@tcp --servicenode=<span class="variable">$new_service_ip</span>@tcp0 --writeconf /dev/md4</span><br><span class="line"></span><br><span class="line">$ mgsinfo=<span class="string">&quot;<span class="variable">$new_mgs_ip</span>@o2ib:<span class="variable">$new_mgs_ip2</span>@o2ib2:<span class="variable">$new_mgs_ip</span>@tcp:<span class="variable">$new_mgs_ip2</span>@tcp&quot;</span></span><br><span class="line">$ serinfo=<span class="string">&quot;<span class="variable">$new_mgs_ip</span>@o2ib:<span class="variable">$new_mgs_ip2</span>@o2ib2:<span class="variable">$new_mgs_ip</span>@tcp:<span class="variable">$new_mgs_ip2</span>@tcp&quot;</span></span><br><span class="line"></span><br><span class="line">$ tunefs.lfs --erase-params --mgsnode=<span class="variable">$mgsinfo</span> --servicenode=<span class="variable">$serinfo</span> --writeconf /dev/md4</span><br></pre></td></tr></table></figure>

<h4 id="EC"><a href="#EC" class="headerlink" title="EC"></a>EC</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">$ lctl --device <span class="variable">$&#123;obdfilter_name&#125;</span>_osc cleanup</span><br><span class="line">$ lctl --device <span class="variable">$&#123;obdfilter_name&#125;</span>_osc detach</span><br><span class="line">$ lctl attach echo_client ec ec_uuid</span><br><span class="line">$ lctl --device ec create 1 | awk <span class="string">&#x27;/object id/ &#123;print $6&#125;&#x27;</span></span><br><span class="line"><span class="comment">## Get the id</span></span><br><span class="line">$ lctl --device ec getattr <span class="variable">$id</span></span><br><span class="line">$ lctl --device ec getattr <span class="variable">$id</span></span><br><span class="line">$ lctl --device ec</span><br><span class="line">$ lctl --device ec destroy <span class="variable">$id</span> 1</span><br><span class="line">$ lctl --device ec cleanup</span><br><span class="line">$ lctl --device ec detach</span><br><span class="line"></span><br><span class="line">$ lfs setstripe -E eof -c 4 -E eof -L erasure_code -ec_data_count 4 -ec_parity_count 2 file</span><br><span class="line">$ lfs mirror resync file</span><br></pre></td></tr></table></figure>

<h4 id="PFL-Progressive-file-layouts"><a href="#PFL-Progressive-file-layouts" class="headerlink" title="PFL Progressive file layouts"></a><a target="_blank" rel="noopener" href="https://docs.aws.amazon.com/fsx/latest/LustreGuide/performance.html">PFL Progressive file layouts</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ lfs setstripe -E 100M -c 1 -E 10G -c 8 -E 100G -c 16 -E -1 -c 32 /mountname/directory</span><br><span class="line"><span class="comment">#The first component (-E 100M -c 1) indicates a stripe count value of 1 for files up to 100MiB in size.</span></span><br><span class="line"><span class="comment">#The second component (-E 10G -c 8) indicates a stripe count of 8 for files up to 10GiB in size.</span></span><br><span class="line"><span class="comment">#The third component (-E 100G -c 16) indicates a stripe count of 16 for files up to 100GiB in size.</span></span><br><span class="line"><span class="comment">#The fourth component (-E -1 -c 32) indicates a stripe count of 32 for files larger than 100GiB.</span></span><br><span class="line"></span><br><span class="line">$ lfs getstripe /mountname/directory</span><br></pre></td></tr></table></figure>

<h3 id="Client-CLIENT"><a href="#Client-CLIENT" class="headerlink" title="Client CLIENT"></a>Client CLIENT</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#skip the deactivate OST</span><br><span class="line">client# mount -o exclude=testfs-OST0000 -t lustre \</span><br><span class="line">           uml1:/testfs /mnt/testfs</span><br><span class="line">            client# lctl get_param lov.testfs-clilov-*.target_obd</span><br><span class="line"></span><br><span class="line">client $ lctl get_param -n llite.*.client_type</span><br><span class="line">local client</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.fsname-OST0000-osc*.resend_count</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.resend_count=10</span><br></pre></td></tr></table></figure>

<h4 id="monitor-1"><a href="#monitor-1" class="headerlink" title="monitor"></a>monitor</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br></pre></td><td class="code"><pre><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">osc.teslustre1-OST0000-osc-ffff9a2736998000.rpc_stats=</span><br><span class="line">snapshot_time:         1647398762.213066047 (secs.nsecs)</span><br><span class="line">read RPCs in flight:  0</span><br><span class="line">write RPCs in flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending read pages:   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                      41   0   0   |         41   1   1</span><br><span class="line">2:                       0   0   0   |         10   0   1</span><br><span class="line">4:                       7   0   1   |         18   0   1</span><br><span class="line">8:                       1   0   1   |         48   1   3</span><br><span class="line">16:                      3   0   1   |         40   1   4</span><br><span class="line">32:                     10   0   1   |         31   0   5</span><br><span class="line">64:                      3   0   1   |         24   0   5</span><br><span class="line">128:                    17   0   1   |         70   1   7</span><br><span class="line">256:                  4279  98 100   |       3343  92 100</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.read_ahead_stats</span><br><span class="line">llite.llustre1-ffff9a2736998000.read_ahead_stats=</span><br><span class="line">snapshot_time             1647398871.147899109 secs.nsecs</span><br><span class="line">hits                      857597 samples [pages]</span><br><span class="line">misses                    664 samples [pages]</span><br><span class="line">failed grab_cache_page    10319 samples [pages]</span><br><span class="line">read but discarded        82760 samples [pages]</span><br><span class="line">zero size window          468754 samples [pages]</span><br><span class="line">read-ahead to EOF         242 samples [pages]</span><br><span class="line">hit max r-a issue         965 samples [pages]</span><br><span class="line">failed to reach end       11560 samples [pages]</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.*read*</span><br><span class="line">llite.llustre1-ffff9a2736998000.fast_read=1</span><br><span class="line">llite.llustre1-ffff9a2736998000.max_read_ahead_mb=64</span><br><span class="line">llite.llustre1-ffff9a2736998000.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.llustre1-ffff9a2736998000.max_read_ahead_whole_mb=64</span><br><span class="line">llite.llustre1-ffff9a2736998000.read_ahead_stats=</span><br><span class="line">snapshot_time             1647398887.020011954 secs.nsecs</span><br><span class="line">hits                      857597 samples [pages]</span><br><span class="line">misses                    664 samples [pages]</span><br><span class="line">failed grab_cache_page    10319 samples [pages]</span><br><span class="line">read but discarded        82760 samples [pages]</span><br><span class="line">zero size window          468754 samples [pages]</span><br><span class="line">read-ahead to EOF         242 samples [pages]</span><br><span class="line">hit max r-a issue         965 samples [pages]</span><br><span class="line">failed to reach end       11560 samples [pages]</span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*.stats</span><br><span class="line"></span><br><span class="line">osc.teslustre1-OST0000-osc-ffff9a2736998000.stats=</span><br><span class="line">snapshot_time             1647398966.425013406 secs.nsecs</span><br><span class="line">req_waittime              10027 samples [usec] 23 5567567 76549428 162550984799808</span><br><span class="line">req_active                10027 samples [reqs] 1 11 36957 214049</span><br><span class="line">ldlm_glimpse_enqueue      796 samples [reqs] 1 1 796 796</span><br><span class="line">ldlm_extent_enqueue       397 samples [reqs] 1 1 397 397</span><br><span class="line">read_bytes                4361 samples [bytes] 0 1048576 3514739172 3636771225513784</span><br><span class="line">write_bytes               3625 samples [bytes] 190 1048576 3514628763 3636770234184707</span><br><span class="line">ost_setattr               435 samples [usec] 40 2063486 2273570 4258904750384</span><br><span class="line">ost_read                  4361 samples [usec] 74 134260 26437092 465273858862</span><br><span class="line">ost_write                 3625 samples [usec] 69 5567567 47628317 157826643363867</span><br><span class="line">ost_connect               1 samples [usec] 3793 3793 3793 14386849</span><br><span class="line">ldlm_cancel               402 samples [usec] 51 5246 86898 115121580</span><br><span class="line">obd_ping                  10 samples [usec] 82 202 1243 168281</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n mdc.*MDT0000*.blocksize</span><br><span class="line">4096</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.active</span><br><span class="line">mdc.fsname-MDT0000-mdc-ffff8dab2cce8800.active=1</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.state</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.stats | grep -Ei &#x27;ost_read|ldlm_glimpse&#x27;</span><br><span class="line">$ lctl get_param mdc.*MDT0000*.timeouts</span><br><span class="line">mdc.fsname-MDT0000-mdc-ffff8dab2cce8800.timeouts=</span><br><span class="line">last reply : 1619838998, 0s ago</span><br><span class="line">network    : cur  50  worst  50 (at 1619704491, 134507s ago)   1   1   1   1</span><br><span class="line">portal 12  : cur  50  worst  50 (at 1619704491, 134507s ago)  50  50  50  50</span><br><span class="line">portal 17  : cur  50  worst  50 (at 1619708441, 130557s ago)  50  50   0  50</span><br><span class="line">portal 23  : cur  50  worst  50 (at 1619769781, 69217s ago)  50  50  50   0</span><br><span class="line">portal 30  : cur  50  worst  50 (at 1619769796, 69202s ago)  50   0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.state</span><br><span class="line">$ lctl get_param osc.*OST0000*.stats</span><br><span class="line">$ lctl get_param osc.*OST0000*.timeouts</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.timeouts=</span><br><span class="line">last reply : 1619838981, 36s ago</span><br><span class="line">network    : cur  50  worst  50 (at 1619704492, 134525s ago)   1   1   1   1</span><br><span class="line">portal 28  : cur  50  worst  50 (at 1619704491, 134526s ago)  50  50  50  50</span><br><span class="line">portal 7   : cur  50  worst  50 (at 1619704492, 134525s ago)  50  50   0   0</span><br><span class="line">portal 6   : cur  50  worst  50 (at 1619769796, 69221s ago)  50   0   0  50</span><br><span class="line">portal 17  : cur  50  worst  50 (at 1619770886, 68131s ago)  50   0   0   0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.unstable_stats</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.unstable_stats=</span><br><span class="line">unstable_pages:                    0</span><br><span class="line">unstable_mb:                       0</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*OST0000*.blocksize</span><br><span class="line">1048576</span><br><span class="line"></span><br><span class="line"># Client-Based I/O Extent Size Survey</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">disabled</span><br><span class="line"> write anything to this file to activate, then &#x27;0&#x27; or &#x27;disable&#x27; to deactivate</span><br><span class="line"></span><br><span class="line"># Monitor client all</span><br><span class="line">client $ lctl get_param llite.*.stats</span><br><span class="line">clinet $ lctl get_param llite.*.read_ahead_stat</span><br><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">osc.teslustre1-OST0000-osc-ffff9a2736998000.rpc_stats=</span><br><span class="line">snapshot_time:         1647398762.213066047 (secs.nsecs)</span><br><span class="line">read RPCs in flight:  0</span><br><span class="line">write RPCs in flight: 0</span><br><span class="line">pending write pages:  0</span><br><span class="line">pending read pages:   0</span><br><span class="line"></span><br><span class="line">                        read                    write</span><br><span class="line">pages per rpc         rpcs   % cum % |       rpcs   % cum %</span><br><span class="line">1:                      41   0   0   |         41   1   1</span><br><span class="line">2:                       0   0   0   |         10   0   1</span><br><span class="line">4:                       7   0   1   |         18   0   1</span><br><span class="line">8:                       1   0   1   |         48   1   3</span><br><span class="line">16:                      3   0   1   |         40   1   4</span><br><span class="line">32:                     10   0   1   |         31   0   5</span><br><span class="line">64:                      3   0   1   |         24   0   5</span><br><span class="line">128:                    17   0   1   |         70   1   7</span><br><span class="line">256:                  4279  98 100   |       3343  92 100</span><br><span class="line"></span><br><span class="line">#The file can be cleared and enabled by issuing the following command:</span><br><span class="line">$ lctl set_param llite.testfs-*.extents_stats=1</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=1</span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838099.035887547 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838110.284058497 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line"></span><br><span class="line">$ dd if=/dev/urandom of=test_5 bs=4k count=100</span><br><span class="line">100+0 records in</span><br><span class="line">100+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.0048456 s, 84.5 MB/s</span><br><span class="line"></span><br><span class="line">$ ls -l test_5</span><br><span class="line">-rw-r--r-- 1 root root 409600 May  1 11:02 test_5</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838165.554498620 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100  100  100</span><br><span class="line">$ dd if=test_5 of=test_6 bs=16k count=25</span><br><span class="line">25+0 records in</span><br><span class="line">25+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.00201437 s, 203 MB/s</span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838225.522203239 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100   80   80</span><br><span class="line">   8K -   16K :              0    0    0  |              0    0   80</span><br><span class="line">  16K -   32K :              0    0    0  |             25   20  100</span><br><span class="line">$ dd if=test_5 of=test_7 bs=16k count=25 iflag=direct</span><br><span class="line">25+0 records in</span><br><span class="line">25+0 records out</span><br><span class="line">409600 bytes (410 kB) copied, 0.010213 s, 40.1 MB/s</span><br><span class="line"></span><br><span class="line">$ lctl get_param llite.fsname-*.extents_stats</span><br><span class="line">llite.fsname-ffff8dab2cce8800.extents_stats=</span><br><span class="line">snapshot_time:         1619838404.667431860 (secs.nsecs)</span><br><span class="line">                               read       |                write</span><br><span class="line">      extents            calls    % cum%  |          calls    % cum%</span><br><span class="line">   0K -    4K :              0    0    0  |              0    0    0</span><br><span class="line">   4K -    8K :              0    0    0  |            100   66   66</span><br><span class="line">   8K -   16K :              0    0    0  |              0    0   66</span><br><span class="line">  16K -   32K :             25  100  100  |             50   33  100</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*0000-osc-*.stats</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.stats=</span><br><span class="line">snapshot_time             1619795318.676450713 secs.nsecs</span><br><span class="line">req_waittime              1321 samples [usec] 282 81340 2770704 62526834430</span><br><span class="line">req_active                1321 samples [reqs] 1 27 2605 27949</span><br><span class="line">read_bytes                10 samples [bytes] 1048576 1048576 10485760 10995116277760</span><br><span class="line">write_bytes               85 samples [bytes] 32768 1048576 87162880 91210072981504</span><br><span class="line">ost_read                  10 samples [usec] 5584 15595 111076 1326778432</span><br><span class="line">ost_write                 85 samples [usec] 3899 81340 1826891 60616504085</span><br><span class="line">ost_connect               1 samples [usec] 2156 2156 2156 4648336</span><br><span class="line">ost_statfs                16 samples [usec] 282 753 8835 5208451</span><br><span class="line">ldlm_cancel               3 samples [usec] 547 1141 2390 2093894</span><br><span class="line">obd_ping                  1198 samples [usec] 319 1351 815234 569338326</span><br><span class="line"></span><br><span class="line">$ lctl get_param mdc.*.import</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;state: FULL&quot;</span><br><span class="line">$ lctl get_param mdc.*.import | grep &quot;connect_flags&quot;</span><br><span class="line">or</span><br><span class="line">$ lctl get_param  mdc.*.connect_flags | grep early_lock_cancel</span><br><span class="line"></span><br><span class="line">$ lctl get_param -n llite.*.sbi_flags</span><br><span class="line">checksum            acl       lru_resize lazy_statfs 64bit_hash agl verbose layout xattr_cache fast_read file_secctx</span><br><span class="line">checksum user_xattr acl flock lru_resize lazy_statfs 64bit_hash agl verbose layout xattr_cache fast_read file_secctx</span><br><span class="line"></span><br><span class="line">$ lfs setstripe -S 65536 /lfs</span><br><span class="line"></span><br><span class="line">#Flush all of the metadata client (mdc) locks on this node</span><br><span class="line">$ lctl set_param ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line"></span><br><span class="line">$ lctl get_param osc.*.grant_shrink_interval</span><br><span class="line">osc.lfs-OST0000-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line">osc.lfs-OST0001-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line">osc.lfs-OST0002-osc-ffff8dab37559000.grant_shrink_interval=1200</span><br><span class="line"></span><br><span class="line">#This example reports the amount of space this client has reserved for writeback cache with each OST</span><br><span class="line">$ lctl get_param osc.*.cur_grant_bytes</span><br><span class="line">osc.fsname-OST0000-osc-ffff8dab2cce8800.cur_grant_bytes=3407872</span><br><span class="line"></span><br><span class="line">#calculator the max_cur_granted</span><br><span class="line">undirty 34209792 + grant_chunk 3407872 = max_cur_granted 37617664</span><br><span class="line"></span><br><span class="line">#grant_chunk</span><br><span class="line">$  lctl get_param osc.*.import  | grep -Ei &#x27;max_brw_size|grant_extent_tax&#x27;</span><br><span class="line">       max_brw_size: 1048576</span><br><span class="line">       grant_extent_tax: 655360</span><br><span class="line">grant_chunk = $(((1048576+655360)*2)) = 3407872 bytes</span><br><span class="line"></span><br><span class="line">#undirty</span><br><span class="line">nrpages:256 rpc_in_flight:8</span><br><span class="line">rpc_in_flight++</span><br><span class="line">nrpegs 256 x  rpc_in_flight 9 = nrpages 2304</span><br><span class="line">max_dirty_mb:32 MB</span><br><span class="line">MB to pages = dirty_max_pages: 8192</span><br><span class="line">if dirty_max_pages 8192 &gt; nrpages 2304; nrpages = dirty_max_pages:8192</span><br><span class="line">nrpages 8192 x 4096 = undirty 33554432 bytes</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lctl grant_max_extent_size 1073741824</span><br><span class="line">lctl grant_max_extent_size = max_extent_size/4096 to pages:262144</span><br><span class="line">(nrpages 8192 + max_extent_pages 262144 -1) / max_extent_pages 262144 = nrextents = 1</span><br><span class="line">grant_extent_tax: 655360</span><br><span class="line">undirty 34209792 bytes = undirty bytes: 33554432 + nrextents 1 * grant_extent_tax (bytes):655360</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client $ lctl get_param osc.*.grant_shrink</span><br><span class="line">client $ lctl set_param osc.*.grant_shrink=0</span><br><span class="line">#replace</span><br><span class="line">$ bpftrace -e &#x27;k:osc_should_shrink_grant &#123; override(0); &#125;&#x27; --unsafe</span><br><span class="line"></span><br><span class="line">### clear cache info</span><br><span class="line">client $ lctl set_param osc.*-osc*.rpc_stats 0</span><br><span class="line">client $ lctl set_param llite.*.read_ahead_stats 0</span><br><span class="line">client $ lctl set_param -n llite.*.max_cached_mb 128 ## 128MB</span><br><span class="line">#randomly read 1000 of 32K chunks from file large than 1G size</span><br><span class="line">find /fsname -type f -size +1G | head -n 128 | while read line; do dd if=$line of=/dev/null bs=128K count=1000 skip=$(shuf -i 2000-65000 -n 1) &amp; done</span><br><span class="line"></span><br><span class="line">#check status</span><br><span class="line">client $ lctl get_param osc.*-osc*.rpc_stats</span><br><span class="line"></span><br><span class="line">client $ lctl get_param llite.*.max_cached_mb</span><br><span class="line">llite.fsname-ffff8dab2cce8800.max_cached_mb=</span><br><span class="line">users: 16</span><br><span class="line">max_cached_mb: 128  ##128MB=32768 pages</span><br><span class="line">used_mb: 128</span><br><span class="line">unused_mb: 0</span><br><span class="line">reclaim_count: 36</span><br><span class="line"></span><br><span class="line">#128M</span><br><span class="line">client lctl set_param llite.*.read_ahead_stats=c</span><br><span class="line">llite.testfs-ffff8daf56159000.read_ahead_stats=c</span><br><span class="line"></span><br><span class="line">llite.testfs-ffff8dcc59a15800.fast_read=1</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_async_active=12</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_async_file_threshold_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_range_kb=1024</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_per_file_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.max_read_ahead_whole_mb=64</span><br><span class="line">llite.testfs-ffff8dcc59a15800.read_ahead_stats=</span><br><span class="line">snapshot_time             1642131829.087651284 secs.nsecs</span><br><span class="line">hits                      2887533 samples [pages]</span><br><span class="line">misses                    627508 samples [pages] &lt;--------------------</span><br><span class="line">readpage not consecutive  1255012 samples [pages]                    |</span><br><span class="line">zero size window          2887886 samples [pages]                    |</span><br><span class="line">failed to fast read       627508 samples [pages]                     |</span><br><span class="line">                                                                     |</span><br><span class="line">lctl get_param llite.testfs*.*read*                                  |</span><br><span class="line">llite.testfs-ffff8db8208be800.fast_read=1                            |</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_mb=64                   |-------------dff misses samples in the same case</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_per_file_mb=64          |</span><br><span class="line">llite.testfs-ffff8db8208be800.max_read_ahead_whole_mb=64             |</span><br><span class="line">llite.testfs-ffff8db8208be800.read_ahead_stats=                      |</span><br><span class="line">snapshot_time             1642130844.639513080 secs.nsecs            |</span><br><span class="line">hits                      19433183 samples [pages]                   |</span><br><span class="line">misses                    5 samples [pages] &lt;-------------------------</span><br><span class="line">readpage not consecutive  4 samples [pages]</span><br><span class="line">zero size window          12 samples [pages]</span><br><span class="line"></span><br><span class="line">failed to reach end       76012 samples [pages]</span><br><span class="line"></span><br><span class="line">Hits</span><br><span class="line">Misses</span><br><span class="line">Readpage not consecutive</span><br><span class="line">Miss inside window</span><br><span class="line">Failed grab_cache_page</span><br><span class="line">Failed lock match</span><br><span class="line">Read but discarded</span><br><span class="line">Zero length file</span><br><span class="line">Zero size window</span><br><span class="line">Read-ahead to EOF</span><br><span class="line">Hit max r-a issue</span><br><span class="line">Wrong page from</span><br><span class="line">grab_cache_page</span><br><span class="line"></span><br><span class="line">$ size=1; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=2; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=4; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=8; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">$ size=16; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2;&#125;&#x27;</span><br><span class="line">36</span><br><span class="line">if size 4 * PAGE_SIZE 4096 = 32768 &lt; rsize 131072(means dd bs=128k); means Small 16384 read IO 36</span><br><span class="line"></span><br><span class="line">1K 2K 4K ... 256 K</span><br><span class="line"></span><br><span class="line">## for example, just print first line</span><br><span class="line">$ size=1; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2; exit&#125;&#x27;</span><br><span class="line">209</span><br><span class="line">the size=1 * PAGE_SIZE 4096 = 4096 &lt; rsize 131072 (means dd bs=128k); means Small 4096 read IO 209</span><br><span class="line">$ size=8; lctl get_param -n osc.*.rpc_stats | awk &#x27;($1 == &quot;&#x27;$size&#x27;:&quot; &amp;&amp; $2!=0) &#123;print $2; exit&#125;&#x27;</span><br><span class="line">14</span><br><span class="line">the size=8 * PAGE_SIZE 4096 = 32768 &lt; rsize 131072 (means dd bs=128k); means Small 32768 read IO 14</span><br><span class="line"></span><br><span class="line">$ lctl set_param osc.*-osc*.rpc_stats 0</span><br><span class="line">$ lctl set_param llite.*.read_ahead_stats 0</span><br><span class="line">$ lctl set_param -n llite.*.max_cached_mb 64 #default 64</span><br><span class="line"></span><br><span class="line">#########################-test- monitor tools##################</span><br><span class="line"># setstripe</span><br><span class="line">$ lfs setstripe -S 4000M -c 50 /mnt/striped</span><br><span class="line"></span><br><span class="line">#Monitor</span><br><span class="line">$ lctl get_param  osc.*OST0000*.stats</span><br><span class="line">osc.$FSNAME-OST0000-osc-ffff8dab37559000.stats=</span><br><span class="line">snapshot_time             1619689024.314392020 secs.nsecs</span><br><span class="line">req_waittime              128 samples [usec] 138 2599 61091 35024171</span><br><span class="line">req_active                128 samples [reqs] 1 1 128 128</span><br><span class="line">ost_connect               1 samples [usec] 2599 2599 2599 6754801</span><br><span class="line">ost_statfs                3 samples [usec] 459 512 1433 686269</span><br><span class="line">obd_ping                  124 samples [usec] 138 575 57059 27583101</span><br><span class="line">$ lctl get_param mdc.lfs*.stats</span><br><span class="line">mdc.lfs-MDT0000-mdc-ffff8dab3afe6000.stats=</span><br><span class="line">snapshot_time             1619689557.206806185 secs.nsecs</span><br><span class="line">req_waittime              112 samples [usec] 71 1525 18376 7048052</span><br><span class="line">req_active                112 samples [reqs] 1 1 112 112</span><br><span class="line">mds_getattr               1 samples [usec] 86 86 86 7396</span><br><span class="line">mds_connect               1 samples [usec] 734 734 734 538756</span><br><span class="line">mds_get_root              1 samples [usec] 101 101 101 10201</span><br><span class="line">mds_statfs                3 samples [usec] 71 145 293 31995</span><br><span class="line">ldlm_cancel               1 samples [usec] 92 92 92 8464</span><br><span class="line">obd_ping                  103 samples [usec] 93 1525 16863 6429731</span><br><span class="line"></span><br><span class="line">#iostat </span><br><span class="line">OSS $ llobdstat $fsname-OST0000 2</span><br><span class="line">Read: 9.56529e+13, Write: 2.76168e+13, create/destroy: 1832/171649, stat: 1.1192e+07, punch: 52249</span><br><span class="line">[NOTE: cx: create, dx: destroy, st: statfs, pu: punch ]</span><br><span class="line"></span><br><span class="line">Timestamp   Read-delta  ReadRate  Write-delta  WriteRate</span><br><span class="line">--------------------------------------------------------</span><br><span class="line">1637629257    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:13</span><br><span class="line">1637629259    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:14</span><br><span class="line">1637629261   16.00MB    8.00MB/s     0.00MB    0.00MB/s st:14</span><br><span class="line">1637629263    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:12</span><br><span class="line">1637629265    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:16</span><br><span class="line">1637629267    0.00MB    0.00MB/s     0.00MB    0.00MB/s st:16</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">OSS $ llstat -i 1 ost</span><br><span class="line">ost.OSS.ost.stats</span><br><span class="line"> @ 1692956281.779740954</span><br><span class="line">Name                    Count  Rate   #Events   Unit        last    min      avg        max   stddev</span><br><span class="line">start_time              0      0      1692348937</span><br><span class="line">elapsed_time            1      1      607344    </span><br><span class="line">req_waittime            66     66     517862    [usec]        42      5       37      24089      177</span><br><span class="line">req_qdepth              66     66     517862    [reqs]         0      0        0          2        0</span><br><span class="line">req_active              66     66     517862    [reqs]         1      1        1          4        0</span><br><span class="line">req_timeout             66     66     517862    [sec]         50     50       50         50        0</span><br><span class="line">reqbuf_avail            132    132    1038968   [bufs]        63     63       63         64        0</span><br><span class="line">ldlm_glimpse_enqueue    0      0      10003     [reqs]         0      1        1          1        0</span><br><span class="line">ldlm_extent_enqueue     64     64     269583    [reqs]         1      1        1          1        0</span><br><span class="line">ost_setattr             0      0      10000     [usec]         0     20       45      26939      320</span><br><span class="line">ost_create              2      2      8765      [usec]      1435      6     1356     316505     4539</span><br><span class="line">ost_destroy             0      0      2         [usec]         0   4235   397885     791535   556705</span><br><span class="line">ost_get_info            0      0      7         [usec]         0     57     9798      29310    11485</span><br><span class="line">ost_connect             0      0      218370    [usec]         0     19      164       6337      178</span><br><span class="line">ost_disconnect          0      0      50        [usec]         0     44      162        538       93</span><br><span class="line">ost_set_info            0      0      12        [usec]         0     24       39         53        9</span><br><span class="line">ost_quotactl            0      0      204       [usec]         0      8       96       3757      297</span><br><span class="line">obd_ping                0      0      866       [usec]         0      3       18         75        8</span><br><span class="line"></span><br><span class="line">MDS $ llstat -i 1 mds</span><br><span class="line"></span><br><span class="line">napshot_time             1637629207.425948584</span><br><span class="line">req_waittime              33338215</span><br><span class="line">req_qdepth                33338215</span><br><span class="line">req_active                33338215</span><br><span class="line">req_timeout               33338215</span><br><span class="line">reqbuf_avail              67080504</span><br><span class="line">ldlm_glimpse_enqueue      13497741</span><br><span class="line">ldlm_extent_enqueue       787370</span><br><span class="line">ost_setattr               388196</span><br><span class="line">ost_create                6643</span><br><span class="line">ost_destroy               690443</span><br><span class="line">ost_get_info              3191</span><br><span class="line">ost_connect               1721355</span><br><span class="line">ost_disconnect            1714914</span><br><span class="line">ost_sync                  79</span><br><span class="line">ost_set_info              5901</span><br><span class="line">ost_quotactl              2214221</span><br><span class="line">obd_ping                  12308161</span><br><span class="line">/proc/fs/lfs/ost/OSS/ost/stats @ 1637629208.426168645</span><br><span class="line">Name                      Cur.Count  Cur.Rate   #Events   Unit           last        min          avg        max    stddev</span><br><span class="line">req_waittime              6          6          33338221  [usec]          178          4       683.36    1846733  10378.79</span><br><span class="line">req_qdepth                6          6          33338221  [reqs]            0          0         0.02       2958      3.50</span><br><span class="line">req_active                6          6          33338221  [reqs]            8          1         2.20         79      7.54</span><br><span class="line">req_timeout               6          6          33338221  [sec]           300         50        50.01         92      0.52</span><br><span class="line">reqbuf_avail              12         12         67080516  [bufs]          739          0        62.16        117      2.89</span><br><span class="line">ldlm_glimpse_enqueue      2          2          13497743  [reqs]            2          1         1.00          1      0.00</span><br><span class="line">ldlm_extent_enqueue       0          0          787370    [reqs]            0          1         1.00          1      0.00</span><br><span class="line">ost_setattr               0          0          388196    [usec]            0          2      9493.19   78450702 241902.62</span><br><span class="line">ost_create                0          0          6643      [usec]            0     150442    994329.00   34318893 1112213.09</span><br><span class="line">ost_destroy               0          0          690443    [usec]            0         74     29416.49   21468510 104767.69</span><br><span class="line">ost_get_info              0          0          3191      [usec]            0          9      4283.79     447398  21558.91</span><br><span class="line">ost_connect               0          0          1721355   [usec]            0          7      3217.33   20717452  29239.94</span><br><span class="line">ost_disconnect            0          0          1714914   [usec]            0         34      5274.31   13105121  33557.39</span><br><span class="line">ost_sync                  0          0          79        [usec]            0          9    555830.46    1691862 476579.36</span><br><span class="line">ost_set_info              0          0          5901      [usec]            0          9        18.16         65      4.85</span><br><span class="line">ost_quotactl              0          0          2214221   [usec]            0          8       798.77    7068501  10658.14</span><br><span class="line">obd_ping                  4          4          12308165  [usec]           39          3         9.30        304      3.42</span><br><span class="line"></span><br><span class="line">$ llstat -i2 lwp/lustre1-MDT0001-lwp-OST0000/stats</span><br><span class="line">/usr/bin/llstat: lwp.lustre1-MDT0001-lwp-OST0000.stats</span><br><span class="line"> at Fri Aug 25 17:34:16 CST 2023</span><br><span class="line"> on 10.53.18.213@tcp</span><br><span class="line">snapshot_time             1692956056.581368615</span><br><span class="line">start_time                1692856082.705177154</span><br><span class="line">elapsed_time              99973.876191461</span><br><span class="line">req_waittime              3986</span><br><span class="line">req_active                3989</span><br><span class="line">mds_connect               3</span><br><span class="line">obd_ping                  3983</span><br><span class="line"></span><br><span class="line">lwp.lustre1-MDT0001-lwp-OST0000.stats</span><br><span class="line"> @ 1692956058.585759109</span><br><span class="line">Name                    Count  Rate   #Events   Unit        last    min      avg        max   stddev</span><br><span class="line">start_time              0      0      1692856082</span><br><span class="line">elapsed_time            2      1      99975     </span><br><span class="line">req_waittime            0      0      3986      [usec]         0    226      444      15427      315</span><br><span class="line">req_active              0      0      3989      [reqs]         0      1        1          1        0</span><br><span class="line">mds_connect             0      0      3         [usec]         0    257     1016       2425     1220</span><br><span class="line">obd_ping                0      0      3983      [usec]         0    226      444      15427      314</span><br></pre></td></tr></table></figure>
<p>This shows that the three files in lost+found have decimal object IDs - 690670, 614725, and 533088, respectively. The object sequence number (formerly object group) is 0 for all current OST objects.  </p>
<p>The MDT parent inode FIDs are hexadecimal numbers of the form sequence:oid:idx. Since the sequence number is below 0x100000000 in all these cases, the FIDs are in the legacy Inode and Generation In FID (IGIF) namespace and are mapped directly to the MDT inode &#x3D; seq and generation &#x3D; oid values; the MDT inodes are 0x751c5, 0x18d11, and 0x21417 respectively. For objects with MDT parent sequence numbers above 0x200000000, this indicates that the FID needs to be mapped via the MDT Object Index (OI) file on the MDT to determine the internal inode number.   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">root@oss1<span class="comment"># cd /mnt/ost/lost+found</span></span><br><span class="line">root@oss1<span class="comment"># ll_decode_filter_fid #12345[4,5,8]</span></span><br><span class="line"><span class="comment">#123454: objid=690670 seq=0 parent=[0x751c5:0xfce6e605:0x0]</span></span><br><span class="line"><span class="comment">#123455: objid=614725 seq=0 parent=[0x18d11:0xebba84eb:0x1]</span></span><br><span class="line"><span class="comment">#123458: objid=533088 seq=0 parent=[0x21417:0x19734d61:0x0]</span></span><br></pre></td></tr></table></figure>
<p>ll_recover_lost_found_objs<br>The ll_recover_lost_found_objs utility was used to help recover Lustre OST objects (file data) from the lost+found directory of an OST and return them to their correct locations based on information stored in the trusted.fid extended attribute stored on every OST object containing data.  </p>
<ul>
<li><p>stripe</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#stripe info</span><br><span class="line">$ lctl get_param  lov.fsname-clilov-\*.stripe*</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripecount=1</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripeoffset=-1</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripesize=1048576</span><br><span class="line">lov.fsname-clilov-ffff8dab2cce8800.stripetype=1</span><br></pre></td></tr></table></figure>
</li>
<li><p>cancel_lru_locks</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ lctl set_param -n ldlm.namespaces.*osc*.lru_size=clear</span><br><span class="line">$ lctl set_param -n ldlm.namespaces.*mdc*.lru_size=clear</span><br><span class="line">$ lctl get_param ldlm.namespaces.*.lock_unused_count</span><br><span class="line">ldlm.namespaces.MGC192.168.0.1@tcp.lock_unused_count=0</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffffa05245a8c800.lock_unused_count=488</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffffa05245a8c800.lock_unused_count=1</span><br><span class="line">ldlm.namespaces.fsname-OST0001-osc-ffffa05245a8c800.lock_unused_count=0</span><br><span class="line"></span><br><span class="line">$ lctl get_param ldlm.namespaces.*mdc-*.lru_size</span><br><span class="line">ldlm.namespaces.fsname-MDT0000-mdc-ffff8dab2cce8800.lru_size=1</span><br><span class="line">$ lctl get_param ldlm.namespaces.*osc-*.lru_size</span><br><span class="line">ldlm.namespaces.fsname-OST0000-osc-ffff8dab2cce8800.lru_size=0</span><br></pre></td></tr></table></figure>
</li>
<li><p>readahead<br>disable readahead and set stripe count &#x3D; 1, compare the read performance, if not aligned, the result will slow than disabled readahead result.  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#disable the client read ahead</span></span><br><span class="line">$ lctl set_param llite.*.max_read_ahead_mb=0</span><br><span class="line"></span><br><span class="line"><span class="comment">#limit ldlm threads, ldlm threads will exhaust all CPUs resources like LU-7330</span></span><br><span class="line">options ptlrpc ldlm_num_threads=16</span><br></pre></td></tr></table></figure></li>
</ul>
<h4 id="complie-client"><a href="#complie-client" class="headerlink" title="complie client"></a>complie client</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br></pre></td><td class="code"><pre><span class="line">#2.12.x build must export openmpi env</span><br><span class="line">$ export PATH=/usr/lib64/openmpi/bin/:$PATH</span><br><span class="line">$ export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib/:$LD_LIBRARY_PATH</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### o2ib loading failed</span><br><span class="line">[Wed Aug 21 10:56:43 2024] Lustre: Lustre: Build Version: 2.15.4</span><br><span class="line">[Wed Aug 21 10:56:43 2024] ko2iblnd: disagrees about version of symbol __ib_alloc_pd</span><br><span class="line">[Wed Aug 21 10:56:43 2024] ko2iblnd: Unknown symbol __ib_alloc_pd (err -22)</span><br><span class="line">[Wed Aug 21 10:56:43 2024] ko2iblnd: disagrees about version of symbol rdma_resolve_addr</span><br><span class="line">[Wed Aug 21 10:56:43 2024] ko2iblnd: Unknown symbol rdma_resolve_addr (err -22)</span><br><span class="line"></span><br><span class="line">failed $ modinfo ib_cm</span><br><span class="line">filename:       /lib/modules/4.18.0-513.18.1.el8_lustre.x86_64/kernel/drivers/infiniband/core/ib_cm.ko.xz</span><br><span class="line"></span><br><span class="line">worked $ modinfo ib_cm</span><br><span class="line">filename:       /lib/modules/4.18.0-513.18.1.el8_lustre.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_cm.ko</span><br><span class="line"></span><br><span class="line">worked $ rpm -qf /lib/modules/4.18.0-513.18.1.el8_lustre.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_cm.ko</span><br><span class="line">mlnx-ofa_kernel-modules-23.10-OFED.23.10.2.1.3.1.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64</span><br><span class="line"></span><br><span class="line">failed $ ls -l /lib/modules/4.18.0-513.18.1.el8_lustre.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_cm.ko </span><br><span class="line">ls: cannot access &#x27;/lib/modules/4.18.0-513.18.1.el8_lustre.x86_64/extra/mlnx-ofa_kernel/drivers/infiniband/core/ib_cm.ko&#x27;: No such file or directory</span><br><span class="line"></span><br><span class="line">failed $ rpm -Uvh kmod-mlnx-ofa_kernel-23.10-OFED.23.10.2.1.3.1.rhel8u9.x86_64.rpm</span><br><span class="line">warning: kmod-mlnx-ofa_kernel-23.10-OFED.23.10.2.1.3.1.rhel8u9.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID e6d6a281: NOKEY</span><br><span class="line">Verifying...                          ################################# [100%]</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">	file /etc/depmod.d/zz01-mlnx-ofa_kernel-mlx_compat.conf from install of kmod-mlnx-ofa_kernel-23.10-OFED.23.10.2.1.3.1.rhel8u9.x86_64 conflicts with file from package mlnx-ofa_kernel-modules-23.10-OFED.23.10.3.2.2.1.kver.4.18.0_553.5.1.el8_lustre.x86_64.x86_64</span><br><span class="line"></span><br><span class="line">failed $  rpm -Uvh --force kmod-mlnx-ofa_kernel-23.10-OFED.23.10.2.1.3.1.rhel8u9.x86_64.rpm</span><br><span class="line">warning: kmod-mlnx-ofa_kernel-23.10-OFED.23.10.2.1.3.1.rhel8u9.x86_64.rpm: Header V4 RSA/SHA256 Signature, key ID e6d6a281: NOKEY</span><br><span class="line">Verifying...                          ################################# [100%]</span><br><span class="line">Preparing...                          ################################# [100%]</span><br><span class="line">Updating / installing...</span><br><span class="line">   1:kmod-mlnx-ofa_kernel-23.10-OFED.2################################# [100%]</span><br><span class="line"></span><br><span class="line">failed $ modinfo ib_cm</span><br><span class="line">filename:       /lib/modules/4.18.0-513.18.1.el8_lustre.x86_64/weak-updates/mlnx-ofa_kernel/drivers/infiniband/core/ib_cm.ko</span><br><span class="line"></span><br><span class="line">failed $ reboot </span><br><span class="line"></span><br><span class="line">#worked, but I&#x27;m not found the mlnx-ofa_kernel-modules-23.10-OFED.23.10.3.2.2.1.kver.4.18.0_553.5.1.el8_lustre.x86_64.x86_64.rpm from source</span><br><span class="line"></span><br><span class="line">#another is</span><br><span class="line">mlnx-ofa_kernel-devel-23.10: /usr/src/ofa_kernel/default is still a non-empty directory. Deleting in preparation for a symlink.</span><br><span class="line"></span><br><span class="line">$ ls -lhs /usr/src/ofa_kernel/default</span><br><span class="line">0 lrwxrwxrwx 1 root root 36 Aug 21 14:18 /usr/src/ofa_kernel/default -&gt; /etc/alternatives/ofa_kernel_headers</span><br><span class="line"></span><br><span class="line">$ ls -l /etc/alternatives/ofa_kernel_headers</span><br><span class="line">lrwxrwxrwx 1 root root 59 Aug  4 22:05 /etc/alternatives/ofa_kernel_headers -&gt; /usr/src/ofa_kernel/x86_64/4.18.0-553.5.1.el8_lustre.x86_64</span><br><span class="line"></span><br><span class="line">$ rpm -qf /usr/src/ofa_kernel/x86_64/4.18.0-553.5.1.el8_lustre.x86_64</span><br><span class="line">mlnx-ofa_kernel-devel-23.10-OFED.23.10.3.2.2.1.kver.4.18.0_553.5.1.el8_lustre.x86_64.x86_64</span><br><span class="line"></span><br><span class="line">#remove /usr/src/ofa_kernel/x86_64/4.18.0-553.5.1.el8_lustre.x86_64 and reinstall this rpm is not worked, the same error too.</span><br><span class="line"></span><br><span class="line">$ rm -rf /usr/src/ofa_kernel/x86_64/4.18.0-553.5.1.el8_lustre.x86_64 /lib/modules/4.18.0-553.5.1.el8_lustre.x86_64/extra/mlnx-ofa_kernel </span><br><span class="line">$ rm -f /etc/alternatives/ofa_kernel_headers /usr/src/ofa_kernel/default</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#if you has the openmpi error when you rpmbuild, find the lfs.spec and delete all openmpi info</span><br><span class="line">$ rpmbuild -bb -v --without servers lfs.spec</span><br><span class="line"></span><br><span class="line">$ rpm --eval %&#123;kernel_module_package_buildreqs&#125;</span><br><span class="line">					kernel-devel kernel-abi-whitelists redhat-rpm-config kernel-rpm-macros elfutils-libelf-devel kmod</span><br><span class="line"></span><br><span class="line">$ rpmbuild --rebuild --without servers  lfs-2.10.3-1.src.rpm</span><br><span class="line">$ rpmbuild  --rebuild --without servers --with-o2ib=/usr/src/ofa_kernel/default --with lnet-dlc  lfs-2.10.3-1.src.rpm</span><br><span class="line">$ rpmbuild --rebuild --without servers --without lfs-tests lfs-2.10.3-1.src.rpm</span><br><span class="line">$ rpmbuild --define &#x27;kversion 2.6.32-220.4.1.el6.x86_64&#x27; --define &#x27;kdir /usr/src/kernels/2.6.32-220.4.1.el6.x86_64/&#x27; --rebuild lfs-client-2.1.0-2.6.32_131.6.1.el6.x86_64_g9d71fe8.src.rpm</span><br><span class="line"></span><br><span class="line">#rebuild with o2ib</span><br><span class="line">#After you have install Mellanox EFOD and you need re-compile the lfs clinet</span><br><span class="line"></span><br><span class="line">#el9 install </span><br><span class="line">$ dnf install keyutils-libs-devel</span><br><span class="line"></span><br><span class="line">rebuild show &quot;If you still want to build Lustre for your OFED I/B stack, you need to install its devel headers RPM.&quot;</span><br><span class="line">$ ./configure --disable-server --with-o2ib=/usr/src/ofa_kernel/default #here is MOFED-devel path</span><br><span class="line"></span><br><span class="line">$ EXTRA_LNET_INCLUDE=&quot;-I/usr/src/ofa_kernel/default/include/ -include /usr/src/ofa_kernel/default/include/linux/compat-2.6.h&quot; ./configure --with-o2ib=/usr/src/ofa_kernel/default/</span><br><span class="line">$ EXTRA_LNET_INCLUDE=&quot;-I/usr/src/ofa_kernel/default/include/ -include /usr/src/ofa_kernel/default/include/linux/compat-2.6.h&quot; make rpms</span><br><span class="line"></span><br><span class="line">IB:</span><br><span class="line">$ ./configure --with-o2ib=/usr/src/ofa_kernel/default</span><br><span class="line"></span><br><span class="line">OPA:</span><br><span class="line">$ ./configure --with-o2ib=yes</span><br><span class="line"></span><br><span class="line">$ make rpms</span><br><span class="line"></span><br><span class="line"># centos 7.7</span><br><span class="line"># /root/rpmbuild/BUILD/lfs-2.10.8/lnet/klnds/o2iblnd/o2iblnd.h:69:27: fatal error: linux/pci-dma.h: No such file or directory</span><br><span class="line"></span><br><span class="line"># copy the file from the old kernel</span><br><span class="line">copy /usr/src/kernels/$&#123;the old version kernel&#125;/include/linux/pci-dma.h /usr/src/kernels/$&#123;the centos 7.7 kernel&#125;/include/linux/pci-dma.h</span><br><span class="line"></span><br><span class="line"># New stupid bug when you compile lfs 2.10.3-1, if you are not export $PATH with openmpi, the compile will failed.</span><br><span class="line">If you want it pass, I was clear /tmp/tmp.* rpmbuild not help I guess maybe the old config in some tmpfs path.</span><br><span class="line">after you reboot and re-export the env, the compile will be successful.</span><br><span class="line">#Is real the realease production ? `too stupid` bug. just waste my time to type these words.</span><br><span class="line">There is no test team in lfs develop team, All users was the test team except you are going to buy DDN.</span><br><span class="line"></span><br><span class="line"># from source</span><br><span class="line">$ ./configure --enable-client --disable-server --with-linux=/usr/src/kernels/$(uname -r) --with-linux-obj=/usr/src/kernels/$(uname -r);make rpms/deps</span><br><span class="line">$ ./configure --enable-client --disable-server --with-linux=/usr/src/kernels/linux-5.4.123 --with-linux-obj=/usr/src/kernels/linux-5.4.123;make rpms/deps</span><br><span class="line">$ cd $&#123;BUILDPATH&#125;/lfs-release</span><br><span class="line">$ git reset --hard &amp;&amp; git clean -dfx</span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ ./configure --disable-server --with-linux=/usr/src/linux-headers-4.15.0-64-generic</span><br><span class="line">$ make install</span><br><span class="line">$ rm -rf  /lib/modules/4.15.0-64-generic/kernel/drivers/staging/lfs/</span><br><span class="line">$ depmod -a</span><br></pre></td></tr></table></figure>
<h5 id="ubuntu-install-client"><a href="#ubuntu-install-client" class="headerlink" title="ubuntu install client"></a>ubuntu install client</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ apt install libncurses5-dev libncurses-dev bison flex gnupg libelf-dev gcc libssl-dev bc bzip2 build-essential udev kmod cpio  libfuse-dev libattr1-dev libblkid-dev uuid-dev devscripts fakeroot kernel-wedge libudev-dev libpython3-dev swig  gettext texinfo debhelper dh-exec update-notifier-common sg3-utils attr mpi-default-bin selinux-utils python2 libpython2-stdlib libsgutils2-2 libpython2.7-stdlib linux-headers-$(<span class="built_in">uname</span> -r) libkeyutils-dev libnl-3-dev pkg-config libhwloc-dev libnl-genl-3-dev libsnmp-dev dpatch libmount-dev libyaml-dev module-assistant libreadline-dev mpi-default-dev</span><br><span class="line"></span><br><span class="line"><span class="comment">#high version remove python2</span></span><br><span class="line">$ apt install libncurses5-dev libncurses-dev bison flex gnupg libelf-dev gcc libssl-dev bc bzip2 build-essential udev kmod cpio  libfuse-dev libattr1-dev libblkid-dev uuid-dev devscripts fakeroot kernel-wedge libudev-dev libpython3-dev swig  gettext texinfo debhelper dh-exec update-notifier-common sg3-utils attr mpi-default-bin selinux-utils libsgutils2-2 libpython2.7-stdlib linux-headers-$(<span class="built_in">uname</span> -r) libkeyutils-dev libnl-3-dev pkg-config libhwloc-dev libnl-genl-3-dev libsnmp-dev dpatch libmount-dev libyaml-dev module-assistant libreadline-dev mpi-default-dev</span><br><span class="line"></span><br><span class="line">$ ./configure --enable-client --disable-server;make debs</span><br><span class="line">$ <span class="built_in">cd</span> debs</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="get-the-status"><a href="#get-the-status" class="headerlink" title="get the status"></a>get the status</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">client $ lfs osts</span><br><span class="line">client $ cat /proc/fs/lfs/lov/$fsname-clilov-fffff882037467800/target_obd</span><br><span class="line">mds    $ cat /proc/fs/lfs/lov/$fsname-MDT0000-mdtlov/target_obd</span><br><span class="line">client $ lctl get_param osc.*-OST*.active</span><br><span class="line"></span><br><span class="line"># check object server status in my production env</span><br><span class="line">$ (lfs osts | awk -F &#x27;[ :_]+&#x27; &#x27;$0~/OST/&#123;print $2&#125;&#x27; | while read line; do grep &quot;FULL&quot; /proc/fs/lfs/osc/$&#123;line&#125;-*/state &gt;/dev/null 2&gt;&amp;1 || echo -e &quot;Got these bad OSTs:&quot; $&#123;RED&#125;$line$&#123;NC&#125; ; done) &amp;&amp; exit 0</span><br><span class="line"></span><br><span class="line"># show ost ip  addr</span><br><span class="line">$ lctl dl -t</span><br><span class="line"></span><br><span class="line"># list nids</span><br><span class="line">$ lctl lst_nids</span><br><span class="line">$ lctl which_nid $your_ipaddr@tcp</span><br><span class="line">$ lctl ping $your_ipaddr@tcp</span><br><span class="line">$ lnetctl peer show --nid $ipaddr</span><br><span class="line">$ lnetctl peer del --prim_nid $ipaddr --nid $ipaddr</span><br><span class="line"></span><br><span class="line"># mount namespace &quot;D&quot; and clear mds info from client</span><br><span class="line">client $ echo 0 &gt; cat /sys/fs/lfs/mdc/$FSNAME-*/active</span><br><span class="line">client $ lctl set_param mdc.$FSNAME-*.active=0 #better than degarde</span><br></pre></td></tr></table></figure>

<h3 id="Backup-and-recovery-lfs-ZFS-OST"><a href="#Backup-and-recovery-lfs-ZFS-OST" class="headerlink" title="Backup and recovery lfs ZFS OST"></a>Backup and recovery lfs ZFS OST</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">OSS $ umount /lfs/test_ost0</span><br><span class="line">OSS $ zfs set canmount=on test_ost0/test_ost0</span><br><span class="line">OSS $ zfs mount -a</span><br><span class="line">OSS $ ls /test_ost0/test_ost0</span><br><span class="line">CONFIGS         nodemap  oi.103  oi.110  oi.118  oi.125  oi.18  oi.25  oi.32  oi.4   oi.47  oi.54  oi.61  oi.69  oi.76  oi.83  oi.90  oi.98</span><br><span class="line">......</span><br><span class="line">OSS $ tar cvf /backup/test_ost0.tar --xattrs-include=&quot;trusted.*&quot; --sparse .</span><br><span class="line">./</span><br><span class="line">./fld</span><br><span class="line">./oi.46/</span><br><span class="line">./oi.31/</span><br><span class="line">./oi.114/</span><br><span class="line">./quota_slave/</span><br><span class="line">./quota_slave/0x20000-OST0000</span><br><span class="line">......</span><br><span class="line">./oi.92/</span><br><span class="line">./oi.76/</span><br><span class="line">./oi.98/</span><br><span class="line">tar: Exiting with failure status due to previous errors</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">OSS $ cd ~</span><br><span class="line">OSS $ umount /test_ost0/test_ost0/</span><br><span class="line">OSS $ mkfs.lfs --reformat --backfstype=zfs --ost  --index=0  --fsname=teslustre1 --servicenode=$&#123;oss_ipaddr&#125;@tcp --mgsnode=$&#123;mds_ipaddr&#125;@tcp  test_ost0/test_ost0</span><br><span class="line">OSS $ zfs set canmount=on test_ost0/test_ost0 </span><br><span class="line">OSS $ zfs mount -a</span><br><span class="line">OSS $ cd /test_ost0/test_ost0</span><br><span class="line">OSS $ tar xvpf /backup/test_ost0.tar --xattrs-include=&quot;trusted.*&quot; --sparse</span><br><span class="line"></span><br><span class="line">#fixed the MDT inactive 2.15.2</span><br><span class="line">#fixed OST inode full cause mount.lustre failed 2.15.2</span><br><span class="line">#don&#x27;t not delete oi.16 under zfs-osd</span><br><span class="line">OSS $ rm -rf oi.16* lfsck_* LFSCK CATALOGS</span><br><span class="line">OSS $ getfattr -n trusted.lma fld </span><br><span class="line"># file: fld</span><br><span class="line">trusted.lma=0sAAAAAAAAAAABAAAAAgAAAAMAAAAAAAAA</span><br><span class="line"></span><br><span class="line">OSS $ rm -f fld</span><br><span class="line">OSS $ umount /test_ost0/test_ost0</span><br><span class="line"></span><br><span class="line">writeconf all OST and MDT</span><br><span class="line">patch https://review.whamcloud.com/#/c/46723/</span><br><span class="line"></span><br><span class="line">OSS $ lfs_rmmod</span><br><span class="line">OSS $ mount.lfs test_ost0/test_ost0 /lfs/test_ost0</span><br></pre></td></tr></table></figure>

<h3 id="lfs-multiple-ethernet-port-for-diff-LAN"><a href="#lfs-multiple-ethernet-port-for-diff-LAN" class="headerlink" title="lfs multiple ethernet port for diff LAN"></a>lfs multiple ethernet port for diff LAN</h3><p>just modify lnet driver parameters in server and client</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=tcp0(eth0),tcp1(ens12) <span class="comment">## in server and client</span></span><br><span class="line"></span><br><span class="line">client $ mount.lfs 192.168.0.1@tcp0:/lfs /mnt</span><br><span class="line">client $ mount.lfs 10.0.0.1@tcp1:/lfs /mnt</span><br><span class="line"></span><br><span class="line">[Fri Mar 25 15:55:47 2022] lfs: lfs-OST0000: Connection restored to ed9931e3-3560-4e6c-0914-9e6e65e6bfc0 (at 10.0.0.1@tcp)</span><br><span class="line">[Fri Mar 25 16:00:08 2022] lfs: lfs-OST0000: Connection restored to ed9931e3-3560-4e6c-0914-9e6e65e6bfc0 (at 10.0.0.1@tcp)</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<h3 id="trace-the-kernel-sock"><a href="#trace-the-kernel-sock" class="headerlink" title="trace the kernel sock"></a>trace the kernel sock</h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">./lnet/include/lnet/lib-types.h</span><br><span class="line">        <span class="type">int</span>                             ln_niinit_self;</span><br><span class="line">        <span class="comment">/* LNetNIInit/LNetNIFini counter */</span></span><br><span class="line">        <span class="type">int</span>                             ln_refcount;</span><br><span class="line">        <span class="comment">/* SHUTDOWN/RUNNING/STOPPING */</span></span><br><span class="line"></span><br><span class="line">$ lctl --net tcp del_peer $ipaddr</span><br><span class="line">lnet/peer.c</span><br><span class="line"> LNetPut</span><br><span class="line">  ksocknal_alloc_tx</span><br><span class="line">   ksocknal_launch_packet</span><br><span class="line"></span><br><span class="line">$ ss -tulpen</span><br><span class="line">udp    UNCONN     <span class="number">0</span>      <span class="number">0</span>                                                             [::]:<span class="number">111</span>                                                                       [::]:*                   users:((<span class="string">&quot;rpcbind&quot;</span>,pid=<span class="number">936</span>,fd=<span class="number">9</span>)) ino:<span class="number">16271</span> sk:ffffa0b3b54a0000 v6only:<span class="number">1</span> &lt;-&gt;</span><br><span class="line"></span><br><span class="line">tcp    LISTEN     <span class="number">0</span>      <span class="number">127</span>                                                              *:<span class="number">988</span>                                                                          *:*                   ino:<span class="number">389210</span> sk:ffffa0b43b7e2e80 &lt;-&gt;</span><br><span class="line"></span><br><span class="line">tcp    LISTEN     <span class="number">0</span>      <span class="number">128</span>                                                              *:<span class="number">111</span>                                                                          *:*                   users:((<span class="string">&quot;rpcbind&quot;</span>,pid=<span class="number">936</span>,fd=<span class="number">8</span>)) ino:<span class="number">16270</span> sk:ffffa0b3b5548000 &lt;-&gt;</span><br><span class="line"></span><br><span class="line">$ crash /usr/lib/debug/usr/lib/modules/<span class="number">3.10</span><span class="number">.0</span><span class="number">-1160.</span>el7.x86_64/vmlinux</span><br><span class="line"></span><br><span class="line">crash&gt; <span class="class"><span class="keyword">struct</span> <span class="title">sock</span>.__<span class="title">sk_common</span>.<span class="title">skc_num</span> <span class="title">ffffa0b43b7e2e80</span></span></span><br><span class="line"><span class="class">  __<span class="title">sk_common</span>.<span class="title">skc_num</span> =</span> <span class="number">988</span></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="class"><span class="keyword">struct</span> <span class="title">sock</span>.<span class="title">sk_rcvbuf</span> <span class="title">ffffa0b43b7e2e80</span></span></span><br><span class="line"><span class="class">  <span class="title">sk_rcvbuf</span> =</span> <span class="number">134217728</span></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="class"><span class="keyword">struct</span> <span class="title">sock</span>.<span class="title">sk_sndbuf</span> <span class="title">ffffa0b43b7e2e80</span></span></span><br><span class="line"><span class="class"><span class="title">sk_sndbuf</span> =</span> <span class="number">134217728</span></span><br><span class="line">net.ipv4.tcp_rmem = <span class="number">65536</span>       <span class="number">134217728</span>       <span class="number">268435456</span></span><br><span class="line">net.ipv4.tcp_wmem = <span class="number">65536</span>       <span class="number">134217728</span>       <span class="number">268435456</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">struct</span> sock.sk_max_ack_backlog ffffa0b43b7e2e80</span><br><span class="line">  sk_max_ack_backlog = <span class="number">127</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.sk_max_ack_backlog ffffa0b3b5548000</span><br><span class="line">  sk_max_ack_backlog = <span class="number">128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.__sk_common.skc_num ffffa0b3b54a0000</span><br><span class="line">  __sk_common.skc_num = <span class="number">111</span></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.sk_rcvbuf ffffa0b3b54a0000</span><br><span class="line">  sk_rcvbuf = <span class="number">212992</span></span><br><span class="line">crash&gt; <span class="keyword">struct</span> sock.sk_rcvbuf ffffa0b3b54a0000</span><br><span class="line">  sk_rcvbuf = <span class="number">212992</span></span><br><span class="line">net.core.rmem_default = <span class="number">212992</span></span><br><span class="line">net.core.rmem_max = <span class="number">212992</span></span><br><span class="line">net.core.wmem_default = <span class="number">212992</span></span><br><span class="line">net.core.wmem_max = <span class="number">212992</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#looks like the ss show the struct sock address</span></span><br><span class="line"></span><br><span class="line"># /proc/net/tcp</span><br><span class="line">netstat -wtpeav</span><br></pre></td></tr></table></figure>

<h3 id="lfs-magic-num"><a href="#lfs-magic-num" class="headerlink" title="lfs magic num"></a>lfs magic num</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">magic: 0xa0629d03</span><br></pre></td></tr></table></figure>

<h3 id="rocev2-test"><a href="#rocev2-test" class="headerlink" title="rocev2 test"></a>rocev2 test</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">options lnet networks=o2ib(ens1f1np1),tcp(ens1f0np0)</span><br><span class="line"></span><br><span class="line">options lnet networks=<span class="string">&quot;tcp1(eth1),tcp2(eth2),o2ib0(ib0)&quot;</span></span><br><span class="line"><span class="comment">###</span></span><br><span class="line">options lnet ip2nets=<span class="string">&quot;tcp1(eth0) 192.168.0.[2,4] \</span></span><br><span class="line"><span class="string"> tcp1 192.168.0.*; o2ib1 132.6.[1-3],[2-8/2]&quot;</span></span><br><span class="line"><span class="comment">### [2-8/2] means 2,4,6,8</span></span><br></pre></td></tr></table></figure>

<h3 id="DOM"><a href="#DOM" class="headerlink" title="DOM"></a>DOM</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br></pre></td><td class="code"><pre><span class="line">mds $ lctl get_param lod.*.dom_stripesize</span><br><span class="line"><span class="comment">#disable DOM stripe</span></span><br><span class="line">mds $ lctl set_param lod.*.dom_stripesize=0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#default is 1MiB</span></span><br><span class="line"><span class="comment">#From 64KiB to 1GiB</span></span><br><span class="line">client $ lctl set_param lod.*MDT000*.dom_stripesize=131072 <span class="comment">#128KiB</span></span><br><span class="line"></span><br><span class="line">client $ lctl conf_param lod.*MDT&lt;index&gt;*.dom_stripesize=&lt;value <span class="keyword">in</span> bytes&gt;</span><br><span class="line">client $ lctl get_param lod.*MDT&lt;index&gt;*.dom_stripesize</span><br><span class="line"></span><br><span class="line"><span class="comment">#find all files on MDT</span></span><br><span class="line">client $ lfs find -L mdt /testfs</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find DoM files with a 1M stripe size</span></span><br><span class="line">client $ lfs find /testfs -L mdt -S 1M</span><br><span class="line"></span><br><span class="line"><span class="comment">#Find DoM directories with stripe size less than 180K</span></span><br><span class="line">client $ lfs find -L mdt -S -180K -<span class="built_in">type</span> d /testfs</span><br><span class="line"></span><br><span class="line">client $ lfs setstripe -L mdt -E 1M DoM.1c.file1 </span><br><span class="line">client $ lfs getstripe DoM.1c.file1 </span><br><span class="line">DoM.1c.file1</span><br><span class="line">  lcm_layout_gen:    1</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   1</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   1048576</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   1048576</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">ls</span> -<span class="built_in">ls</span> DoM.1c.file1</span><br><span class="line">0 -rw-r--r-- 1 root root 0 Jul 24 15:58 DoM.1c.file1</span><br><span class="line">client $ lfs setstripe -L mdt -E 1M -E -1 -S 1M DoM.2c.file1</span><br><span class="line">client $ lfs getstripe DoM.2c.file1</span><br><span class="line">DoM.2c.file1</span><br><span class="line">  lcm_layout_gen:    2</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   2</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   1048576</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   1048576</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">    lcme_id:             2</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          0</span><br><span class="line">    lcme_extent.e_start: 1048576</span><br><span class="line">    lcme_extent.e_end:   EOF</span><br><span class="line">      lmm_stripe_count:  1</span><br><span class="line">      lmm_stripe_size:   1048576</span><br><span class="line">      lmm_pattern:       raid0</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: -1</span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">ls</span> -<span class="built_in">ls</span> DoM.2c.file1</span><br><span class="line">0 -rw-r--r-- 1 root root 0 Jul 24 15:58 DoM.2c.file1</span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/urandom of=DoM.2c.file1 bs=4K count=32 oflag=append conv=notrunc</span><br><span class="line">32+0 records <span class="keyword">in</span></span><br><span class="line">32+0 records out</span><br><span class="line">131072 bytes (131 kB, 128 KiB) copied, 0.00426249 s, 30.8 MB/s</span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">ls</span> -<span class="built_in">ls</span> DoM.2c.file1</span><br><span class="line">133 -rw-r--r-- 1 root root 131072 Jul 24 16:00 DoM.2c.file1</span><br><span class="line"></span><br><span class="line">client $ lfs getstripe DoM.2c.file1</span><br><span class="line">DoM.2c.file1</span><br><span class="line">  lcm_layout_gen:    3</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   2</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   1048576</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   1048576</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">    lcme_id:             2</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init</span><br><span class="line">    lcme_extent.e_start: 1048576</span><br><span class="line">    lcme_extent.e_end:   EOF</span><br><span class="line">      lmm_stripe_count:  1</span><br><span class="line">      lmm_stripe_size:   1048576</span><br><span class="line">      lmm_pattern:       raid0</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 4</span><br><span class="line">      lmm_objects:</span><br><span class="line">      - 0: &#123; l_ost_idx: 4, l_fid: [0x100040000:0x444b2c:0x0] &#125;</span><br><span class="line"></span><br><span class="line">client $ <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/urandom of=DoM.2c.file1 bs=1M count=32 oflag=append conv=notrunc</span><br><span class="line">32+0 records <span class="keyword">in</span></span><br><span class="line">32+0 records out</span><br><span class="line">33554432 bytes (34 MB, 32 MiB) copied, 0.263022 s, 128 MB/s</span><br><span class="line"></span><br><span class="line">client $ lfs getstripe DoM.2c.file1</span><br><span class="line">DoM.2c.file1</span><br><span class="line">  lcm_layout_gen:    3</span><br><span class="line">  lcm_mirror_count:  1</span><br><span class="line">  lcm_entry_count:   2</span><br><span class="line">    lcme_id:             1</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init &lt;---0~1MiB on MDT</span><br><span class="line">    lcme_extent.e_start: 0</span><br><span class="line">    lcme_extent.e_end:   1048576</span><br><span class="line">      lmm_stripe_count:  0</span><br><span class="line">      lmm_stripe_size:   1048576</span><br><span class="line">      lmm_pattern:       mdt</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">    lcme_id:             2</span><br><span class="line">    lcme_mirror_id:      0</span><br><span class="line">    lcme_flags:          init </span><br><span class="line">    lcme_extent.e_start: 1048576 &lt;---1MiB~EOF on OST</span><br><span class="line">    lcme_extent.e_end:   EOF</span><br><span class="line">      lmm_stripe_count:  1</span><br><span class="line">      lmm_stripe_size:   1048576</span><br><span class="line">      lmm_pattern:       raid0</span><br><span class="line">      lmm_layout_gen:    0</span><br><span class="line">      lmm_stripe_offset: 4</span><br><span class="line">      lmm_objects:</span><br><span class="line">      - 0: &#123; l_ost_idx: 4, l_fid: [0x100040000:0x444b2c:0x0] &#125;</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -<span class="built_in">ls</span> DoM.2c.file1</span><br><span class="line">31193 -rw-r--r-- 1 root root 33685504 Jul 24 16:01 DoM.2c.file1</span><br><span class="line"></span><br><span class="line">mds $ lctl get_param -n lod.*MDT0000*.dom_stripesize</span><br><span class="line">1048576</span><br><span class="line">mds $ lctl conf_param testfs-MDT0000.lod.dom_stripesize=128K</span><br><span class="line"></span><br><span class="line"><span class="comment">#disable DOM stripe</span></span><br><span class="line">mds $ lctl conf_param lfs-MDT0000.lod.dom_stripesize=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># not test, https://wiki.lustre.org/images/9/94/LUG2018-Lustre_Data_on_MDT_An_Early_Look-Leers.pdf</span></span><br><span class="line">client $ lfs setstripe -E 1M -L mdt -E 64M -c1 -S 1M -E 8G -c4 -E -1 -c -1 -S 4M DNE.PFL.file1</span><br><span class="line">DoM.PFL.file1</span><br><span class="line">lcm_layout_gen: 6</span><br><span class="line">lcm_mirror_count: 1</span><br><span class="line">lcm_entry_count: 4</span><br><span class="line">lcme_id: 1</span><br><span class="line">lcme_mirror_id: 0</span><br><span class="line">lcme_flags: init  -------&gt;0-1MB on DoM MDT</span><br><span class="line">lcme_extent.e_start: 0</span><br><span class="line">lcme_extent.e_end: 1048576</span><br><span class="line">lmm_stripe_count: 0</span><br><span class="line">lmm_stripe_size: 1048576</span><br><span class="line">lmm_pattern: mdt</span><br><span class="line">lmm_layout_gen: 0</span><br><span class="line">lmm_stripe_offset: 0</span><br><span class="line"></span><br><span class="line">lcme_id: 2</span><br><span class="line">lcme_mirror_id: 0</span><br><span class="line">lcme_flags: init</span><br><span class="line">lcme_extent.e_start: 1048576  -------&gt; 1-64MB on 1 stripe of 1MB stripesz</span><br><span class="line">lcme_extent.e_end: 67108864</span><br><span class="line">lmm_stripe_count: 1</span><br><span class="line">lmm_stripe_size: 1048576</span><br><span class="line">lmm_pattern: raid0</span><br><span class="line">lmm_layout_gen: 0</span><br><span class="line">lmm_stripe_offset: 10</span><br><span class="line">lmm_objects:</span><br><span class="line">- 0: &#123; l_ost_idx: 10, l_fid: [0x1000a0000:0x349339:0x0] &#125;</span><br><span class="line"></span><br><span class="line">lcme_id: 3</span><br><span class="line">lcme_mirror_id: 0</span><br><span class="line">lcme_flags: init</span><br><span class="line">lcme_extent.e_start: 67108864 -------&gt; 64M-8GB on 4 OSTs at default stripesz</span><br><span class="line">lcme_extent.e_end: 8589934592</span><br><span class="line">lmm_stripe_count: 4</span><br><span class="line">lmm_stripe_size: 1048576</span><br><span class="line">lmm_pattern: raid0</span><br><span class="line">lmm_layout_gen: 0</span><br><span class="line">lmm_stripe_offset: 0</span><br><span class="line">lmm_objects:</span><br><span class="line">- 0: &#123; l_ost_idx: 0, l_fid: [0x100000000:0x345c67:0x0] &#125;</span><br><span class="line">- 1: &#123; l_ost_idx: 7, l_fid: [0x100070000:0x34967a:0x0] &#125;</span><br><span class="line">- 2: &#123; l_ost_idx: 3, l_fid: [0x100030000:0x3475ea:0x0] &#125;</span><br><span class="line">- 3: &#123; l_ost_idx: 9, l_fid: [0x100090000:0x34612a:0x0] &#125;</span><br><span class="line"></span><br><span class="line">lcme_id: 4</span><br><span class="line">lcme_mirror_id: 0</span><br><span class="line">lcme_flags: init</span><br><span class="line">lcme_extent.e_start: 8589934592  ------&gt; 8GB-EOF across all OSTs at 4MB stripesz</span><br><span class="line">lcme_extent.e_end: EOF</span><br><span class="line">lmm_stripe_count: 14</span><br><span class="line">lmm_stripe_size: 4194304</span><br><span class="line">lmm_pattern: raid0</span><br><span class="line">lmm_layout_gen: 0</span><br><span class="line">lmm_stripe_offset: 13</span><br><span class="line">lmm_objects:</span><br><span class="line">- 0: &#123; l_ost_idx: 13, l_fid: [0x1000d0000:0x34682a:0x0] &#125;</span><br><span class="line">- 1: &#123; l_ost_idx: 5, l_fid: [0x100050000:0x36420a:0x0] &#125;</span><br><span class="line">- 2: &#123; l_ost_idx: 11, l_fid: [0x1000b0000:0x34874a:0x0] &#125;</span><br><span class="line">- 3: &#123; l_ost_idx: 6, l_fid: [0x100060000:0x34540a:0x0] &#125;</span><br><span class="line">- 4: &#123; l_ost_idx: 12, l_fid: [0x1000c0000:0x34989a:0x0] &#125;</span><br><span class="line">- 5: &#123; l_ost_idx: 1, l_fid: [0x100010000:0x34b78a:0x0] &#125;</span><br><span class="line">- 6: &#123; l_ost_idx: 8, l_fid: [0x100080000:0x34963b:0x0] &#125;</span><br><span class="line">- 7: &#123; l_ost_idx: 4, l_fid: [0x100040000:0x34d339:0x0] &#125;</span><br><span class="line">- 8: &#123; l_ost_idx: 2, l_fid: [0x100020000:0x3471aa:0x0] &#125;</span><br><span class="line">- 9: &#123; l_ost_idx: 10, l_fid: [0x1000a0000:0x34933a:0x0] &#125;</span><br><span class="line">- 10: &#123; l_ost_idx: 0, l_fid: [0x100000000:0x345c68:0x0] &#125;</span><br><span class="line">- 11: &#123; l_ost_idx: 7, l_fid: [0x100070000:0x34967b:0x0] &#125;</span><br><span class="line">- 12: &#123; l_ost_idx: 3, l_fid: [0x100030000:0x3475eb:0x0] &#125;</span><br><span class="line">- 13: &#123; l_ost_idx: 9, l_fid: [0x100090000:0x34612b:0x0] &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 0 1 2 3 4 5 6 7</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  lfs <span class="built_in">mkdir</span> -i <span class="variable">$&#123;i&#125;</span> DOM-<span class="variable">$&#123;i&#125;</span>-MDT</span><br><span class="line">  lfs setstripe -L mdt --component-end 1M DOM-<span class="variable">$&#123;i&#125;</span>-MDT</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">mdtest -v -i 2 -n 10000 -w 8192 -u -F -C -T -r -d /domfs/DOM-0-MDT/mdtestdir@</span><br><span class="line">/domfs/DOM-1-MDT/mdtestdir@/domfs/DOM-2-MDT/mdtestdir@</span><br><span class="line">/domfs/DOM-3-MDT/mdtestdir@/domfs/DOM-4-MDT/mdtestdir@</span><br><span class="line">/domfs/DOM-5-MDT/mdtestdir@/domfs/DOM-6-MDT/mdtestdir@</span><br><span class="line">/domfs/DOM-7-MDT/mdtestdir</span><br><span class="line"></span><br><span class="line"><span class="comment">#https://wiki.lustre.org/images/f/ff/LUG2021-Evaluation_DoM_DNE_Scaling-Simmons.pdf</span></span><br><span class="line"><span class="comment"># DNE3 is new auto striping across MDTs feature</span></span><br><span class="line">mds $ lctl set_param mdt.*.enable_dir_auto_split=1</span><br><span class="line">mds $ lctl <span class="built_in">set</span> param mdt.*.dir_split_delta=1</span><br><span class="line">mds $ lctl <span class="built_in">set</span> param mdt.*.dir_split_count=15000</span><br></pre></td></tr></table></figure>


<h4 id="lnet-health"><a href="#lnet-health" class="headerlink" title="lnet health"></a><a target="_blank" rel="noopener" href="http://wiki.lfsfs.cn/index.php?title=LNet%E5%81%A5%E5%BA%B7%E5%8A%9F%E8%83%BD%E7%94%A8%E6%88%B7%E6%89%8B%E5%86%8C">lnet health</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># config</span></span><br><span class="line">lctl network up/down</span><br><span class="line">lctl list_nids</span><br><span class="line">lctl ping xxxxx@tcp</span><br><span class="line">lctl network unconfigure</span><br><span class="line"><span class="comment">### from lfs 2.7</span></span><br><span class="line">lnetctl lnet configure/unconfigure</span><br><span class="line">lnetctl net show -v</span><br><span class="line">lnetctl peer show -v</span><br><span class="line">lnetctl net add --net LNET --<span class="keyword">if</span> eth0</span><br><span class="line">lnetctl net del --net LNET</span><br><span class="line">// To <span class="built_in">export</span> the current configuration to a YAML file</span><br><span class="line">lnetctl <span class="built_in">export</span> FILE.yaml</span><br><span class="line">lnetctl <span class="built_in">export</span> &gt; FILE.yaml</span><br><span class="line">// To import the configuration from a YAML file</span><br><span class="line">lnetctl import FILE.yaml</span><br><span class="line">lnetctl import &lt; FILE.yaml</span><br><span class="line"></span><br><span class="line"><span class="comment"># the more value, the long recovery time, the default is disabled means lnet_health_sensitivity and lnet_retry_count set to 0</span></span><br><span class="line">$ lnetctl <span class="built_in">set</span> health_sensitivity: sensitivity to failure</span><br><span class="line">        0 - turn off health evaluation</span><br><span class="line">        &gt;0 - sensitivity value not more than 1000</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">set</span> recovery_interval: interval to ping unhealthy interfaces</span><br><span class="line">        &gt;0 - <span class="built_in">timeout</span> <span class="keyword">in</span> seconds</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">set</span> retry_count: number of retries</span><br><span class="line">        0 - turn of retries</span><br><span class="line">        &gt;0 - number of retries</span><br><span class="line"></span><br><span class="line"><span class="comment">#Important</span></span><br><span class="line">$ lnetctl <span class="built_in">set</span> transaction_timeout: Message/Response <span class="built_in">timeout</span></span><br><span class="line">        &gt;0 - <span class="built_in">timeout</span> <span class="keyword">in</span> seconds</span><br><span class="line">$ lnetctl <span class="built_in">set</span> transaction_timeout 50</span><br><span class="line">$ lnetctl <span class="built_in">set</span> retry_count 2</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lnet_lnd_timeout = lnet_transaction_timeout / retry_count</span><br><span class="line"></span><br><span class="line"><span class="comment">#dump all config</span></span><br><span class="line">$ lnetctl global show</span><br><span class="line">global:</span><br><span class="line">    numa_range: 0</span><br><span class="line">    max_intf: 200</span><br><span class="line">    discovery: 1</span><br><span class="line">    drop_asym_route: 0</span><br><span class="line">    retry_count: 2</span><br><span class="line">    transaction_timeout: 50</span><br><span class="line">    health_sensitivity: 100</span><br><span class="line">    recovery_interval: 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#show local health</span></span><br><span class="line">$ lnetctl net show -v 4</span><br><span class="line"></span><br><span class="line"><span class="comment">#show remote health</span></span><br><span class="line">$ lnetctl peer show -v 4</span><br><span class="line"></span><br><span class="line"><span class="comment">#show status</span></span><br><span class="line">$ lnetctl stats show</span><br><span class="line"></span><br><span class="line"><span class="comment"># mark the route status, set 0 means disable</span></span><br><span class="line">options lnet auto_down=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># avoid_asym_router_failure, avoid push data to the block route cause data loss</span></span><br><span class="line">options lnet avoid_asym_router_failure=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping check the active route time interval, default: 60s</span></span><br><span class="line">options lnet live_router_check_interval=50</span><br><span class="line"></span><br><span class="line"><span class="comment"># dead_router_check_interval, default:60s</span></span><br><span class="line">options lnet dead_router_check_interval=50</span><br><span class="line"></span><br><span class="line"><span class="comment"># ping timeout</span></span><br><span class="line">options lnet router_ping_timeout=60</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the route status from</span></span><br><span class="line">$ <span class="built_in">cat</span> /sys/kernel/debug/lnet/stats</span><br><span class="line">0 23 0 49912 49912 0 0 18468672 27625832 0 0</span><br><span class="line"></span><br><span class="line">$ lnetctl stats show</span><br><span class="line">statistics:</span><br><span class="line">    msgs_alloc: 0</span><br><span class="line">    msgs_max: 23</span><br><span class="line">    rst_alloc: 0</span><br><span class="line">    errors: 0</span><br><span class="line">    send_count: 49912</span><br><span class="line">    resend_count: 0</span><br><span class="line">    response_timeout_count: 0</span><br><span class="line">    local_interrupt_count: 0</span><br><span class="line">    local_dropped_count: 0</span><br><span class="line">    local_aborted_count: 0</span><br><span class="line">    local_no_route_count: 0</span><br><span class="line">    local_timeout_count: 0</span><br><span class="line">    local_error_count: 0</span><br><span class="line">    remote_dropped_count: 0</span><br><span class="line">    remote_error_count: 0</span><br><span class="line">    remote_timeout_count: 0</span><br><span class="line">    network_timeout_count: 0</span><br><span class="line">    recv_count: 49912</span><br><span class="line">    route_count: 0</span><br><span class="line">    drop_count: 0</span><br><span class="line">    send_length: 18468672</span><br><span class="line">    recv_length: 27625832</span><br><span class="line">    route_length: 0</span><br><span class="line">    drop_length: 0</span><br><span class="line"></span><br><span class="line">Servers:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib1(ib0)&quot;</span> routes=<span class="string">&quot;o2ib2 10.10.0.20@o2ib1&quot;</span></span><br><span class="line">Routers:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib1(ib0),o2ib2(ib1)&quot;</span> <span class="string">&quot;forwarding=enabled&quot;</span></span><br><span class="line">Clients:</span><br><span class="line">options lnet networks=<span class="string">&quot;o2ib2(ib0)&quot;</span> routes=<span class="string">&quot;o2ib1 10.20.0.29@o2ib2&quot;</span></span><br><span class="line"></span><br><span class="line">Servers:</span><br><span class="line">lnetctl net add --net o2ib1 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl route add --net o2ib2 --gateway 10.10.0.20@o2ib1</span><br><span class="line">lnetctl peer add --nid 10.10.0.20@o2ib1,10.10.0.21@o2ib1</span><br><span class="line"></span><br><span class="line">Routers:</span><br><span class="line">lnetctl net add --net o2ib1 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl net add --net o2ib2 --<span class="keyword">if</span> ib2,ib3</span><br><span class="line">lnetctl peer add --nid 10.10.0.1@o2ib1,10.10.0.2@o2ib1</span><br><span class="line">lnetctl peer add --nid 10.20.0.1@o2ib2,10.20.0.2@o2ib2</span><br><span class="line">lnetctl <span class="built_in">set</span> routing 1</span><br><span class="line"></span><br><span class="line">Clients:</span><br><span class="line">lnetctl net add --net o2ib2 --<span class="keyword">if</span> ib0,ib1</span><br><span class="line">lnetctl route add --net o2ib1 --gateway 10.20.0.29@o2ib2</span><br><span class="line">lnetctl peer add --nid 10.20.0.29@o2ib2,10.20.0.30@o2ib2</span><br><span class="line"></span><br><span class="line">$ lnetctl <span class="built_in">export</span> FILE.yaml</span><br><span class="line">$ lnetctl <span class="built_in">export</span> &gt; FILE.yaml</span><br><span class="line">$ lnetctl import FILE.yaml</span><br><span class="line">$ lnetctl import &lt; FILE.yaml</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="OPA-setting"><a href="#OPA-setting" class="headerlink" title="OPA setting"></a>OPA setting</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> ko2iblnd-opa ko2iblnd</span><br><span class="line">options ko2iblnd-opa peer_credits=128 peer_credits_hiw=64 credits=1024 concurrent_sends=256 ntx=2048 map_on_demand=32 fmr_pool_size=2048 fmr_flush_trigger=512 fmr_cache=1</span><br><span class="line"></span><br><span class="line">//The default value is 8, /sys/kernel/debug/lnet/peers, LNet使用peer_credits和network_interface_credits通过网络发送块大小固定为1MB的数据。peer_credits参数管理可以同时发送到单个对等节点的并行</span><br><span class="line">数量</span><br><span class="line">ko2iblnd-opa peer_credits=128</span><br><span class="line"></span><br><span class="line">增加peer_credits的数量并不一定能获得良好的性能，因为在大型的配置中，增加数量会导致网络过载并增加OFED堆栈的内存利用率。可调参数network interface credits(credits)能够限制并行发送到单个网络的数</span><br><span class="line">量，并通过proc/sys/lnet/nis接口进行监控。可以通过特定lfs网络驱动程序(LND)的模块参数来增加network interface credits的数量:</span><br><span class="line">// The default value is 64 and is shared across all the CPU partitions (CPTs).</span><br><span class="line">ko2iblnd-opa credits=1024</span><br><span class="line"></span><br><span class="line">// The default value is 0</span><br><span class="line">ko2iblnd-opa map_on_demand=32</span><br></pre></td></tr></table></figure>


<h4 id="llmount"><a href="#llmount" class="headerlink" title="llmount"></a>llmount</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">https://wiki.whamcloud.com/display/PUB/Testing+a+Lustre+filesystem</span><br><span class="line"></span><br><span class="line">$ /usr/lib64/lfs/tests/llmount.sh</span><br><span class="line"></span><br><span class="line">$ export NAME=testfs                    # used by test-framework.sh to find the configuration file</span><br><span class="line">$ cat lfs/tests/cfg/$NAME.sh</span><br><span class="line">FSNAME=testfs</span><br><span class="line">MDSDEVBASE=/dev/vg_testfs/lvmdt</span><br><span class="line">OSTDEVBASE=/dev/vg_testfs/lvost</span><br><span class="line">OSTCOUNT=$&#123;OSTCOUNT:-5&#125;</span><br><span class="line">MODOPTS_LIBCFS=&quot;libcfs_panic_on_lbug=0&quot;</span><br><span class="line">FAIL_ON_ERROR=$&#123;FAIL_ON_ERROR:-true&#125;</span><br><span class="line">export SHARED_DIRECTORY=&quot;/tmp&quot;          # /tmp is shared for all services on the test node</span><br><span class="line">. $LUSTRE/tests/cfg/local.sh            # source all of the other configuration defaults</span><br><span class="line">unset OSTSIZE                           # use the size of the lvost devices, not a fixed size</span><br><span class="line">unset MDSSIZE                           # use the size of the lvmdt devices, not a fixed size</span><br></pre></td></tr></table></figure>

<h4 id="Skip-recovery"><a href="#Skip-recovery" class="headerlink" title="Skip recovery"></a>Skip recovery</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">mds $ cat /proc/fs/lustre/mdt/test-MDT0000/recovery_status</span><br><span class="line">status: INACTIVE</span><br><span class="line"></span><br><span class="line">mds $ cat /proc/fs/lustre/mdt/test-MDT0001/recovery_status</span><br><span class="line">status: COMPLETE</span><br><span class="line">recovery_start: 1683514491</span><br><span class="line">recovery_duration: 0</span><br><span class="line">completed_clients: 1/1</span><br><span class="line">replayed_requests: 0</span><br><span class="line">last_transno: 115964116992</span><br><span class="line">VBR: DISABLED</span><br><span class="line">IR: ENABLED</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">mds $ mds lctl dl | grep osc</span><br><span class="line">8 UP mdt testfs0-MDT0000 testfs0-MDT0000_UUID 10</span><br><span class="line">mds $ lctl dl | grep &quot;UP mdt&quot; | awk &#x27;&#123;print $1&#125;&#x27;</span><br><span class="line">8</span><br><span class="line">mds $ lctl --device 8 abort_recovery</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line"></span><br><span class="line">mount.lfs xxx xxx -o abort_recov</span><br></pre></td></tr></table></figure>

<h4 id="lfs-migarate"><a href="#lfs-migarate" class="headerlink" title="lfs migarate"></a>lfs migarate</h4><p>Strong not recommand this command, because the command will cause loss the data, I suggest you copy data by index and checksum the copy file</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ lfs_migrate /lfs/</span><br><span class="line"></span><br><span class="line"><span class="comment">#single ost</span></span><br><span class="line">$ lfs find --ost 6 /lfs</span><br><span class="line">$ time lfs find --obd lfs-OST0006_UUID /lfs | lfs_migrate -sy</span><br><span class="line"><span class="comment">#ADD ost</span></span><br><span class="line">$ lfs <span class="built_in">df</span></span><br><span class="line">$ lfs_migrate /lfs/</span><br><span class="line">$ lfs <span class="built_in">df</span></span><br><span class="line"></span><br><span class="line">or custome script when these files not be written</span><br><span class="line">$ lfs setstripe -c 1  -i 4 /lfs/dir1</span><br><span class="line">$ copy /lfs/old_dir1/file1 /lfs/dir1</span><br><span class="line">$ <span class="built_in">md5sum</span> /lfs/old_dir1/file1 /lfs/dir1/file1</span><br><span class="line"></span><br><span class="line"><span class="comment"># dont &#x27;t use lfs migrate, it &#x27;s too dangerous, it will cause data loss</span></span><br><span class="line"><span class="comment">## lfs find /opt/lfswh -obd lfswh-OST000c_UUID -size +4G | lfs_migrate -y</span></span><br><span class="line"><span class="comment">## lfs migrate -c 1  -i 4 filepath</span></span><br></pre></td></tr></table></figure>

<h4 id="Job-status"><a href="#Job-status" class="headerlink" title="Job status"></a>Job status</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">client $  lctl get_param jobid_var</span><br><span class="line">client $  jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line">SLURM: jobid_var=SLURM_JOB_ID</span><br><span class="line">SGE: jobid_var=JOB_ID</span><br><span class="line">LSF: jobid_var=LSB_JOBID</span><br><span class="line">Loadleveler: jobid_var=LOADL_STEP_ID</span><br><span class="line">PBS: jobid_var=PBS_JOBID</span><br><span class="line">Maui/MOAB: jobid_var=PBS_JOBID</span><br><span class="line"><span class="comment"># Enable for sge</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=JOB_ID</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=<span class="built_in">disable</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># If there isn&#x27;t any job scheduler is running over the system, or user just want to collect the stats for process &amp; uid:</span></span><br><span class="line">mds $ lctl conf_param testfs.sys.jobid_var=procname_uid</span><br><span class="line"></span><br><span class="line"><span class="comment"># Check Job status</span></span><br><span class="line">oss $ lctl get_param obdfilter.testfs5-OST0004.job_stats</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          9158530</span><br><span class="line">  snapshot_time:   1503038800</span><br><span class="line">  read_bytes:      &#123; samples:           0, unit: bytes, min:       0, max:       0, <span class="built_in">sum</span>:               0 &#125;</span><br><span class="line">  write_bytes:     &#123; samples:       32452, unit: bytes, min:  262144, max: 1048576, <span class="built_in">sum</span>:     34009513984 &#125;</span><br><span class="line">  getattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  setattr:         &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># get mdt ops</span></span><br><span class="line">mds $ lctl get_param mdt.*.job_stats</span><br><span class="line">mds $ lctl get_param  mdt.testfs5-MDT0000.job_stats</span><br><span class="line">mdt.testfs5-MDT0000.job_stats=</span><br><span class="line">job_stats:</span><br><span class="line">- job_id:          278685</span><br><span class="line">  snapshot_time:   1503068243</span><br><span class="line">  open:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  close:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">mknod</span>:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">link</span>:            &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">unlink</span>:          &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line">  <span class="built_in">mkdir</span>:           &#123; samples:           0, unit:  reqs &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># clear stats for all job on testfs-OST0001</span></span><br><span class="line">oss $ lctl set_param obdfilter.testfs-OST0001.job_stats=clear</span><br><span class="line"><span class="comment"># Clear stats for job &quot;dd.0&quot; on lfs-MDT0000</span></span><br><span class="line">mds $ lctl set_param mdt.lfs-MDT0000.job_stats=dd.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># cleanup interval (seconds)</span></span><br><span class="line">lctl set_param -P testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">lctl set_param  testfs5.mdt.job_cleanup_interval=604800</span><br><span class="line">mds $  <span class="built_in">cat</span> /proc/fs/lfs/mdt/testfs5-MDT0000/job_cleanup_interval</span><br></pre></td></tr></table></figure>

<h4 id="lfs-fid-and-path"><a href="#lfs-fid-and-path" class="headerlink" title="lfs fid and path"></a>lfs fid and path</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[client]# lfs fid2path /mnt      [0x200000400:0x1:0x0]</span><br><span class="line">                                       |         |   |</span><br><span class="line">                                       |         |   -- version</span><br><span class="line">                                       |         ---- object id</span><br><span class="line">                                       ----------Sequence</span><br><span class="line">[client]# lfs path2fid /mnt</span><br><span class="line">[0x200000007:0x1:0x0]</span><br></pre></td></tr></table></figure>


<h4 id="trace-with-debugfs"><a href="#trace-with-debugfs" class="headerlink" title="trace with debugfs"></a>trace with debugfs</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line">$ dd if=RHEL5.5_x86_64.iso of=rhel5-1 bs=1M count=1 oflag=sync</span><br><span class="line"></span><br><span class="line">$ cat $(blktrace.log)</span><br><span class="line">  8,32   0     2591  1720.406537446 14972  Q   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2592  1720.406539742 14972  G   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2593  1720.406542136 14972  P   N [ll_ost_io00_002]</span><br><span class="line">  8,32   0     2594  1720.406543690 14972  I   W 19144704 + 2048 [ll_ost_io00_002]</span><br><span class="line"></span><br><span class="line">$ line=19144704</span><br><span class="line">$ debugfs -c -R &quot;icheck $(($line/8))&quot; /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">/dev/sdc: catastrophic mode - not reading inode or group bitmaps</span><br><span class="line">Block   Inode number</span><br><span class="line">2393088 2623</span><br><span class="line">$ debugfs -R &quot;ncheck 2623&quot; /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">Inode   Pathname</span><br><span class="line">2623    /O/0/d13/2669</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ lfs getstripe rhel5-1</span><br><span class="line">rhel5-1</span><br><span class="line">lmm_stripe_count:   1</span><br><span class="line">lmm_stripe_size:    1048576</span><br><span class="line">lmm_pattern:        1</span><br><span class="line">lmm_layout_gen:     0</span><br><span class="line">lmm_stripe_offset:  3</span><br><span class="line">lmm_pool:           OST0003</span><br><span class="line">        obdidx           objid           objid           group</span><br><span class="line">             3            2669          0xa6d                0</span><br><span class="line"></span><br><span class="line">$ debugfs -c -R &quot;stat &lt;2623&gt;&quot; /dev/sdc</span><br><span class="line">debugfs 1.42.12.wc1 (15-Sep-2014)</span><br><span class="line">/dev/sdc: catastrophic mode - not reading inode or group bitmaps</span><br><span class="line">Inode: 2623   Type: regular    Mode:  0666   Flags: 0x80000</span><br><span class="line">Generation: 2790813459    Version: 0x00000005:00002bf8</span><br><span class="line">User:     0   Group:     0   Size: 1048576</span><br><span class="line">File ACL: 0    Directory ACL: 0</span><br><span class="line">Links: 1   Blockcount: 2048</span><br><span class="line">Fragment:  Address: 0    Number: 0    Size: 0</span><br><span class="line"> ctime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016</span><br><span class="line"> atime: 0x00000000:00000000 -- Thu Jan  1 08:00:00 1970</span><br><span class="line"> mtime: 0x5715e344:00000000 -- Tue Apr 19 15:50:28 2016</span><br><span class="line">crtime: 0x57148057:dfa2ff44 -- Mon Apr 18 14:36:07 2016</span><br><span class="line">Size of extra inode fields: 28</span><br><span class="line">Extended attributes stored in inode body:</span><br><span class="line">  lma = &quot;08 00 00 00 00 00 00 00 00 00 00 00 01 00 00 00 6d 0a 00 00 00 00 00 00 &quot; (24)</span><br><span class="line">  lma: fid=[0x100000000:0xa6d:0x0] compat=8 incompat=0</span><br><span class="line">  fid = &quot;d2 0b 00 00 02 00 00 00 40 00 00 00 00 00 00 00 &quot; (16)</span><br><span class="line">  fid: parent=[0x200000bd2:0x40:0x0] stripe=0</span><br><span class="line">EXTENTS:</span><br><span class="line">(0-255):2393088-2393343</span><br><span class="line"></span><br><span class="line">$ lfs fid2path /opt/ [0x200000bd2:0x40:0x0]</span><br><span class="line">/opt/OST0003/rhel5-1</span><br></pre></td></tr></table></figure>

<h4 id="llog-reader"><a href="#llog-reader" class="headerlink" title="llog_reader"></a>llog_reader</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ mount -t ldiskfs /dev/sda /mnt/mgs</span><br><span class="line">$ llog_reader /mnt/mgs/CONFIGS/tfs-client</span><br><span class="line"></span><br><span class="line">$ debugfs -c -R <span class="string">&#x27;dump CONFIGS/tfs-client /tmp/tfs-client&#x27;</span> /dev/sda</span><br><span class="line">$ llog_reader /tmp/tfs-client</span><br></pre></td></tr></table></figure>

<h3 id="2-12-8-client-performance-degraded-in-the-lfs-2-15-0-server"><a href="#2-12-8-client-performance-degraded-in-the-lfs-2-15-0-server" class="headerlink" title="2.12.8 client performance degraded in the lfs 2.15.0 server"></a>2.12.8 client performance degraded in the lfs 2.15.0 server</h3><p>2.15.0 OSS server time not sync<br>2.12.8_6_g5457c37 the 12 cores physical server</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ time <span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..65535&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> $(openssl rand -hex 16) &gt;&gt;  $(openssl rand -hex 8)_<span class="variable">$i</span> ; <span class="keyword">done</span></span><br><span class="line">real    40m53.549s</span><br></pre></td></tr></table></figure>

<p>2.15.0 dual core, KVM</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$time</span> <span class="keyword">for</span> i <span class="keyword">in</span> &#123;0..65535&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> $(openssl rand -hex 16) &gt;&gt;  $(openssl rand -hex 8)_<span class="variable">$i</span> ; <span class="keyword">done</span></span><br><span class="line">real    11m20.142s</span><br></pre></td></tr></table></figure>

<h4 id="lfs-zfs-direct-IO-support"><a href="#lfs-zfs-direct-IO-support" class="headerlink" title="lfs zfs direct IO support"></a><a target="_blank" rel="noopener" href="https://lfs-discuss.lfs.narkive.com/S7kbvnG2/lfs-on-zfs-pooer-direct-i-o-performance">lfs zfs direct IO support</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">John, with newer Lfs clients it is possible <span class="keyword">for</span> multiple threads to submit non-overlapping writes concurrently (also not conflicting within a single page), see LU-1669 <span class="keyword">for</span> details.</span><br><span class="line"></span><br><span class="line">Even so, O_DIRECT writes need to be synchronous to disk on the OSS, as Patrick reports, because <span class="keyword">if</span> the OSS fails before the write is on disk there is no cached copy of the data on the client that can be used to resend the RPC.</span><br><span class="line"></span><br><span class="line">The problem is that the ZFS OSD has very long transaction commit <span class="built_in">times</span> <span class="keyword">for</span> synchronous writes because it does not yet have support <span class="keyword">for</span> the ZIL. Using buffered writes, or having very large O_DIRECT writes (e.g. 40MB or larger) and large RPCs (4MB, or up to 16MB <span class="keyword">in</span> 2.9.0) to amortize the <span class="built_in">sync</span> overhead may be beneficial <span class="keyword">if</span> you really want to use O_DIRECT.</span><br></pre></td></tr></table></figure>

<h3 id="Not-wait-the-zfs-sync"><a href="#Not-wait-the-zfs-sync" class="headerlink" title="Not wait the zfs sync"></a>Not wait the zfs sync</h3><p><code>this setting will cause data loss, if client roll back log failed</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">lctl set_param osd-zfs.*.osd_obj_sync_delay_us=0</span><br><span class="line">or</span><br><span class="line">lctl set_param osd-zfs.*.osd_object_sync_delay_us=0</span><br><span class="line">or</span><br><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/osd_zfs/parameters/osd_object_sync_delay_us</span><br><span class="line"></span><br><span class="line"><span class="comment">## to default</span></span><br><span class="line"><span class="built_in">echo</span> -1 &gt; /sys/module/osd_zfs/parameters/osd_object_sync_delay_us</span><br><span class="line">osd_object_sync_delay_us</span><br><span class="line">To improve fsync() performance <span class="keyword">until</span> ZIL device,it is possible <span class="built_in">disable</span> the code <span class="built_in">which</span> causes Lfs to block waiting on a TXG to <span class="built_in">sync</span></span><br></pre></td></tr></table></figure>

<h3 id="errors"><a href="#errors" class="headerlink" title="errors"></a>errors</h3><h4 id="network"><a href="#network" class="headerlink" title="network"></a>network</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">client_bulk_callback() event <span class="built_in">type</span> 2, status -5, desc ffff</span><br></pre></td></tr></table></figure>

<h3 id="Build-lfs-MASTER-with-zfs-under-almalinux-8-7"><a href="#Build-lfs-MASTER-with-zfs-under-almalinux-8-7" class="headerlink" title="Build lfs MASTER with zfs under almalinux 8.7"></a><a target="_blank" rel="noopener" href="https://wiki.whamcloud.com/pages/viewpage.action?pageId=154144662">Build lfs MASTER with zfs under almalinux 8.7</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br></pre></td><td class="code"><pre><span class="line">$ yum remove kernel-selftests-internal-4.18.0-372.9.1.el8_lfs.x86_64 kernel-debuginfo-common-x86_64-4.18.0-372.9.1.el8_lfs.x86_64 kernel-devel-4.18.0-348.2.1.el8_lfs.x86_64 kernel-debuginfo-4.18.0-372.9.1.el8_lfs.x86_64 kernel-4.18.0-348.2.1.el8_lfs.x86_64 kernel-modules-internal-4.18.0-372.9.1.el8_lfs.x86_64 kernel-modules-internal-4.18.0-348.2.1.el8_lfs.x86_64</span><br><span class="line"></span><br><span class="line"><span class="comment"># install el8.8 kernel</span></span><br><span class="line">$ rpm -Uvh  --force kernel-headers-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-devel-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-core-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-modules-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-tools-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-core-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-modules-extra-4.18.0-477.27.2.el8_8.x86_64.rpm kernel-tools-libs-4.18.0-477.27.2.el8_8.x86_64.rpm</span><br><span class="line"></span><br><span class="line">$ dnf remove $(rpm -qa | grep lfs)</span><br><span class="line">$ dnf remove libss-devel-1.46.2.wc5-0.el8.x86_64</span><br><span class="line">$ rpm -Uhv --force e2fsprogs-libs-1.45.6-5.el8.x86_64.rpm libcom_err-1.45.6-5.el8.x86_64.rpm ../../../../devel/x86_64/os/Packages/e2fsprogs-static-1.45.6-5.el8.x86_64.rpm e2fsprogs-libs-1.45.6-5.el8.x86_64.rpm e2fsprogs-1.45.6-5.el8.x86_64.rpm libss-1.45.6-5.el8.x86_64.rpm libss-1.45.6-5.el8.x86_64.rpm e2fsprogs-devel-1.45.6-5.el8.x86_64.rpm libcom_err-1.45.6-5.el8.x86_64.rpm libcom_err-devel-1.45.6-5.el8.x86_64.rpm ../../../debug/x86_64/Packages/e2fsprogs-debugsource-1.45.6-5.el8.x86_64.rpm libss-1.45.6-5.el8.x86_64.rpm</span><br><span class="line"></span><br><span class="line">$ dnf install -y  audit-libs-devel binutils-devel elfutils-devel kabi-dw ncurses-devel newt-devel numactl-devel openssl-devel pciutils-devel perl perl-devel python2 python3-docutils xmlto xz-devel elfutils-libelf-devel libcap-devel libcap-ng-devel llvm-toolset libyaml libyaml-devel kernel-rpm-macros kernel-abi-whitelists uuid libuuid-devel libblkid libblkid-devel libtirpc-devel libtirpc libaio-devel libattr-devel   libffi-devel libudev-devel ncompress python3-cffi python3-devel python3-packaging  libmount libmount-devel make cmake automake gdb gcc libyaml-devel e2fsprogs e2fsprogs-devel e2fsprogs-libs libcom_err libcom_err-devel libss libss-devel libnl3-devel bpftool dwarves java-devel  libbabeltrace-devel libmnl-devel libbpf-devel python3-sphinx asciidoc libtraceevent-devel nss-tools perl-generators pesign</span><br><span class="line"></span><br><span class="line">$ git <span class="built_in">clone</span> git://git.whamcloud.com/fs/lustre-release.git</span><br><span class="line">$ <span class="built_in">cd</span> lustre-release</span><br><span class="line"><span class="comment">#if you want switch the version</span></span><br><span class="line">$ git tag</span><br><span class="line"></span><br><span class="line">$ git checkout -b v2_15_4</span><br><span class="line">or</span><br><span class="line">$ git switch -c 2.15.4</span><br><span class="line"></span><br><span class="line">$ git describe --tags</span><br><span class="line">v2_15_4</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line">$ sh ./autogen.sh</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> <span class="variable">$HOME</span>; <span class="built_in">mkdir</span> -p kernel/rpmbuild/&#123;BUILD,RPMS,SOURCES,SPECS,SRPMS&#125;; <span class="built_in">cd</span> kernel; <span class="built_in">echo</span> <span class="string">&#x27;%_topdir %(echo $HOME)/kernel/rpmbuild&#x27;</span> &gt; ~/.rpmmacros</span><br><span class="line">$ wget https://repo.almalinux.org/vault/8.7/BaseOS/Source/Packages/kernel-4.18.0-477.27.2.el8_7.src.rpm</span><br><span class="line"></span><br><span class="line">$ rpm -ivh kernel-4.18.0-477.27.2.el8_8.src.rpm</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel/rpmbuild</span><br><span class="line">$ rpmbuild -bp --target=`<span class="built_in">uname</span> -m` ./SPECS/kernel.spec</span><br><span class="line">......</span><br><span class="line">Processed config files are <span class="keyword">in</span> /root/kernel/rpmbuild/BUILD/kernel-4.18.0-477.27.2.el8_7/linux-4.18.0-477.27.2.el8.x86_64/configs</span><br><span class="line">+ <span class="built_in">cd</span> ..</span><br><span class="line">+ find . <span class="string">&#x27;(&#x27;</span> -name <span class="string">&#x27;*.orig&#x27;</span> -o -name <span class="string">&#x27;*~&#x27;</span> <span class="string">&#x27;)&#x27;</span> -<span class="built_in">exec</span> <span class="built_in">rm</span> -f <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;;&#x27;</span></span><br><span class="line">+ find . -name .gitignore -<span class="built_in">exec</span> <span class="built_in">rm</span> -f <span class="string">&#x27;&#123;&#125;&#x27;</span> <span class="string">&#x27;;&#x27;</span></span><br><span class="line">+ <span class="built_in">cd</span> ..</span><br><span class="line">+ <span class="built_in">exit</span> 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/lustre-release/lustre/kernel_patches/series</span><br><span class="line">$ <span class="built_in">rm</span> -f <span class="variable">$HOME</span>/lustre-kernel-x86_64-lustre.patch</span><br><span class="line">$ <span class="keyword">for</span> patch <span class="keyword">in</span> $(&lt;<span class="string">&quot;4.18-rhel8.9.series&quot;</span>); </span><br><span class="line">  <span class="keyword">do</span> </span><br><span class="line">     patch_file=<span class="string">&quot;<span class="variable">$HOME</span>/lustre-release/lustre/kernel_patches/patches/<span class="variable">$&#123;patch&#125;</span>&quot;</span>; </span><br><span class="line">     <span class="built_in">cat</span> <span class="string">&quot;<span class="variable">$&#123;patch_file&#125;</span>&quot;</span> &gt;&gt; <span class="string">&quot;<span class="variable">$HOME</span>/lustre-kernel-x86_64-lustre.patch&quot;</span></span><br><span class="line">  <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> <span class="variable">$HOME</span>/lustre-kernel-x86_64-lustre.patch</span><br><span class="line">...</span><br><span class="line">Index: linux-4.18.0-240.22.1.el8_3/block/bio-integrity.c</span><br><span class="line">===================================================================</span><br><span class="line">--- linux-4.18.0-240.22.1.el8_3.orig/block/bio-integrity.c</span><br><span class="line">+++ linux-4.18.0-240.22.1.el8_3/block/bio-integrity.c</span><br><span class="line">@@ -39,7 +39,7 @@ void blk_flush_integrity(void)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cp</span> ~/lustre-kernel-x86_64-lustre.patch ~/kernel/rpmbuild/SOURCES/patch-4.18.0-lustre.patch</span><br><span class="line"></span><br><span class="line">$ sed -i.inst -e <span class="string">&#x27;/^    find $RPM_BUILD_ROOT\/lib\/modules\/$KernelVer/a\</span></span><br><span class="line"><span class="string">    cp -a fs/ext4/* $RPM_BUILD_ROOT/lib/modules/$KernelVer/build/fs/ext4\</span></span><br><span class="line"><span class="string">    rm -f $RPM_BUILD_ROOT/lib/modules/$KernelVer/build/fs/ext4/ext4-inode-test*&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/^# empty final patch to facilitate testing of kernel patches/i\</span></span><br><span class="line"><span class="string">Patch99995: patch-%&#123;version&#125;-lustre.patch&#x27;</span> \</span><br><span class="line">-e <span class="string">&#x27;/^ApplyOptionalPatch linux-kernel-test.patch/i\</span></span><br><span class="line"><span class="string">ApplyOptionalPatch patch-%&#123;version&#125;-lustre.patch&#x27;</span> \</span><br><span class="line">~/kernel/rpmbuild/SPECS/kernel.spec</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&#x27;# x86_64&#x27;</span> &gt; ~/kernel/rpmbuild/SOURCES/kernel-4.18.0-x86_64.config</span><br><span class="line"></span><br><span class="line"><span class="comment"># miss match with 8.7 config, failed !!!</span></span><br><span class="line"><span class="comment">#$ cat ~/lustre-release/lustre/kernel_patches/kernel_configs/kernel-4.18.0-4.18-rhel8-x86_64.config &gt;&gt; ~/kernel/rpmbuild/SOURCES/kernel-4.18.0-x86_64.config</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#replace</span></span><br><span class="line">$ <span class="built_in">cat</span> /boot/config-4.18.0-513.18.1.el8_lustre.x86_64 &gt;&gt; ~/kernel/rpmbuild/SOURCES/kernel-4.18.0-x86_64.config</span><br><span class="line">Find the line with <span class="string">&#x27;# IO Schedulers&#x27;</span> and insert following two lines below it: <span class="comment">#Enabled in the default</span></span><br><span class="line">CONFIG_IOSCHED_DEADLINE=y</span><br><span class="line">CONFIG_DEFAULT_IOSCHED=<span class="string">&quot;deadline&quot;</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel/rpmbuild</span><br><span class="line">$ buildid=<span class="string">&quot;_lustre&quot;</span></span><br><span class="line">$ <span class="built_in">export</span> C_INCLUDE_PATH=<span class="string">&quot;/usr/include&quot;</span></span><br><span class="line">$ rpmbuild -ba --with firmware --target x86_64 --with baseonly \</span><br><span class="line">           --without kabichk --define <span class="string">&quot;buildid <span class="variable">$&#123;buildid&#125;</span>&quot;</span> \</span><br><span class="line">           ~/kernel/rpmbuild/SPECS/kernel.spec</span><br><span class="line"></span><br><span class="line"><span class="comment">#some errors resloved: https://discussion.fedoraproject.org/t/rebuilding-fedora-32-kernel-unknown-type-name-elf32-word/78541</span></span><br><span class="line"></span><br><span class="line">....</span><br><span class="line">Checking <span class="keyword">for</span> unpackaged file(s): /usr/lib/rpm/check-files /root/kernel/rpmbuild/BUILDROOT/kernel-4.18.0-477.27.2.el8_lustre.x86_64</span><br><span class="line">Wrote: /root/kernel/rpmbuild/SRPMS/kernel-4.18.0-477.27.2.el8_lfs.src.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/noarch/kernel-doc-4.18.0-477.27.2.el8_lfs.noarch.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-headers-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-debuginfo-common-x86_64-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-core-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-devel-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-modules-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-modules-extra-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-modules-internal-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Wrote: /root/kernel/rpmbuild/RPMS/x86_64/kernel-debuginfo-4.18.0-477.27.2.el8_lfs.x86_64.rpm</span><br><span class="line">Executing(%clean): /bin/sh -e /var/tmp/rpm-tmp.gTAWhY</span><br><span class="line">+ <span class="built_in">umask</span> 022</span><br><span class="line">+ <span class="built_in">cd</span> /root/kernel/rpmbuild/BUILD</span><br><span class="line">+ <span class="built_in">cd</span> kernel-4.18.0-477.27.2.el8_7</span><br><span class="line">+ <span class="built_in">rm</span> -rf /root/kernel/rpmbuild/BUILDROOT/kernel-4.18.0-477.27.2.el8_lfs.x86_64</span><br><span class="line">+ <span class="built_in">exit</span> 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#build perf tools</span></span><br><span class="line">$ <span class="built_in">cd</span> ~/kernel/rpmbuild/BUILD/kernel-4.18.0-477.13.1.el8_8/linux-4.18.0-477.27.2.el8_lustre.x86_64/tools/perf</span><br><span class="line">$ make -j 8</span><br><span class="line">bpf_program__title is deprecated error</span><br><span class="line"></span><br><span class="line"><span class="comment">#https://lore.kernel.org/bpf/20200909071920.GA1498025@krava/T/</span></span><br><span class="line">$ vi ~/kernel/rpmbuild/BUILD/kernel-4.18.0-477.13.1.el8_8/linux-4.18.0-477.27.2.el8_lustre.x86_64/tools/perf/util/bpf-loader.c</span><br><span class="line">        //config_str = bpf_program__title(prog, <span class="literal">false</span>);</span><br><span class="line">        config_str = bpf_program__section_name(prog);  &lt;---332</span><br><span class="line"></span><br><span class="line">                //title = bpf_program__title(prog, <span class="literal">false</span>);</span><br><span class="line">                title = bpf_program__section_name(prog); &lt;---459</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span>  ~/kernel/rpmbuild/BUILD/kernel-4.18.0-477.13.1.el8_8/linux-4.18.0-477.27.2.el8_lustre.x86_64/tools/perf</span><br><span class="line">$ make -j 8</span><br><span class="line">......</span><br><span class="line">  LD      perf-in.o</span><br><span class="line">  LINK    perf</span><br><span class="line"></span><br><span class="line"><span class="comment">#kernel version 6.5</span></span><br><span class="line">$ make -C tools/perf/ WERROR=0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">mkdir</span> -p /root/2.15.4/src</span><br><span class="line">$ <span class="built_in">mv</span> /root/kernel/rpmbuild/SRPMS/kernel-4.18.0-477.27.2.el8_lustre.src.rpm /root/2.15.4/src</span><br><span class="line">$ <span class="built_in">mv</span> /root/kernel/rpmbuild/RPMS/x86_64/*rpm /root/kernel/rpmbuild/RPMS/noarch/*rpm /root/2.15.4/</span><br><span class="line"></span><br><span class="line"><span class="comment"># if mellanox</span></span><br><span class="line">$ <span class="built_in">cd</span> MLNX_OFED_LINUX-5.9-0.5.6.0-rhel8.7-x86_64</span><br><span class="line"><span class="comment">#if need nvme over fabrics</span></span><br><span class="line">$ ./mlnxofedinstall --distro rhel8.8 --add-kernel-support --with-nvmf </span><br><span class="line"></span><br><span class="line"><span class="comment">#else</span></span><br><span class="line">$ ./mlnxofedinstall --add-kernel-support --skip-distro-check  --skip-repo --with-nvmf</span><br><span class="line"></span><br><span class="line"><span class="comment">#if you want install in another node by rpm, no use mlxofedinstall</span></span><br><span class="line">$ scp -r /tmp/MLNX_OFED_LINUX-23.10-2.1.3.1-4.18.0-513.18.1.el8_lustre.x86_64/MLNX_OFED_LINUX-23.10-2.1.3.1-rhel8.9-ext/RPMS root@testnode:</span><br><span class="line"><span class="comment">#login testnode, install these pacages, rpm -ivh --force --nodeps</span></span><br><span class="line">mlnx-tools-24.01-0.2310213.x86_64.rpm </span><br><span class="line">knem-modules-1.1.4.90mlnx3-OFED.23.10.0.2.1.1.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">rdma-core-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">librdmacm-utils-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">srp_daemon-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">ucx-rdmacm-1.16.0-1.2310213.x86_64.rpm </span><br><span class="line">ibarr-0.1.3-1.2310055.x86_64.rpm </span><br><span class="line">mlnx-ofa_kernel-modules-23.10-OFED.23.10.2.1.3.1.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">iser-23.10-OFED.23.10.2.1.3.1.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">infiniband-diags-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">opensm-devel-5.17.0.1.MLNX20240219.0eca20cc-0.1.2310213.x86_64.rpm </span><br><span class="line">ucx-1.16.0-1.2310213.x86_64.rpm </span><br><span class="line">openmpi-4.1.7a1-1.2310055.x86_64.rpm </span><br><span class="line">mlnx-ofa_kernel-23.10-OFED.23.10.2.1.3.1.rhel8u9.x86_64.rpm </span><br><span class="line">kernel-mft-4.26.1-3.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">xpmem-modules-2.7.3-1.2310055.rhel8u9.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">libxpmem-2.7.3-1.2310055.rhel8u9.x86_64.rpm </span><br><span class="line">libibumad-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">ibsim-0.12-1.2310055.x86_64.rpm </span><br><span class="line">opensm-5.17.0.1.MLNX20240219.0eca20cc-0.1.2310213.x86_64.rpm </span><br><span class="line">mstflint-4.16.1-2.2310055.x86_64.rpm </span><br><span class="line">ibdump-6.0.0-1.2310055.x86_64.rpm </span><br><span class="line">ucx-cma-1.16.0-1.2310213.x86_64.rpm </span><br><span class="line">hcoll-4.8.3223-1.2310055.x86_64.rpm </span><br><span class="line">mlnx-iproute2-6.4.0-1.2310055.x86_64.rpm </span><br><span class="line">mlnx-fw-updater-23.10-2.1.3.1.x86_64.rpm </span><br><span class="line">libibverbs-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">mlnx-ofa_kernel-source-23.10-OFED.23.10.2.1.3.1.rhel8u9.x86_64.rpm </span><br><span class="line">xpmem-2.7.3-1.2310055.rhel8u9.x86_64.rpm </span><br><span class="line">isert-23.10-OFED.23.10.2.1.3.1.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">librdmacm-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">libibverbs-utils-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">opensm-libs-5.17.0.1.MLNX20240219.0eca20cc-0.1.2310213.x86_64.rpm </span><br><span class="line">perftest-23.10.0-0.29.g0705c22.2310055.x86_64.rpm </span><br><span class="line">ibutils2-2.1.1-0.1.MLNX20240219.g79770a56.2310213.x86_64.rpm </span><br><span class="line">sharp-3.5.1.MLNX20240219.7fcef5af-1.2310213.x86_64.rpm </span><br><span class="line">ucx-xpmem-1.16.0-1.2310213.x86_64.rpm </span><br><span class="line">mlnx-ethtool-6.4-1.2310055.x86_64.rpm </span><br><span class="line">mlnxofed-docs-23.10-2.1.3.1.noarch.rpm </span><br><span class="line">mlnx-ofa_kernel-devel-23.10-OFED.23.10.2.1.3.1.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">srp-23.10-OFED.23.10.2.1.3.1.kver.4.18.0_513.18.1.el8_lustre.x86_64.x86_64.rpm </span><br><span class="line">rdma-core-devel-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">opensm-static-5.17.0.1.MLNX20240219.0eca20cc-0.1.2310213.x86_64.rpm </span><br><span class="line">ucx-devel-1.16.0-1.2310213.x86_64.rpm </span><br><span class="line">mpitests_openmpi-3.2.21-8418f75.2310055.x86_64.rpm </span><br><span class="line">knem-1.1.4.90mlnx3-OFED.23.10.0.2.1.1.rhel8u9.x86_64.rpm </span><br><span class="line">ofed-scripts-23.10-OFED.23.10.2.1.3.x86_64.rpm </span><br><span class="line">ibacm-2307mlnx47-1.2310213.x86_64.rpm </span><br><span class="line">mft-4.26.1-3.x86_64.rpm </span><br><span class="line">ucx-ib-1.16.0-1.2310213.x86_64.rpm </span><br><span class="line">rshim-2.0.19-0.gbf7f1f2.x86_64.rpm </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#build zfs,downloda zfs-2.1.5.tar.gz</span></span><br><span class="line">$ dnf install gcc make autoconf automake libtool rpm-build libtirpc-devel libblkid-devel libuuid-devel libudev-devel openssl-devel zlib-devel libaio-devel libattr-devel elfutils-libelf-devel kernel-devel-$(<span class="built_in">uname</span> -r) python3 python3-devel python3-setuptools python3-cffi libffi-devel git ncompress libcurl-devel python3-packaging dkms</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">-------------------&gt; reboot, boot by lustre kernel</span><br><span class="line"></span><br><span class="line">$ zfs zfs-2.1.5</span><br><span class="line"><span class="comment">#$ git checkout master</span></span><br><span class="line">$ sh autogen.sh</span><br><span class="line">$ ./configure; make -s -j $(grep -c MHz /proc/cpuinfo); make rpms</span><br><span class="line"></span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzpool5-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libnvpair3-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libuutil3-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzfs5-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzfs5-devel-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-test-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/noarch/zfs-dracut-2.1.5-1.el8.noarch.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/noarch/python3-pyzfs-2.1.5-1.el8.noarch.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-debugsource-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzpool5-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libnvpair3-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libuutil3-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/libzfs5-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/zfs-build-root-MdXhLosA/RPMS/x86_64/zfs-test-debuginfo-2.1.5-1.el8.x86_64.rpm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#ldiskfs only, no zfs</span></span><br><span class="line">--with-ldiskfsprogs</span><br><span class="line"></span><br><span class="line"><span class="comment">#fix the lustre-release bad file time</span></span><br><span class="line">$ find lustre-release -<span class="built_in">type</span> f -<span class="built_in">exec</span> <span class="built_in">touch</span> &#123;&#125; +</span><br><span class="line"></span><br><span class="line"><span class="comment">#build lustre with mellanox</span></span><br><span class="line">$ ./configure --enable-quota --with-linux=/root/kernel/rpmbuild/BUILD/kernel-4.18.0-477.27.2.el8_7/linux-4.18.0-477.27.2.el8_lustre.x86_64 --enable-server --with-zfs=/root/zfs/zfs-2.1.15 --with-o2ib=/usr/src/ofa_kernel/default</span><br><span class="line"></span><br><span class="line"><span class="comment">#-------------------------------------------------------------------------------here is 13.1 not 477.27.2</span></span><br><span class="line">$ ./configure --enable-quota --with-linux=/root/kernel/rpmbuild/BUILD/kernel-4.18.0-477.13.1.el8_8/linux-4.18.0-477.27.2.el8_lustre.x86_64 --enable-server --with-zfs=/root/zfs-2.1.15 --with-o2ib=/usr/src/ofa_kernel/default</span><br><span class="line"></span><br><span class="line">or by defualt o2ib </span><br><span class="line">$ dnf install librdmacm-debuginfo rdma-core-debuginfo rdma-core-debugsource rdma-core-debugsource</span><br><span class="line"></span><br><span class="line">$ ./configure --enable-quota --with-linux=/root/kernel/rpmbuild/BUILD/kernel-4.18.0-477.13.1.el8_8/linux-4.18.0-477.27.2.el8_lfs.x86_64 --enable-server --with-zfs=/root/zfs/zfs-2.1.15 --with-o2ib=/usr/src/debug/rdma-core-41.0-1.el8.x86_64</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">find /usr/src/ofa_kernel/default -<span class="built_in">type</span> d -name ofed_scripts</span><br><span class="line"><span class="built_in">dirname</span>: missing operand</span><br><span class="line">Try <span class="string">&#x27;dirname --help&#x27;</span> <span class="keyword">for</span> more information.</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line"><span class="comment">#the bash is wrong in 2.15.61</span></span><br><span class="line">grep -Rin ofed_scripts </span><br><span class="line">contrib/lbuild/lbuild:1737:     o2ib_location=$(find $(<span class="built_in">pwd</span>)/usr/src/<span class="variable">$&#123;kib_prefix&#125;</span> -<span class="built_in">type</span> -name ofed_scripts)</span><br><span class="line">modify to</span><br><span class="line">contrib/lbuild/lbuild:1737:     o2ib_location=$(find -L $(<span class="built_in">pwd</span>)/usr/src/<span class="variable">$&#123;kib_prefix&#125;</span> -<span class="built_in">type</span> -name ofed_scripts)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">config.status: executing libtool commands</span><br><span class="line"></span><br><span class="line">CC:            gcc</span><br><span class="line">LD:            /usr/bin/ld -m elf_x86_64</span><br><span class="line">CPPFLAGS:      -include /root/lfs-release/undef.h -include /root/lfs-release/config.h -I/root/lfs-release/lnet/include/uapi -I/root/lfs-release/lfs/include/uapi -I/root/lfs-release/libcfs/include -I/root/lfs-release/lnet/utils/ -I/root/lfs-release/lfs/include </span><br><span class="line">CFLAGS:        -g -O2 -Wall -Werror</span><br><span class="line">EXTRA_KCFLAGS: -include /root/lfs-release/undef.h -include /root/lfs-release/config.h  -g -I/root/lfs-release/libcfs/include -I/root/lfs-release/libcfs/include/libcfs -I/root/lfs-release/lnet/include/uapi -I/root/lfs-release/lnet/include -I/root/lfs-release/lfs/include/uapi -I/root/lfs-release/lfs/include -Wno-format-truncation -Wno-stringop-truncation -Wno-stringop-overflow</span><br><span class="line"></span><br><span class="line">Type <span class="string">&#x27;make&#x27;</span> to build Lustre.</span><br><span class="line"></span><br><span class="line">$ make rpms</span><br><span class="line">......</span><br><span class="line">Checking <span class="keyword">for</span> unpackaged file(s): /usr/lib/rpm/check-files /tmp/rpmbuild-lfs-root-UgvtYxFm/BUILDROOT/lfs-2.15.2-1.el8.x86_64</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-osd-ldiskfs-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-osd-ldiskfs-mount-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-osd-zfs-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-osd-zfs-mount-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-resource-agents-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-devel-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-tests-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-tests-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-iokit-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-debugsource-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-osd-ldiskfs-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-osd-ldiskfs-mount-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-osd-zfs-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-osd-zfs-mount-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/lfs-tests-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Wrote: /tmp/rpmbuild-lfs-root-UgvtYxFm/RPMS/x86_64/kmod-lfs-tests-debuginfo-2.15.2-1.el8.x86_64.rpm</span><br><span class="line">Executing(%clean): /bin/sh -e /tmp/rpmbuild-lfs-root-UgvtYxFm/TMP/rpm-tmp.2EyaTj</span><br><span class="line"></span><br><span class="line">first time mount show input/output error when mellanox mount the broadcom RocEv2 or broadcom mount the mellanox</span><br><span class="line">each server mkfs as a lfs server</span><br><span class="line">mount twice sucess</span><br><span class="line"></span><br><span class="line">LustreError: 166-1: MGC192.168.0.16@o2ib2: Connection to MGS (at 192.168.0.16@o2ib2) was lost; <span class="keyword">in</span> progress operations using this service will fail</span><br><span class="line">LustreError: 15c-8: MGC192.168.0.16@o2ib2: Confguration from <span class="built_in">log</span> testfs6-client failed from MGS -5. Communication error between node &amp; MGS, a bad configuration, or other errors. See syslog <span class="keyword">for</span> more info</span><br><span class="line"></span><br><span class="line">bnxt_en 0000:8a:00.0: QPLIB: FP: CQ Processed Req wr_id[39] = 0xff39ee65e0331739 with status 0x3</span><br><span class="line">Lustre: Mounted lfs4-client</span><br><span class="line">Lustre: MGS: Client d1a8ba05-ce20-4632-8409-13306103f0d9 (at 192.168.0.17@o2ib2) reconnecting</span><br><span class="line">Lustre: MGS: haven<span class="string">&#x27;t heard from client d1a8ba05-ce20-4632-8409-13306103f0d9 (at 192.168.0.17@o2ib2) in 227 seconds. I think it&#x27;</span>s dead, and I am evicting it. exp 00000000eb5ecb96, cur 1676287681 expire 1676287531 last 1676287454</span><br></pre></td></tr></table></figure>

<h4 id="kernel-modules"><a href="#kernel-modules" class="headerlink" title="kernel modules"></a>kernel modules</h4><ul>
<li>&#x2F;sys&#x2F;module&#x2F;osd_zfs&#x2F;parameters&#x2F;osd_object_sync_delay_us</li>
</ul>
<h4 id="idmapped-mounts"><a href="#idmapped-mounts" class="headerlink" title="idmapped mounts"></a>idmapped mounts</h4><p><a target="_blank" rel="noopener" href="https://lwn.net/Articles/838916/">1</a><br><a target="_blank" rel="noopener" href="https://lpc.events/event/11/contributions/1086/attachments/926/1826/christian_brauner_idmapped_mounts.pdf">2</a><br><a target="_blank" rel="noopener" href="https://github.com/containerd/containerd/pull/4734">3</a><br><a target="_blank" rel="noopener" href="https://github.com/containers/podman/issues/10374">4</a></p>
<h4 id="debug"><a href="#debug" class="headerlink" title="debug"></a>debug</h4><ul>
<li>Lost Connection<ul>
<li>Error messages with signature of “-107” and “-108” indicate communication problem somewhere in the Lustre stack. It can lay anywhere, but there will always be information from the Lustre Network Driver, for infiniband: “o2iblnd”. If the LND does not directly point to the problem, look for a downed node. Note that one cause can be Out Of Memory condition causing dropped connections and reconnect cycles. It may be complex to figure out the root cause for network related problems.</li>
</ul>
</li>
<li>RPC Debug Messages<ul>
<li>One very common message in the log files refers to some activity taking longer than expected and related possibility of timeout. Such as example below. Highlighted key parts of the message. These are most common indicators for user to debug the root cause:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Lustre: 10763:0:(service.c:1393:ptlrpc_server_handl e_request()) @@@ Request x1367071000625897 took longer than estimated (888+12s); client may <span class="built_in">timeout</span>. req@ffff880068217400 x1367071000625897/t133143988007 o101 - &gt;316a078c - 99d7 - fda8 - 5d6a - e357a4eba5a9@NET_0x40000000000c7_UUID:0/0 lens 680/680 e 2 to 0 dl 1303746736 ref 1 fl Complete:/0/0 rc 301/301</span><br></pre></td></tr></table></figure></li>
<li>To decipher the message, you can note the eye-catcher, and req@ which indicates the transaction number (x…&#x2F;t…) following the memory address from ptlrpc_request. o101 LDLM (Lustre Distributed Lock Manager) requeue request, which are very common. o400is indicative of obd ping. Last part is the request&#x2F;reply status, which is normally an errorno, but higher values are lfs specific. Status rc 301 refers to lock aborted.</li>
</ul>
</li>
<li>Lustre Log Eye-catcher <ul>
<li>It is common to find strings ### (LDLM_ERROR) and @@@ (DEBUG_REQ) on the logs to act as an eye-catcher. When looking for error messages potentially related to the event at hand, it is useful to seek these strings.</li>
</ul>
</li>
</ul>
<h4 id="show-scrub-status"><a href="#show-scrub-status" class="headerlink" title="show scrub status"></a>show scrub status</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ lctl get_param -n osd-ldiskfs.test-MDT0001.oi_scrub</span><br><span class="line">name: OI_scrub</span><br><span class="line">magic: 0x4c5fd252</span><br><span class="line">oi_files: 64</span><br><span class="line">status: init</span><br><span class="line">flags:</span><br><span class="line">param:</span><br><span class="line">time_since_last_completed: N/A</span><br><span class="line">time_since_latest_start: N/A</span><br><span class="line">time_since_last_checkpoint: N/A</span><br><span class="line">latest_start_position: N/A</span><br><span class="line">last_checkpoint_position: N/A</span><br><span class="line">first_failure_position: N/A</span><br><span class="line">checked: 0</span><br><span class="line">updated: 0</span><br><span class="line">failed: 0</span><br><span class="line">prior_updated: 0</span><br><span class="line">noscrub: 0</span><br><span class="line">igif: 0</span><br><span class="line">success_count: 0</span><br><span class="line">run_time: 0 seconds</span><br><span class="line">average_speed: 0 objects/sec</span><br><span class="line">real_time_speed: N/A</span><br><span class="line">current_position: N/A</span><br><span class="line">lf_scanned: 0</span><br><span class="line">lf_repaired: 0</span><br><span class="line">lf_failed: 0</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://github.com/DDNStorage/lustre_manual_markdown/blob/master/03.11-File%20Level%20Redundancy%20(FLR).md">File Level Redundancy (FLR)</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Note: For redundancy and fault-tolerance, <span class="built_in">users</span> need to make sure that different mirrors must be on different OSTs, even OSSs and racks. An understanding of cluster topology is necessary to achieve this architecture. In the initial implementation the use of the existing OST pools mechanism will allow separating OSTs by any arbitrary criteria: i.e. fault domain. In practice, <span class="built_in">users</span> can take advantage of OST pools by grouping OSTs by topological information. Therefore, when creating a mirrored file, <span class="built_in">users</span> can indicate <span class="built_in">which</span> OST pools can be used by mirrors.</span><br><span class="line"></span><br><span class="line">The first mirror has 4MB stripe size and two stripes across OSTs <span class="keyword">in</span> the “flash” OST pool. The second mirror has 4MB stripe size inherited from the first mirror, and stripes across all of the available OSTs <span class="keyword">in</span> the “archive” OST pool.</span><br><span class="line"></span><br><span class="line">client<span class="comment"># lfs mirror create -N -S 4M -c 2 -p flash \</span></span><br><span class="line">                          -N -c -1 -p archive /mnt/testfs/file1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">init - indicates mirrored component has been initialized (has allocated OST objects).</span><br><span class="line">stale - indicates mirrored component does not have up-to-date data. Stale components will not be used <span class="keyword">for</span> <span class="built_in">read</span> or write operations, and need to be resynchronized by running lfs mirror resync <span class="built_in">command</span> before they can be accessed again.</span><br><span class="line">prefer - indicates mirrored component is preferred <span class="keyword">for</span> <span class="built_in">read</span> or write. For example, the mirror is located on SSD-based OSTs or is closer, fewer hops, on the network to the client. This flag can be <span class="built_in">set</span> by <span class="built_in">users</span> at mirror creation time.</span><br><span class="line">The following <span class="built_in">command</span> creates a mirrored file with 3 PFL mirrors:</span><br><span class="line"></span><br><span class="line">client<span class="comment"># lfs mirror create -N -E 4M -p flash --flags=prefer -E eof -c 2 \</span></span><br><span class="line">-N -E 16M -S 8M -c 4 -p archive --comp-flags=prefer -E eof -c -1 \</span><br><span class="line">-N -E 32M -c 1 -p none -E eof -c -1 /mnt/testfs/file2</span><br></pre></td></tr></table></figure>

<h3 id="lfs-extended-attributes"><a href="#lfs-extended-attributes" class="headerlink" title="lfs extended attributes"></a>lfs extended attributes</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">$ zdb -vv -ddddd -O tank-11/zstd bio_test_suite/IO/run_io.sh</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">         2    1   128K     1K  11.5K      1K     1K  100.00  ZFS plain file</span><br><span class="line">                                               532   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED </span><br><span class="line">	dnode maxblkid: 0</span><br><span class="line">	path	/bio_test_suite/IO/run_io.sh</span><br><span class="line">	uid     99</span><br><span class="line">	gid     99</span><br><span class="line">	atime	Tue Jun  6 15:28:48 2023</span><br><span class="line">	mtime	Fri May 26 09:30:18 2023</span><br><span class="line">	ctime	Fri May 26 09:30:18 2023</span><br><span class="line">	crtime	Mon May 22 09:44:59 2023</span><br><span class="line">	gen	1033</span><br><span class="line">	mode	100644</span><br><span class="line">	size	734</span><br><span class="line">	parent	6240</span><br><span class="line">	links	1</span><br><span class="line">	pflags	840800000004</span><br><span class="line">	SA xattrs: 348 bytes, 5 entries</span><br><span class="line"></span><br><span class="line">		trusted.version = \206g\211\032\013\000\000\000</span><br><span class="line">		trusted.som = \004\000\000\000\000\000\000\000\336\002\000\000\000\000\000\000\016\000\000\000\000\000\000\000</span><br><span class="line">		trusted.link = \337\361\352\021\001\000\000\0003\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\033\000\000\000\002\000\000^3\000\000\000e\000\000\000\000run_io.sh</span><br><span class="line">		trusted.lov = \320\013\321\013\001\000\000\000\022\000\000\000\000\000\000\000\340e\000\000\002\000\000\000\000\000\020\000\001\000\000\000$\255\034\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000P\000\000\000</span><br><span class="line">		trusted.lma = \000\000\000\000\000\000\000\000\340e\000\000\002\000\000\000\022\000\000\000\000\000\000\000</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 1:56c00000000:4000 400L/400P F=1 B=4727/4727 <span class="built_in">cksum</span>=39831943e7:24b520795602:d04b5cf4d8504:3560da12be8eb50</span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000000400) size    1K</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ zdb -vv -ddddd tank-11/zstd 2</span><br><span class="line">Dataset tank-11/zstd [ZPL], ID 160, cr_txg 40, 7.50T, 7542 objects, rootbp DVA[0]=&lt;2:3001102000:1000&gt; DVA[1]=&lt;2:3101101000:1000&gt; [L0 DMU objset] fletcher4 uncompressed unencrypted LE contiguous unique double size=1000L/1000P birth=197721L/197721P fill=7542 <span class="built_in">cksum</span>=15c0016c50:390952d4849f:4ef1be86f619e9:4ce1a63092608951</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">         2    1   128K     1K  11.5K      1K     1K  100.00  ZFS plain file (K=inherit) (Z=inherit=zstd-unknown)</span><br><span class="line">                                               532   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED </span><br><span class="line">	dnode maxblkid: 0</span><br><span class="line">	path	/bio_test_suite/IO/run_io.sh</span><br><span class="line">	uid     99</span><br><span class="line">	gid     99</span><br><span class="line">	atime	Tue Jun  6 15:28:48 2023</span><br><span class="line">	mtime	Fri May 26 09:30:18 2023</span><br><span class="line">	ctime	Fri May 26 09:30:18 2023</span><br><span class="line">	crtime	Mon May 22 09:44:59 2023</span><br><span class="line">	gen	1033</span><br><span class="line">	mode	100644</span><br><span class="line">	size	734</span><br><span class="line">	parent	6240</span><br><span class="line">	links	1</span><br><span class="line">	pflags	840800000004</span><br><span class="line">	SA xattrs: 348 bytes, 5 entries</span><br><span class="line"></span><br><span class="line">		trusted.version = \206g\211\032\013\000\000\000</span><br><span class="line">		trusted.som = \004\000\000\000\000\000\000\000\336\002\000\000\000\000\000\000\016\000\000\000\000\000\000\000</span><br><span class="line">		trusted.link = \337\361\352\021\001\000\000\0003\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\033\000\000\000\002\000\000^3\000\000\000e\000\000\000\000run_io.sh</span><br><span class="line">		trusted.lov = \320\013\321\013\001\000\000\000\022\000\000\000\000\000\000\000\340e\000\000\002\000\000\000\000\000\020\000\001\000\000\000$\255\034\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000P\000\000\000</span><br><span class="line">		trusted.lma = \000\000\000\000\000\000\000\000\340e\000\000\002\000\000\000\022\000\000\000\000\000\000\000</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 1:56c00000000:4000 400L/400P F=1 B=4727/4727 <span class="built_in">cksum</span>=39831943e7:24b520795602:d04b5cf4d8504:3560da12be8eb50</span><br><span class="line"></span><br><span class="line">		segment [0000000000000000, 0000000000000400) size    1K</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ getfattr -n trusted.lov --absolute-names -e hex /tank/test_IO/run_io.sh</span><br><span class="line"><span class="comment"># file: /tank/test_IO/run_io.sh</span></span><br><span class="line">trusted.lov=0xd00bd10b010000001200000000000000e065000002000000000010000100000024ad1c000000000000000000000000000000000050000000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ getfattr -n trusted.lov --only-values /tank/test_IO/run_io.sh | hexdump -C</span><br><span class="line">getfattr: Removing leading <span class="string">&#x27;/&#x27;</span> from absolute path names</span><br><span class="line">00000000  d0 0b d1 0b 01 00 00 00  12 00 00 00 00 00 00 00  |................|</span><br><span class="line">00000010  e0 65 00 00 02 00 00 00  00 00 10 00 01 00 00 00  |.e..............|</span><br><span class="line">00000020  24 ad 1c 00 00 00 00 00  00 00 00 00 00 00 00 00  |$...............|</span><br><span class="line">00000030  00 00 00 00 50 00 00 00                           |....P...|</span><br><span class="line">00000038</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -li run_io.sh </span><br><span class="line">2 -rw-r--r-- 1 99 99 734 May 26 09:30 run_io.sh</span><br><span class="line"></span><br><span class="line">another example------------</span><br><span class="line">$ lfs getstripe -v test_file_300M </span><br><span class="line">test_file_300M</span><br><span class="line">lmm_magic:         0x0BD10BD0</span><br><span class="line">lmm_seq:           0x200000402</span><br><span class="line">lmm_object_id:     0xd3c</span><br><span class="line">lmm_fid:           [0x200000402:0xd3c:0x0]</span><br><span class="line">lmm_stripe_count:  1</span><br><span class="line">lmm_stripe_size:   1048576</span><br><span class="line">lmm_pattern:       raid0</span><br><span class="line">lmm_layout_gen:    0</span><br><span class="line">lmm_stripe_offset: 2</span><br><span class="line">	obdidx		 objid		 objid		 group</span><br><span class="line">	     2	          1923	        0x783	             0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -lhi /test_ost2/test_ost2/O/0/d3/1923</span><br><span class="line">2753 -rw-rw-rw- 1 root root 300M Aug 25 16:02 /test_ost2/test_ost2/O/0/d3/1923</span><br><span class="line"></span><br><span class="line">$ zdb -vv -bbbb -O test_ost2/test_ost2 O/0/d3/1923</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">      2753    2   128K     1M   299M     512   300M  100.00  ZFS plain file</span><br><span class="line">                                               192   bonus  System attributes</span><br><span class="line">	dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED SPILL_BLKPTR</span><br><span class="line">	dnode maxblkid: 299</span><br><span class="line">	uid     0</span><br><span class="line">	gid     0</span><br><span class="line">	atime	Thu Jan  1 08:00:00 1970</span><br><span class="line">	mtime	Fri Aug 25 16:02:04 2023</span><br><span class="line">	ctime	Fri Aug 25 16:02:04 2023</span><br><span class="line">	crtime	Fri Aug 25 16:00:43 2023</span><br><span class="line">	gen	77</span><br><span class="line">	mode	100666</span><br><span class="line">	size	314572800</span><br><span class="line">	parent	648</span><br><span class="line">	links	1</span><br><span class="line">	pflags	0</span><br><span class="line">	rdev	0x0000000000000000</span><br><span class="line">	SA xattrs: 204 bytes, 3 entries</span><br><span class="line"></span><br><span class="line">		trusted.lma = \010\000\000\000\000\000\000\000\000\000\002\000\001\000\000\000\203\007\000\000\000\000\000\000</span><br><span class="line">		trusted.fid = \002\004\000\000\002\000\000\000&lt;\015\000\000\000\000\000\000\000\000\020\000\001\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000</span><br><span class="line">		trusted.version = \255\010\000\000\001\000\000\000</span><br><span class="line"></span><br><span class="line">trusted.fid = \002\004\000\000\002 ---&gt; = 0200000402</span><br><span class="line">                                  \000\000\000&lt;\015\000\000\000 ---&gt;  <span class="string">&quot;&lt;&quot;</span>=074=3C=60 015=0d=13 = 0x0d3c</span><br><span class="line">                                                               \000\000\000\000\000\020\000\001\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000\000</span><br><span class="line"></span><br><span class="line">$ ./fid2path test_ost2/test_ost2 2753</span><br><span class="line">[0x200000402:0xd3c:0x0]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="Dell-lustre-tuning-with-OPF"><a href="#Dell-lustre-tuning-with-OPF" class="headerlink" title="Dell lustre tuning with OPF"></a><a target="_blank" rel="noopener" href="https://www.delltechnologies.com/content/dam/digitalassets/active/en/unauth/white-papers/products/ready-solutions/dell-data-accelerator-cambridge.pdf">Dell lustre tuning with OPF</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#lnet</span></span><br><span class="line">options hfi1 krcvqs=8 pcie_caps=0x51 rcvhdrcnt=8192</span><br><span class="line"><span class="comment">#OSS</span></span><br><span class="line">mds$ <span class="built_in">echo</span> 127 &gt; /sys/module/mdt/parameters/max_mod_rpcs_per_client</span><br><span class="line">oss$ lctl set_param obdfilter.*OST*.brw_size=16</span><br><span class="line">client$ lctl set_param osc.*.checksums=0</span><br><span class="line"></span><br><span class="line">client$ lctl set_param osc.*OST*.max_pages_per_rpc=16M</span><br><span class="line">client$ lctl client$ lctl set_param mdc.*.max_rpcs_in_flight=128 osc.*.max_rpcs_in_flight=128 mdc.*.max_mod_rpcs_in_flight=127 osc.*OST*.max_pages_per_rpc=16M</span><br><span class="line">client$ lctl set_param llite.*.max_read_ahead_mb=2048 llite.*.max_read_ahead_per_file_mb=256</span><br><span class="line">client$ lctl set_param osc.*OST*.max_dirty_mb=512</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="http://lists.lustre.org/pipermail/lustre-discuss-lustre.org/2023-January/018413.html"></a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">cat /sys/fs/lustre/osp/tfs3-OST0002-osc-MDT0000/max_rpcs_in_progress</span><br><span class="line">4096 #max_rpcs_in_progress</span><br><span class="line"></span><br><span class="line">$ lctl list_param -R &#x27;*&#x27; | grep lfsck -v | grep -E &#x27;sync_in_flight|sync_changes|max_rpcs_in_progress|max_rpcs_in_flight|max_rpcs_in_progress|destroys_in_fligh&#x27; | while read line; do echo $line; lctl get_param -n $line ; done</span><br><span class="line"></span><br><span class="line">$ lctl set_param mdt.*-MDT0000.md_stats=clear</span><br><span class="line">$ sleep 10</span><br><span class="line">$ lctl get_param mdt.*-MDT0000.md_stats</span><br></pre></td></tr></table></figure>

<h3 id="Lustre-word"><a href="#Lustre-word" class="headerlink" title="Lustre word"></a>Lustre word</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">MGS Management Server</span><br><span class="line">MGT Management Target</span><br><span class="line">MDS Metadata Server</span><br><span class="line">MDT Metadata Target</span><br><span class="line">OSS Object Storage Server</span><br><span class="line">OST Object Storage Target</span><br><span class="line">FDR Fourteen Data Rate</span><br><span class="line">IB Infiniband</span><br><span class="line">OPA Omni-Path Architecture</span><br><span class="line">LNet Lustre Networking</span><br><span class="line">RAID Redundant Array of Independent Disks</span><br><span class="line">PFL Progressive File Layout</span><br><span class="line">DoM Data on MDT</span><br><span class="line">SEL Self Extending Layout</span><br><span class="line">DNE Distributed Namespace</span><br><span class="line">EA Extended Attribute</span><br><span class="line">OSC Object Storage Client</span><br><span class="line">LMV Logical Metadata Volume</span><br><span class="line">VFS Virtual File System</span><br><span class="line">LOV Logical Object Volume</span><br><span class="line">PTL-RPC Portal Remote Procedure Call</span><br><span class="line">LDLM Lustre Distributed Lock Manager</span><br><span class="line">OBD Object Based Disk</span><br><span class="line">OFD OBD Filter Device</span><br><span class="line">OSD Object Storage Device</span><br><span class="line">MDC Metadata Client</span><br><span class="line">LTS Lustre Test Suites</span><br><span class="line">HSM Hierarchical Storage Manager</span><br><span class="line">IOR Interleaved or Random</span><br><span class="line">MGC Management Client</span><br><span class="line">LRU Least Recently Used</span><br><span class="line">FLD FID Location Database</span><br><span class="line">FID File Identifier</span><br><span class="line">LWP Light Weight Proxy</span><br><span class="line">CPT CPU Partition Table</span><br><span class="line">NUMA Non-Uniform Memory Access</span><br><span class="line">SMP Symmetric Multi-Processing</span><br><span class="line">OSP Object Storage Proxy</span><br><span class="line">OI Object Index</span><br><span class="line">IGIF Inode and Generation In FID</span><br><span class="line">IDIF Object ID In FID</span><br><span class="line">UUID Universally Unique Identifier</span><br><span class="line">NID Network Identifier</span><br></pre></td></tr></table></figure>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-Maintenance"><span class="toc-number">1.</span> <span class="toc-text">lfs Maintenance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#reinstall-not-working"><span class="toc-number">2.</span> <span class="toc-text">reinstall not working</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-bugs-in-test"><span class="toc-number">3.</span> <span class="toc-text">lfs bugs in test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#print-OST"><span class="toc-number">4.</span> <span class="toc-text">print OST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#hot-file"><span class="toc-number">5.</span> <span class="toc-text">[hot file]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NET"><span class="toc-number">6.</span> <span class="toc-text">NET</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#UDSP"><span class="toc-number">6.1.</span> <span class="toc-text">UDSP</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lustre-test"><span class="toc-number">6.2.</span> <span class="toc-text">lustre test</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#project-quota"><span class="toc-number">6.3.</span> <span class="toc-text">project quota</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#User-group-quota"><span class="toc-number">7.</span> <span class="toc-text">User group quota</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#re-writeconf"><span class="toc-number">7.1.</span> <span class="toc-text">re-writeconf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#changelog"><span class="toc-number">7.2.</span> <span class="toc-text">changelog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#kdump"><span class="toc-number">7.3.</span> <span class="toc-text">kdump</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Zero-Pages-2-Cache-Pages-4-Cache-Private-8-User-Pages-16-Free-Pages"><span class="toc-number"></span> <span class="toc-text">1 Zero Pages 2 Cache Pages 4 Cache Private 8 User Pages 16 Free Pages</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#1-Progress-Indicators-2-Common-Messages-4-Error-Messages-8-Debug-Messages-16-Report-Messages"><span class="toc-number"></span> <span class="toc-text">1 Progress Indicators 2 Common Messages 4 Error Messages 8 Debug Messages 16 Report Messages</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#reaadonly-mount"><span class="toc-number">0.1.</span> <span class="toc-text">reaadonly mount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DNE"><span class="toc-number">0.2.</span> <span class="toc-text">DNE</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ALL"><span class="toc-number">1.</span> <span class="toc-text">ALL</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#collect-log"><span class="toc-number">2.</span> <span class="toc-text">collect log</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#list-all-parameters"><span class="toc-number">2.1.</span> <span class="toc-text">list all parameters</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MDS"><span class="toc-number">3.</span> <span class="toc-text">MDS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor"><span class="toc-number">3.1.</span> <span class="toc-text">monitor</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#OSS"><span class="toc-number">4.</span> <span class="toc-text">OSS</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Monitor"><span class="toc-number">4.1.</span> <span class="toc-text">Monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#format"><span class="toc-number">4.2.</span> <span class="toc-text">format</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#change-ipaddr"><span class="toc-number">4.3.</span> <span class="toc-text">change ipaddr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#EC"><span class="toc-number">4.4.</span> <span class="toc-text">EC</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PFL-Progressive-file-layouts"><span class="toc-number">4.5.</span> <span class="toc-text">PFL Progressive file layouts</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Client-CLIENT"><span class="toc-number">5.</span> <span class="toc-text">Client CLIENT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#monitor-1"><span class="toc-number">5.1.</span> <span class="toc-text">monitor</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#complie-client"><span class="toc-number">5.2.</span> <span class="toc-text">complie client</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#ubuntu-install-client"><span class="toc-number">5.2.1.</span> <span class="toc-text">ubuntu install client</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#get-the-status"><span class="toc-number">5.3.</span> <span class="toc-text">get the status</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backup-and-recovery-lfs-ZFS-OST"><span class="toc-number">6.</span> <span class="toc-text">Backup and recovery lfs ZFS OST</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-multiple-ethernet-port-for-diff-LAN"><span class="toc-number">7.</span> <span class="toc-text">lfs multiple ethernet port for diff LAN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#trace-the-kernel-sock"><span class="toc-number">8.</span> <span class="toc-text">trace the kernel sock</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-magic-num"><span class="toc-number">9.</span> <span class="toc-text">lfs magic num</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rocev2-test"><span class="toc-number">10.</span> <span class="toc-text">rocev2 test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DOM"><span class="toc-number">11.</span> <span class="toc-text">DOM</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lnet-health"><span class="toc-number">11.1.</span> <span class="toc-text">lnet health</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OPA-setting"><span class="toc-number">11.2.</span> <span class="toc-text">OPA setting</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llmount"><span class="toc-number">11.3.</span> <span class="toc-text">llmount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Skip-recovery"><span class="toc-number">11.4.</span> <span class="toc-text">Skip recovery</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-migarate"><span class="toc-number">11.5.</span> <span class="toc-text">lfs migarate</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Job-status"><span class="toc-number">11.6.</span> <span class="toc-text">Job status</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-fid-and-path"><span class="toc-number">11.7.</span> <span class="toc-text">lfs fid and path</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#trace-with-debugfs"><span class="toc-number">11.8.</span> <span class="toc-text">trace with debugfs</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#llog-reader"><span class="toc-number">11.9.</span> <span class="toc-text">llog_reader</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-12-8-client-performance-degraded-in-the-lfs-2-15-0-server"><span class="toc-number">12.</span> <span class="toc-text">2.12.8 client performance degraded in the lfs 2.15.0 server</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#lfs-zfs-direct-IO-support"><span class="toc-number">12.1.</span> <span class="toc-text">lfs zfs direct IO support</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Not-wait-the-zfs-sync"><span class="toc-number">13.</span> <span class="toc-text">Not wait the zfs sync</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#errors"><span class="toc-number">14.</span> <span class="toc-text">errors</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#network"><span class="toc-number">14.1.</span> <span class="toc-text">network</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Build-lfs-MASTER-with-zfs-under-almalinux-8-7"><span class="toc-number">15.</span> <span class="toc-text">Build lfs MASTER with zfs under almalinux 8.7</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#kernel-modules"><span class="toc-number">15.1.</span> <span class="toc-text">kernel modules</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#idmapped-mounts"><span class="toc-number">15.2.</span> <span class="toc-text">idmapped mounts</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#debug"><span class="toc-number">15.3.</span> <span class="toc-text">debug</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#show-scrub-status"><span class="toc-number">15.4.</span> <span class="toc-text">show scrub status</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#lfs-extended-attributes"><span class="toc-number">16.</span> <span class="toc-text">lfs extended attributes</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dell-lustre-tuning-with-OPF"><span class="toc-number">17.</span> <span class="toc-text">Dell lustre tuning with OPF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Lustre-word"><span class="toc-number">18.</span> <span class="toc-text">Lustre word</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2019/12/29/lfs_cmd/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2019/12/29/lfs_cmd/&text=lfs command"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2019/12/29/lfs_cmd/&is_video=false&description=lfs command"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=lfs command&body=Check out this article: http://example.com/2019/12/29/lfs_cmd/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2019/12/29/lfs_cmd/&title=lfs command"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2019/12/29/lfs_cmd/&name=lfs command&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2019/12/29/lfs_cmd/&t=lfs command"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    John Doe
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = '67e8c052/67e8c052.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'blog-comments';
      var utterances_theme = 'github-dark';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
