<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="HardwareHost Memory Buffer (HMB) HMB vs DRAM-less DRAM-less has the lower BW and OPS HMB has the more stable throughput for a long time    PCI-E GT&#x2F;s to GB&#x2F;sGT per sec to GB per sec 12345678">
<meta property="og:type" content="article">
<meta property="og:title" content="memory">
<meta property="og:url" content="http://example.com/2022/11/14/mem/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="HardwareHost Memory Buffer (HMB) HMB vs DRAM-less DRAM-less has the lower BW and OPS HMB has the more stable throughput for a long time    PCI-E GT&#x2F;s to GB&#x2F;sGT per sec to GB per sec 12345678">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/top-mem1.png">
<meta property="og:image" content="http://example.com/img/slabtop1.png">
<meta property="article:published_time" content="2022-11-14T07:35:11.000Z">
<meta property="article:modified_time" content="2025-03-04T08:29:52.767Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="mem">
<meta property="article:tag" content="memory">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/top-mem1.png">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>memory</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2022/11/14/issues/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2022/11/14/benchmark/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/11/14/mem/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/11/14/mem/&text=memory"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/11/14/mem/&is_video=false&description=memory"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=memory&body=Check out this article: http://example.com/2022/11/14/mem/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/11/14/mem/&name=memory&description=&lt;h3 id=&#34;Hardware&#34;&gt;&lt;a href=&#34;#Hardware&#34; class=&#34;headerlink&#34; title=&#34;Hardware&#34;&gt;&lt;/a&gt;Hardware&lt;/h3&gt;&lt;h4 id=&#34;Host-Memory-Buffer-HMB&#34;&gt;&lt;a href=&#34;#Host-Memory-Buffer-HMB&#34; class=&#34;headerlink&#34; title=&#34;Host Memory Buffer (HMB)&#34;&gt;&lt;/a&gt;Host Memory Buffer (HMB)&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.virtium.com/knowledge-base/how-to-select-between-dram-vs-dram-less-ssds/&#34;&gt;HMB vs DRAM-less&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;DRAM-less has the lower BW and OPS&lt;/li&gt;
&lt;li&gt;HMB has the more stable throughput for a long time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;PCI-E-GT-s-to-GB-s&#34;&gt;&lt;a href=&#34;#PCI-E-GT-s-to-GB-s&#34; class=&#34;headerlink&#34; title=&#34;PCI-E GT&amp;#x2F;s to GB&amp;#x2F;s&#34;&gt;&lt;/a&gt;PCI-E GT&amp;#x2F;s to GB&amp;#x2F;s&lt;/h4&gt;&lt;p&gt;GT per sec to GB per sec&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#PCIe 2.0 协议支持 5.0 GT/s 的传输速率，但是由于采用了 8b/10b 编码方案，导致每条通道的实际有效速率为 5*8/10=4 Gbps，也就是 500 MB/s&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;BEGIN&amp;#123;print &amp;quot;64GT/s to GB/s pcie gen3: &amp;quot; 64*(128/130)/8&amp;quot; GB/s&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;64GT/s to GB/s pcie gen3: 7.87692 GB/s&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;BEGIN&amp;#123;print &amp;quot;40GT/s to GB/s pcie gen2: &amp;quot; 40*(8/10)/8&amp;quot; GB/s&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;40GT/s to GB/s pcie gen2: 4 GB/s&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;DDR refers to double data rate, which means that the transfer rate (MT&amp;#x2F;s) is double what the speed rating is in MHz. Non-ECC DDR RAM is 64 bits (8 bytes) wide - so you’d do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RAM speed * 2 &amp;#x3D; MT&amp;#x2F;s * RAM width &amp;#x3D; bandwidth (in MB&amp;#x2F;s) 2933MHz * 2 &amp;#x3D; 5866 MT&amp;#x2F;s * 64bit&amp;#x2F;8(gb&amp;#x2F;s to GB&amp;#x2F;s) &amp;#x3D; 46,928 MB&amp;#x2F;s (approx 45.8GB&amp;#x2F;s)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GDDR6 in GPU&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GDDR6 pin 16Gb&amp;#x2F;s(16GT&amp;#x2F;s) is the maximum data rate per pin for GDDR6&lt;ul&gt;
&lt;li&gt;16Gbps * 384bit&amp;#x2F;8(bytes)&amp;#x3D; 768GB&amp;#x2F;s&lt;/li&gt;
&lt;li&gt;awk ‘BEGIN{print 46928&amp;#x2F;64*8&amp;#x2F;2}’ &amp;#x3D; 2933MT&amp;#x2F;s&lt;/li&gt;
&lt;li&gt;awk ‘BEGIN{print 768&amp;#x2F;384*8&amp;#x2F;2}’ &amp;#x3D; 8GT&amp;#x2F;s&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GDDR6X can transfer two bits per pin instead of just one. This raises overall memory speed to 19–21GT&amp;#x2F;s when compared to standard GDDR6 that tends to top out at 16GT&amp;#x2F;s.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/&#34;&gt;Memory Barriers&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C++11 atomic types, such as load(std::memory_order_acquire)&lt;/li&gt;
&lt;li&gt;POSIX mutexes, such as pthread_mutex_lock&lt;/li&gt;
&lt;li&gt;LoadLoad&lt;ul&gt;
&lt;li&gt;A LoadLoad barrier effectively prevents reordering of loads performed before the barrier with loads performed after the barrier.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;StoreStore&lt;ul&gt;
&lt;li&gt;A StoreStore barrier effectively prevents reordering of stores performed before the barrier with stores performed after the barrier.&lt;figure class=&#34;highlight c&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;meta&#34;&gt;#&lt;span class=&#34;keyword&#34;&gt;define&lt;/span&gt; barrier() __asm__ __volatile__(&lt;span class=&#34;string&#34;&gt;&amp;quot;&amp;quot;&lt;/span&gt;: : :&lt;span class=&#34;string&#34;&gt;&amp;quot;memory&amp;quot;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;barrier()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;mb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;rmb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;wmb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;smp_mb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;smp_rmb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;smp_wmb()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;DDR5&#34;&gt;&lt;a href=&#34;#DDR5&#34; class=&#34;headerlink&#34; title=&#34;DDR5&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://www.crucial.com/articles/about-memory/everything-about-ddr5-ram#scanner&#34;&gt;DDR5&lt;/a&gt;&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;DDR5-5600	69.21 GB/s&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-5200	66.12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4800	62.74&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4400	58.81&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4000	54.65&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-3600	50.26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-3200	45.62&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-3200	33.57&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;DDR5’s latency is virtually the same as DDR4&lt;ul&gt;
&lt;li&gt;CAS latency is often misunderstood because of its naming convention, but it’s only half of the true memory latency equation. True memory latency is measured in nanoseconds and is a combination of RAM speed and CAS latency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;highlight plaintext&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;True memory latency (ns) = (2000/RAM Speed) (ns) x CAS latency&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;true memory latency of DDR4-3200 CL22 = 13.75 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;true memory latency of DDR5-4800 CL40 = 16.67 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Memory experts and system architects know that the latency matters only at the system level since that is what users typically experience. That said, system latency is also measured in nanoseconds and is a combination of host memory controller features and behavior, number of module ranks, memory speed, and true memory latency.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;CAS latency (CL)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4800 CL40 92.8 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR4-3200 CL22 90.0 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;So, what is CAS latency? At a basic level, latency refers to the time delay between when a command is entered and when the data is available. Latency is the gap between these two events. When the memory controller tells the memory to access a particular location, the data must go through a number of clock cycles in the column address strobe (CAS) to get to its desired location and complete the command. There are two main variables that determine a module&amp;#x27;s latency:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;The total number of clock cycles the data must go through (measured in CAS latency, or CL, on data sheets) The duration of each clock cycle (measured in nanoseconds)  Combining these two variables gives us the latency equation: &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;latency (ns) = clock cycle time (ns) x number of clock cycles &lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h3 id=&#34;Linux&#34;&gt;&lt;a href=&#34;#Linux&#34; class=&#34;headerlink&#34; title=&#34;Linux&#34;&gt;&lt;/a&gt;Linux&lt;/h3&gt;&lt;h4 id=&#34;ECC-disabled-error&#34;&gt;&lt;a href=&#34;#ECC-disabled-error&#34; class=&#34;headerlink&#34; title=&#34;ECC disabled error&#34;&gt;&lt;/a&gt;ECC disabled error&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ dmesg &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;EDAC sbridge: CPU SrcID &lt;span class=&#34;comment&#34;&gt;#0, Ha #0, Channel #1 has DIMMs, but ECC is disabled&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;EDAC sbridge: Couldn&lt;span class=&#34;string&#34;&gt;&amp;#x27;t find mci handler&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;EDAC sbridge: Couldn&amp;#x27;&lt;/span&gt;t find mci handler&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;EDAC sbridge: Failed to register device with error -19.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;segment-fault&#34;&gt;&lt;a href=&#34;#segment-fault&#34; class=&#34;headerlink&#34; title=&#34;segment fault&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://utcc.utoronto.ca/~cks/space/blog/linux/KernelSegfaultErrorCodes&#34;&gt;segment fault&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;error 4: (Data) read from an unmapped area&lt;ul&gt;
&lt;li&gt;This is your classic wild pointer read. On 64-bit x86, most of the address space is unmapped so even a program that uses a relatively large amount of memory is hopefully going to have most bad pointers go to memory that has no mappings at all.&lt;/li&gt;
&lt;li&gt;A faulting address of 0 is a NULL pointer and falls into page zero, the lowest page in memory. The kernel prevents people from mapping page zero, and in general low memory is never mapped, so reads from small faulting addresses should always be error 4s.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 5: read from a memory area that’s mapped but not readable&lt;ul&gt;
&lt;li&gt;This is probably a pointer read of a pointer that is so wild that it’s pointing somewhere in the kernel’s area of the address space. It might be a guard page, but at least some of the time mmap()’ing things with PROT_NONE appears to make Linux treat them as unmapped areas so you get error code 4 instead. You might think this could be an area mmap()’d with other permissions but without PROT_READ, but it appears that in practice other permissions imply the ability to read the memory as well.&lt;/li&gt;
&lt;li&gt;(I assume that the Linux kernel is optimizing PROT_NONE mappings by not even creating page table entries for the memory area, rather than carefully assembling PTEs that deny all permissions. The error bits come straight from the CPU, so if there are no PTEs the CPU says ‘fault for an unmapped area’ regardless of what Linux thinks and will report in, eg, &amp;#x2F;proc&amp;#x2F;PID&amp;#x2F;maps.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 6: (data) write to an unmapped area.&lt;ul&gt;
&lt;li&gt;This is your classic write to a wild or corrupted pointer, including to (or through) a null pointer. As with reads, writes to guard pages mmap()’d with PROT_NONE will generally show up as this, not as ‘write to a mapped area that denies permissions’.&lt;/li&gt;
&lt;li&gt;(As with reads, all writes with small faulting addresses should be error 6s because no one sane allows low memory to be mapped.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 7: write to a mapped area that isn’t writable.&lt;ul&gt;
&lt;li&gt;This is either a wild pointer that was unlucky enough to wind up pointing to a bit of memory that was mapped, or an attempt to change read-only data, for example the classical C mistake of trying to modify a string constant (as seen in the first entry). You might also be trying to write to a file that was mmap()’d read only, or in general a memory mapping that lacks PROT_WRITE.&lt;/li&gt;
&lt;li&gt;(All attempts to write to the kernel’s area of address space also get this error, instead of error 6.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 14: attempt to execute code from an unmapped area&lt;ul&gt;
&lt;li&gt;This is the sign of trying to call through a mangled function pointer (or a NULL one), or perhaps returning from a call when the stack is in an unexpected or corrupted state so that the return address isn’t valid. One source of mangled function pointers is use-after-free issues where the (freed) object contains embedded function pointers&lt;/li&gt;
&lt;li&gt;(Error 14 with a faulting address of 0 often means a function call through a NULL pointer, which in turn often means ‘making an indirect call to a function without checking that it’s defined’. There are various larger scale causes of this in code.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 15: attempt to execute code from a mapped memory area that isn’t executable&lt;ul&gt;
&lt;li&gt;This is probably still a mangled function pointer or return address, it’s just that you’re unlucky (or lucky) and there’s mapped memory there instead of nothing.&lt;/li&gt;
&lt;li&gt;(Your code could have confused a function pointer with a data pointer somehow, but this is a lot rarer a mistake than confusing writable data with read-only data.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;x86-64-5-level-page-table&#34;&gt;&lt;a href=&#34;#x86-64-5-level-page-table&#34; class=&#34;headerlink&#34; title=&#34;x86_64 5-level page table&#34;&gt;&lt;/a&gt;x86_64 5-level page table&lt;/h4&gt;&lt;p&gt;Linear-address Translation Using 5-level paging   &lt;/p&gt;
&lt;h4 id=&#34;Memroy-allocators&#34;&gt;&lt;a href=&#34;#Memroy-allocators&#34; class=&#34;headerlink&#34; title=&#34;Memroy allocators&#34;&gt;&lt;/a&gt;Memroy allocators&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/5684365/what-causes-page-faults&#34;&gt;page fault&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Accessing a page that isn’t resident in memory but is on disk in a page file or a mapped file&lt;ul&gt;
&lt;li&gt;Allocate a physical page, and read the desired page from disk and into the relevant working set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Accessing a page that is on the standby or modified list&lt;ul&gt;
&lt;li&gt;Transition the page to the relevant process, session, or system working set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Accessing a demand-zero page&lt;ul&gt;
&lt;li&gt;Add a zero-filled page to the relevant working set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Writing to a copy-on-write page&lt;ul&gt;
&lt;li&gt;Make process-private (or session-private) copy of page, and replace original in process or system working set&lt;br&gt;page fault并不是说进程要访问的内存(地址)不在虚拟地址空间，那对应segment fault。&lt;br&gt;page fault应该指进程访问的虚拟地址尚未建立虚拟地址与物理地址对应表，或表存在但物理地址未被缓存。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;brk是将数据段(.data)的最高地址指针_edata往高地址推，mmap是在进程的虚拟地址空间中（一般是堆和栈中间）找一块空闲的。这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址&lt;br&gt;空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系。       &lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/1420726&#34;&gt;如果一个进程使用了mmap将很大的数据文件映射到进程的虚拟地址空间，我们需要重点关注majflt的值，因为相比minflt，majflt对于性能的损害是致命的，随机读一次磁盘的耗时数量级在几个毫秒，而minflt只有&amp;gt;在大量的时候才会对性能产生影响。&lt;/a&gt;    &lt;/p&gt;
&lt;p&gt;In my case, ftrace and perf show the very slow about munmap system call, there are tons of minflt in per secs(3000,000~8000,000),  issue in the hardware memory&lt;br&gt;upgrade firmware, disable ksm and disable powersave and export all numa cores and disable HT, reduce the minflt about 100x in the same applications&lt;br&gt;and I ‘m sure it ‘s not the memory fragment  &lt;/p&gt;
&lt;p&gt;Page faults can occur for a variety of reasons, as you can see above. Only one of them has to do with reading from the disk. If you try to allocate a block from the heap and the heap manager allocates new pages, then accesses those pages, you’ll get a demand-zero page fault. If you try to hook a function in kernel32 by writing to kernel32’s pages, you’ll get a copy-on-write fault because those pages are silently being copied so your changes don’t affect other processes.  &lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;禁止malloc调用mmap分配内存，禁止内存紧缩。&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;在进程启动时候，加入以下两行代码：&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;mallopt(M_MMAP_MAX, 0); // 禁止malloc调用mmap分配内存&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;mallopt(M_TRIM_THRESHOLD, -1); // 禁止内存紧缩&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;从操作系统角度来看，进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap（不考虑共享内存）。brk是将数据段(.data)的最高地址指针_edata往高地址推，mmap是在进程的虚拟地址空间中（一般是堆和&amp;gt;栈中间）找一块空闲的。这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;。在标准C库中，提供了malloc/free函数分配释放内存，这两个函数底层是由brk，mmap，munmap这些系统调用实现的。  &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ sar -B 2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:     pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:        35.18     30.30 6259790.16    306.97 4470216.70      0.00      0.00      0.00      0.00&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ pidstat -r 2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:      UID       PID  minflt/s  majflt/s     VSZ    RSS   %MEM  Command&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:    1001429      4061      2.65      0.00 1062056 149640   0.01  node&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:        0      6313    672.57      0.00   44260  22652   0.00  condor_procd&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ ps -o majflt,minflt,pid,c,&lt;span class=&#34;built_in&#34;&gt;comm&lt;/span&gt; -C python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;MAJFLT MINFLT    PID  C COMMAND&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   231 5394232  4494  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;     6   8981   5305  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;     0   4387  10567  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   935  33308  36002  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;     8  18544  36005  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;       maj_flt     MAJFLT    The number of major page faults that have occurred with this process.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;       min_flt     MINFLT    The number of minor page faults that have occurred with this process.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;maj_flt low value, min_flt high value, check the python application code&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;Process                         kernel          Slab allocator    page allocator&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  |                               |                   |                |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Segments    libc allocator     ext4/scsi module-----caches-----------free lists-------DRAM(physical)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  |             |                                                      |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Heap----------memory----------------------------------------------------&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Application                    virtual mem&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   |                               |   ----------------------------------------physical mem&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   |&amp;lt;-----------------------------||   |                                            ^&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Allocator (libc)                  ||   V                                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;1.malloc()--------2a blk---------&amp;gt;heap(from high to low)----&amp;gt;3.lookup--mmu--&amp;gt;4.page fault&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  free()       |                   |  |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  realloc()    |                   V  |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  calloc()     |                      ------------------5.page out------------&amp;gt;swap device&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;               |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;               -2b mmap/munmap---&amp;gt;mappings&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (processs address space)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;https://stackoverflow.com/questions/9819186/munmap-performance-on-linux&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;The actual reason it gets slow is that munmap() takes the mm-&amp;gt;mmap_sem lock &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; the entire duration of the syscall. &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Several other operations are liable to be blocked by this, &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; example (but not limited to) fork()/mmap(). This is especially important to note &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; architectures that &lt;span class=&#34;keyword&#34;&gt;do&lt;/span&gt; not implement a lockless get_user_pages_fast() operation &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; pages already in-memory, &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;because a bunch of futex operations (that underpin pthread primitives) will call get_user_pages_fast() and the default implementation will try to take a &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; lock on mmap_sem.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;tcmalloc and jemalloc provide their own allocator along with garbage collection&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;38&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;page cache &amp;lt;--&amp;gt; filesystem              RAW block device&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                   |                        |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                   |                        |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                   --------------------------&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                     block device interface&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                     Volume manager (&lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; used)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                     Device Mapper (&lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; used)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                         Block layer&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;            classic scheduler  or  Multi queue scheduler&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                    Host Bus adaptor Driver(scsi)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                        Disk devices&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;# blk_fill_rwbs&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;R: &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;W: Write&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;M: Metadata&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;S: Synchronous&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;A: Readahead&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;F: Flush or force unit access&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;D: Discard&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;E: Erase&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;N: None&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;IO scduler&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Noop/Dealine/CFQ&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;blk-mq (linux 3.13)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;In the event of a system failure while there are active writes, the parity of a stripe may become inconsistent with the data. If this is not detected and repaired before a disk or block fails, data loss may ensue as incorrect parity will be used to reconstruct the missing block in that stripe. This potential vulnerability is sometimes known as the write hole. Battery-backed cache and similar techniques are commonly used to reduce the window of opportunity for this to occur. The same issue occurs for RAID-6.&lt;/p&gt;
&lt;h4 id=&#34;slabtop&#34;&gt;&lt;a href=&#34;#slabtop&#34; class=&#34;headerlink&#34; title=&#34;slabtop&#34;&gt;&lt;/a&gt;slabtop&lt;/h4&gt;&lt;p&gt;In-kernel data structures cache&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#slab &amp;gt; 100M object&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /proc/slabinfo |awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;if($3*$4/1024/1024 &amp;gt; 100)&amp;#123;print $1,$3*$4/1024/1024&amp;#125; &amp;#125;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2554 x slabs&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;56 x obj &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; a slab&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;total 8128 = 254 x 32, obj sieze=1K&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8128 x 1K = cache size = 8128K&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt; 8128   7360  90%    1.00K    254        32      8128K kmalloc-1024&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;To free pagecache:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 1 &amp;gt; /proc/sys/vm/drop_caches&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;To free dentries and inodes:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 2 &amp;gt; /proc/sys/vm/drop_caches&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;To free pagecache, dentries and inodes:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 3 &amp;gt; /proc/sys/vm/drop_caches&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#Got the free pages from buddyinfo&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;sum=0;for(i=5;i&amp;lt;=NF;i++) sum+=$i*(2^(i-5))&amp;#125;;&amp;#123;total+=sum*4096/1024/1024&amp;#125;;&amp;#123;print $1 &amp;quot; &amp;quot; $2 &amp;quot; &amp;quot; $3 &amp;quot; &amp;quot; $4 &amp;quot;\t : &amp;quot; sum*4096/1024/1024 &amp;quot;M&amp;quot;&amp;#125; END &amp;#123;print &amp;quot;total\t\t\t : &amp;quot; total &amp;quot;M&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/buddyinfo&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;All processes mem= $(grep Pss /proc/[1-9]*/smaps | awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;total+=$2&amp;#125;; END &amp;#123;print total&amp;#125;&amp;#x27;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;RSS mem = awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;sum+=$2&amp;#125; END&amp;#123;print sum*4&amp;quot; KiB&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/[1-9]*/statm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#there are little diff result with python version, because awk and python is not the same application, malloc diff mem&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;slab size = awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;sum=sum+$3*$4;&amp;#125;END&amp;#123;print sum/1024&amp;quot; KiB&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/slabinfo&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;pagetable size = awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;$0~/PageTables/&amp;#123;print $2&amp;quot; KiB&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/meminfo&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The RSS contains some of duplicate part, some share lib, if you want the real value, get it from pmap    &lt;/p&gt;
&lt;h5 id=&#34;slab-exhaust-all-memory-because-a-lot-of-scan&#34;&gt;&lt;a href=&#34;#slab-exhaust-all-memory-because-a-lot-of-scan&#34; class=&#34;headerlink&#34; title=&#34;slab exhaust all memory because a lot of scan&#34;&gt;&lt;/a&gt;slab exhaust all memory because a lot of scan&lt;/h5&gt;&lt;p&gt;&lt;img src=&#34;/img/top-mem1.png&#34;&gt;&lt;br&gt;Why Slab&amp;#x3D;24021820 kB (24GB)&lt;br&gt;slabtop&lt;br&gt;&lt;img src=&#34;/img/slabtop1.png&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;Single-process-memory&#34;&gt;&lt;a href=&#34;#Single-process-memory&#34; class=&#34;headerlink&#34; title=&#34;Single process memory&#34;&gt;&lt;/a&gt;Single process memory&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;       /proc/[pid]/statm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;              Provides information about memory usage, measured &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; pages.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;              The columns are:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  size       (1) total program size&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (same as VmSize &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; /proc/[pid]/status)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  resident   (2) resident &lt;span class=&#34;built_in&#34;&gt;set&lt;/span&gt; size&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (same as VmRSS &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; /proc/[pid]/status)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  shared     (3) number of resident shared pages (i.e., backed by a file)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (same as RssFile+RssShmem &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; /proc/[pid]/status)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  text       (4) text (code)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  lib        (5) library (unused since Linux 2.6; always 0)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  data       (6) data + stack&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  dt         (7) dirty pages (unused since Linux 2.6; always 0)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /proc/3760/statm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;400865 96456 37653 27355 0 157019 0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Second field means res (resident)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;pmap $(pgrep bash)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;pmap &lt;span class=&#34;variable&#34;&gt;$pid&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;.....&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;ffffffffff600000      4K r-x--   [ anon ]&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt; total           795256K &amp;lt;--- &lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; the size too large, maybe the memory leak&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;There are some of share library &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; each resident&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;If you want get share library memory consumption.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;/proc/[pid]/smaps (since Linux 2.6.14)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;              This file shows memory consumption &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; each of the process&lt;span class=&#34;string&#34;&gt;&amp;#x27;s&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;              mappings.  (The pmap(1) command displays similar information,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;              in a form that may be easier for parsing.)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;


&lt;h4 id=&#34;Struct-page&#34;&gt;&lt;a href=&#34;#Struct-page&#34; class=&#34;headerlink&#34; title=&#34;Struct page&#34;&gt;&lt;/a&gt;Struct page&lt;/h4&gt;&lt;p&gt;page frame minimum unit. every page frame has a struct page to point&lt;br&gt;&lt;a href=&#34;https://stackoverflow.com/questions/34836806/how-to-get-physical-address-from-struct-page-in-linux-kernel&#34;&gt;struct page could mapping page frame to physical address&lt;/a&gt;&lt;br&gt;all page frame in the LUR list.&lt;br&gt;There are 2.3%(96&amp;#x2F;4096) usage in linux 2.6.32&lt;/p&gt;
&lt;h4 id=&#34;smem&#34;&gt;&lt;a href=&#34;#smem&#34; class=&#34;headerlink&#34; title=&#34;smem&#34;&gt;&lt;/a&gt;smem&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ smem -u&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;User     Count     Swap      USS      PSS      RSS&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;chrony       1        0      584      621     1316&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;rpc          1        0      604      633     1120&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;...&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -m&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -w&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -t&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -c &lt;span class=&#34;string&#34;&gt;&amp;quot;name user pss&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem --bar &lt;span class=&#34;variable&#34;&gt;$pid&lt;/span&gt; -c &lt;span class=&#34;string&#34;&gt;&amp;quot;pss uss&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;Kernel-Memory-Leak-Detector&#34;&gt;&lt;a href=&#34;#Kernel-Memory-Leak-Detector&#34; class=&#34;headerlink&#34; title=&#34;Kernel Memory Leak Detector&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://www.kernel.org/doc/html/latest/dev-tools/kmemleak.html&#34;&gt;Kernel Memory Leak Detector&lt;/a&gt;&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ modprobe kmemleak-test&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; scan &amp;gt; /sys/kernel/debug/kmemleak&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /sys/kernel/debug/kmemleak&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;## centos not enabled by defualt&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ grep CONFIG_DEBUG_KMEMLEAK /boot/config-4.18.0-305.el8.x86_64&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ CONFIG_DEBUG_KMEMLEAK is not &lt;span class=&#34;built_in&#34;&gt;set&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;zram&#34;&gt;&lt;a href=&#34;#zram&#34; class=&#34;headerlink&#34; title=&#34;zram&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://www.wowotech.net/memory_management/zram.html&#34;&gt;zram&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;CentOS 7 only work for lzo and the performance was not good enough  &lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;77&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; lz4 &amp;gt; /sys/block/zram0/comp_algorithm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;-bash: &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt;: write error: Invalid argument&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /sys/block/zram0/comp_algorithm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;[lzo]&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ modprobe zram num_devices=2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 8589934592 &amp;gt; /sys/block/zram0/disksize&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 8589934592 &amp;gt; /sys/block/zram1/disksize&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ swapoff -a&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ mkswap /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ mkswap /dev/zram1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ swapon /dev/zram0 /dev/zram1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;or&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ modprobe zram num_devices=1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 68719476736 &amp;gt; /sys/block/zram0/disksize&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ mkfs.ext4 /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ tune2fs -m0 /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;tune2fs 1.42.9 (28-Dec-2013)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Setting reserved blocks percentage to 0% (0 blocks)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;## no compression function&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ zpool create -o ashift=0 -o multihost=off -O compression=zstd -O primarycache=none -O secondarycache=none tank /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ sh zram.info&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Physical memory:     134,928,179,200 bytes&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Buffers and cache:       812,810,240 bytes /  0.6% of physical memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Unallocated:             825,503,744 bytes /  0.6% of physical memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Compressed:           19,495,145,472 bytes / 14.4% of physical memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Decompressed size:    33,168,183,296 bytes / 22.3% of decompressed memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Compression ratio: 1.701&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#!/bin/sh -eu&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;TOTAL_DATA=0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;TOTAL_COMP=0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;HAS_ZRAM=0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;# Iterate through swap devices searching for compressed ones&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; NAME _; &lt;span class=&#34;keyword&#34;&gt;do&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;comment&#34;&gt;# Filter zram swaps and let&amp;#x27;s hope your ordinary swap doesn&amp;#x27;t have&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;comment&#34;&gt;# &amp;quot;zram&amp;quot; in its name :D&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;keyword&#34;&gt;case&lt;/span&gt; &lt;span class=&#34;variable&#34;&gt;$NAME&lt;/span&gt; &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        *zram*) ;;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        *) &lt;span class=&#34;built_in&#34;&gt;continue&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;keyword&#34;&gt;esac&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    DIR=/sys`udevadm info --query=path --name=&lt;span class=&#34;variable&#34;&gt;$NAME&lt;/span&gt;`&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    DATA=$(awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;print $1&amp;#125;&amp;#x27;&lt;/span&gt; &lt;span class=&#34;variable&#34;&gt;$DIR&lt;/span&gt;/mm_stat)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    COMP=$(awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;print $3&amp;#125;&amp;#x27;&lt;/span&gt; &lt;span class=&#34;variable&#34;&gt;$DIR&lt;/span&gt;/mm_stat)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    TOTAL_DATA=$((TOTAL_DATA + DATA))&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    TOTAL_COMP=$((TOTAL_COMP + COMP))&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    HAS_ZRAM=1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;done&lt;/span&gt; &amp;lt;/proc/swaps&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;# Extract physical memory in kibibytes and scale back to bytes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ MEM_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ FREE_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ BUF_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ CACHE_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;#125; &amp;lt;/proc/meminfo&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;/usr/bin/printf &lt;span class=&#34;string&#34;&gt;&amp;quot;\&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Physical memory:   %&amp;#x27;17d bytes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Buffers and cache: %&amp;#x27;17d bytes / %4.1f%% of physical memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Unallocated:       %&amp;#x27;17d bytes / %4.1f%% of physical memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;&amp;quot;&lt;/span&gt; `&lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;scale=6; 1024*&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;; 1024*(&lt;span class=&#34;variable&#34;&gt;$BUF_KIB&lt;/span&gt;+&lt;span class=&#34;variable&#34;&gt;$CACHE_KIB&lt;/span&gt;); 100*(&lt;span class=&#34;variable&#34;&gt;$BUF_KIB&lt;/span&gt;+&lt;span class=&#34;variable&#34;&gt;$CACHE_KIB&lt;/span&gt;)/&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;; 1024*&lt;span class=&#34;variable&#34;&gt;$FREE_KIB&lt;/span&gt;; 100*&lt;span class=&#34;variable&#34;&gt;$FREE_KIB&lt;/span&gt;/&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;&amp;quot;&lt;/span&gt;|bc`&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;built_in&#34;&gt;test&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;&lt;span class=&#34;variable&#34;&gt;$HAS_ZRAM&lt;/span&gt;&amp;quot;&lt;/span&gt;; &lt;span class=&#34;keyword&#34;&gt;then&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    /usr/bin/printf &lt;span class=&#34;string&#34;&gt;&amp;quot;\&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Compressed:        %&amp;#x27;17d bytes / %4.1f%% of physical memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Decompressed size: %&amp;#x27;17d bytes / %4.1f%% of decompressed memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Compression ratio: %.3f&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;&amp;quot;&lt;/span&gt; `&lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;scale=6; &lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;; 100*&lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;/&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;/1024; &lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;; 100*&lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;/(1024*&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;-&lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;+&lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;); &lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;/&lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;&amp;quot;&lt;/span&gt;|bc`&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;else&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;Compressed:                        0 bytes / zram not in use&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;fi&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;Make-sure-the-ECC-work-in-Linux&#34;&gt;&lt;a href=&#34;#Make-sure-the-ECC-work-in-Linux&#34; class=&#34;headerlink&#34; title=&#34;Make sure the ECC work in Linux&#34;&gt;&lt;/a&gt;Make sure the ECC work in Linux&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ dmidecode -t memory  | grep -Ei &lt;span class=&#34;string&#34;&gt;&amp;#x27;error correction type&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        Error Correction Type: Single-bit ECC&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ dmidecode -t memory  | grep -Ei &lt;span class=&#34;string&#34;&gt;&amp;#x27;error correction type&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        Error Correction Type: Multi-bit ECC&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;"><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/11/14/mem/&t=memory"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hardware"><span class="toc-number">1.</span> <span class="toc-text">Hardware</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Host-Memory-Buffer-HMB"><span class="toc-number">1.1.</span> <span class="toc-text">Host Memory Buffer (HMB)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCI-E-GT-s-to-GB-s"><span class="toc-number">1.2.</span> <span class="toc-text">PCI-E GT&#x2F;s to GB&#x2F;s</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DDR5"><span class="toc-number">1.3.</span> <span class="toc-text">DDR5</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux"><span class="toc-number">2.</span> <span class="toc-text">Linux</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ECC-disabled-error"><span class="toc-number">2.1.</span> <span class="toc-text">ECC disabled error</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#segment-fault"><span class="toc-number">2.2.</span> <span class="toc-text">segment fault</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#x86-64-5-level-page-table"><span class="toc-number">2.3.</span> <span class="toc-text">x86_64 5-level page table</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Memroy-allocators"><span class="toc-number">2.4.</span> <span class="toc-text">Memroy allocators</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#slabtop"><span class="toc-number">2.5.</span> <span class="toc-text">slabtop</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#slab-exhaust-all-memory-because-a-lot-of-scan"><span class="toc-number">2.5.1.</span> <span class="toc-text">slab exhaust all memory because a lot of scan</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Single-process-memory"><span class="toc-number">2.6.</span> <span class="toc-text">Single process memory</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Struct-page"><span class="toc-number">2.7.</span> <span class="toc-text">Struct page</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#smem"><span class="toc-number">2.8.</span> <span class="toc-text">smem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kernel-Memory-Leak-Detector"><span class="toc-number">2.9.</span> <span class="toc-text">Kernel Memory Leak Detector</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zram"><span class="toc-number">2.10.</span> <span class="toc-text">zram</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Make-sure-the-ECC-work-in-Linux"><span class="toc-number">2.11.</span> <span class="toc-text">Make sure the ECC work in Linux</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Monitor-ECC-error"><span class="toc-number">2.12.</span> <span class="toc-text">Monitor ECC error</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#From-OS"><span class="toc-number">2.12.1.</span> <span class="toc-text">From OS</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#From-IPMI"><span class="toc-number">2.12.2.</span> <span class="toc-text">From IPMI</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ECC-err"><span class="toc-number">2.12.3.</span> <span class="toc-text">ECC err</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#memory-fragmention"><span class="toc-number">2.13.</span> <span class="toc-text">memory fragmention</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#check-the-fragment"><span class="toc-number">2.13.1.</span> <span class="toc-text">check the fragment</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ZRAM"><span class="toc-number">2.14.</span> <span class="toc-text">ZRAM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#numa"><span class="toc-number">2.15.</span> <span class="toc-text">numa</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Soft-lockup-detected-on-a-large-NUMA-system-under-a-heavy-memory-usage"><span class="toc-number">2.15.1.</span> <span class="toc-text">Soft lockup detected on a large NUMA system under a heavy memory usage</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transparent-Huge-Page"><span class="toc-number">2.16.</span> <span class="toc-text">Transparent Huge Page</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Use-hugetlbfs"><span class="toc-number">2.17.</span> <span class="toc-text">Use hugetlbfs</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#set-hugepage"><span class="toc-number">2.17.1.</span> <span class="toc-text">set hugepage</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#free-hugepage"><span class="toc-number">2.17.2.</span> <span class="toc-text">free hugepage</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#memlock"><span class="toc-number">2.18.</span> <span class="toc-text">memlock</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#get-pagesize"><span class="toc-number">2.19.</span> <span class="toc-text">get pagesize</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#memmap"><span class="toc-number">2.20.</span> <span class="toc-text">memmap</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#mapping-dev-pmemX-can-be-used-to-create-a-DAX-filesystem"><span class="toc-number">2.20.1.</span> <span class="toc-text">mapping &#x2F;dev&#x2F;pmemX can be used to create a DAX filesystem</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Benchmark-mem-by-perf"><span class="toc-number">2.21.</span> <span class="toc-text">Benchmark mem by perf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dirty-page"><span class="toc-number">2.22.</span> <span class="toc-text">dirty page</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#meminfo"><span class="toc-number">2.23.</span> <span class="toc-text">meminfo</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#proc-vmstat"><span class="toc-number">2.24.</span> <span class="toc-text">&#x2F;proc&#x2F;vmstat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OOM"><span class="toc-number">2.25.</span> <span class="toc-text">OOM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#show-the-swap"><span class="toc-number">2.26.</span> <span class="toc-text">show the swap</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#valgrind"><span class="toc-number">2.27.</span> <span class="toc-text">valgrind</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#vmtouch"><span class="toc-number">2.28.</span> <span class="toc-text">vmtouch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#java-malloc"><span class="toc-number">2.29.</span> <span class="toc-text">java malloc</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Deactivating-KSM"><span class="toc-number">2.30.</span> <span class="toc-text">Deactivating KSM</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#twice-memset-speed-compare"><span class="toc-number">3.</span> <span class="toc-text">twice memset speed compare</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#optimze-by-hugepage"><span class="toc-number">3.1.</span> <span class="toc-text">optimze by hugepage</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        memory
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">John Doe</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2022-11-14T07:35:11.000Z" itemprop="datePublished">2022-11-14</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Buffer/">Buffer</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/mem/" rel="tag">mem</a>, <a class="tag-link-link" href="/tags/memory/" rel="tag">memory</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h3 id="Hardware"><a href="#Hardware" class="headerlink" title="Hardware"></a>Hardware</h3><h4 id="Host-Memory-Buffer-HMB"><a href="#Host-Memory-Buffer-HMB" class="headerlink" title="Host Memory Buffer (HMB)"></a>Host Memory Buffer (HMB)</h4><ul>
<li><a target="_blank" rel="noopener" href="https://www.virtium.com/knowledge-base/how-to-select-between-dram-vs-dram-less-ssds/">HMB vs DRAM-less</a><ul>
<li>DRAM-less has the lower BW and OPS</li>
<li>HMB has the more stable throughput for a long time</li>
</ul>
</li>
</ul>
<h4 id="PCI-E-GT-s-to-GB-s"><a href="#PCI-E-GT-s-to-GB-s" class="headerlink" title="PCI-E GT&#x2F;s to GB&#x2F;s"></a>PCI-E GT&#x2F;s to GB&#x2F;s</h4><p>GT per sec to GB per sec</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#PCIe 2.0 协议支持 5.0 GT/s 的传输速率，但是由于采用了 8b/10b 编码方案，导致每条通道的实际有效速率为 5*8/10=4 Gbps，也就是 500 MB/s</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ awk <span class="string">&#x27;BEGIN&#123;print &quot;64GT/s to GB/s pcie gen3: &quot; 64*(128/130)/8&quot; GB/s&quot;&#125;&#x27;</span></span><br><span class="line">64GT/s to GB/s pcie gen3: 7.87692 GB/s</span><br><span class="line"></span><br><span class="line">$ awk <span class="string">&#x27;BEGIN&#123;print &quot;40GT/s to GB/s pcie gen2: &quot; 40*(8/10)/8&quot; GB/s&quot;&#125;&#x27;</span></span><br><span class="line">40GT/s to GB/s pcie gen2: 4 GB/s</span><br></pre></td></tr></table></figure>
<ul>
<li><p>DDR refers to double data rate, which means that the transfer rate (MT&#x2F;s) is double what the speed rating is in MHz. Non-ECC DDR RAM is 64 bits (8 bytes) wide - so you’d do the following:</p>
<ul>
<li>RAM speed * 2 &#x3D; MT&#x2F;s * RAM width &#x3D; bandwidth (in MB&#x2F;s) 2933MHz * 2 &#x3D; 5866 MT&#x2F;s * 64bit&#x2F;8(gb&#x2F;s to GB&#x2F;s) &#x3D; 46,928 MB&#x2F;s (approx 45.8GB&#x2F;s)</li>
</ul>
</li>
<li><p>GDDR6 in GPU</p>
<ul>
<li>GDDR6 pin 16Gb&#x2F;s(16GT&#x2F;s) is the maximum data rate per pin for GDDR6<ul>
<li>16Gbps * 384bit&#x2F;8(bytes)&#x3D; 768GB&#x2F;s</li>
<li>awk ‘BEGIN{print 46928&#x2F;64*8&#x2F;2}’ &#x3D; 2933MT&#x2F;s</li>
<li>awk ‘BEGIN{print 768&#x2F;384*8&#x2F;2}’ &#x3D; 8GT&#x2F;s</li>
</ul>
</li>
<li>GDDR6X can transfer two bits per pin instead of just one. This raises overall memory speed to 19–21GT&#x2F;s when compared to standard GDDR6 that tends to top out at 16GT&#x2F;s.</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/">Memory Barriers</a></p>
<ul>
<li>C++11 atomic types, such as load(std::memory_order_acquire)</li>
<li>POSIX mutexes, such as pthread_mutex_lock</li>
<li>LoadLoad<ul>
<li>A LoadLoad barrier effectively prevents reordering of loads performed before the barrier with loads performed after the barrier.</li>
</ul>
</li>
<li>StoreStore<ul>
<li>A StoreStore barrier effectively prevents reordering of stores performed before the barrier with stores performed after the barrier.<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> barrier() __asm__ __volatile__(<span class="string">&quot;&quot;</span>: : :<span class="string">&quot;memory&quot;</span>)</span></span><br><span class="line"></span><br><span class="line">barrier()</span><br><span class="line">mb()</span><br><span class="line">rmb()</span><br><span class="line">wmb()</span><br><span class="line">smp_mb()</span><br><span class="line">smp_rmb()</span><br><span class="line">smp_wmb()</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="DDR5"><a href="#DDR5" class="headerlink" title="DDR5"></a><a target="_blank" rel="noopener" href="https://www.crucial.com/articles/about-memory/everything-about-ddr5-ram#scanner">DDR5</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">DDR5-5600	69.21 GB/s</span><br><span class="line">DDR5-5200	66.12</span><br><span class="line">DDR5-4800	62.74</span><br><span class="line">DDR5-4400	58.81</span><br><span class="line">DDR5-4000	54.65</span><br><span class="line">DDR5-3600	50.26</span><br><span class="line">DDR5-3200	45.62</span><br><span class="line">DDR5-3200	33.57</span><br></pre></td></tr></table></figure>
<ul>
<li>DDR5’s latency is virtually the same as DDR4<ul>
<li>CAS latency is often misunderstood because of its naming convention, but it’s only half of the true memory latency equation. True memory latency is measured in nanoseconds and is a combination of RAM speed and CAS latency.</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">True memory latency (ns) = (2000/RAM Speed) (ns) x CAS latency</span><br><span class="line">true memory latency of DDR4-3200 CL22 = 13.75 ns</span><br><span class="line">true memory latency of DDR5-4800 CL40 = 16.67 ns</span><br><span class="line">Memory experts and system architects know that the latency matters only at the system level since that is what users typically experience. That said, system latency is also measured in nanoseconds and is a combination of host memory controller features and behavior, number of module ranks, memory speed, and true memory latency.</span><br><span class="line"></span><br><span class="line">CAS latency (CL)</span><br><span class="line">DDR5-4800 CL40 92.8 ns</span><br><span class="line">DDR4-3200 CL22 90.0 ns</span><br><span class="line"></span><br><span class="line">So, what is CAS latency? At a basic level, latency refers to the time delay between when a command is entered and when the data is available. Latency is the gap between these two events. When the memory controller tells the memory to access a particular location, the data must go through a number of clock cycles in the column address strobe (CAS) to get to its desired location and complete the command. There are two main variables that determine a module&#x27;s latency:</span><br><span class="line"></span><br><span class="line">The total number of clock cycles the data must go through (measured in CAS latency, or CL, on data sheets) The duration of each clock cycle (measured in nanoseconds)  Combining these two variables gives us the latency equation: </span><br><span class="line">latency (ns) = clock cycle time (ns) x number of clock cycles </span><br></pre></td></tr></table></figure>

<h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><h4 id="ECC-disabled-error"><a href="#ECC-disabled-error" class="headerlink" title="ECC disabled error"></a>ECC disabled error</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ dmesg </span><br><span class="line">EDAC sbridge: CPU SrcID <span class="comment">#0, Ha #0, Channel #1 has DIMMs, but ECC is disabled</span></span><br><span class="line">EDAC sbridge: Couldn<span class="string">&#x27;t find mci handler</span></span><br><span class="line"><span class="string">EDAC sbridge: Couldn&#x27;</span>t find mci handler</span><br><span class="line">EDAC sbridge: Failed to register device with error -19.</span><br></pre></td></tr></table></figure>

<h4 id="segment-fault"><a href="#segment-fault" class="headerlink" title="segment fault"></a><a target="_blank" rel="noopener" href="https://utcc.utoronto.ca/~cks/space/blog/linux/KernelSegfaultErrorCodes">segment fault</a></h4><ul>
<li>error 4: (Data) read from an unmapped area<ul>
<li>This is your classic wild pointer read. On 64-bit x86, most of the address space is unmapped so even a program that uses a relatively large amount of memory is hopefully going to have most bad pointers go to memory that has no mappings at all.</li>
<li>A faulting address of 0 is a NULL pointer and falls into page zero, the lowest page in memory. The kernel prevents people from mapping page zero, and in general low memory is never mapped, so reads from small faulting addresses should always be error 4s.</li>
</ul>
</li>
<li>error 5: read from a memory area that’s mapped but not readable<ul>
<li>This is probably a pointer read of a pointer that is so wild that it’s pointing somewhere in the kernel’s area of the address space. It might be a guard page, but at least some of the time mmap()’ing things with PROT_NONE appears to make Linux treat them as unmapped areas so you get error code 4 instead. You might think this could be an area mmap()’d with other permissions but without PROT_READ, but it appears that in practice other permissions imply the ability to read the memory as well.</li>
<li>(I assume that the Linux kernel is optimizing PROT_NONE mappings by not even creating page table entries for the memory area, rather than carefully assembling PTEs that deny all permissions. The error bits come straight from the CPU, so if there are no PTEs the CPU says ‘fault for an unmapped area’ regardless of what Linux thinks and will report in, eg, &#x2F;proc&#x2F;PID&#x2F;maps.)</li>
</ul>
</li>
<li>error 6: (data) write to an unmapped area.<ul>
<li>This is your classic write to a wild or corrupted pointer, including to (or through) a null pointer. As with reads, writes to guard pages mmap()’d with PROT_NONE will generally show up as this, not as ‘write to a mapped area that denies permissions’.</li>
<li>(As with reads, all writes with small faulting addresses should be error 6s because no one sane allows low memory to be mapped.)</li>
</ul>
</li>
<li>error 7: write to a mapped area that isn’t writable.<ul>
<li>This is either a wild pointer that was unlucky enough to wind up pointing to a bit of memory that was mapped, or an attempt to change read-only data, for example the classical C mistake of trying to modify a string constant (as seen in the first entry). You might also be trying to write to a file that was mmap()’d read only, or in general a memory mapping that lacks PROT_WRITE.</li>
<li>(All attempts to write to the kernel’s area of address space also get this error, instead of error 6.)</li>
</ul>
</li>
<li>error 14: attempt to execute code from an unmapped area<ul>
<li>This is the sign of trying to call through a mangled function pointer (or a NULL one), or perhaps returning from a call when the stack is in an unexpected or corrupted state so that the return address isn’t valid. One source of mangled function pointers is use-after-free issues where the (freed) object contains embedded function pointers</li>
<li>(Error 14 with a faulting address of 0 often means a function call through a NULL pointer, which in turn often means ‘making an indirect call to a function without checking that it’s defined’. There are various larger scale causes of this in code.)</li>
</ul>
</li>
<li>error 15: attempt to execute code from a mapped memory area that isn’t executable<ul>
<li>This is probably still a mangled function pointer or return address, it’s just that you’re unlucky (or lucky) and there’s mapped memory there instead of nothing.</li>
<li>(Your code could have confused a function pointer with a data pointer somehow, but this is a lot rarer a mistake than confusing writable data with read-only data.)</li>
</ul>
</li>
</ul>
<h4 id="x86-64-5-level-page-table"><a href="#x86-64-5-level-page-table" class="headerlink" title="x86_64 5-level page table"></a>x86_64 5-level page table</h4><p>Linear-address Translation Using 5-level paging   </p>
<h4 id="Memroy-allocators"><a href="#Memroy-allocators" class="headerlink" title="Memroy allocators"></a>Memroy allocators</h4><ul>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/5684365/what-causes-page-faults">page fault</a><ul>
<li>Accessing a page that isn’t resident in memory but is on disk in a page file or a mapped file<ul>
<li>Allocate a physical page, and read the desired page from disk and into the relevant working set</li>
</ul>
</li>
<li>Accessing a page that is on the standby or modified list<ul>
<li>Transition the page to the relevant process, session, or system working set</li>
</ul>
</li>
<li>Accessing a demand-zero page<ul>
<li>Add a zero-filled page to the relevant working set</li>
</ul>
</li>
<li>Writing to a copy-on-write page<ul>
<li>Make process-private (or session-private) copy of page, and replace original in process or system working set<br>page fault并不是说进程要访问的内存(地址)不在虚拟地址空间，那对应segment fault。<br>page fault应该指进程访问的虚拟地址尚未建立虚拟地址与物理地址对应表，或表存在但物理地址未被缓存。</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>brk是将数据段(.data)的最高地址指针_edata往高地址推，mmap是在进程的虚拟地址空间中（一般是堆和栈中间）找一块空闲的。这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址<br>空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系。       </p>
<p><a target="_blank" rel="noopener" href="https://cloud.tencent.com/developer/article/1420726">如果一个进程使用了mmap将很大的数据文件映射到进程的虚拟地址空间，我们需要重点关注majflt的值，因为相比minflt，majflt对于性能的损害是致命的，随机读一次磁盘的耗时数量级在几个毫秒，而minflt只有&gt;在大量的时候才会对性能产生影响。</a>    </p>
<p>In my case, ftrace and perf show the very slow about munmap system call, there are tons of minflt in per secs(3000,000~8000,000),  issue in the hardware memory<br>upgrade firmware, disable ksm and disable powersave and export all numa cores and disable HT, reduce the minflt about 100x in the same applications<br>and I ‘m sure it ‘s not the memory fragment  </p>
<p>Page faults can occur for a variety of reasons, as you can see above. Only one of them has to do with reading from the disk. If you try to allocate a block from the heap and the heap manager allocates new pages, then accesses those pages, you’ll get a demand-zero page fault. If you try to hook a function in kernel32 by writing to kernel32’s pages, you’ll get a copy-on-write fault because those pages are silently being copied so your changes don’t affect other processes.  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">禁止malloc调用mmap分配内存，禁止内存紧缩。</span><br><span class="line">在进程启动时候，加入以下两行代码：</span><br><span class="line">mallopt(M_MMAP_MAX, 0); // 禁止malloc调用mmap分配内存</span><br><span class="line">mallopt(M_TRIM_THRESHOLD, -1); // 禁止内存紧缩</span><br><span class="line"></span><br><span class="line">从操作系统角度来看，进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap（不考虑共享内存）。brk是将数据段(.data)的最高地址指针_edata往高地址推，mmap是在进程的虚拟地址空间中（一般是堆和&gt;栈中间）找一块空闲的。这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系</span><br><span class="line">。在标准C库中，提供了malloc/free函数分配释放内存，这两个函数底层是由brk，mmap，munmap这些系统调用实现的。  </span><br><span class="line"></span><br><span class="line">$ sar -B 2</span><br><span class="line">Average:     pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff</span><br><span class="line">Average:        35.18     30.30 6259790.16    306.97 4470216.70      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">$ pidstat -r 2</span><br><span class="line">Average:      UID       PID  minflt/s  majflt/s     VSZ    RSS   %MEM  Command</span><br><span class="line">Average:    1001429      4061      2.65      0.00 1062056 149640   0.01  node</span><br><span class="line">Average:        0      6313    672.57      0.00   44260  22652   0.00  condor_procd</span><br><span class="line"></span><br><span class="line">$ ps -o majflt,minflt,pid,c,<span class="built_in">comm</span> -C python</span><br><span class="line">MAJFLT MINFLT    PID  C COMMAND</span><br><span class="line">   231 5394232  4494  0 python</span><br><span class="line">     6   8981   5305  0 python</span><br><span class="line">     0   4387  10567  0 python</span><br><span class="line">   935  33308  36002  0 python</span><br><span class="line">     8  18544  36005  0 python</span><br><span class="line"></span><br><span class="line">       maj_flt     MAJFLT    The number of major page faults that have occurred with this process.</span><br><span class="line">       min_flt     MINFLT    The number of minor page faults that have occurred with this process.</span><br><span class="line"></span><br><span class="line">maj_flt low value, min_flt high value, check the python application code</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Process                         kernel          Slab allocator    page allocator</span><br><span class="line">  |                               |                   |                |</span><br><span class="line">Segments    libc allocator     ext4/scsi module-----caches-----------free lists-------DRAM(physical)</span><br><span class="line">  |             |                                                      |</span><br><span class="line">Heap----------memory----------------------------------------------------</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Application                    virtual mem</span><br><span class="line">   |                               |   ----------------------------------------physical mem</span><br><span class="line">   |&lt;-----------------------------||   |                                            ^</span><br><span class="line">Allocator (libc)                  ||   V                                            |</span><br><span class="line">1.malloc()--------2a blk---------&gt;heap(from high to low)----&gt;3.lookup--mmu--&gt;4.page fault</span><br><span class="line">  free()       |                   |  |</span><br><span class="line">  realloc()    |                   V  |</span><br><span class="line">  calloc()     |                      ------------------5.page out------------&gt;swap device</span><br><span class="line">               |</span><br><span class="line">               -2b mmap/munmap---&gt;mappings</span><br><span class="line">                             (processs address space)</span><br><span class="line"></span><br><span class="line">https://stackoverflow.com/questions/9819186/munmap-performance-on-linux</span><br><span class="line">The actual reason it gets slow is that munmap() takes the mm-&gt;mmap_sem lock <span class="keyword">for</span> the entire duration of the syscall. </span><br><span class="line">Several other operations are liable to be blocked by this, </span><br><span class="line"><span class="keyword">for</span> example (but not limited to) fork()/mmap(). This is especially important to note <span class="keyword">for</span> architectures that <span class="keyword">do</span> not implement a lockless get_user_pages_fast() operation <span class="keyword">for</span> pages already in-memory, </span><br><span class="line">because a bunch of futex operations (that underpin pthread primitives) will call get_user_pages_fast() and the default implementation will try to take a <span class="built_in">read</span> lock on mmap_sem.</span><br></pre></td></tr></table></figure>
<p>tcmalloc and jemalloc provide their own allocator along with garbage collection</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">page cache &lt;--&gt; filesystem              RAW block device</span><br><span class="line">                   |                        |</span><br><span class="line">                   |                        |</span><br><span class="line">                   --------------------------</span><br><span class="line">                              |</span><br><span class="line">                              V</span><br><span class="line">                     block device interface</span><br><span class="line">                              |</span><br><span class="line">                              V</span><br><span class="line">                     Volume manager (<span class="keyword">if</span> used)</span><br><span class="line">                              |</span><br><span class="line">                              V</span><br><span class="line">                     Device Mapper (<span class="keyword">if</span> used)</span><br><span class="line">                              |</span><br><span class="line">                              V</span><br><span class="line">                         Block layer</span><br><span class="line">            classic scheduler  or  Multi queue scheduler</span><br><span class="line">                              |</span><br><span class="line">                              V</span><br><span class="line">                    Host Bus adaptor Driver(scsi)</span><br><span class="line">                              |</span><br><span class="line">                              V</span><br><span class="line">                        Disk devices</span><br><span class="line"></span><br><span class="line"><span class="comment"># blk_fill_rwbs</span></span><br><span class="line">R: <span class="built_in">read</span></span><br><span class="line">W: Write</span><br><span class="line">M: Metadata</span><br><span class="line">S: Synchronous</span><br><span class="line">A: Readahead</span><br><span class="line">F: Flush or force unit access</span><br><span class="line">D: Discard</span><br><span class="line">E: Erase</span><br><span class="line">N: None</span><br><span class="line"></span><br><span class="line">IO scduler</span><br><span class="line">Noop/Dealine/CFQ</span><br><span class="line">blk-mq (linux 3.13)</span><br></pre></td></tr></table></figure>
<p>In the event of a system failure while there are active writes, the parity of a stripe may become inconsistent with the data. If this is not detected and repaired before a disk or block fails, data loss may ensue as incorrect parity will be used to reconstruct the missing block in that stripe. This potential vulnerability is sometimes known as the write hole. Battery-backed cache and similar techniques are commonly used to reduce the window of opportunity for this to occur. The same issue occurs for RAID-6.</p>
<h4 id="slabtop"><a href="#slabtop" class="headerlink" title="slabtop"></a>slabtop</h4><p>In-kernel data structures cache</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#slab &gt; 100M object</span></span><br><span class="line">$ <span class="built_in">cat</span> /proc/slabinfo |awk <span class="string">&#x27;&#123;if($3*$4/1024/1024 &gt; 100)&#123;print $1,$3*$4/1024/1024&#125; &#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">2554 x slabs</span><br><span class="line">56 x obj <span class="keyword">in</span> a slab</span><br><span class="line">total 8128 = 254 x 32, obj sieze=1K</span><br><span class="line">8128 x 1K = cache size = 8128K</span><br><span class="line">  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME</span><br><span class="line"> 8128   7360  90%    1.00K    254        32      8128K kmalloc-1024</span><br><span class="line"></span><br><span class="line">To free pagecache:</span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"></span><br><span class="line">To free dentries and inodes:</span><br><span class="line">$ <span class="built_in">echo</span> 2 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"></span><br><span class="line">To free pagecache, dentries and inodes:</span><br><span class="line">$ <span class="built_in">echo</span> 3 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"></span><br><span class="line"><span class="comment">#Got the free pages from buddyinfo</span></span><br><span class="line">$ awk <span class="string">&#x27;&#123;sum=0;for(i=5;i&lt;=NF;i++) sum+=$i*(2^(i-5))&#125;;&#123;total+=sum*4096/1024/1024&#125;;&#123;print $1 &quot; &quot; $2 &quot; &quot; $3 &quot; &quot; $4 &quot;\t : &quot; sum*4096/1024/1024 &quot;M&quot;&#125; END &#123;print &quot;total\t\t\t : &quot; total &quot;M&quot;&#125;&#x27;</span> /proc/buddyinfo</span><br><span class="line"></span><br><span class="line">All processes mem= $(grep Pss /proc/[1-9]*/smaps | awk <span class="string">&#x27;&#123;total+=$2&#125;; END &#123;print total&#125;&#x27;</span>)</span><br><span class="line"></span><br><span class="line">RSS mem = awk <span class="string">&#x27;&#123;sum+=$2&#125; END&#123;print sum*4&quot; KiB&quot;&#125;&#x27;</span> /proc/[1-9]*/statm</span><br><span class="line"><span class="comment">#there are little diff result with python version, because awk and python is not the same application, malloc diff mem</span></span><br><span class="line"></span><br><span class="line">slab size = awk <span class="string">&#x27;&#123;sum=sum+$3*$4;&#125;END&#123;print sum/1024&quot; KiB&quot;&#125;&#x27;</span> /proc/slabinfo</span><br><span class="line"></span><br><span class="line">pagetable size = awk <span class="string">&#x27;$0~/PageTables/&#123;print $2&quot; KiB&quot;&#125;&#x27;</span> /proc/meminfo</span><br></pre></td></tr></table></figure>
<p>The RSS contains some of duplicate part, some share lib, if you want the real value, get it from pmap    </p>
<h5 id="slab-exhaust-all-memory-because-a-lot-of-scan"><a href="#slab-exhaust-all-memory-because-a-lot-of-scan" class="headerlink" title="slab exhaust all memory because a lot of scan"></a>slab exhaust all memory because a lot of scan</h5><p><img src="/img/top-mem1.png"><br>Why Slab&#x3D;24021820 kB (24GB)<br>slabtop<br><img src="/img/slabtop1.png"></p>
<h4 id="Single-process-memory"><a href="#Single-process-memory" class="headerlink" title="Single process memory"></a>Single process memory</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">       /proc/[pid]/statm</span><br><span class="line">              Provides information about memory usage, measured <span class="keyword">in</span> pages.</span><br><span class="line">              The columns are:</span><br><span class="line"></span><br><span class="line">                  size       (1) total program size</span><br><span class="line">                             (same as VmSize <span class="keyword">in</span> /proc/[pid]/status)</span><br><span class="line">                  resident   (2) resident <span class="built_in">set</span> size</span><br><span class="line">                             (same as VmRSS <span class="keyword">in</span> /proc/[pid]/status)</span><br><span class="line">                  shared     (3) number of resident shared pages (i.e., backed by a file)</span><br><span class="line">                             (same as RssFile+RssShmem <span class="keyword">in</span> /proc/[pid]/status)</span><br><span class="line">                  text       (4) text (code)</span><br><span class="line">                  lib        (5) library (unused since Linux 2.6; always 0)</span><br><span class="line">                  data       (6) data + stack</span><br><span class="line">                  dt         (7) dirty pages (unused since Linux 2.6; always 0)</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> /proc/3760/statm</span><br><span class="line">400865 96456 37653 27355 0 157019 0</span><br><span class="line"></span><br><span class="line">Second field means res (resident)</span><br><span class="line">pmap $(pgrep bash)</span><br><span class="line"></span><br><span class="line">pmap <span class="variable">$pid</span></span><br><span class="line">.....</span><br><span class="line">ffffffffff600000      4K r-x--   [ anon ]</span><br><span class="line"> total           795256K &lt;--- <span class="keyword">if</span> the size too large, maybe the memory leak</span><br><span class="line"></span><br><span class="line">There are some of share library <span class="keyword">in</span> each resident</span><br><span class="line"></span><br><span class="line">If you want get share library memory consumption.</span><br><span class="line">/proc/[pid]/smaps (since Linux 2.6.14)</span><br><span class="line">              This file shows memory consumption <span class="keyword">for</span> each of the process<span class="string">&#x27;s</span></span><br><span class="line"><span class="string">              mappings.  (The pmap(1) command displays similar information,</span></span><br><span class="line"><span class="string">              in a form that may be easier for parsing.)</span></span><br></pre></td></tr></table></figure>


<h4 id="Struct-page"><a href="#Struct-page" class="headerlink" title="Struct page"></a>Struct page</h4><p>page frame minimum unit. every page frame has a struct page to point<br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/34836806/how-to-get-physical-address-from-struct-page-in-linux-kernel">struct page could mapping page frame to physical address</a><br>all page frame in the LUR list.<br>There are 2.3%(96&#x2F;4096) usage in linux 2.6.32</p>
<h4 id="smem"><a href="#smem" class="headerlink" title="smem"></a>smem</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ smem -u</span><br><span class="line">User     Count     Swap      USS      PSS      RSS</span><br><span class="line">chrony       1        0      584      621     1316</span><br><span class="line">rpc          1        0      604      633     1120</span><br><span class="line">...</span><br><span class="line">$ smem -m</span><br><span class="line">$ smem -w</span><br><span class="line">$ smem -t</span><br><span class="line">$ smem -c <span class="string">&quot;name user pss&quot;</span></span><br><span class="line">$ smem --bar <span class="variable">$pid</span> -c <span class="string">&quot;pss uss&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="Kernel-Memory-Leak-Detector"><a href="#Kernel-Memory-Leak-Detector" class="headerlink" title="Kernel Memory Leak Detector"></a><a target="_blank" rel="noopener" href="https://www.kernel.org/doc/html/latest/dev-tools/kmemleak.html">Kernel Memory Leak Detector</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ modprobe kmemleak-test</span><br><span class="line">$ <span class="built_in">echo</span> scan &gt; /sys/kernel/debug/kmemleak</span><br><span class="line">$ <span class="built_in">cat</span> /sys/kernel/debug/kmemleak</span><br><span class="line"></span><br><span class="line"><span class="comment">## centos not enabled by defualt</span></span><br><span class="line">$ grep CONFIG_DEBUG_KMEMLEAK /boot/config-4.18.0-305.el8.x86_64</span><br><span class="line">$ CONFIG_DEBUG_KMEMLEAK is not <span class="built_in">set</span></span><br></pre></td></tr></table></figure>

<h4 id="zram"><a href="#zram" class="headerlink" title="zram"></a><a target="_blank" rel="noopener" href="http://www.wowotech.net/memory_management/zram.html">zram</a></h4><p>CentOS 7 only work for lzo and the performance was not good enough  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> lz4 &gt; /sys/block/zram0/comp_algorithm</span><br><span class="line">-bash: <span class="built_in">echo</span>: write error: Invalid argument</span><br><span class="line">$ <span class="built_in">cat</span> /sys/block/zram0/comp_algorithm</span><br><span class="line">[lzo]</span><br><span class="line"></span><br><span class="line">$ modprobe zram num_devices=2</span><br><span class="line">$ <span class="built_in">echo</span> 8589934592 &gt; /sys/block/zram0/disksize</span><br><span class="line">$ <span class="built_in">echo</span> 8589934592 &gt; /sys/block/zram1/disksize</span><br><span class="line">$ swapoff -a</span><br><span class="line">$ mkswap /dev/zram0</span><br><span class="line">$ mkswap /dev/zram1</span><br><span class="line">$ swapon /dev/zram0 /dev/zram1</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">$ modprobe zram num_devices=1</span><br><span class="line">$ <span class="built_in">echo</span> 68719476736 &gt; /sys/block/zram0/disksize</span><br><span class="line">$ mkfs.ext4 /dev/zram0</span><br><span class="line">$ tune2fs -m0 /dev/zram0</span><br><span class="line">tune2fs 1.42.9 (28-Dec-2013)</span><br><span class="line">Setting reserved blocks percentage to 0% (0 blocks)</span><br><span class="line"><span class="comment">## no compression function</span></span><br><span class="line">$ zpool create -o ashift=0 -o multihost=off -O compression=zstd -O primarycache=none -O secondarycache=none tank /dev/zram0</span><br><span class="line"></span><br><span class="line">$ sh zram.info</span><br><span class="line">Physical memory:     134,928,179,200 bytes</span><br><span class="line">Buffers and cache:       812,810,240 bytes /  0.6% of physical memory</span><br><span class="line">Unallocated:             825,503,744 bytes /  0.6% of physical memory</span><br><span class="line">Compressed:           19,495,145,472 bytes / 14.4% of physical memory</span><br><span class="line">Decompressed size:    33,168,183,296 bytes / 22.3% of decompressed memory</span><br><span class="line">Compression ratio: 1.701</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/sh -eu</span></span><br><span class="line"></span><br><span class="line">TOTAL_DATA=0</span><br><span class="line">TOTAL_COMP=0</span><br><span class="line">HAS_ZRAM=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># Iterate through swap devices searching for compressed ones</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> NAME _; <span class="keyword">do</span></span><br><span class="line">    <span class="comment"># Filter zram swaps and let&#x27;s hope your ordinary swap doesn&#x27;t have</span></span><br><span class="line">    <span class="comment"># &quot;zram&quot; in its name :D</span></span><br><span class="line">    <span class="keyword">case</span> <span class="variable">$NAME</span> <span class="keyword">in</span></span><br><span class="line">        *zram*) ;;</span><br><span class="line">        *) <span class="built_in">continue</span></span><br><span class="line">    <span class="keyword">esac</span></span><br><span class="line"></span><br><span class="line">    DIR=/sys`udevadm info --query=path --name=<span class="variable">$NAME</span>`</span><br><span class="line">    DATA=$(awk <span class="string">&#x27;&#123;print $1&#125;&#x27;</span> <span class="variable">$DIR</span>/mm_stat)</span><br><span class="line">    COMP=$(awk <span class="string">&#x27;&#123;print $3&#125;&#x27;</span> <span class="variable">$DIR</span>/mm_stat)</span><br><span class="line">    TOTAL_DATA=$((TOTAL_DATA + DATA))</span><br><span class="line">    TOTAL_COMP=$((TOTAL_COMP + COMP))</span><br><span class="line">    HAS_ZRAM=1</span><br><span class="line"><span class="keyword">done</span> &lt;/proc/swaps</span><br><span class="line"></span><br><span class="line"><span class="comment"># Extract physical memory in kibibytes and scale back to bytes</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">read</span> _ MEM_KIB _</span><br><span class="line">    <span class="built_in">read</span> _ FREE_KIB _</span><br><span class="line">    <span class="built_in">read</span> _ BUF_KIB _</span><br><span class="line">    <span class="built_in">read</span> _ CACHE_KIB _</span><br><span class="line">&#125; &lt;/proc/meminfo</span><br><span class="line"></span><br><span class="line">/usr/bin/printf <span class="string">&quot;\</span></span><br><span class="line"><span class="string">Physical memory:   %&#x27;17d bytes</span></span><br><span class="line"><span class="string">Buffers and cache: %&#x27;17d bytes / %4.1f%% of physical memory</span></span><br><span class="line"><span class="string">Unallocated:       %&#x27;17d bytes / %4.1f%% of physical memory</span></span><br><span class="line"><span class="string">&quot;</span> `<span class="built_in">echo</span> <span class="string">&quot;scale=6; 1024*<span class="variable">$MEM_KIB</span>; 1024*(<span class="variable">$BUF_KIB</span>+<span class="variable">$CACHE_KIB</span>); 100*(<span class="variable">$BUF_KIB</span>+<span class="variable">$CACHE_KIB</span>)/<span class="variable">$MEM_KIB</span>; 1024*<span class="variable">$FREE_KIB</span>; 100*<span class="variable">$FREE_KIB</span>/<span class="variable">$MEM_KIB</span>&quot;</span>|bc`</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">test</span> <span class="string">&quot;<span class="variable">$HAS_ZRAM</span>&quot;</span>; <span class="keyword">then</span></span><br><span class="line">    /usr/bin/printf <span class="string">&quot;\</span></span><br><span class="line"><span class="string">Compressed:        %&#x27;17d bytes / %4.1f%% of physical memory</span></span><br><span class="line"><span class="string">Decompressed size: %&#x27;17d bytes / %4.1f%% of decompressed memory</span></span><br><span class="line"><span class="string">Compression ratio: %.3f</span></span><br><span class="line"><span class="string">&quot;</span> `<span class="built_in">echo</span> <span class="string">&quot;scale=6; <span class="variable">$TOTAL_COMP</span>; 100*<span class="variable">$TOTAL_COMP</span>/<span class="variable">$MEM_KIB</span>/1024; <span class="variable">$TOTAL_DATA</span>; 100*<span class="variable">$TOTAL_DATA</span>/(1024*<span class="variable">$MEM_KIB</span>-<span class="variable">$TOTAL_COMP</span>+<span class="variable">$TOTAL_DATA</span>); <span class="variable">$TOTAL_DATA</span>/<span class="variable">$TOTAL_COMP</span>&quot;</span>|bc`</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;Compressed:                        0 bytes / zram not in use&quot;</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>

<h4 id="Make-sure-the-ECC-work-in-Linux"><a href="#Make-sure-the-ECC-work-in-Linux" class="headerlink" title="Make sure the ECC work in Linux"></a>Make sure the ECC work in Linux</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ dmidecode -t memory  | grep -Ei <span class="string">&#x27;error correction type&#x27;</span></span><br><span class="line">        Error Correction Type: Single-bit ECC</span><br><span class="line">$ dmidecode -t memory  | grep -Ei <span class="string">&#x27;error correction type&#x27;</span></span><br><span class="line">        Error Correction Type: Multi-bit ECC</span><br></pre></td></tr></table></figure>
<span id="more"></span>
<h4 id="Monitor-ECC-error"><a href="#Monitor-ECC-error" class="headerlink" title="Monitor ECC error"></a>Monitor ECC error</h4><h5 id="From-OS"><a href="#From-OS" class="headerlink" title="From OS"></a>From OS</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#Intel x86 scalable</span></span><br><span class="line">skx_edac</span><br><span class="line"></span><br><span class="line"><span class="comment">#Intel x86 E3</span></span><br><span class="line">ie31200-edac</span><br><span class="line">$ edac-util -vs</span><br><span class="line">edac-util: EDAC drivers are loaded. 1 MC detected:</span><br><span class="line">  mc0:IE31200</span><br><span class="line"></span><br><span class="line">$ edac-util -vs</span><br><span class="line">edac-util: EDAC drivers loaded. No memory controllers found</span><br><span class="line">$ <span class="built_in">echo</span> $?</span><br><span class="line">1</span><br><span class="line"></span><br><span class="line">$ edac-util -v</span><br><span class="line">mc0: 0 Uncorrected Errors with no DIMM info</span><br><span class="line">mc0: 0 Corrected Errors with no DIMM info</span><br><span class="line">mc0: csrow0: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow0: mc<span class="comment">#0csrow#0channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow0: mc<span class="comment">#0csrow#0channel#1: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow1: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow1: mc<span class="comment">#0csrow#1channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow1: mc<span class="comment">#0csrow#1channel#1: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow2: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow2: mc<span class="comment">#0csrow#2channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow2: mc<span class="comment">#0csrow#2channel#1: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow3: 0 Uncorrected Errors</span><br><span class="line">mc0: csrow3: mc<span class="comment">#0csrow#3channel#0: 0 Corrected Errors</span></span><br><span class="line">mc0: csrow3: mc<span class="comment">#0csrow#3channel#1: 0 Corrected Errors</span></span><br><span class="line">edac-util: No errors to report.</span><br><span class="line"></span><br><span class="line"><span class="comment">#CentOS 7.7 not support Xeon E2000, the kernel merge the patch at 2019-06, maybe wait CentOS 7.8</span></span><br><span class="line"><span class="comment">#Intel x86 E2000</span></span><br><span class="line">ie31200-edac</span><br><span class="line">$ edac-util -vs</span><br><span class="line">edac-util: EDAC drivers loaded. No memory controllers found</span><br></pre></td></tr></table></figure>
<h5 id="From-IPMI"><a href="#From-IPMI" class="headerlink" title="From IPMI"></a>From IPMI</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ipmitool sel elist</span><br></pre></td></tr></table></figure>

<h5 id="ECC-err"><a href="#ECC-err" class="headerlink" title="ECC err"></a>ECC err</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ dmesg -T |  grep -Ei <span class="string">&quot;Err.*bit ECC&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># x86 Monitor ecc</span></span><br><span class="line"><span class="comment"># is the ecc active ?</span></span><br><span class="line">  <span class="keyword">if</span> [[ -f /sys/devices/system/edac/mc/mc0/ce_count ]]</span><br><span class="line">  <span class="keyword">then</span></span><br><span class="line">    awk <span class="string">&#x27;$1&gt;0 &#123;printf &quot;Mem err. &quot;; exit 2&#125;&#x27;</span> /sys/devices/system/edac/mc/mc*/*e_count</span><br><span class="line">    <span class="built_in">exit</span> 2</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;The system not active edac&quot;</span></span><br><span class="line">    <span class="built_in">exit</span> 2</span><br><span class="line">  <span class="keyword">fi</span></span><br></pre></td></tr></table></figure>


<h4 id="memory-fragmention"><a href="#memory-fragmention" class="headerlink" title="memory fragmention"></a>memory fragmention</h4><ul>
<li><a target="_blank" rel="noopener" href="https://www.elastic.co/es/blog/elastic-metrics-7-8-0-released">suggestion from elastic</a><ul>
<li>pageinfo: collects metrics on the buddy paging algorithm, which can be used to determine memory fragmentation</li>
<li>ksm: reports data from Kernel Samepage Merging. In order to take advantage of KSM, applications must use the madvise system call to mark memory regions for merging. KSM is not enabled on all distros, and KSM status is set with the CONFIG_KSM kernel flag</li>
<li>conntrack: reports on performance counters for the Linux connection tracking component of netfilter</li>
</ul>
</li>
</ul>
<p>kswapd high cpu usage in some of not enough memory case<br>reslove the issue temporary</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">sar -rB 2 <span class="comment"># check system vm status</span></span><br><span class="line"></span><br><span class="line">sar -r 2</span><br><span class="line">               ------------------free --&gt; malloc --&gt; release or reclaim</span><br><span class="line">               |</span><br><span class="line">03:32:14 PM kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</span><br><span class="line">03:32:26 PM 122884328   8881472      6.74      1968     68528 118503440     88.53   5951984     60828         4</span><br><span class="line">03:33:22 PM  29706588 102059212     77.46      1968     75940 118507844     88.53  98904256     67008         0</span><br><span class="line">03:33:30 PM 128777516   2988284      2.27      1968     75940    282000      0.21     63112     66988         0</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"></span><br><span class="line"><span class="comment">#reduce the memory fragmentation</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/compact_memory</span><br><span class="line"><span class="comment">#When 1 is written to the file, all zones are compacted such that free memory is available in contiguous blocks where possible. This can be important for example in the allocation of huge pages although processes will also directly compact memory as required.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#Available only when CONFIG_COMPACTION is set. When 1 is written to the file, all zones are compacted such that free memory is available in contiguous blocks where possible</span></span><br><span class="line"><span class="comment">#reduce memory compaction ratio, default is 500</span></span><br><span class="line"><span class="built_in">echo</span> 1000 &gt;  /proc/sys/vm/extfrag_threshold <span class="comment"># 0~1000 ,1000 means the minimum trigger memory compaction operation, it means kernel memory is required to be a contiguous block of a certain size. The kernel memory may be all consumed or fragmented, hence why a contiguous block could not be allocated.</span></span><br><span class="line"></span><br><span class="line">This parameter affects whether the kernel will compact memory or direct reclaim to satisfy a high-order allocation.</span><br><span class="line">The extfrag/extfrag_index file <span class="keyword">in</span> debugfs shows what the fragmentation index <span class="keyword">for</span> each order is <span class="keyword">in</span> each zone <span class="keyword">in</span> the system.</span><br><span class="line">Values tending towards 0 imply allocations would fail due to lack of memory, values towards 1000 imply failures are due to fragmentation and -1 implies that the allocation will succeed as long as watermarks are met.</span><br><span class="line"></span><br><span class="line">The kernel will not compact memory <span class="keyword">in</span> a zone <span class="keyword">if</span> the fragmentation index is &lt;= extfrag_threshold. The default value is 500</span><br><span class="line"></span><br><span class="line">        /*</span><br><span class="line">         * Index is between 0 and 1000</span><br><span class="line">         *</span><br><span class="line">         * 0 =&gt; allocation would fail due to lack of memory</span><br><span class="line">         * 1000 =&gt; allocation would fail due to fragmentation</span><br><span class="line">         */</span><br><span class="line"></span><br><span class="line">sar -r -B -S 2</span><br><span class="line">Average:     pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff</span><br><span class="line">Average:         0.00      8.92 3163363.44      0.00 177917.30      0.00  39200.92  39093.80     99.73</span><br><span class="line"></span><br><span class="line">Average:    kbmemfree kbmemused  %memused kbbuffers  kbcached  kbcommit   %commit  kbactive   kbinact   kbdirty</span><br><span class="line">Average:     17337774 510492746     96.72    140728 342605322 133693955     24.93 266439056 230275118    344656</span><br><span class="line"></span><br><span class="line">Average:    kbswpfree kbswpused  %swpused  kbswpcad   %swpcad</span><br><span class="line">Average:      8075700    312904      3.73      1948      0.62</span><br><span class="line"></span><br><span class="line">pgscank/s: Number of pages scanned by the kswapd daemon per second.</span><br><span class="line">pgscand/s: Number of pages scanned directly per second.</span><br><span class="line">%vmeff = pgsteal / pgscanin, In the kswapd scan pages(pagecache and swapcache), how many page banned and re-use the pages ratio</span><br><span class="line">pgsteal/s: Number of pages the system has reclaimed from cache (pagecache and swapcache)  per  second  to  satisfy  its  memory demands.</span><br><span class="line"></span><br><span class="line">Linux memory allocate base the buddy algorithm, When allocate a huge page and no free, trigger direct reclaim or memory compaction</span><br></pre></td></tr></table></figure>

<h5 id="check-the-fragment"><a href="#check-the-fragment" class="headerlink" title="check the fragment"></a><a target="_blank" rel="noopener" href="http://linux.laoqinren.net/kernel/memory-compaction/">check the fragment</a></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /sys/kernel/debug/extfrag/unusable_index</span><br><span class="line">Node 0, zone      DMA 0.000 0.000 0.000 0.000 0.000 0.008 0.016 0.032 0.032 0.097 0.226</span><br><span class="line">Node 0, zone    DMA32 0.000 0.000 0.039 0.236 0.328 0.353 0.365 0.376 0.390 0.406 0.414</span><br><span class="line">Node 0, zone   Normal 0.000 0.003 0.011 0.020 0.048 0.091 0.152 0.205 0.258 0.383 1.000</span><br><span class="line">Node 1, zone   Normal 0.000 0.001 0.007 0.008 0.044 0.115 0.180 0.240 0.383 0.544 0.611 &lt;---61.1% free space could not <span class="keyword">for</span> 2^10 memory malloc</span><br><span class="line">unusable index=(free pages−free blocks suitable)/free pages</span><br><span class="line">        /*</span><br><span class="line">         * Index should be a value between 0 and 1. Return a value to 3</span><br><span class="line">         * decimal places.</span><br><span class="line">         *</span><br><span class="line">         * 0 =&gt; no fragmentation</span><br><span class="line">         * 1 =&gt; high fragmentation</span><br><span class="line">         */</span><br><span class="line"></span><br><span class="line">$ numactl --hardware; <span class="built_in">cat</span> /sys/kernel/debug/extfrag/extfrag_index /proc/pagetypeinfo /proc/buddyinfo</span><br><span class="line"><span class="comment">#show the failed reson</span></span><br><span class="line">$ <span class="built_in">cat</span> /sys/kernel/debug/extfrag/extfrag_index</span><br><span class="line">Node 0, zone      DMA -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000</span><br><span class="line">Node 0, zone    DMA32 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000  &lt;--- -1.000 means could not be failed</span><br><span class="line">Node 0, zone   Normal -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.979 0.990 0.995 0.998 0.999</span><br><span class="line">Node 1, zone   Normal -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.991 0.996 0.998 0.999</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">not-busy_system $ <span class="built_in">cat</span> /proc/vmstat  | grep compact</span><br><span class="line">compact_migrate_scanned 465594270</span><br><span class="line">compact_free_scanned 30261638</span><br><span class="line">compact_isolated 126803981</span><br><span class="line">compact_stall 2087</span><br><span class="line">compact_fail 1590</span><br><span class="line">compact_success 497</span><br><span class="line">compact_daemon_wake 141163</span><br><span class="line">compact_daemon_migrate_scanned 215029058</span><br><span class="line">compact_daemon_free_scanned 21945759</span><br><span class="line"></span><br><span class="line">issue-epyc2 $ <span class="built_in">cat</span> /proc/vmstat  | grep compact</span><br><span class="line">compact_migrate_scanned 92428646009</span><br><span class="line">compact_free_scanned 4698526918775</span><br><span class="line">compact_isolated 34726066192</span><br><span class="line">compact_stall 131374</span><br><span class="line">compact_fail 127303</span><br><span class="line">compact_success 4071</span><br><span class="line"></span><br><span class="line">normal-epyc2 $ <span class="built_in">cat</span> /proc/vmstat  | grep compact</span><br><span class="line">compact_migrate_scanned 1401765817</span><br><span class="line">compact_free_scanned 621557349415</span><br><span class="line">compact_isolated 761675264</span><br><span class="line">compact_stall 28996</span><br><span class="line">compact_fail 19342</span><br><span class="line">compact_success 9650</span><br><span class="line"></span><br><span class="line">mm/vmstat.c</span><br><span class="line"></span><br><span class="line">/*</span><br><span class="line"> * A fragmentation index only makes sense <span class="keyword">if</span> an allocation of a requested</span><br><span class="line"> * size would fail. If that is <span class="literal">true</span>, the fragmentation index indicates</span><br><span class="line"> * whether external fragmentation or a lack of memory was the problem.</span><br><span class="line"> * The value can be used to determine <span class="keyword">if</span> page reclaim or compaction</span><br><span class="line"> * should be used</span><br><span class="line"> */</span><br><span class="line">static int __fragmentation_index(unsigned int order, struct contig_page_info *info)</span><br><span class="line">&#123;</span><br><span class="line">        unsigned long requested = 1UL &lt;&lt; <span class="string">order;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if (!info-&gt;free_blocks_total)</span></span><br><span class="line"><span class="string">                return 0;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        /* Fragmentation index only makes sense when a request would fail */</span></span><br><span class="line"><span class="string">        if (info-&gt;free_blocks_suitable)</span></span><br><span class="line"><span class="string">                return -1000;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        /*</span></span><br><span class="line"><span class="string">         * Index is between 0 and 1 so return within 3 decimal places</span></span><br><span class="line"><span class="string">         *</span></span><br><span class="line"><span class="string">         * 0 =&gt; allocation would fail due to lack of memory</span></span><br><span class="line"><span class="string">         * 1 =&gt; allocation would fail due to fragmentation</span></span><br><span class="line"><span class="string">         */</span></span><br><span class="line"><span class="string">        return 1000 - div_u64( (1000+(div_u64(info-&gt;free_pages * 1000ULL, requested))), info-&gt;free_blocks_total);</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        /*</span></span><br><span class="line"><span class="string">         * Index is between 0 and 1 so return within 3 decimal places</span></span><br><span class="line"><span class="string">         *</span></span><br><span class="line"><span class="string">         * 0 =&gt; allocation would fail due to lack of memory</span></span><br><span class="line"><span class="string">         * 1 =&gt; allocation would fail due to fragmentation</span></span><br><span class="line"><span class="string">         */</span></span><br><span class="line"><span class="string">        if (info-&gt;free_blocks_suitable) //enough mem</span></span><br><span class="line"><span class="string">                return -1000;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Each zone has the watermark, cat /proc/zoneinfo</span></span><br><span class="line"><span class="string"> |</span></span><br><span class="line"><span class="string"> |                availlable memory back to high watermark level, swapd goes sleep</span></span><br><span class="line"><span class="string"> |                                   |    .</span></span><br><span class="line"><span class="string"> |      kswapd work up               |   .</span></span><br><span class="line"><span class="string"> |    .     |      direct reclam     | ..</span></span><br><span class="line"><span class="string">f|     .    |            |           v.</span></span><br><span class="line"><span class="string">r|------.----------------------------.----------- high watermark</span></span><br><span class="line"><span class="string">e|       .  |            |          .</span></span><br><span class="line"><span class="string">e|        . |            |         .</span></span><br><span class="line"><span class="string"> |         .V            |        .</span></span><br><span class="line"><span class="string">p|----------.--------------------.--------------- low watermark</span></span><br><span class="line"><span class="string">a|            ......     |    ..</span></span><br><span class="line"><span class="string">g|                ^ ..   |  ..</span></span><br><span class="line"><span class="string">e|                |   .. v..</span></span><br><span class="line"><span class="string">s|-----------------------.----------------------- min watermark</span></span><br><span class="line"><span class="string"> |                |</span></span><br><span class="line"><span class="string"> |                |</span></span><br><span class="line"><span class="string"> ------------------------------------------------</span></span><br><span class="line"><span class="string">                 |</span></span><br><span class="line"><span class="string">           kswapd running still rate of allocation is high</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://farseerfc.me/zhs/followup-about-swap.html</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ grep &quot;pages free&quot; -B 1 -A  6 /proc/zoneinfo</span></span><br><span class="line"><span class="string">      nr_kernel_stack 10512</span></span><br><span class="line"><span class="string">  pages free     3975</span></span><br><span class="line"><span class="string">        min      17</span></span><br><span class="line"><span class="string">        low      21</span></span><br><span class="line"><span class="string">        high     25</span></span><br><span class="line"><span class="string">        spanned  4095</span></span><br><span class="line"><span class="string">        present  3996</span></span><br><span class="line"><span class="string">        managed  3975</span></span><br><span class="line"><span class="string">--</span></span><br><span class="line"><span class="string">Node 0, zone    DMA32</span></span><br><span class="line"><span class="string">  pages free     297942</span></span><br><span class="line"><span class="string">        min      1242</span></span><br><span class="line"><span class="string">        low      1552</span></span><br><span class="line"><span class="string">        high     1862</span></span><br><span class="line"><span class="string">        spanned  1044480</span></span><br><span class="line"><span class="string">        present  314699</span></span><br><span class="line"><span class="string">        managed  298315</span></span><br><span class="line"><span class="string">--</span></span><br><span class="line"><span class="string">Node 0, zone   Normal</span></span><br><span class="line"><span class="string">  pages free     2075318</span></span><br><span class="line"><span class="string">        min      15635</span></span><br><span class="line"><span class="string">        low      19543</span></span><br><span class="line"><span class="string">        high     23451</span></span><br><span class="line"><span class="string">        spanned  3596288</span></span><br><span class="line"><span class="string">        present  3596288</span></span><br><span class="line"><span class="string">        managed  3520874</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> * Initialise min_free_kbytes.</span></span><br><span class="line"><span class="string"> *</span></span><br><span class="line"><span class="string"> * For small machines we want it small (128k min).  For large machines</span></span><br><span class="line"><span class="string"> * we want it large (64MB max).  But it is not linear, because network</span></span><br><span class="line"><span class="string"> * bandwidth does not increase linearly with machine size.  We use</span></span><br><span class="line"><span class="string"> *</span></span><br><span class="line"><span class="string"> *  min_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:</span></span><br><span class="line"><span class="string"> *  min_free_kbytes = sqrt(lowmem_kbytes * 16)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ sysctl vm.min_free_kbytes vm.watermark_scale_factor vm.watermark_boost_factor</span></span><br><span class="line"><span class="string">vm.min_free_kbytes = 67584</span></span><br><span class="line"><span class="string">vm.watermark_scale_factor = 10</span></span><br><span class="line"><span class="string">vm.watermark_boost_factor = 15000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">spanned = (4095 + 1044480 + 3596288)*4/1024 = 18144 MiB</span></span><br><span class="line"><span class="string">present = (3996 + 314699 + 3596288)*4/1024 = 15292.9 MiB</span></span><br><span class="line"><span class="string">free    = (3975 + 297942 + 2075318)*4/1024 = 9286.07 MiB</span></span><br><span class="line"><span class="string">high    = (25 + 1862 + 23451)*4/1024 = 98.97 MiB</span></span><br><span class="line"><span class="string">low     = (21 + 1552 + 19543)*4/1024 = 82.48 MiB</span></span><br><span class="line"><span class="string">min     = (17 + 1242 + 15635)*4/1024 = 65.99 MiB</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">wmark_high * watermark_boost_factor / 10000</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">watermark_scale_factor:</span></span><br><span class="line"><span class="string">The unit is in fractions of 10,000. The default value of 10 means the distances between watermarks are 0.1% of the available memory in the node/system. The maximum value is 1000, or 10% of memory.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Node 0 DMA min = 17 low = 21 high = 25; present = 3996 ; 3996*0.001=3.997 = 4, 17+4 = 21, 21 + 4 = 25</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">##there is the different algorithms in the low memory system</span></span><br><span class="line"><span class="string">cgroup v2</span></span><br><span class="line"><span class="string">memory.min</span></span><br><span class="line"><span class="string">memory.low</span></span><br><span class="line"><span class="string">memory.high</span></span><br><span class="line"><span class="string">memory.max</span></span><br><span class="line"><span class="string">memory.swap.high</span></span><br><span class="line"><span class="string">memory.swap.max</span></span><br><span class="line"><span class="string">kernel parameter systemd.unified_cgroup_hierarchy=1</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#limit mem</span></span><br><span class="line"><span class="string">$ systemctl edit user@1000.service</span></span><br><span class="line"><span class="string">[Service]</span></span><br><span class="line"><span class="string">Delegate=yes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ cat ~/.config/systemd/user/limit-mem.slice</span></span><br><span class="line"><span class="string">[Slice]</span></span><br><span class="line"><span class="string">MemoryHigh=3G</span></span><br><span class="line"><span class="string">MemoryMax=4G</span></span><br><span class="line"><span class="string">MemorySwapMax=2G</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ systemd-run --user --slice=limit-mem.slice --shell</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ type limit-mem</span></span><br><span class="line"><span class="string">limit-mem is an alias for /usr/bin/time systemd-run --user --pty --same-dir --wait --collect --slice=limit-mem.slice</span></span><br><span class="line"><span class="string">$ limit-mem cp some-large-file dest/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">available: 2 nodes (0-1)</span></span><br><span class="line"><span class="string">node 0 cpus: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71</span></span><br><span class="line"><span class="string">node 0 size: 257436 MB</span></span><br><span class="line"><span class="string">node 0 free: 3644 MB</span></span><br><span class="line"><span class="string">node 1 cpus: 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95</span></span><br><span class="line"><span class="string">node 1 size: 258023 MB</span></span><br><span class="line"><span class="string">node 1 free: 428 MB</span></span><br><span class="line"><span class="string">node distances:</span></span><br><span class="line"><span class="string">node   0   1</span></span><br><span class="line"><span class="string">  0:  10  21</span></span><br><span class="line"><span class="string">  1:  21  10</span></span><br><span class="line"><span class="string">Node 0, zone      DMA -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000</span></span><br><span class="line"><span class="string">Node 0, zone    DMA32 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 -1.000 0.986 0.993</span></span><br><span class="line"><span class="string">Node 0, zone   Normal -1.000 -1.000 -1.000 -1.000 0.925 0.963 0.982 0.991 0.996 0.998 0.999</span></span><br><span class="line"><span class="string">Node 1, zone   Normal -1.000 -1.000 0.750 0.875 0.938 0.969 0.985 0.993 0.997 0.999 1.000</span></span><br><span class="line"><span class="string">Page block order</span>: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">Free pages count per migrate <span class="built_in">type</span> at order       0      1      2      3      4      5      6      7      8      9     10</span><br><span class="line">Node    0, zone      DMA, <span class="built_in">type</span>    Unmovable      1      0      1      1      1      1      1      0      1      0      0</span><br><span class="line">Node    0, zone      DMA, <span class="built_in">type</span>  Reclaimable      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone      DMA, <span class="built_in">type</span>      Movable      0      0      0      0      0      0      0      0      0      1      3</span><br><span class="line">Node    0, zone      DMA, <span class="built_in">type</span>      Reserve      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone      DMA, <span class="built_in">type</span>          CMA      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone      DMA, <span class="built_in">type</span>      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, <span class="built_in">type</span>    Unmovable    301   1638   1860      0   1252     75    303    228     15      0      0</span><br><span class="line">Node    0, zone    DMA32, <span class="built_in">type</span>  Reclaimable      0     24     13   1900   1473    399      2      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, <span class="built_in">type</span>      Movable   7829   8977   8590   6422   3501   1001     98      2      0      0      0</span><br><span class="line">Node    0, zone    DMA32, <span class="built_in">type</span>      Reserve      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, <span class="built_in">type</span>          CMA      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone    DMA32, <span class="built_in">type</span>      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, <span class="built_in">type</span>    Unmovable &gt;100000  64223   7587      3      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, <span class="built_in">type</span>  Reclaimable  45808  13050    380      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, <span class="built_in">type</span>      Movable  12480    373      4      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, <span class="built_in">type</span>      Reserve      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, <span class="built_in">type</span>          CMA      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line">Node    0, zone   Normal, <span class="built_in">type</span>      Isolate      0      0      0      0      0      0      0      0      0      0      0</span><br><span class="line"></span><br><span class="line">Number of blocks <span class="built_in">type</span>     Unmovable  Reclaimable      Movable      Reserve          CMA      Isolate</span><br><span class="line">Node 0, zone      DMA            1            0            7            0            0            0</span><br><span class="line">Node 0, zone    DMA32          593          192          615            0            0            0</span><br><span class="line">Node 0, zone   Normal         9082          899       119555            0            0            0</span><br><span class="line">Page block order: 9</span><br><span class="line">Pages per block:  512</span><br><span class="line"></span><br><span class="line">如何限制系统减少出现memory compaction的概率.  查阅资料后, 发现Linux有一个参数是控制这个的: /proc/sys/vm/extfrag_threshold  这个参数是一个0 ~ 1000的整数. 如果出现内存不够用的情况, Linux会为当</span><br><span class="line">前系统的内存碎片情况打一个分, 如果超过了extfrag_threshold这个值, kswapd就会触发memory compaction .  所以, 这个值设置接近1000, 说明系统在内存碎片的处理倾向于把旧的页换出, 以符合申请的需要; 而</span><br><span class="line">设置接近0, 表示系统在内存碎片的处理倾向于做memory compaction.   </span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/drop_caches</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/compact_memory</span><br><span class="line">这个实质就是减少memory compaction的内存总量</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>In heavey loading, it ‘s too slow, go to memory compaction   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> &gt; /proc/sys/vm/compact_memory</span><br></pre></td></tr></table></figure>

<p>Got the free pages from buddyinfo</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ awk <span class="string">&#x27;&#123;sum=0;for(i=5;i&lt;=NF;i++) sum+=$i*(2^(i-5))&#125;;&#123;total+=sum*4096/1024/1024&#125;;&#123;print $1 &quot; &quot; $2 &quot; &quot; $3 &quot; &quot; $4 &quot;\t : &quot; sum*4096/1024/1024 &quot;M&quot;&#125; END &#123;print &quot;total\t\t\t : &quot; total &quot;M&quot;&#125;&#x27;</span> /proc/buddyinfo</span><br></pre></td></tr></table></figure>
<p>memory compaction cause system responding slow, but it ‘s not cause the TcpExtPFMemallocDrop     </p>
<h4 id="ZRAM"><a href="#ZRAM" class="headerlink" title="ZRAM"></a><a target="_blank" rel="noopener" href="http://liujunming.top/2016/07/04/Linux%E5%86%85%E6%A0%B8%E4%B8%ADzram%E6%A8%A1%E5%9D%97%E7%9A%84%E7%90%86%E8%A7%A3/">ZRAM</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">description <span class="string">&quot;Initializes zram swaping&quot;</span></span><br><span class="line">start on runlevel [2345]</span><br><span class="line">stop on runlevel [!2345]</span><br><span class="line">pre-start script</span><br><span class="line"><span class="comment"># load dependency modules</span></span><br><span class="line">modprobe zram num_devices=2</span><br><span class="line"><span class="comment"># initialize the devices</span></span><br><span class="line"><span class="built_in">echo</span> 1073741824 &gt; /sys/block/zram0/disksize</span><br><span class="line"><span class="built_in">echo</span> 1073741824 &gt; /sys/block/zram1/disksize</span><br><span class="line"><span class="comment"># Creating swap filesystems</span></span><br><span class="line">mkswap /dev/zram0</span><br><span class="line">mkswap /dev/zram1</span><br><span class="line"><span class="comment"># Switch the swaps on</span></span><br><span class="line">swapon -p 5 /dev/zram0</span><br><span class="line">swapon -p 5 /dev/zram1</span><br><span class="line">end script</span><br><span class="line">post-stop script</span><br><span class="line"><span class="comment"># Switching off swap</span></span><br><span class="line">swapoff /dev/zram0</span><br><span class="line">swapoff /dev/zram1</span><br><span class="line">rmmod zram</span><br><span class="line">end script</span><br><span class="line"></span><br><span class="line"><span class="comment">##test zram</span></span><br><span class="line"><span class="comment">#include &lt;stdio.h&gt;</span></span><br><span class="line"><span class="comment">#include &lt;stdlib.h&gt;</span></span><br><span class="line">int <span class="function"><span class="title">main</span></span>()</span><br><span class="line">&#123;</span><br><span class="line">        //printf(<span class="string">&quot;%d\n&quot;</span>, sizeof(int));</span><br><span class="line">        int  *mem;</span><br><span class="line">        int i, size;</span><br><span class="line">        size = 0x70000000;</span><br><span class="line">        mem = (int*)malloc(size*sizeof(int));</span><br><span class="line">        <span class="keyword">for</span>(i = 0; i &lt; size; i++)</span><br><span class="line">                mem[i] = (i%1024);</span><br><span class="line">        getchar();</span><br><span class="line">        free(mem);</span><br><span class="line">        <span class="built_in">return</span> 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="numa"><a href="#numa" class="headerlink" title="numa"></a>numa</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">numactl --interleave=all <span class="comment">#Two/Four socket CPU will share all memory, but you can&#x27; t got the best performance except you need more memory</span></span><br><span class="line">vm.zone_reclaim_mode = 0  <span class="comment">#</span></span><br><span class="line"><span class="built_in">echo</span> -15 &gt; /proc/&lt;pid&gt;/oom_adj <span class="comment">#reduce OOM kill ratio</span></span><br><span class="line">huge page will not be swaped</span><br><span class="line"><span class="comment">#numa info in /proc/vmstat</span></span><br><span class="line">https://documentation.suse.com/sles/12-SP3/html/SLES-all/cha-tuning-numactl.html</span><br><span class="line"></span><br><span class="line">numa_pte_updates</span><br><span class="line">The amount of base pages that were marked <span class="keyword">for</span> NUMA hinting faults.</span><br><span class="line"></span><br><span class="line">numa_huge_pte_updates</span><br><span class="line">The amount of transparent huge pages that were marked <span class="keyword">for</span> NUMA hinting faults. In combination with numa_pte_updates the total address space that was marked can be calculated.</span><br><span class="line"></span><br><span class="line">numa_hint_faults</span><br><span class="line">Records how many NUMA hinting faults were trapped.</span><br><span class="line"></span><br><span class="line">numa_hint_faults_local</span><br><span class="line">Shows how many of the hinting faults were to <span class="built_in">local</span> nodes. In combination with numa_hint_faults, the percentage of <span class="built_in">local</span> versus remote faults can be calculated. A high percentage of <span class="built_in">local</span> hinting faults indicates that the workload is closer to being converged.</span><br><span class="line"></span><br><span class="line">numa_pages_migrated</span><br><span class="line">Records how many pages were migrated because they were misplaced. As migration is a copying operation, it contributes the largest part of the overhead created by NUMA balancing.</span><br><span class="line"></span><br><span class="line"><span class="comment">#check the process</span></span><br><span class="line">$ grep -e AnonHugePages  /proc/*/smaps | awk -F <span class="string">&#x27;[/ ]+&#x27;</span> <span class="string">&#x27;$(NF-1)&gt;4 &#123;system(&quot;ps -fp  &quot;$3)&#125;&#x27;</span></span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">nobody    5023  5016  3 Jun07 ?        22:28:27 /sbin/lustre_exporter --collector.ost=disabled --collector.mdt=core</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">polkitd    743     1  0 Jun07 ?        00:00:19 /usr/lib/polkit-1/polkitd --no-debug</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="Soft-lockup-detected-on-a-large-NUMA-system-under-a-heavy-memory-usage"><a href="#Soft-lockup-detected-on-a-large-NUMA-system-under-a-heavy-memory-usage" class="headerlink" title="Soft lockup detected on a large NUMA system under a heavy memory usage"></a><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/1560893">Soft lockup detected on a large NUMA system under a heavy memory usage</a></h5><p>Systems with numa factor lower than or equal to 30 may hang under the high load.<br>Soft lockup detected under a heavy memory pressure on a large NUMA system.</p>
<p>For RHEL 7, users must be aware of the following steps that can avoid the soft lockup from occurring. Disable Transparent Huge Page (THP) to avoid such busy memory-compaction, and add “numa_balancing&#x3D;disable” to the kernel parameter followed by reboot, OR set &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;zone_reclaim_mode to 1.<br>For RHEL 6,<br>disable Transparent Huge Page (THP) to avoid such busy memory-compaction OR set &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;zone_reclaim_mode to 1.</p>
<p>The default value(zero) of &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;zone_reclaim_mode results in the CPUs running on the memory exhausted nodes to skip over to the next node with available memory to attempt the memory allocation.</p>
<p><a target="_blank" rel="noopener" href="https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/">That’s the reason there is Linux Transparent Hugepage Support in Linux. It’s an optimization! It manages large pages automatically and transparently for applications. The benefits are pretty obvious: no changes required on application side; it reduces the number of TLB misses; page table walking becomes cheaper. The feature logically can be divided into two parts: allocation and maintenance. The THP takes the regular (“higher-order”) memory allocation path and it requires that the OS be able to find contiguous and aligned block of memory. It suffers from the same issues as the regular pages, namely fragmentation. If the OS can’t find a contiguous block of memory, it will try to compact, reclaim or page out other pages. That process is expensive and could cause latency spikes (up to seconds). This issue was addressed in the 4.6 kernel (via “defer” option), the OS falls back to a regular page if it can’t allocate a large one. The second part is maintenance. Even if an application touches just 1 byte of memory, it will consume whole 2 MB large page. This is obviously a waste of memory. So there’s a background kernel thread called “khugepaged”. It scans pages and tries to defragment and collapse them into one huge page. Despite it’s a background thread, it locks pages it works with, hence could cause latency spikes too. Another pitfall lays in large page splitting, not all parts of the OS work with large pages, e.g. swap. The OS splits large pages into regular ones for them. It could also degrade the performance and increase memory fragmentation.</a></p>
<h4 id="Transparent-Huge-Page"><a href="#Transparent-Huge-Page" class="headerlink" title="Transparent Huge Page"></a><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/46111">Transparent Huge Page</a></h4><p>The transparent Huge Page implementation in the Linux kernel includes functionality that provides compaction. Compaction operations are system level processes that are resource intensive</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##### Enable or disable</span></span><br><span class="line">$ <span class="built_in">cat</span> /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">[always] madvise never</span><br><span class="line">$ <span class="built_in">cat</span> /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line">[always] madvise never</span><br><span class="line"></span><br><span class="line"><span class="comment"># monitor Thp</span></span><br><span class="line">$ egrep <span class="string">&#x27;trans|thp&#x27;</span> /proc/vmstat</span><br><span class="line">$ awk  <span class="string">&#x27;/AnonHugePages/ &#123; if($2&gt;4)&#123;print FILENAME &quot; &quot; $0; system(&quot;ps -fp &quot; gensub(/.*\/([0-9]+).*/, &quot;\\1&quot;, &quot;g&quot;, FILENAME))&#125;&#125;&#x27;</span> /proc/*/smaps</span><br><span class="line"></span><br><span class="line">To prevent applications from allocating more memory resources than necessary, you can <span class="built_in">disable</span> huge pages system-wide and only <span class="built_in">enable</span> them inside MADV_HUGEPAGE madvise regions by running:</span><br><span class="line">$ <span class="built_in">echo</span> madvise &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"></span><br><span class="line">To <span class="built_in">disable</span> direct compaction, run:</span><br><span class="line">$ <span class="built_in">echo</span> madvise &gt; /sys/kernel/mm/transparent_hugepage/defrag</span><br><span class="line"></span><br><span class="line">$ grep AnonHugePages /proc/meminfo</span><br><span class="line">AnonHugePages:  19523584 kB</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/meminfo|grep Huge</span><br><span class="line">AnonHugePages:  1681086464 kB</span><br><span class="line">HugePages_Total:       0</span><br><span class="line">HugePages_Free:        0</span><br><span class="line">HugePages_Rsvd:        0</span><br><span class="line">HugePages_Surp:        0</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line"></span><br><span class="line"><span class="comment">#means 1681086464/2048 = 820843x 2MB huge pages</span></span><br><span class="line"></span><br><span class="line">$ grep -Ei <span class="string">&#x27;trans|thp&#x27;</span> /proc/vmstat</span><br><span class="line">nr_anon_transparent_hugepages 9533</span><br><span class="line">thp_fault_alloc 24017</span><br><span class="line">thp_fault_fallback 16231</span><br><span class="line">thp_collapse_alloc 1687</span><br><span class="line">thp_collapse_alloc_failed 39449</span><br><span class="line">thp_split 2926</span><br><span class="line">thp_zero_page_alloc 1</span><br><span class="line">thp_zero_page_alloc_failed 0</span><br></pre></td></tr></table></figure>

<h4 id="Use-hugetlbfs"><a href="#Use-hugetlbfs" class="headerlink" title="Use hugetlbfs"></a><a target="_blank" rel="noopener" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/5/html/tuning_and_optimizing_red_hat_enterprise_linux_for_oracle_9i_and_10g_databases/sect-oracle_9i_and_10g_tuning_guide-large_memory_optimization_big_pages_and_huge_pages-configuring_huge_pages_in_red_hat_enterprise_linux_4_or_5">Use hugetlbfs</a></h4><h5 id="set-hugepage"><a href="#set-hugepage" class="headerlink" title="set hugepage"></a><a target="_blank" rel="noopener" href="https://paolozaino.wordpress.com/2016/10/02/how-to-force-any-linux-application-to-use-hugepages-without-modifying-the-source-code/">set hugepage</a></h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">        a) hugeadm --create-global-mounts</span><br><span class="line">        b) hugeadm --pool-pages-max DEFAULT:8G</span><br><span class="line">        c) hugeadm --set-recommended-min_free_kbytes</span><br><span class="line">        d) hugeadm --set-recommended-shmmax</span><br><span class="line"></span><br><span class="line">$ yum -y install libhugetlbfs-utils libhugetlbfs</span><br><span class="line"><span class="comment"># To set the 2MB pool minimum to 512 pages:</span></span><br><span class="line">$ hugeadm --pool-pages-min 2MB:512</span><br><span class="line">$ hugeadm --pool-pages-max 2MB:2048</span><br><span class="line">$ hugeadm --pool-list</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">mkdir</span> -p /mnt/hugetlbfs-64K</span><br><span class="line">$ mount -t hugetlbfs none -opagesize=64k /mnt/hugetlbfs-64K</span><br><span class="line">or</span><br><span class="line">$ mount -t hugetlbfs none /mnt/hugetlbfs -o uid=postfix -o gid=postfix</span><br><span class="line">$ hugeadm --set-recommended-shmmax</span><br><span class="line">$ hugeadm --explain</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ LD_PRELOAD=libhugetlbfs.so HUGETLB_MORECORE=<span class="built_in">yes</span> ./run_your_cmd</span><br><span class="line"><span class="comment">## looks like it &#x27;s ok</span></span><br><span class="line">  PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND</span><br><span class="line">31723 root      20   0  642.7g  94996   1348 S  1734  0.0 101:41.53 ./ft.E.x</span><br><span class="line"></span><br><span class="line"><span class="comment">#ftrace</span></span><br><span class="line"><span class="comment">#Trace the mm_page_alloc_extfrag event with ftrace. Due to memory fragmentation, the migration type steals physical pages from the backup migration type</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/kernel/debug/tracing/events/kmem/mm_page_alloc_extfrag/enable</span><br><span class="line"><span class="built_in">cat</span> /sys/kernel/debug/tracing/trace</span><br><span class="line"></span><br><span class="line">           &lt;...&gt;-48794 [003] d... 29700.500381: mm_page_alloc_extfrag: page=ffffe547d61d0000 pfn=22574080 alloc_order=9 fallback_order=10 pageblock_order=9 alloc_migratetype=2 fallback_migratetype=2 fragmenting=0 change_ownership=1</span><br><span class="line">           &lt;...&gt;-48794 [003] d... 29701.045678: mm_page_alloc_extfrag: page=ffffe547d60e0000 pfn=22558720 alloc_order=9 fallback_order=10 pageblock_order=9 alloc_migratetype=2 fallback_migratetype=2 fragmenting=0 change_ownership=1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">$ grep Hugepagesize /proc/meminfo</span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">$ <span class="built_in">echo</span> 512 &gt; /proc/sys/vm/nr_hugepages</span><br><span class="line">This means <span class="keyword">if</span> a 1GB Huge Pages pool should be allocated, <span class="keyword">then</span> 512 Huge Pages need to be allocated</span><br><span class="line">it <span class="string">&#x27;s the static huge page</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">echo 1000 &gt; /proc/sys/vm/hugetlb_shm_group</span></span><br><span class="line"><span class="string">##means only members of group testuser(1000) can allocate &quot;huge&quot; Shared memory segment</span></span><br><span class="line"><span class="string">mount -t hugetlbfs -o uid=1000,gid=1000,mode=775,size=10g none /data/hugeshm</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#kernel parameter, 4x1G, 1024x2M</span></span><br><span class="line"><span class="string">default_hugepagesz=1G hugepagesz=1G hugepages=4 hugepagesz=2M hugepages=1024</span></span><br><span class="line"><span class="string">mount -t hugetlbfs -o pagesize=1G none /dev/hugepages1G</span></span><br><span class="line"><span class="string">mount -t hugetlbfs -o pagesize=2M none /dev/hugepages2M</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">echo 4 &gt; /sys/devices/system/node/node1/hugepages/hugepages-1048576kB/nr_hugepages</span></span><br><span class="line"><span class="string">echo 1024 &gt; /sys/devices/system/node/node3/hugepages/hugepages-2048kB/nr_hugepages</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">/proc/sys/vm/nr_overcommit_hugepages</span></span><br><span class="line"><span class="string">Defines the maximum number of additional huge pages that can be created and used by the system through overcommitting memory. Writing any non-zero value into this file indicates that the system obtains that number of huge pages from the kernel&#x27;</span>s normal page pool <span class="keyword">if</span> the persistent huge page pool is exhausted. As these surplus huge pages become unused, they are <span class="keyword">then</span> freed and returned to the kernel<span class="string">&#x27;s normal page pool.</span></span><br></pre></td></tr></table></figure>
<h5 id="free-hugepage"><a href="#free-hugepage" class="headerlink" title="free hugepage"></a>free hugepage</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 0 &gt; /proc/sys/vm/nr_hugepages</span><br></pre></td></tr></table></figure>

<ul>
<li>Hugepages is a feature that allows the Linux kernel to utilize the multiple page size capabilities of<br>modern hardware architectures.<br>  * A page is the basic unit of virtual memory, with the default page size being 4 KB in the x86 architecture.</li>
<li>Leave sufficient memory for OS use<br>  * no longer subject to normal memory allocations</li>
<li>Huge Pages are ‘pinned’ to physical RAM and cannot be swapped&#x2F;paged out.<br>  * Preference is for 1G hugepages.<br>  * Each hugepage requires a TLB entry. Smaller hugepages &#x3D;&gt; more TLBs and TLB lookups due to page faults &#x3D;&gt; higher probability of packet drop blips</li>
</ul>
<p>hugepagesz<br>[HW,IA-64,PPC,X86-64] The size of the HugeTLB pages.<br>On x86-64 and powerpc, this option can be specified multiple times interleaved with hugepages&#x3D; to reserve huge pages of different sizes. Valid pages sizes on x86-64 are 2M (when the CPU supports “pse”) and 1G (when the CPU supports the “pdpe1gb” cpuinfo flag).    </p>
<p>hugepages<br>[HW,X86-32,IA-64] HugeTLB pages to allocate at boot.     </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hugepagesz=1G hugepages=224</span><br></pre></td></tr></table></figure>

<h4 id="memlock"><a href="#memlock" class="headerlink" title="memlock"></a>memlock</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">oracle           soft    memlock         1048576</span><br><span class="line">oracle           hard    memlock         1048576</span><br></pre></td></tr></table></figure>
<p>The memlock parameter specifies how much memory the oracle user can lock into its address space. Note that Huge Pages are locked in physical memory. The memlock setting is specified in KB and must match the memory size of the number of Huge Pages that Oracle should be able to allocate. So if the Oracle database should be able to use 512 Huge Pages, then memlock must be set to at least 512 * Hugepagesize, which on this system would be 1048576 KB (512<em>1024</em>2). If memlock is too small, then no single Huge Page will be allocated when the Oracle database starts </p>
<h4 id="get-pagesize"><a href="#get-pagesize" class="headerlink" title="get pagesize"></a>get pagesize</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">getconf PAGESIZE</span><br><span class="line">4096</span><br></pre></td></tr></table></figure>

<h4 id="memmap"><a href="#memmap" class="headerlink" title="memmap"></a><a target="_blank" rel="noopener" href="https://docs.pmem.io/getting-started-guide/creating-development-environments/linux-environments/linux-memmap">memmap</a></h4><p><a target="_blank" rel="noopener" href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt">https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">memmap=exactmap [KNL,X86] Enable setting of an exact</span><br><span class="line">                        E820 memory map, as specified by the user.</span><br><span class="line">                        Such memmap=exactmap lines can be constructed based on</span><br><span class="line">                        BIOS output or other requirements. See the memmap=nn@ss</span><br><span class="line">                        option description.</span><br><span class="line"></span><br><span class="line">        memmap=nn[KMG]@ss[KMG]</span><br><span class="line">                        [KNL] Force usage of a specific region of memory.</span><br><span class="line">                        Region of memory to be used is from ss to ss+nn.</span><br><span class="line">                        If @ss[KMG] is omitted, it is equivalent to mem=nn[KMG],</span><br><span class="line">                        <span class="built_in">which</span> limits max address to nn[KMG].</span><br><span class="line">                        Multiple different regions can be specified,</span><br><span class="line">                        comma delimited.</span><br><span class="line">                        Example:</span><br><span class="line">                                memmap=100M@2G,100M<span class="comment">#3G,1G!1024G</span></span><br><span class="line"></span><br><span class="line">        memmap=nn[KMG]<span class="comment">#ss[KMG]</span></span><br><span class="line">                        [KNL,ACPI] Mark specific memory as ACPI data.</span><br><span class="line">                        Region of memory to be marked is from ss to ss+nn.</span><br><span class="line"></span><br><span class="line">        memmap=nn[KMG]<span class="variable">$ss</span>[KMG]</span><br><span class="line">                        [KNL,ACPI] Mark specific memory as reserved.</span><br><span class="line">                        Region of memory to be reserved is from ss to ss+nn.</span><br><span class="line">                        Example: Exclude memory from 0x18690000-0x1869ffff</span><br><span class="line">                                 memmap=64K<span class="variable">$0x18690000</span></span><br><span class="line">                                 or</span><br><span class="line">                                 memmap=0x10000<span class="variable">$0x18690000</span></span><br><span class="line">                        Some bootloaders may need an escape character before <span class="string">&#x27;$&#x27;</span>,</span><br><span class="line">                        like Grub2, otherwise <span class="string">&#x27;$&#x27;</span> and the following number</span><br><span class="line">                        will be eaten.</span><br><span class="line"></span><br><span class="line">        memmap=nn[KMG]!ss[KMG]</span><br><span class="line">                        [KNL,X86] Mark specific memory as protected.</span><br><span class="line">                        Region of memory to be used, from ss to ss+nn.</span><br><span class="line">                        The memory region may be marked as e820 <span class="built_in">type</span> 12 (0xc)</span><br><span class="line">                        and is NVDIMM or ADR memory.</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ dmesg | grep e820</span><br><span class="line">$ grubby --args=<span class="string">&quot;memmap=96G:32G&quot;</span> --update-kernel=ALL</span><br><span class="line">$ grubby --args=<span class="string">&quot;mem=194G&quot;</span> --update-kernel=ALL</span><br></pre></td></tr></table></figure>

<h5 id="mapping-dev-pmemX-can-be-used-to-create-a-DAX-filesystem"><a href="#mapping-dev-pmemX-can-be-used-to-create-a-DAX-filesystem" class="headerlink" title="mapping &#x2F;dev&#x2F;pmemX can be used to create a DAX filesystem"></a>mapping &#x2F;dev&#x2F;pmemX can be used to create a DAX filesystem</h5><ul>
<li>DAX bypass the pagecache<ul>
<li>DAX use NVRAM for pagecache</li>
<li>DAX can map pagecache NVRAM into user buffer</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ sudo parted /dev/pmem0</span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition type?  primary/extended? p</span><br><span class="line">File system type?  [ext2]? ext4</span><br><span class="line">Start? 2MiB</span><br><span class="line">End? 100GiB</span><br><span class="line">(parted)</span><br><span class="line"></span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition type?  primary/extended? p</span><br><span class="line">File system type?  [ext2]? ext4</span><br><span class="line">Start? 100GiB</span><br><span class="line">End? 200GiB</span><br><span class="line"></span><br><span class="line">$  getconf PAGE_SIZE</span><br><span class="line">4096</span><br><span class="line"></span><br><span class="line">$ sudo mkfs.ext4 -b 4096 -E stride=512 -F /dev/pmem0</span><br><span class="line">$ sudo mkdir /pmem</span><br><span class="line">$ sudo mount -o dax /dev/pmem0p1 /pmem</span><br><span class="line">$ sudo mount -v | grep /pmem</span><br><span class="line">$ fallocate --length 1G /pmem/data</span><br><span class="line">$ echo 1 &gt; /sys/kernel/debug/tracing/events/fs_dax/dax_pmd_fault_done/enable</span><br><span class="line">$ echo 0 &gt; /sys/kernel/debug/tracing/events/fs_dax/dax_pmd_fault_done/enable</span><br><span class="line"></span><br><span class="line"># Verify the namespace is in fsdax mode</span><br><span class="line">$ ndctl list -u</span><br><span class="line">$ cat /proc/iomem</span><br></pre></td></tr></table></figure>

<h4 id="Benchmark-mem-by-perf"><a href="#Benchmark-mem-by-perf" class="headerlink" title="Benchmark mem by perf"></a>Benchmark mem by perf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">$ lscpu</span><br><span class="line">......</span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19</span><br><span class="line"></span><br><span class="line">$ perf bench numa mem  -p 1 -t 10  -T 12000  -Irq -H 1 -C 1,3,5,7,9,11,13,15,17,19 -M 0,0,0,0,0,0,0,0,0,0</span><br><span class="line"><span class="comment"># Running &#x27;numa/mem&#x27; benchmark:</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># Running main, &quot;perf bench numa numa-mem -p 1 -t 10 -T 12000 -Irq -H 1 -C 1,3,5,7,9,11,13,15,17,19 -M 0,0,0,0,0,0,0,0,0,0&quot;</span></span><br><span class="line">          8.837 secs slowest (max) thread-runtime</span><br><span class="line">          8.000 secs fastest (min) thread-runtime</span><br><span class="line">          8.182 secs average thread-runtime</span><br><span class="line">          4.738 % difference between max/avg runtime</span><br><span class="line">         12.584 GB data processed, per thread</span><br><span class="line">        125.840 GB data processed, total</span><br><span class="line">          0.702 nsecs/byte/thread runtime</span><br><span class="line">          1.424 GB/sec/thread speed</span><br><span class="line">         14.239 GB/sec total speed</span><br><span class="line"></span><br><span class="line">$ perf bench numa mem  -p 1 -t 10  -T 12000  -Irq -H 1 -C 0,2,4,6,8,10,12,14,16,18 -M 0,0,0,0,0,0,0,0,0,0</span><br><span class="line"><span class="comment"># Running &#x27;numa/mem&#x27; benchmark:</span></span><br><span class="line"></span><br><span class="line"> <span class="comment"># Running main, &quot;perf bench numa numa-mem -p 1 -t 10 -T 12000 -Irq -H 1 -C 0,2,4,6,8,10,12,14,16,18 -M 0,0,0,0,0,0,0,0,0,0&quot;</span></span><br><span class="line">          5.528 secs slowest (max) thread-runtime</span><br><span class="line">          5.000 secs fastest (min) thread-runtime</span><br><span class="line">          5.084 secs average thread-runtime</span><br><span class="line">          4.776 % difference between max/avg runtime</span><br><span class="line">         12.584 GB data processed, per thread</span><br><span class="line">        125.840 GB data processed, total</span><br><span class="line">          0.439 nsecs/byte/thread runtime</span><br><span class="line">          2.276 GB/sec/thread speed</span><br><span class="line">         22.764 GB/sec total speed</span><br></pre></td></tr></table></figure>

<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line">$ dmidecode -t memory | grep -Ei <span class="string">&#x27;1638|32&#x27;</span></span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line">        Size: 16384 MB</span><br><span class="line">        Speed: 3200 MT/s</span><br><span class="line">        Configured Memory Speed: 3200 MT/s</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> &#123;5..8&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$i</span>; perf bench numa mem  -p 1 -t 2  -T 15000  -Irq -H 1 -C 4,<span class="variable">$i</span> | grep <span class="string">&quot;sec total speed&quot;</span>; <span class="keyword">done</span></span><br><span class="line">5</span><br><span class="line">         12.818 GB/sec total speed</span><br><span class="line">6</span><br><span class="line">         12.757 GB/sec total speed</span><br><span class="line">7</span><br><span class="line">         12.810 GB/sec total speed</span><br><span class="line">8</span><br><span class="line">         13.142 GB/sec total speed</span><br><span class="line"></span><br><span class="line">$ <span class="keyword">for</span> i <span class="keyword">in</span> &#123;21..24&#125;; <span class="keyword">do</span> <span class="built_in">echo</span> <span class="variable">$i</span>; perf bench numa mem  -p 1 -t 2  -T 15000  -Irq -H 1 -C 20,<span class="variable">$i</span> | grep <span class="string">&quot;sec total speed&quot;</span>; <span class="keyword">done</span></span><br><span class="line">21</span><br><span class="line">         12.836 GB/sec total speed</span><br><span class="line">22</span><br><span class="line">         12.824 GB/sec total speed</span><br><span class="line">23</span><br><span class="line">         12.847 GB/sec total speed</span><br><span class="line">24</span><br><span class="line">         13.183 GB/sec total speed</span><br><span class="line"></span><br><span class="line">$ perf bench numa mem  -p 1 -t 2  -T 15000  -Irq -H 1 -C 20,46 | grep <span class="string">&quot;sec total speed&quot;</span></span><br><span class="line">         13.125 GB/sec total speed</span><br><span class="line">$ perf bench numa mem  -p 1 -t 2  -T 15000  -Irq -H 1 -C 20,47 | grep <span class="string">&quot;sec total speed&quot;</span></span><br><span class="line">         13.137 GB/sec total speed</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>AMD roma looks like 4 cores share a mem channel, 2 threads x 15000MB &#x3D; 30G , 12GB&#x2F;s means 1x channel, 13GB&#x2F;s means pass 2x channels</p>
<h4 id="dirty-page"><a href="#dirty-page" class="headerlink" title="dirty page"></a>dirty page</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">sysctl -a | grep dirty</span><br><span class="line">vm.dirty_background_bytes = 0</span><br><span class="line">vm.dirty_background_ratio = 10</span><br><span class="line">vm.dirty_bytes = 0</span><br><span class="line">vm.dirty_expire_centisecs = 3000</span><br><span class="line">vm.dirty_ratio = 20</span><br><span class="line">vm.dirty_writeback_centisecs = 500</span><br><span class="line"></span><br><span class="line">vfs_cache_pressure</span><br><span class="line">------------------</span><br><span class="line">This percentage value controls the tendency of the kernel to reclaim the memory <span class="built_in">which</span> is used <span class="keyword">for</span> caching of directory and inode objects.</span><br><span class="line"></span><br><span class="line">At the default value of vfs_cache_pressure=100 the kernel will attempt to reclaim dentries and inodes at a <span class="string">&quot;fair&quot;</span> rate with respect to pagecache and swapcache reclaim.  Decreasing vfs_cache_pressure causes the kernel to prefer to retain dentry and inode caches. When vfs_cache_pressure=0, the kernel will never reclaim dentries and inodes due to memory pressure and this can easily lead to out-of-memory conditions. Increasing vfs_cache_pressure beyond 100 causes the kernel to prefer to reclaim dentries and inodes.</span><br><span class="line"></span><br><span class="line">Increasing vfs_cache_pressure significantly beyond 100 may have negative performance impact. Reclaim code needs to take various locks to find freeable directory and inode objects. With vfs_cache_pressure=1000, it will look <span class="keyword">for</span> ten <span class="built_in">times</span> more freeable objects than there are.</span><br><span class="line"></span><br><span class="line">sysctl -w vm.vfs_cache_pressure=200</span><br></pre></td></tr></table></figure>

<ul>
<li><p>writeback</p>
<ul>
<li>On very large memory systems, consider using more granularity by using<ul>
<li>&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;dirty_background_ratio</li>
<li>&#x2F;proc&#x2F;sys&#x2F;vm&#x2F;dirty_ratio</li>
</ul>
</li>
<li>If there is a lot of pagecache pressure one would want to start background flushing sooner and delay the synchronous writes<ul>
<li>lower diretry_background_ratio</li>
<li>increase the dirty_ratio</li>
</ul>
</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/22370102/difference-between-kmalloc-and-kmem-cache-alloc">Kmalloc</a> - allocates contiguous region from the physical memory. But keep in mind, allocating and free’ing memory is a lot of work.</p>
</li>
<li><p>Kmem_cache_alloc - Here, your process keeps some copies of the some pre-defined size objects pre-allocated. Say you have struct that you know you will be requiring very frequently, so instead of allocating it from the main memory (kmalloc) when you need it, you already keep multiple copies of it allocated &amp; when you want it, it returns the address of the block already allocated (saves a lot of time). Similarly, when you free it, you don’t give it back, it actually isn’t free’d, it goes back to the allocated pool so that if some process again asks for it, you can return this address of the already allocated struct.</p>
</li>
<li><p>Reclaim ratios</p>
<ul>
<li>swappiness</li>
<li>min_free_kbytes</li>
<li>vfs_cache_pressure</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">GFP_ATOMIC  (__GFP_HIGH|__GFP_ATIOMIC|__GFP_KSWAPD_RECLAIM eg:alloc_skb) could malloc some memory under the lower watermark   </span><br><span class="line"><span class="keyword">if</span> under lower watermark, you could see the PF_MEMALLOC, <span class="keyword">in</span> our <span class="keyword">case</span> ,it <span class="string">&#x27;s the PF_MEMALLOC failed (out of memory)  </span></span><br><span class="line"><span class="string">but it could failed too, because some memory will to scan and reclaim   </span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[reference](https://github.com/haiwei-li/study/blob/3f40f1406620fb0bb7300a1d3ff7320b180875bc/Linux/Memory/zone%20%E6%B0%B4%E4%BD%8D/zone%20watermask.md)</span></span><br><span class="line"><span class="string">一种是默认的操作，此时分配器将同步等待内存回收完成，再进行内存分配，也就是direct reclaim   </span></span><br><span class="line"><span class="string">还有一种特殊情况，如果内存分配的请求是带了 PF_MEMALLOC 标志位的，并且现在空余内存的大小可以满足本次内存分配的需求，那么也将是先分配，再回收。   </span></span><br><span class="line"><span class="string">使用PF_MEMALLOC(&quot;PF&quot;表示per-process flag)相当于是忽略了watermark，因此它对应的内存分配的标志是ALLOC_NO_WATERMARK。能够获取&quot;min&quot;值以下的内存，也意味着该process有动用几乎所有内存的权利，因此它</span></span><br><span class="line"><span class="string">也对应GFP的标志__GFP_MEMALLOC。     </span></span><br><span class="line"><span class="string">```c</span></span><br><span class="line"><span class="string">if (gfp_mask &amp; __GFP_MEMALLOC)</span></span><br><span class="line"><span class="string">        return ALLOC_NO_WATERMARKS;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">if (alloc_flags &amp; ALLOC_NO_WATERMARKS)</span></span><br><span class="line"><span class="string">        set_page_pfmemalloc(page);</span></span><br><span class="line"><span class="string">可以在内存严重短缺的时候，不等待回收而强行分配内存呢？其中的一个人物就是kswapd啦，因为kswapd本身就是负责回收内存的，它只需要占用一小部分内存支撑其正常运行(就像启动资金一样)，就可以去回收更多</span></span><br><span class="line"><span class="string">的内存(赚更多的钱回来)。    </span></span><br><span class="line"><span class="string">虽然kswapd是在&quot;low&quot;到&quot;min&quot;的这段区间被唤醒加入调度队列的，但当它真正执行的时候，空余内存的值可能已经掉到&quot;min&quot;以下了。可见，&quot;min&quot;值存在的一个意义是保证像kswapd这样的特殊任务能够在需要的时候立</span></span><br><span class="line"><span class="string">刻获得所需内存。</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#linux-3.10.0-1127.el7/mm/page_alloc.c</span></span><br><span class="line"><span class="string">int __meminit init_per_zone_wmark_min(void)</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">        unsigned long lowmem_kbytes;</span></span><br><span class="line"><span class="string">        int new_min_free_kbytes;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        lowmem_kbytes = nr_free_buffer_pages() * (PAGE_SIZE &gt;&gt; 10);</span></span><br><span class="line"><span class="string">        new_min_free_kbytes = int_sqrt(lowmem_kbytes * 16);  &lt;------------------min_free_kbytes</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        if (new_min_free_kbytes &gt; user_min_free_kbytes) &#123;</span></span><br><span class="line"><span class="string">                min_free_kbytes = new_min_free_kbytes;</span></span><br><span class="line"><span class="string">                if (min_free_kbytes &lt; 128)</span></span><br><span class="line"><span class="string">                        min_free_kbytes = 128;</span></span><br><span class="line"><span class="string">                if (min_free_kbytes &gt; 65536)</span></span><br><span class="line"><span class="string">                        min_free_kbytes = 65536;</span></span><br><span class="line"><span class="string">        &#125; else &#123;</span></span><br><span class="line"><span class="string">                pr_warn(&quot;min_free_kbytes is not updated to %d because user defined value %d is preferred\n&quot;,</span></span><br><span class="line"><span class="string">                                new_min_free_kbytes, user_min_free_kbytes);</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string">        setup_per_zone_wmarks();</span></span><br><span class="line"><span class="string">        refresh_zone_stat_thresholds();</span></span><br><span class="line"><span class="string">        setup_per_zone_lowmem_reserve();</span></span><br><span class="line"><span class="string">        setup_per_zone_inactive_ratio();</span></span><br><span class="line"><span class="string">        return 0;</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">static void __setup_per_zone_wmarks(void)</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">        unsigned long pages_min = min_free_kbytes &gt;&gt; (PAGE_SHIFT - 10);</span></span><br><span class="line"><span class="string">        unsigned long lowmem_pages = 0;</span></span><br><span class="line"><span class="string">        struct zone *zone;</span></span><br><span class="line"><span class="string">        unsigned long flags;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        /* Calculate total number of !ZONE_HIGHMEM pages */ &lt;--------------------------------------</span></span><br><span class="line"><span class="string">        for_each_zone(zone) &#123;</span></span><br><span class="line"><span class="string">                if (!is_highmem(zone))</span></span><br><span class="line"><span class="string">                        lowmem_pages += zone-&gt;managed_pages;</span></span><br><span class="line"><span class="string">        &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        for_each_zone(zone) &#123;</span></span><br><span class="line"><span class="string">                u64 tmp;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">可以看出这里每一个 zone 的 wmark_min 的根据自己的内存大小比例分配对应百分比的 min_free_kbytes. 也就是所有 zone 的 wmark_min 加起来就是这个 min_free_kbytes</span></span><br><span class="line"><span class="string">wmark_low = 5/4 * wmark_min</span></span><br><span class="line"><span class="string">wmark_high = 3/2 * wmark_min</span></span><br><span class="line"><span class="string">每一个zone 还有一个reserve page, 用来限制在 high level zone 满足不了请求的情况下, low level zone 自己需要保留的page数.具体的初始化在</span></span><br><span class="line"><span class="string">setup_per_zone_lowmem_reserve()</span></span><br><span class="line"><span class="string">那么这里来理解一下设置这些wmark_min, wmark_low, wmark_high 的目的了.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">这里min_free_kbytes 主要是kernel 为了留给__GFP_ATOMIC 类型的内存申请操作, 因为在操作系统里面有一些内存申请操作是不允许切换的,也就是不能在这个时候把当前这个 cpu 交给别的进程, 比如handling an interrupt or executing code inside an critical region. 那么这时候肯定也是希望kernel 内存申请操作应该是非阻塞的. 因此希望系统至少能够留下 min_free_kbytes 的空间用户__GFP_ATOMIC 类型的内存申请</span></span><br><span class="line"><span class="string">操作.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">wmark_min 是说当前的这个空闲的 page frame 已经极地了, 当有内存申请操作的时候, 如果是非内核的内存申请操作, 那么就返回失败, 如果申请操作来自kernel, 比如调用的是 __alloc_pages_high_priority() &gt;的时候, 就可以返回内存</span></span><br><span class="line"><span class="string">wmark_low 是用来唤醒 kswap 进程, 当我们某一个__alloc_pages 的时候发现 free page fram 小于 wmark_low 的时候, 就会唤醒这个kswapd 进程, 进行 page reclaim</span></span><br><span class="line"><span class="string">wmark_high 是当 kswapd 这个进程进行 page reclaim 了以后, 什么时候停止的标志, 只有当 page frame 大于这个 pagh_high 的时候, kswapd 进程才会停止, 继续sleep </span></span><br><span class="line"><span class="string">所以其实wmark_min, wmark_low, wmark_high 都是为了kernel 能够允许atomic 类型的申请操作成功服务的</span></span><br><span class="line"><span class="string">.......</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://github.com/baotiao/baotiao.github.com/blob/6448a8ba675cbb6ea9ec002c4d8792a2b50226d5/_posts/2016-06-02-page-reclaim.md</span></span><br></pre></td></tr></table></figure>


<p>The “burst allocation” cause the high latency direct reclaim, for prevent this situation …..    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">------------------------------------------------------------------</span><br><span class="line">    &lt;---allocate | reclaim---&gt;                               used    RAM</span><br><span class="line">------------------------------------------------------------------</span><br><span class="line"></span><br><span class="line">        ------------------------watermark_scale_factor increase min to high</span><br><span class="line">        |                   |</span><br><span class="line">        |-------------------|---extra_free_kbytes increase this area from min to low</span><br><span class="line">        |        |          |</span><br><span class="line">  wmark min  wmark low    wmark high</span><br><span class="line">--------|--------|----------|-------------------------------------</span><br><span class="line">        --delta--</span><br><span class="line">    &lt;---allocate | reclaim---&gt;                               used    RAM</span><br><span class="line">----|-------------------|-----------------------------------------</span><br><span class="line">    -----alloc size------</span><br><span class="line"></span><br><span class="line">Lower the wmark low will wake up the kswapd...</span><br><span class="line">In the android add the extra_free_kbytes for increase the memory size from wmark min to the wmark low</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93841288">about avaiable memory</a><br>si_mem_available   </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">             |---unreclaimble</span><br><span class="line">page cache &lt;-----reclaimble --------------|</span><br><span class="line">                 SReclaimable  -------------&gt; available</span><br><span class="line">             |---free   ------------------|</span><br><span class="line">free pages &lt;-----zone watermark (low? or high)</span><br><span class="line">             |---lowmem reserve (min)</span><br><span class="line"></span><br><span class="line">avaiable = free pages - all zone lowmem reserve - high watermark + page cache + could be reclaim from slab</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">## too high lru_lock--- &gt; lru cache once write a lot of pages to inactive list</span><br><span class="line">2 x list in linux ----&gt; active FIFO list (PG_active=1 in the flag, PG_referenced means used it recently ?)</span><br><span class="line">                   |--&gt; inactive FIFO list (PG_active=0)</span><br><span class="line"></span><br><span class="line">对于anonymous pages，总是需要先写入swap area才能回收。而对于page cache，有一些可以直接discard（比如elf的text段对应的页面，data段对应的页面中clean的部分），有一些dirty的页面需要先write back同</span><br><span class="line">步到磁盘。</span><br><span class="line">由于有flusher thread定期的write back，回收时还是dirty的page cache页面不会太多。而且，page cache中的页面有对应的文件和在文件中的位置信息，需要换入恢复的时候也更加容易。</span><br><span class="line">因此，内核通常更倾向于换出page cache中的页面，只有当内存压力变得相对严重时，才会选择回收 anonymous pages。用户可以根据具体应用场景的需要，通过&quot;/proc/sys/vm/swappiness&quot;调节内存回收时anonymous pages和page cache的比重。</span><br></pre></td></tr></table></figure>

<h4 id="meminfo"><a href="#meminfo" class="headerlink" title="meminfo"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/93228929">meminfo</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">|------------------------buff/cache-----------------------------|             ---&gt; some of swapcached</span><br><span class="line">           |---buffers---|------------cached(mapped)------------|             |</span><br><span class="line">|---slab---|---Active(file)+Inactive(file)---|---shmem shared---|---anonpages---|</span><br><span class="line">                                             |---active(anon)+Inactive(anon)----|</span><br><span class="line"></span><br><span class="line">$ dmesg  | grep Memory</span><br><span class="line">                                                                            <span class="built_in">read</span>,write      <span class="built_in">readonly</span>              not init</span><br><span class="line">[    0.059168] Memory: 7776940K/8200176K available (14345K kernel code, 3481K rwdata, 10348K rodata, 2688K init, 5976K bss, 422976K reserved, 0K cma-reserved)</span><br><span class="line">[    0.112476] x86/mm: Memory block size: 128MB</span><br><span class="line"></span><br><span class="line">- KernelStack</span><br><span class="line">  - grep -i stack /proc/meminfo</span><br><span class="line">- kernel heap   </span><br><span class="line">  - kmalloc() samll block by slab allocator</span><br><span class="line">  - alloc_page or __get_free_page or kmalloc big part or vmalloc</span><br><span class="line">    - /proc/vmallocinfo</span><br><span class="line">- Page tables</span><br><span class="line">  - All: grep pagetables /proc/meminfo -i</span><br><span class="line">  - single process: grep -E <span class="string">&#x27;VmPTE|VmPMD&#x27;</span> /proc/[PID]/status</span><br><span class="line"></span><br><span class="line">- cache:           file cache page + shm page</span><br><span class="line">- rss:             anonymous page + swap cache page</span><br><span class="line">- (<span class="keyword">in</span>)active_anon: anonymous page + swap cache page + shm page</span><br><span class="line">- (<span class="keyword">in</span>)active_file: file cache page</span><br><span class="line">- rss + cache = (<span class="keyword">in</span>)active_anon + (<span class="keyword">in</span>)active_file</span><br><span class="line"></span><br><span class="line">The memory does not automatically count by alloc_pages assigned, unless alloc_pages kernel module or driver calls the initiative to statistics, otherwise we can only see free memory is reduced, but the /proc/meminfo was not clear <span class="built_in">where</span> their specific use went. For example, there is a FAQ on the VMware guest, is the opportunity to take up guest VMWare ESX host memory via Balloon driver (vmware_balloon module) on the guest, and sometimes take up too much will cause no guest memory is available, <span class="keyword">then</span> the guest to check the /proc/meminfo MemFree see only rarely, but can not see the whereabouts of memory, reason is Balloon driver by alloc_pages allocate memory, leaving no statistical value <span class="keyword">in</span> /proc/meminfo <span class="keyword">in</span>, it is difficult to track.    </span><br><span class="line"></span><br><span class="line">[meminfo with Linux kernel](http://linuxperf.com/?p=142)</span><br><span class="line">```bash</span><br><span class="line">   reserved (dmesg -T | grep reserved)</span><br><span class="line">   slab</span><br><span class="line">     SReclaimable</span><br><span class="line">     SUnreclaim</span><br><span class="line">   VmallocUsed (grep vmalloc /proc/vmallocinfo)  kernel/module.c</span><br><span class="line">      awk <span class="string">&#x27;$0~/vmalloc/&#123;total+=$2&#125;; END &#123;print total&#125;&#x27;</span> /proc/vmallocinfo</span><br><span class="line">      kernel modules (lsmod size field)</span><br><span class="line">        seq_printf(m, <span class="string">&quot;%s %u&quot;</span>,mod-&gt;name, mod-&gt;init_size + mod-&gt;core_size)</span><br><span class="line">   HardwareCorrupted (mm/memory-failure.c: memory_failure())</span><br><span class="line">   PageTables</span><br><span class="line">      not contains Page Frame, it <span class="string">&#x27;s the base unit by physical memory</span></span><br><span class="line"><span class="string">   KernelStack (2048k)</span></span><br><span class="line"><span class="string">   Bounce( bounce buffering in the old device)</span></span><br><span class="line"><span class="string">   Hugepages( diff with Transparent HugePage)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">     HugePages_Total = vm.nr_hugepages</span></span><br><span class="line"><span class="string">       $ echo 128 &gt; /proc/sys/vm/nr_hugepages</span></span><br><span class="line"><span class="string">       $ cat /proc/meminfo</span></span><br><span class="line"><span class="string">       HugePages_Total: 128</span></span><br><span class="line"><span class="string">       HugePages_Free: 128</span></span><br><span class="line"><span class="string">       Hugepagesize: 2048 kB</span></span><br><span class="line"><span class="string">       hugetlbfs mmap() or read(), not support write()</span></span><br><span class="line"><span class="string">       not count by PSS and RSS, not in RU Active/Inactive, not in cache/buffer</span></span><br><span class="line"><span class="string">       VmFlags: xxx ht(hugepages) in smaps</span></span><br><span class="line"><span class="string">       THP will count by PSS and RSS</span></span><br><span class="line"><span class="string">       AnonHugePages count in AnonPages and in PSS/RSS</span></span><br><span class="line"><span class="string">       $ cat /sys/devices/node/*/hugepages/*/nrhugepages</span></span><br><span class="line"><span class="string">       Used via hugetlbfs</span></span><br><span class="line"><span class="string">   LUR (Struct of Page Frame Reclaiming)</span></span><br><span class="line"><span class="string">     LRU_INACTIVE_ANON &lt;--&gt; Inactive(anon)</span></span><br><span class="line"><span class="string">     LRU_ACTIVE_ANON &lt;--&gt; Active(anon)</span></span><br><span class="line"><span class="string">     LRU_INACTIVE_FILE &lt;--&gt; Inactive(file)</span></span><br><span class="line"><span class="string">     LRU_ACTIVE_FILE &lt;--&gt; Active(file)</span></span><br><span class="line"><span class="string">     LRU_UNEVICTABLE &lt;--&gt; Unevictable</span></span><br><span class="line"><span class="string">     Cached+AnonPages</span></span><br><span class="line"><span class="string">       User process</span></span><br><span class="line"><span class="string">         /proc/&lt;pid&gt;/smaps</span></span><br><span class="line"><span class="string">       Page cache</span></span><br><span class="line"><span class="string">   Shmem (page cache or Mapped(attach shmem))</span></span><br><span class="line"><span class="string">     Inactive(anon) or Active(anon), in anon list but not count by AnonPages()</span></span><br><span class="line"><span class="string">     share mem</span></span><br><span class="line"><span class="string">     tmpfs</span></span><br><span class="line"><span class="string">     devtmpfs</span></span><br><span class="line"><span class="string">     used size, allocate not means used</span></span><br><span class="line"><span class="string">   AnonPages</span></span><br><span class="line"><span class="string">     User pages: 1. Anonpages; 2. file-backed page</span></span><br><span class="line"><span class="string">     Anonymous Pages release when the user process exit</span></span><br><span class="line"><span class="string">     page cache(Cached) is the file-backed page, not Anonymous pages</span></span><br><span class="line"><span class="string">     anonymous pages</span></span><br><span class="line"><span class="string">     AnonPages not in share memory, it&#x27;</span>s the Cached</span><br><span class="line">     mmap private anonymous pages is AnonPages(Anonymous Pages),mmap shared anonymous pages is Cached(file-backed pages),shared anonymous mmap is tmpfs</span><br><span class="line">     the Transparent HugePages (THP) AnonHugePages (fs/proc/meminfo.c meminfo_proc_show)</span><br><span class="line">       Transparent HugePages</span><br><span class="line">         <span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepages=never</span><br><span class="line">         <span class="built_in">echo</span> always &gt; /sys/kernel/mm/redhat_transparent_hugepage/enabled</span><br><span class="line">   Cached</span><br><span class="line">     Mapped</span><br><span class="line">       User file-backed pages</span><br><span class="line">       Share lib file/mmap file/executable file</span><br><span class="line">       All process PSS == Mapped + AnonPages</span><br><span class="line">     Unmapped (Cached - Mapped)</span><br><span class="line">       The process <span class="built_in">exit</span> and the file-backed pages will not be reclaimed</span><br><span class="line">     tmpfs</span><br><span class="line">       POSIX/SysV shared memory</span><br><span class="line">       shared anonymous mmap</span><br><span class="line">         not swap-out</span><br><span class="line">   SwapCached ( fs/proc/meminfo.c, meminfo_proc_show, total_swapcache_pages )</span><br><span class="line">     AnonPages( make sure the mapping when swap-out;mm/vmscan.c: shrink_page_list() )</span><br><span class="line">     Shmem</span><br><span class="line">     the pages swap-out -&gt; swap-in, the swapcache will not be reclaimed right now, it <span class="string">&#x27;s the cache too</span></span><br><span class="line"><span class="string">   Mlocked</span></span><br><span class="line"><span class="string">     From Active/Inactive LRU list move to the Unevictable LRU list</span></span><br><span class="line"><span class="string">     It could not page-out and swap-out</span></span><br><span class="line"><span class="string">     location: Contain in the LRU Unevictable，AnonPages，Shmem，Mapped</span></span><br><span class="line"><span class="string">   Buffer (the block dev cache page, filemap.c: do_generic_file_read &gt; add_to_page_cache_lru)</span></span><br><span class="line"><span class="string">     read/write/metadata cache, the diff with cache, it &#x27;</span>s not linux file cache, just block dev</span><br><span class="line">     location: LRU list/Active(file)/Inactive(file)</span><br><span class="line">   DirectMap (4K,2M,1G..)</span><br><span class="line">     TLB(Translation Lookaside Buffer) efficiency</span><br><span class="line">   Dirty pages</span><br><span class="line">     dirty pages = Dirty + NFS_Unstable + Writeback</span><br><span class="line">     Active(file) + Inactive(file) + Shmem + mlock_file】== Cached + Buffers</span><br><span class="line">     Slab+ VmallocUsed + PageTables + KernelStack + HardwareCorrupted + Bounce + X(black hole, alloc_pages/__get_free_page not count <span class="keyword">in</span> meminfo)</span><br><span class="line">   The user used</span><br><span class="line">     (Active + Inactive + Unevictable) + (HugePages_Total * Hugepagesize)</span><br><span class="line">     <span class="keyword">if</span> swapcache=0 (Cached + AnonPages + Buffers) + (HugePages_Total * Hugepagesize)</span><br><span class="line">     All processes = $(grep Pss /proc/[1-9]*/smaps | awk <span class="string">&#x27;&#123;total+=$2&#125;; END &#123;print total&#125;&#x27;</span>)</span><br><span class="line">     All processes + (Cached - mapped) + Buffers + (HugePages_Total * Hugepagesize)</span><br><span class="line">   The collectl info</span><br><span class="line">     <span class="built_in">dd</span> wirte file: collectl Inact(file)</span><br><span class="line">     user process: collectl Anon(contains AnonH)</span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">[lower watermark is min_free_kbytes 125%, hiher watermark is min_free_kbytes 150%](http://linux.laoqinren.net/kernel/vm-sysctl-min_free_kbytes/)</span><br><span class="line">```bash</span><br><span class="line">/*</span><br><span class="line"> * Initialise min_free_kbytes.</span><br><span class="line"> *</span><br><span class="line"> * For small machines we want it small (128k min).  For large machines</span><br><span class="line"> * we want it large (64MB max).  But it is not linear, because network</span><br><span class="line"> * bandwidth does not increase linearly with machine size.  We use</span><br><span class="line"> *</span><br><span class="line"> *      min_free_kbytes = 4 * sqrt(lowmem_kbytes), for better accuracy:</span><br><span class="line"> *      min_free_kbytes = sqrt(lowmem_kbytes * 16)</span><br><span class="line"> *</span><br><span class="line"> * which yields</span><br><span class="line"> *</span><br><span class="line"> * 16MB:        512k</span><br><span class="line"> * 32MB:        724k</span><br><span class="line"> * 64MB:        1024k</span><br><span class="line"> * 128MB:       1448k</span><br><span class="line"> * 256MB:       2048k</span><br><span class="line"> * 512MB:       2896k</span><br><span class="line"> * 1024MB:      4096k</span><br><span class="line"> * 2048MB:      5792k</span><br><span class="line"> * 4096MB:      8192k</span><br><span class="line"> * 8192MB:      11584k</span><br><span class="line"> * 16384MB:     16384k</span><br><span class="line"> */</span><br><span class="line"></span><br><span class="line">watermark[high] - watermark[low] </span><br><span class="line">                = watermark[min] * 3 / 2 - watermark[min] * 5 / 4</span><br><span class="line">                = watermark[min] * 1 / 4</span><br><span class="line">                = per_zone_min_free_pages * 1/4</span><br><span class="line"></span><br><span class="line">$ systctl -w vm.min_free_kbytes=70336</span><br><span class="line">$ grep -A 3 -B 1 &quot;  pages free&quot; /proc/zoneinfo</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3969</span><br><span class="line">        min      2</span><br><span class="line">        low      2</span><br><span class="line">        high     3</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     451904</span><br><span class="line">        min      241</span><br><span class="line">        low      301 (125%)</span><br><span class="line">        high     361 (150%)</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     15162636</span><br><span class="line">        min      8533</span><br><span class="line">        low      10666 (125%)</span><br><span class="line">        high     12799 (150%)</span><br><span class="line">--</span><br><span class="line">Node 1, zone   Normal</span><br><span class="line">  pages free     15810809</span><br><span class="line">        min      8806</span><br><span class="line">        low      11007</span><br><span class="line">        high     13209</span><br><span class="line">$ sysctl -w vm.min_free_kbytes=570336</span><br><span class="line">vm.min_free_kbytes = 570336</span><br><span class="line"></span><br><span class="line">$ grep -A 3 -B 1 &quot;  pages free&quot; /proc/zoneinfo</span><br><span class="line">Node 0, zone      DMA</span><br><span class="line">  pages free     3969</span><br><span class="line">        min      17</span><br><span class="line">        low      21</span><br><span class="line">        high     25</span><br><span class="line">--</span><br><span class="line">Node 0, zone    DMA32</span><br><span class="line">  pages free     451904</span><br><span class="line">        min      1957</span><br><span class="line">        low      2446</span><br><span class="line">        high     2935</span><br><span class="line">--</span><br><span class="line">Node 0, zone   Normal</span><br><span class="line">  pages free     15162723</span><br><span class="line">        min      69196</span><br><span class="line">        low      86495</span><br><span class="line">        high     103794</span><br><span class="line">--</span><br><span class="line">Node 1, zone   Normal</span><br><span class="line">  pages free     15810673</span><br><span class="line">        min      71413</span><br><span class="line">        low      89266</span><br><span class="line">        high     107119</span><br><span class="line">$ grep -A 3 -B 1 &quot;  pages free&quot; /proc/zoneinfo  | awk &#x27;$0~/min/&#123;sum+=$NF&#125; END&#123;print sum*4+4&#125;&#x27;</span><br><span class="line">570336</span><br><span class="line"></span><br><span class="line"># you could see the same ratio from min to low</span><br></pre></td></tr></table></figure>

<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">linux<span class="number">-3.10</span><span class="number">.0</span><span class="number">-1127.</span>el7/mm/vmscan.c</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * The background pageout daemon, started as a kernel thread</span></span><br><span class="line"><span class="comment"> * from the init process.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * This basically trickles out pages so that we have _some_</span></span><br><span class="line"><span class="comment"> * free memory available even if there is no other activity</span></span><br><span class="line"><span class="comment"> * that frees anything up. This is needed for things like routing</span></span><br><span class="line"><span class="comment"> * etc, where we otherwise might have all activity going on in</span></span><br><span class="line"><span class="comment"> * asynchronous contexts that cannot page things out.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * If there are applications that are active memory-allocators</span></span><br><span class="line"><span class="comment"> * (most normal use), this basically shouldn&#x27;t matter.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">static</span> <span class="type">int</span> <span class="title function_">kswapd</span><span class="params">(<span class="type">void</span> *p)</span></span><br><span class="line">&#123;</span><br><span class="line">        <span class="type">unsigned</span> <span class="type">long</span> order, new_order;</span><br><span class="line">        <span class="type">unsigned</span> balanced_order;</span><br><span class="line">        <span class="type">int</span> classzone_idx, new_classzone_idx;</span><br><span class="line">        <span class="type">int</span> balanced_classzone_idx;</span><br><span class="line">        <span class="type">pg_data_t</span> *pgdat = (<span class="type">pg_data_t</span>*)p;</span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">task_struct</span> *<span class="title">tsk</span> =</span> current;</span><br><span class="line"></span><br><span class="line">        <span class="class"><span class="keyword">struct</span> <span class="title">reclaim_state</span> <span class="title">reclaim_state</span> =</span> &#123;</span><br><span class="line">                .reclaimed_slab = <span class="number">0</span>,</span><br><span class="line">        &#125;;</span><br><span class="line">        <span class="type">const</span> <span class="class"><span class="keyword">struct</span> <span class="title">cpumask</span> *<span class="title">cpumask</span> =</span> cpumask_of_node(pgdat-&gt;node_id);</span><br><span class="line"></span><br><span class="line">        lockdep_set_current_reclaim_state(GFP_KERNEL);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (!cpumask_empty(cpumask))</span><br><span class="line">                set_cpus_allowed_ptr(tsk, cpumask);</span><br><span class="line">        current-&gt;reclaim_state = &amp;reclaim_state;</span><br><span class="line"></span><br><span class="line">        <span class="comment">/*</span></span><br><span class="line"><span class="comment">         * Tell the memory management that we&#x27;re a &quot;memory allocator&quot;,</span></span><br><span class="line"><span class="comment">         * and that if we need more memory we should get access to it</span></span><br><span class="line"><span class="comment">         * regardless (see &quot;__alloc_pages()&quot;). &quot;kswapd&quot; should</span></span><br><span class="line"><span class="comment">         * never get caught in the normal page freeing logic.</span></span><br><span class="line"><span class="comment">         *</span></span><br><span class="line"><span class="comment">         * (Kswapd normally doesn&#x27;t need memory anyway, but sometimes</span></span><br><span class="line"><span class="comment">         * you need a small amount of memory in order to be able to</span></span><br><span class="line"><span class="comment">         * page out something else, and this flag essentially protects</span></span><br><span class="line"><span class="comment">         * us from recursively trying to free more memory as we&#x27;re</span></span><br><span class="line"><span class="comment">         * trying to free the first piece of memory in the first place).</span></span><br><span class="line"><span class="comment">         */</span></span><br><span class="line">        tsk-&gt;flags |= PF_MEMALLOC | PF_SWAPWRITE | PF_KSWAPD;</span><br><span class="line">        set_freezable();</span><br></pre></td></tr></table></figure>
<ul>
<li>vm.min_free_kbytes: kernel reserve for emergency</li>
<li>vm.extra_free_kbytes: reserve for application emergency<ul>
<li>not in centos 7 and ubuntu 20.04</li>
<li>work in centos 6<br>free mem &lt; vm.min_free_kbytes + vm.extra_free_kbytes —&gt; kswapd scan and reclaim pages until high in &#x2F;proc&#x2F;zoneinfo</li>
</ul>
</li>
</ul>
<h4 id="proc-vmstat"><a href="#proc-vmstat" class="headerlink" title="&#x2F;proc&#x2F;vmstat"></a>&#x2F;proc&#x2F;vmstat</h4><ul>
<li>pgscan_kswapd reclaim<ul>
<li>pgscan_kswapd<ul>
<li>kswapd background scan page numbers</li>
</ul>
</li>
<li>pgsteal_kswapd<ul>
<li>kswapd background reclaim page numbers</li>
</ul>
</li>
<li>pgscan_direct<ul>
<li>Number of pages scanned by the kswapd daemon per second</li>
</ul>
</li>
<li>pgsteal_direct<ul>
<li>Number of pages scanned directly per second</li>
</ul>
</li>
</ul>
</li>
<li>fragmentation<ul>
<li>compact_stall</li>
<li>compact_fail</li>
<li>compact_success</li>
</ul>
</li>
<li>nr_dirty</li>
<li>drop_cache<ul>
<li>drop_pagecache<ul>
<li>drop page cache number</li>
</ul>
</li>
<li>drop_slab<ul>
<li>drop slab number</li>
</ul>
</li>
<li>pginodesteal<ul>
<li>direct reclaim inode page cache number</li>
</ul>
</li>
<li>kswapd_inodesteal<ul>
<li>kswapd background reclaim page cache number</li>
</ul>
</li>
</ul>
</li>
<li>IO<ul>
<li>pgpgin</li>
<li>pgpgout</li>
</ul>
</li>
<li>SWAP IO<ul>
<li>pswpin</li>
<li>pswpout</li>
</ul>
</li>
<li>workingset<ul>
<li>workingset_refault<ul>
<li>release pages read from disk to memory</li>
</ul>
</li>
<li>workingset_restore<ul>
<li>How many pages avoid to reclaimed, before release the page re-use<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ grep -Ei &#x27;drop|inodesteal|nr_dirty_&#x27; /proc/vmstat</span><br><span class="line">nr_dirty_threshold 48600882</span><br><span class="line">nr_dirty_background_threshold 24300441</span><br><span class="line">pginodesteal 255519573</span><br><span class="line">kswapd_inodesteal 902259110</span><br><span class="line">drop_pagecache 32</span><br><span class="line">drop_slab 48</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="OOM"><a href="#OOM" class="headerlink" title="OOM"></a>OOM</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">echo -15 &gt; /proc/&lt;pid&gt;/oom_adj #reduce OOM kill ratio</span><br><span class="line"></span><br><span class="line">[Mon Apr 19 13:38:55 2021] Node 0 DMA free:14400kB min:28kB low:32kB high:40kB active_anon:0kB inactive_anon:0kB active_file:0kB inactive_file:0kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:15984kB managed:15900kB mlocked:0kB dirty:0kB writeback:0kB mapped:0kB shmem:0kB slab_reclaimable:0kB slab_unreclaimable:0kB kernel_stack:0kB pagetables:0kB unstable:0kB bounce:0kB free_pcp:0kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:0 all_unreclaimable? yes</span><br><span class="line">[Mon Apr 19 13:38:55 2021] lowmem_reserve[]: 0 1408 22531 22531</span><br></pre></td></tr></table></figure>

<p>lowmem_reserve<a href="22531*4kB">2</a> + low_watermark(low: 32kB) &gt; free pages(14400kB) - malloc N x pages (no enough malloc 1 page, because 22531*4kB &gt; free pages 14400kB)<br>failed to get memory from DMA    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[Mon Apr 19 13:38:55 2021] Node 0 DMA32 free:87208kB min:2716kB low:3392kB high:4072kB active_anon:94576kB inactive_anon:94732kB active_file:328kB inactive_file:2784kB unevictable:0kB isolated(anon):0kB isolated(file):0kB present:1675200kB managed:1442720kB mlocked:0kB dirty:0kB writeback:0kB mapped:120kB shmem:4kB slab_reclaimable:114200kB slab_unreclaimable:32456kB kernel_stack:992kB pagetables:4212kB unstable:0kB bounce:0kB free_pcp:124kB local_pcp:0kB free_cma:0kB writeback_tmp:0kB pages_scanned:5910 all_unreclaimable? yes</span><br><span class="line">[Mon Apr 19 13:38:55 2021] lowmem_reserve[]: 0 0 21122 21122</span><br><span class="line">......</span><br><span class="line">[Mon Apr 19 13:38:55 2021] Free swap  = 0kB</span><br><span class="line">[Mon Apr 19 13:38:55 2021] Total swap = 16383996kB</span><br><span class="line">[Mon Apr 19 13:38:55 2021] 12219276 pages RAM</span><br><span class="line">[Mon Apr 19 13:38:55 2021] 0 pages HighMem/MovableOnly</span><br><span class="line">[Mon Apr 19 13:38:55 2021] 262387 pages reserved</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>lowmem_reserve<a href="21122*4kB">2</a> + low_watermark(low: 3392kB) &gt; free pages(87208kB) - malloc N x pages (no enough malloc 1 page, because 87208-21122*4-3392&#x3D;-672)<br>For a memory request to be granted&#x2F;allocated the following check must pass:<br>low watermark + lowmem_reserve[2] &lt; free pages - n pages<br>low watermark - is the low watermark value in the zone;<br>lowmem_reserve - is a buffer watermark value to protect lower zones;<br>n pages - is the number of pages requested, in this case, it is 1;    </p>
<h4 id="show-the-swap"><a href="#show-the-swap" class="headerlink" title="show the swap"></a>show the swap</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment"># Get current swap usage for all running processes</span></span><br><span class="line"><span class="comment"># Erik Ljungstrom 27/05/2011</span></span><br><span class="line"><span class="comment"># Modified by Mikko Rantalainen 2012-08-09</span></span><br><span class="line"><span class="comment"># Pipe the output to &quot;sort -nk3&quot; to get sorted output</span></span><br><span class="line"><span class="comment"># Modified by Marc Methot 2014-09-18</span></span><br><span class="line"><span class="comment"># removed the need for sudo</span></span><br><span class="line"></span><br><span class="line">SUM=0</span><br><span class="line">OVERALL=0</span><br><span class="line"><span class="keyword">for</span> DIR <span class="keyword">in</span> `find /proc/ -maxdepth 1 -<span class="built_in">type</span> d -regex <span class="string">&quot;^/proc/[0-9]+&quot;</span>`</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">    PID=`<span class="built_in">echo</span> <span class="variable">$DIR</span> | <span class="built_in">cut</span> -d / -f 3`</span><br><span class="line">    PROGNAME=`ps -p <span class="variable">$PID</span> -o <span class="built_in">comm</span> --no-headers`</span><br><span class="line">    <span class="keyword">for</span> SWAP <span class="keyword">in</span> `grep VmSwap <span class="variable">$DIR</span>/status 2&gt;/dev/null | awk <span class="string">&#x27;&#123; print $2 &#125;&#x27;</span>`</span><br><span class="line">    <span class="keyword">do</span></span><br><span class="line">        <span class="built_in">let</span> SUM=<span class="variable">$SUM</span>+<span class="variable">$SWAP</span></span><br><span class="line">    <span class="keyword">done</span></span><br><span class="line">    <span class="keyword">if</span> (( <span class="variable">$SUM</span> &gt; <span class="number">0</span> )); <span class="keyword">then</span></span><br><span class="line">        <span class="built_in">echo</span> <span class="string">&quot;PID=<span class="variable">$PID</span> swapped <span class="variable">$SUM</span> KB (<span class="variable">$PROGNAME</span>)&quot;</span></span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    <span class="built_in">let</span> OVERALL=<span class="variable">$OVERALL</span>+<span class="variable">$SUM</span></span><br><span class="line">    SUM=0</span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;Overall swap used: <span class="variable">$OVERALL</span> KB&quot;</span></span><br></pre></td></tr></table></figure>

<h4 id="valgrind"><a href="#valgrind" class="headerlink" title="valgrind"></a>valgrind</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">valgrind --tool=memcheck --leak-check=yes --show-reachable=yes --num-callers=20 --track-fds=yes ./lustre_exporter</span><br></pre></td></tr></table></figure>

<h4 id="vmtouch"><a href="#vmtouch" class="headerlink" title="vmtouch"></a>vmtouch</h4><p>Show the pagecahe for the file   </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ vmtouch  ixgbe-5.12.5.tar.gz</span><br><span class="line">           Files: 1</span><br><span class="line">     Directories: 0</span><br><span class="line">  Resident Pages: 0/127  0/508K  0%</span><br><span class="line">         Elapsed: 4.1e-05 seconds</span><br><span class="line">$ cat ixgbe-5.12.5.tar.gz &gt; /dev/null</span><br><span class="line">$ vmtouch  ixgbe-5.12.5.tar.gz</span><br><span class="line">           Files: 1</span><br><span class="line">     Directories: 0</span><br><span class="line">  Resident Pages: 127/127  508K/508K  100%</span><br><span class="line">         Elapsed: 5.2e-05 seconds</span><br><span class="line"></span><br><span class="line">$ dd if=ixgbe-5.12.5.tar.gz of=/dev/null bs=1M iflag=nocache</span><br><span class="line">0+1 records in</span><br><span class="line">0+1 records out</span><br><span class="line">518142 bytes (518 kB, 506 KiB) copied, 0.000246479 s, 2.1 GB/s</span><br><span class="line"></span><br><span class="line">$ vmtouch  ixgbe-5.12.5.tar.gz</span><br><span class="line">           Files: 1</span><br><span class="line">     Directories: 0</span><br><span class="line">  Resident Pages: 0/127  0/508K  0%</span><br><span class="line">         Elapsed: 4.1e-05 seconds</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h4 id="java-malloc"><a href="#java-malloc" class="headerlink" title="java malloc"></a>java malloc</h4><ul>
<li>glibc ptmalloc2</li>
<li>google tcmalloc</li>
<li>facebook jemalloc<ul>
<li>compare with ptmalloc2, In the multiple threads apps(fixed threads number), could get the beeter malloc(litt x bytes ~ xxx bytes) free performance</li>
<li>reduce the mem fragment, not test</li>
</ul>
</li>
<li>reduce arenas<ul>
<li>glibc ptmalloc2<ul>
<li>malloc per-thread memory arenas will case the mem fragment</li>
<li>MALLOC_ARENA_MAX &#x3D; 1 # disable per thread arena, and all threads share the main arena</li>
<li>MALLOC_ARENA_MAX &#x3D; 2</li>
<li><a target="_blank" rel="noopener" href="https://devcenter.heroku.com/articles/tuning-glibc-memory-behavior">Setting MALLOC_ARENA_MAX to “2” or “1” makes glibc use fewer memory pools and potentially less memory, but this may reduce performance</a></li>
</ul>
</li>
<li>Switch to jemalloc</li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://yoloz.github.io/2018/12/14/java/java%E8%BF%9B%E7%A8%8B%E5%8D%A0%E7%94%A8virt%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E9%AB%98%E7%9A%84%E9%97%AE%E9%A2%98%E7%A0%94%E7%A9%B6/">java</a><ul>
<li>limit mem by -Xmx, looks like there is no help</li>
<li>pmap -x $java_pid<ul>
<li>too many anon 64M memory<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Address           Kbytes     RSS   Dirty Mode  Mapping</span><br><span class="line">00007ff638021000   65404       0       0 -----    [ anon ]</span><br><span class="line">00007ff63c000000     132      36      36 rw---    [ anon ]</span><br><span class="line">00007ff63c021000   65404       0       0 -----    [ anon ]</span><br><span class="line">00007ff640000000     132      28      28 rw---    [ anon ]</span><br><span class="line">00007ff640021000   65404       0       0 -----    [ anon ]</span><br><span class="line">00007ff644000000     132       8       8 rw---    [ anon ]</span><br><span class="line">00007ff644021000   65404       0       0 -----    [ anon ]</span><br><span class="line">00007ff648000000     184     184     184 rw---    [ anon ]</span><br><span class="line">00007ff64802e000   65352       0       0 -----    [ anon ]</span><br><span class="line">00007ff64c000000     132     100     100 rw---    [ anon ]</span><br><span class="line">00007ff64c021000   65404       0       0 -----    [ anon ]</span><br><span class="line">00007ff650000000     132      56      56 rw---    [ anon ]</span><br><span class="line">00007ff650021000   65404       0       0 -----    [ anon ]</span><br><span class="line">00007ff654000000     132      16      16 rw---    [ anon ]</span><br><span class="line">00007ff654021000   65404       0       0 -----    [ anon ]</span><br><span class="line"></span><br><span class="line">These memory pools are called arenas and the implementation is in arena.c. The first important macro is HEAP_MAX_SIZE which is the maximum size of an arena and it is basically 1MB on 32-bit and 64MB on 64-bit:</span><br><span class="line"></span><br><span class="line">HEAP_MAX_SIZE = (2 * DEFAULT_MMAP_THRESHOLD_MAX)</span><br><span class="line">32-bit [DEFAULT_MMAP_THRESHOLD_MAX = (512 * 1024)] = 1, 048, 576 (1MB)</span><br><span class="line">64-bit [DEFAULT_MMAP_THRESHOLD_MAX = (4 1024 1024 * sizeof(long))] = 67, 108, 864 (64MB)</span><br><span class="line"></span><br><span class="line">p2 = (char *)mmap(aligned_heap_area, HEAP_MAX_SIZE, PROT_NONE,MAP_NORESERVE | MAP_ANONYMOUS | MAP_PRIVATE, -1, 0)</span><br><span class="line"></span><br><span class="line">export MALLOC_ARENA_MAX=1 means disable per thread arena, and all threads share the main arena</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="Deactivating-KSM"><a href="#Deactivating-KSM" class="headerlink" title="Deactivating KSM"></a><a target="_blank" rel="noopener" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/virtualization_tuning_and_optimization_guide/sect-ksm-deactivating_ksm">Deactivating KSM</a></h4><p>When I deactivating the KSM under the RHEL 7.9, the basic system call latency issue has gone, eg : ssh login, top strace command , there is no 2~3 secs to wait when you are run it.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop ksmtuned</span><br><span class="line">$ systemctl stop ksm</span><br><span class="line">$ systemctl <span class="built_in">disable</span> ksm</span><br><span class="line">$ systemctl <span class="built_in">disable</span> ksmtuned</span><br><span class="line"><span class="comment">#When KSM is disabled, any memory pages that were shared prior to deactivating KSM are still shared. To delete all of the PageKSM in the system, use the following command</span></span><br><span class="line">$ <span class="built_in">echo</span> 2 &gt;/sys/kernel/mm/ksm/run</span><br></pre></td></tr></table></figure>

<h3 id="twice-memset-speed-compare"><a href="#twice-memset-speed-compare" class="headerlink" title="twice memset speed compare"></a><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/uqJt-MflK0qBcDUV-PUNNA">twice memset speed compare</a></h3><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">                     <span class="type">char</span> *p = (<span class="type">char</span> *)<span class="built_in">malloc</span>(<span class="number">0x40000000</span>);</span><br><span class="line">                     <span class="type">clock_t</span> t1 = clock();</span><br><span class="line">                     <span class="built_in">memset</span>(p, <span class="number">0</span>, <span class="number">0x40000000</span>);</span><br><span class="line">                     <span class="type">clock_t</span> t2 = clock();</span><br><span class="line"></span><br><span class="line">                     <span class="built_in">memset</span>(p, <span class="number">1</span>, <span class="number">0x40000000</span>);</span><br><span class="line">                     <span class="type">clock_t</span> t3 = clock();</span><br><span class="line"></span><br><span class="line">                     <span class="built_in">memset</span>(p, <span class="number">2</span>, <span class="number">0x40000000</span>);</span><br><span class="line">                     <span class="type">clock_t</span> t4 = clock();</span><br><span class="line"></span><br><span class="line">                     <span class="type">double</span> cost1 = (<span class="type">double</span>)(t2-t1)/CLOCKS_PER_SEC;</span><br><span class="line">                     <span class="type">double</span> cost2 = (<span class="type">double</span>)(t3 - t2)/CLOCKS_PER_SEC;</span><br><span class="line">                     <span class="type">double</span> cost3 = (<span class="type">double</span>)(t4 - <span class="number">3</span>)/CLOCKS_PER_SEC;</span><br><span class="line">                     <span class="built_in">printf</span>(<span class="string">&quot;Time Cost: %.3fs vs %.3fs vs %.3fs\n&quot;</span>,</span><br><span class="line">                     cost1, cost2, cost3);</span><br><span class="line">                     <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>假设，我们要设计一个学号查询系统，给全国所有的大、中、小学生分配一个全国唯一的学号，给定一个学号能够立即对应到具体的学生。再假设，要求我们必须用一个48bit的数字来表示这个学号。<br>我们可以这样设计学生学号：用9个bit表示省份，9个bit表示城市，9个bit表示区县，9个bit表示学校，12个bit表示学生在学校内的编号。<br>省份表：包含所有的省份，一个省份对应一个表项，每个表项是一个指针，指向本省的城市表<br>城市表：每个省都有一个自己的城市表，包含本省所有的城市，一个城市对应一个表项，每个表项是一个指针，指向本市的区县表<br>区县表：每个市都有一个自己的区县表，包含本市所有的县&#x2F;区，每个区县对应一个表项，每个表项是一个指针，指向本县&#x2F;区内的学校表<br> 学校表：每个县&#x2F;区都有一个学校表，包含本县&#x2F;区内所有的大、中、小学，每个学校对应一个表项，每个表项是一个指针，指向本学校的学生表<br> 学生表：包含一个学校内的所有的学生，每个表项是一个特定的学生。<br>现在，给定一个学号：0x010203040506，我们可以立即根据这个学号找到对应的学生<br> 我们把省份表存放在固定的一个地方，称之为全局表指针，每次查询时直接从这里获取省份表<br> 根据省份索引在省份表中找到了浙江省的城市表<br>      根据城市索引在城市表中找到了杭州市的区县表<br> 根据区县索引在区县表中找到了西湖区的学校表<br> 根据学校索引在学校表中找到了浙江大学的学生表<br> 根据学生校内编号在学生表中找到了对应的学生，张三<br>这个过程是不是很简单？给定任意一个学号，我们立刻可以对应到：A省B市C区D学校的学生E。<br>MMU(Memory Management Unit)<br>下面以X64 CPU为例进行讲解。X64虽然理论地址宽度是64bit，但目前大部分CPU只支持48bit共256TB的虚拟地址空间。对于4KB大小的页面，MMU采用4级页表进行地址转换，分别是：<br> PML4(Page Map Level 4)，第4级页映射表，每个表项指向一个PDPT<br> PDPT(Page Directory Pointer Table)，页目录指针表，每个表项指向一个PDT<br> PDT(Page Directory Table)，页目录表，每个表项指向一个PT<br> PT(Page Table)，页表，每个表项指向一个物理页面<br>和学生学号一样，48bit的有效虚拟地址被划分成了5部分：<br> 9bit PML4索引<br> 9bit PDPT索引<br> 9bit PDT索引<br> 9bit PT索引<br> 12bit 页内偏移  </p>
<p>test.c用malloc()分配了1GB的虚拟内存，内核并没有立即为其分配物理内存，所以每当第一次访问一个虚拟页面，都会触发缺页异常。<br>也就意味着，第一次memset()的时候，会触发1GB &#x2F; 4KB &#x3D; 262144次缺页异常，而当第二次和第三次memset()的时候，内核已经分配好了物理内存，不会再次触发缺页异常，所以性能比第一次memset()要好的多。  </p>
<p>把第二次和第三次memset()注释掉  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">perf <span class="built_in">stat</span> -d -d ./test  </span><br><span class="line">155,486  	page-faults  </span><br></pre></td></tr></table></figure>
<p>把第二次和第三次加回去, 结果是一样的  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">perf <span class="built_in">stat</span> -d -d ./test  </span><br><span class="line">       	155,486  	page-faults  </span><br></pre></td></tr></table></figure>

<h4 id="optimze-by-hugepage"><a href="#optimze-by-hugepage" class="headerlink" title="optimze by hugepage"></a>optimze by hugepage</h4><p>64上，使用2MB的页面，只需要3级页表就可以了，相比4KB的页面，减少一级页表<br>以4GB虚拟内存为例，如果采用4KB页面进行管理，就要有4GB&#x2F;4KB &#x3D; 1048576个Page，也就意味着需要1048576个页表项，再加上中间各级页表目录，就会占用大量的内存来存储各级页表。<br>样的4GB虚拟内存，使用2MB的大页，只需要4GB&#x2F;2MB &#x3D; 2048个Page，缺页异常和TLB都会大幅下降，能显著提升程序性能。而用1GB的页面的话，只需4GB &#x2F; 1GB &#x3D; 4个Page，此时，缺页异常和TLB Miss的影响被降到了最小。   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line">root@ubuntu:<span class="built_in">test</span><span class="comment"># cat /proc/meminfo | grep Hugepagesize</span></span><br><span class="line">Hugepagesize:       2048 kB</span><br><span class="line">root@ubuntu:<span class="built_in">test</span><span class="comment"># cat /proc/sys/vm/nr_hugepages</span></span><br><span class="line">0</span><br><span class="line">root@ubuntu:<span class="built_in">test</span><span class="comment"># echo 1024 &gt; /proc/sys/vm/nr_hugepages</span></span><br><span class="line"><span class="built_in">cat</span> /proc/meminfo | grep Huge</span><br><span class="line">HugePages_Total:    1024</span><br></pre></td></tr></table></figure>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;sys/mman.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="type">int</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line"></span><br><span class="line">                 	<span class="type">char</span> *p = (<span class="type">char</span> *)mmap(<span class="literal">NULL</span>, <span class="number">0x40000000</span>, PROT_READ | PROT_WRITE,</span><br><span class="line">                  	MAP_ANONYMOUS | MAP_PRIVATE | MAP_HUGETLB, <span class="number">-1</span>, <span class="number">0</span>);</span><br><span class="line">                 	<span class="type">clock_t</span> t1 = clock();</span><br><span class="line">                 	<span class="built_in">memset</span>(p, <span class="number">0</span>, <span class="number">0x40000000</span>);</span><br><span class="line">                 	<span class="type">clock_t</span> t2 = clock();</span><br><span class="line"></span><br><span class="line">                 	<span class="built_in">memset</span>(p, <span class="number">1</span>, <span class="number">0x40000000</span>);</span><br><span class="line">                 	<span class="type">clock_t</span> t3 = clock();</span><br><span class="line"></span><br><span class="line">                 	<span class="built_in">memset</span>(p, <span class="number">2</span>, <span class="number">0x40000000</span>);</span><br><span class="line">                 	<span class="type">clock_t</span> t4 = clock();</span><br><span class="line"></span><br><span class="line">                 	<span class="type">double</span> cost1 = (<span class="type">double</span>)(t2-t1)/CLOCKS_PER_SEC;</span><br><span class="line">                 	<span class="type">double</span> cost2 = (<span class="type">double</span>)(t3 - t2)/CLOCKS_PER_SEC;</span><br><span class="line">                 	<span class="type">double</span> cost3 = (<span class="type">double</span>)(t4 - <span class="number">3</span>)/CLOCKS_PER_SEC;</span><br><span class="line">                 	<span class="built_in">printf</span>(<span class="string">&quot;Time Cost: %.3fs vs %.3fs vs %.3fs\n&quot;</span>,</span><br><span class="line">                 	cost1, cost2, cost3);</span><br><span class="line">                 	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> never &gt; /sys/kernel/mm/transparent_hugepage/enabled</span><br><span class="line"><span class="built_in">echo</span> 1024 &gt; /proc/sys/vm/nr_hugepages</span><br><span class="line"><span class="built_in">cat</span> /proc/meminfo | grep Huge</span><br><span class="line">AnonHugePages:  	2048 kB</span><br><span class="line">ShmemHugePages:    	0 kB</span><br><span class="line">FileHugePages:     	0 kB</span><br><span class="line">HugePages_Total:	1024</span><br><span class="line">HugePages_Free: 	1024</span><br><span class="line">HugePages_Rsvd:    	0</span><br><span class="line">HugePages_Surp:    	0</span><br><span class="line">Hugepagesize:   	2048 kB</span><br><span class="line">Hugetlb:     	2097152 kB</span><br><span class="line"></span><br><span class="line">Perf <span class="built_in">stat</span> -d -d ./test</span><br><span class="line">562  	page-faults</span><br><span class="line"></span><br><span class="line"><span class="comment">#mmap提供了一个MAP_POPULATE标记，可以在分配内存的时候提前把页面映射好，从而避免运行时的缺页异常。</span></span><br><span class="line">char *p = (char *)mmap(NULL, 0x40000000, PROT_READ | PROT_WRITE,</span><br><span class="line">                  	MAP_ANONYMOUS | MAP_PRIVATE | MAP_HUGETLB| MAP_POPULATE, -1, 0);</span><br><span class="line"></span><br><span class="line">Perf <span class="built_in">stat</span> -d -d ./test</span><br><span class="line">50  	page-faults</span><br><span class="line"></span><br><span class="line"><span class="comment">#让程序保持较好的局部性，顺序访问优于随机访问，如数组访问优于链表访问。</span></span><br><span class="line"><span class="comment">#预读或预热，如进入关键路径之前，先确保物理内存已经被分配。比如，有些系统设计为了解决这类问题，甚至会专门创建一个helper线程，专门负责预读，不仅适合文件I/O，也适合内存访问量比较大的场景。</span></span><br><span class="line"><span class="comment">#禁用swap。swap虽然可以一定程度上缓解物理内存短缺的问题，但是却可能会影响系统整体性能，尤其在热点路径上，如果发生页面swap，会对程序性能产生严重影响。</span></span><br><span class="line"><span class="comment">#考虑NUMA的影响，程序访问本地node上的内存比访问远程node内存快的多</span></span><br></pre></td></tr></table></figure>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hardware"><span class="toc-number">1.</span> <span class="toc-text">Hardware</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Host-Memory-Buffer-HMB"><span class="toc-number">1.1.</span> <span class="toc-text">Host Memory Buffer (HMB)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#PCI-E-GT-s-to-GB-s"><span class="toc-number">1.2.</span> <span class="toc-text">PCI-E GT&#x2F;s to GB&#x2F;s</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DDR5"><span class="toc-number">1.3.</span> <span class="toc-text">DDR5</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Linux"><span class="toc-number">2.</span> <span class="toc-text">Linux</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#ECC-disabled-error"><span class="toc-number">2.1.</span> <span class="toc-text">ECC disabled error</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#segment-fault"><span class="toc-number">2.2.</span> <span class="toc-text">segment fault</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#x86-64-5-level-page-table"><span class="toc-number">2.3.</span> <span class="toc-text">x86_64 5-level page table</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Memroy-allocators"><span class="toc-number">2.4.</span> <span class="toc-text">Memroy allocators</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#slabtop"><span class="toc-number">2.5.</span> <span class="toc-text">slabtop</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#slab-exhaust-all-memory-because-a-lot-of-scan"><span class="toc-number">2.5.1.</span> <span class="toc-text">slab exhaust all memory because a lot of scan</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Single-process-memory"><span class="toc-number">2.6.</span> <span class="toc-text">Single process memory</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Struct-page"><span class="toc-number">2.7.</span> <span class="toc-text">Struct page</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#smem"><span class="toc-number">2.8.</span> <span class="toc-text">smem</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Kernel-Memory-Leak-Detector"><span class="toc-number">2.9.</span> <span class="toc-text">Kernel Memory Leak Detector</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zram"><span class="toc-number">2.10.</span> <span class="toc-text">zram</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Make-sure-the-ECC-work-in-Linux"><span class="toc-number">2.11.</span> <span class="toc-text">Make sure the ECC work in Linux</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Monitor-ECC-error"><span class="toc-number">2.12.</span> <span class="toc-text">Monitor ECC error</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#From-OS"><span class="toc-number">2.12.1.</span> <span class="toc-text">From OS</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#From-IPMI"><span class="toc-number">2.12.2.</span> <span class="toc-text">From IPMI</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ECC-err"><span class="toc-number">2.12.3.</span> <span class="toc-text">ECC err</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#memory-fragmention"><span class="toc-number">2.13.</span> <span class="toc-text">memory fragmention</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#check-the-fragment"><span class="toc-number">2.13.1.</span> <span class="toc-text">check the fragment</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ZRAM"><span class="toc-number">2.14.</span> <span class="toc-text">ZRAM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#numa"><span class="toc-number">2.15.</span> <span class="toc-text">numa</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Soft-lockup-detected-on-a-large-NUMA-system-under-a-heavy-memory-usage"><span class="toc-number">2.15.1.</span> <span class="toc-text">Soft lockup detected on a large NUMA system under a heavy memory usage</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Transparent-Huge-Page"><span class="toc-number">2.16.</span> <span class="toc-text">Transparent Huge Page</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Use-hugetlbfs"><span class="toc-number">2.17.</span> <span class="toc-text">Use hugetlbfs</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#set-hugepage"><span class="toc-number">2.17.1.</span> <span class="toc-text">set hugepage</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#free-hugepage"><span class="toc-number">2.17.2.</span> <span class="toc-text">free hugepage</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#memlock"><span class="toc-number">2.18.</span> <span class="toc-text">memlock</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#get-pagesize"><span class="toc-number">2.19.</span> <span class="toc-text">get pagesize</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#memmap"><span class="toc-number">2.20.</span> <span class="toc-text">memmap</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#mapping-dev-pmemX-can-be-used-to-create-a-DAX-filesystem"><span class="toc-number">2.20.1.</span> <span class="toc-text">mapping &#x2F;dev&#x2F;pmemX can be used to create a DAX filesystem</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Benchmark-mem-by-perf"><span class="toc-number">2.21.</span> <span class="toc-text">Benchmark mem by perf</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#dirty-page"><span class="toc-number">2.22.</span> <span class="toc-text">dirty page</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#meminfo"><span class="toc-number">2.23.</span> <span class="toc-text">meminfo</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#proc-vmstat"><span class="toc-number">2.24.</span> <span class="toc-text">&#x2F;proc&#x2F;vmstat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#OOM"><span class="toc-number">2.25.</span> <span class="toc-text">OOM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#show-the-swap"><span class="toc-number">2.26.</span> <span class="toc-text">show the swap</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#valgrind"><span class="toc-number">2.27.</span> <span class="toc-text">valgrind</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#vmtouch"><span class="toc-number">2.28.</span> <span class="toc-text">vmtouch</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#java-malloc"><span class="toc-number">2.29.</span> <span class="toc-text">java malloc</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Deactivating-KSM"><span class="toc-number">2.30.</span> <span class="toc-text">Deactivating KSM</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#twice-memset-speed-compare"><span class="toc-number">3.</span> <span class="toc-text">twice memset speed compare</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#optimze-by-hugepage"><span class="toc-number">3.1.</span> <span class="toc-text">optimze by hugepage</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/11/14/mem/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/11/14/mem/&text=memory"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/11/14/mem/&is_video=false&description=memory"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=memory&body=Check out this article: http://example.com/2022/11/14/mem/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/11/14/mem/&title=memory"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/11/14/mem/&name=memory&description=&lt;h3 id=&#34;Hardware&#34;&gt;&lt;a href=&#34;#Hardware&#34; class=&#34;headerlink&#34; title=&#34;Hardware&#34;&gt;&lt;/a&gt;Hardware&lt;/h3&gt;&lt;h4 id=&#34;Host-Memory-Buffer-HMB&#34;&gt;&lt;a href=&#34;#Host-Memory-Buffer-HMB&#34; class=&#34;headerlink&#34; title=&#34;Host Memory Buffer (HMB)&#34;&gt;&lt;/a&gt;Host Memory Buffer (HMB)&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.virtium.com/knowledge-base/how-to-select-between-dram-vs-dram-less-ssds/&#34;&gt;HMB vs DRAM-less&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;DRAM-less has the lower BW and OPS&lt;/li&gt;
&lt;li&gt;HMB has the more stable throughput for a long time&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;PCI-E-GT-s-to-GB-s&#34;&gt;&lt;a href=&#34;#PCI-E-GT-s-to-GB-s&#34; class=&#34;headerlink&#34; title=&#34;PCI-E GT&amp;#x2F;s to GB&amp;#x2F;s&#34;&gt;&lt;/a&gt;PCI-E GT&amp;#x2F;s to GB&amp;#x2F;s&lt;/h4&gt;&lt;p&gt;GT per sec to GB per sec&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#PCIe 2.0 协议支持 5.0 GT/s 的传输速率，但是由于采用了 8b/10b 编码方案，导致每条通道的实际有效速率为 5*8/10=4 Gbps，也就是 500 MB/s&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;BEGIN&amp;#123;print &amp;quot;64GT/s to GB/s pcie gen3: &amp;quot; 64*(128/130)/8&amp;quot; GB/s&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;64GT/s to GB/s pcie gen3: 7.87692 GB/s&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;BEGIN&amp;#123;print &amp;quot;40GT/s to GB/s pcie gen2: &amp;quot; 40*(8/10)/8&amp;quot; GB/s&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;40GT/s to GB/s pcie gen2: 4 GB/s&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;DDR refers to double data rate, which means that the transfer rate (MT&amp;#x2F;s) is double what the speed rating is in MHz. Non-ECC DDR RAM is 64 bits (8 bytes) wide - so you’d do the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RAM speed * 2 &amp;#x3D; MT&amp;#x2F;s * RAM width &amp;#x3D; bandwidth (in MB&amp;#x2F;s) 2933MHz * 2 &amp;#x3D; 5866 MT&amp;#x2F;s * 64bit&amp;#x2F;8(gb&amp;#x2F;s to GB&amp;#x2F;s) &amp;#x3D; 46,928 MB&amp;#x2F;s (approx 45.8GB&amp;#x2F;s)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;GDDR6 in GPU&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GDDR6 pin 16Gb&amp;#x2F;s(16GT&amp;#x2F;s) is the maximum data rate per pin for GDDR6&lt;ul&gt;
&lt;li&gt;16Gbps * 384bit&amp;#x2F;8(bytes)&amp;#x3D; 768GB&amp;#x2F;s&lt;/li&gt;
&lt;li&gt;awk ‘BEGIN{print 46928&amp;#x2F;64*8&amp;#x2F;2}’ &amp;#x3D; 2933MT&amp;#x2F;s&lt;/li&gt;
&lt;li&gt;awk ‘BEGIN{print 768&amp;#x2F;384*8&amp;#x2F;2}’ &amp;#x3D; 8GT&amp;#x2F;s&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;GDDR6X can transfer two bits per pin instead of just one. This raises overall memory speed to 19–21GT&amp;#x2F;s when compared to standard GDDR6 that tends to top out at 16GT&amp;#x2F;s.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;a href=&#34;http://preshing.com/20120710/memory-barriers-are-like-source-control-operations/&#34;&gt;Memory Barriers&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;C++11 atomic types, such as load(std::memory_order_acquire)&lt;/li&gt;
&lt;li&gt;POSIX mutexes, such as pthread_mutex_lock&lt;/li&gt;
&lt;li&gt;LoadLoad&lt;ul&gt;
&lt;li&gt;A LoadLoad barrier effectively prevents reordering of loads performed before the barrier with loads performed after the barrier.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;StoreStore&lt;ul&gt;
&lt;li&gt;A StoreStore barrier effectively prevents reordering of stores performed before the barrier with stores performed after the barrier.&lt;figure class=&#34;highlight c&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;meta&#34;&gt;#&lt;span class=&#34;keyword&#34;&gt;define&lt;/span&gt; barrier() __asm__ __volatile__(&lt;span class=&#34;string&#34;&gt;&amp;quot;&amp;quot;&lt;/span&gt;: : :&lt;span class=&#34;string&#34;&gt;&amp;quot;memory&amp;quot;&lt;/span&gt;)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;barrier()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;mb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;rmb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;wmb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;smp_mb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;smp_rmb()&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;smp_wmb()&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;DDR5&#34;&gt;&lt;a href=&#34;#DDR5&#34; class=&#34;headerlink&#34; title=&#34;DDR5&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://www.crucial.com/articles/about-memory/everything-about-ddr5-ram#scanner&#34;&gt;DDR5&lt;/a&gt;&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;DDR5-5600	69.21 GB/s&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-5200	66.12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4800	62.74&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4400	58.81&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4000	54.65&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-3600	50.26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-3200	45.62&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-3200	33.57&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;DDR5’s latency is virtually the same as DDR4&lt;ul&gt;
&lt;li&gt;CAS latency is often misunderstood because of its naming convention, but it’s only half of the true memory latency equation. True memory latency is measured in nanoseconds and is a combination of RAM speed and CAS latency.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;figure class=&#34;highlight plaintext&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;True memory latency (ns) = (2000/RAM Speed) (ns) x CAS latency&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;true memory latency of DDR4-3200 CL22 = 13.75 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;true memory latency of DDR5-4800 CL40 = 16.67 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Memory experts and system architects know that the latency matters only at the system level since that is what users typically experience. That said, system latency is also measured in nanoseconds and is a combination of host memory controller features and behavior, number of module ranks, memory speed, and true memory latency.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;CAS latency (CL)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR5-4800 CL40 92.8 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;DDR4-3200 CL22 90.0 ns&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;So, what is CAS latency? At a basic level, latency refers to the time delay between when a command is entered and when the data is available. Latency is the gap between these two events. When the memory controller tells the memory to access a particular location, the data must go through a number of clock cycles in the column address strobe (CAS) to get to its desired location and complete the command. There are two main variables that determine a module&amp;#x27;s latency:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;The total number of clock cycles the data must go through (measured in CAS latency, or CL, on data sheets) The duration of each clock cycle (measured in nanoseconds)  Combining these two variables gives us the latency equation: &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;latency (ns) = clock cycle time (ns) x number of clock cycles &lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h3 id=&#34;Linux&#34;&gt;&lt;a href=&#34;#Linux&#34; class=&#34;headerlink&#34; title=&#34;Linux&#34;&gt;&lt;/a&gt;Linux&lt;/h3&gt;&lt;h4 id=&#34;ECC-disabled-error&#34;&gt;&lt;a href=&#34;#ECC-disabled-error&#34; class=&#34;headerlink&#34; title=&#34;ECC disabled error&#34;&gt;&lt;/a&gt;ECC disabled error&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ dmesg &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;EDAC sbridge: CPU SrcID &lt;span class=&#34;comment&#34;&gt;#0, Ha #0, Channel #1 has DIMMs, but ECC is disabled&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;EDAC sbridge: Couldn&lt;span class=&#34;string&#34;&gt;&amp;#x27;t find mci handler&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;EDAC sbridge: Couldn&amp;#x27;&lt;/span&gt;t find mci handler&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;EDAC sbridge: Failed to register device with error -19.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;segment-fault&#34;&gt;&lt;a href=&#34;#segment-fault&#34; class=&#34;headerlink&#34; title=&#34;segment fault&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://utcc.utoronto.ca/~cks/space/blog/linux/KernelSegfaultErrorCodes&#34;&gt;segment fault&lt;/a&gt;&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;error 4: (Data) read from an unmapped area&lt;ul&gt;
&lt;li&gt;This is your classic wild pointer read. On 64-bit x86, most of the address space is unmapped so even a program that uses a relatively large amount of memory is hopefully going to have most bad pointers go to memory that has no mappings at all.&lt;/li&gt;
&lt;li&gt;A faulting address of 0 is a NULL pointer and falls into page zero, the lowest page in memory. The kernel prevents people from mapping page zero, and in general low memory is never mapped, so reads from small faulting addresses should always be error 4s.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 5: read from a memory area that’s mapped but not readable&lt;ul&gt;
&lt;li&gt;This is probably a pointer read of a pointer that is so wild that it’s pointing somewhere in the kernel’s area of the address space. It might be a guard page, but at least some of the time mmap()’ing things with PROT_NONE appears to make Linux treat them as unmapped areas so you get error code 4 instead. You might think this could be an area mmap()’d with other permissions but without PROT_READ, but it appears that in practice other permissions imply the ability to read the memory as well.&lt;/li&gt;
&lt;li&gt;(I assume that the Linux kernel is optimizing PROT_NONE mappings by not even creating page table entries for the memory area, rather than carefully assembling PTEs that deny all permissions. The error bits come straight from the CPU, so if there are no PTEs the CPU says ‘fault for an unmapped area’ regardless of what Linux thinks and will report in, eg, &amp;#x2F;proc&amp;#x2F;PID&amp;#x2F;maps.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 6: (data) write to an unmapped area.&lt;ul&gt;
&lt;li&gt;This is your classic write to a wild or corrupted pointer, including to (or through) a null pointer. As with reads, writes to guard pages mmap()’d with PROT_NONE will generally show up as this, not as ‘write to a mapped area that denies permissions’.&lt;/li&gt;
&lt;li&gt;(As with reads, all writes with small faulting addresses should be error 6s because no one sane allows low memory to be mapped.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 7: write to a mapped area that isn’t writable.&lt;ul&gt;
&lt;li&gt;This is either a wild pointer that was unlucky enough to wind up pointing to a bit of memory that was mapped, or an attempt to change read-only data, for example the classical C mistake of trying to modify a string constant (as seen in the first entry). You might also be trying to write to a file that was mmap()’d read only, or in general a memory mapping that lacks PROT_WRITE.&lt;/li&gt;
&lt;li&gt;(All attempts to write to the kernel’s area of address space also get this error, instead of error 6.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 14: attempt to execute code from an unmapped area&lt;ul&gt;
&lt;li&gt;This is the sign of trying to call through a mangled function pointer (or a NULL one), or perhaps returning from a call when the stack is in an unexpected or corrupted state so that the return address isn’t valid. One source of mangled function pointers is use-after-free issues where the (freed) object contains embedded function pointers&lt;/li&gt;
&lt;li&gt;(Error 14 with a faulting address of 0 often means a function call through a NULL pointer, which in turn often means ‘making an indirect call to a function without checking that it’s defined’. There are various larger scale causes of this in code.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;error 15: attempt to execute code from a mapped memory area that isn’t executable&lt;ul&gt;
&lt;li&gt;This is probably still a mangled function pointer or return address, it’s just that you’re unlucky (or lucky) and there’s mapped memory there instead of nothing.&lt;/li&gt;
&lt;li&gt;(Your code could have confused a function pointer with a data pointer somehow, but this is a lot rarer a mistake than confusing writable data with read-only data.)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;x86-64-5-level-page-table&#34;&gt;&lt;a href=&#34;#x86-64-5-level-page-table&#34; class=&#34;headerlink&#34; title=&#34;x86_64 5-level page table&#34;&gt;&lt;/a&gt;x86_64 5-level page table&lt;/h4&gt;&lt;p&gt;Linear-address Translation Using 5-level paging   &lt;/p&gt;
&lt;h4 id=&#34;Memroy-allocators&#34;&gt;&lt;a href=&#34;#Memroy-allocators&#34; class=&#34;headerlink&#34; title=&#34;Memroy allocators&#34;&gt;&lt;/a&gt;Memroy allocators&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://stackoverflow.com/questions/5684365/what-causes-page-faults&#34;&gt;page fault&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Accessing a page that isn’t resident in memory but is on disk in a page file or a mapped file&lt;ul&gt;
&lt;li&gt;Allocate a physical page, and read the desired page from disk and into the relevant working set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Accessing a page that is on the standby or modified list&lt;ul&gt;
&lt;li&gt;Transition the page to the relevant process, session, or system working set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Accessing a demand-zero page&lt;ul&gt;
&lt;li&gt;Add a zero-filled page to the relevant working set&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Writing to a copy-on-write page&lt;ul&gt;
&lt;li&gt;Make process-private (or session-private) copy of page, and replace original in process or system working set&lt;br&gt;page fault并不是说进程要访问的内存(地址)不在虚拟地址空间，那对应segment fault。&lt;br&gt;page fault应该指进程访问的虚拟地址尚未建立虚拟地址与物理地址对应表，或表存在但物理地址未被缓存。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;brk是将数据段(.data)的最高地址指针_edata往高地址推，mmap是在进程的虚拟地址空间中（一般是堆和栈中间）找一块空闲的。这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址&lt;br&gt;空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系。       &lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://cloud.tencent.com/developer/article/1420726&#34;&gt;如果一个进程使用了mmap将很大的数据文件映射到进程的虚拟地址空间，我们需要重点关注majflt的值，因为相比minflt，majflt对于性能的损害是致命的，随机读一次磁盘的耗时数量级在几个毫秒，而minflt只有&amp;gt;在大量的时候才会对性能产生影响。&lt;/a&gt;    &lt;/p&gt;
&lt;p&gt;In my case, ftrace and perf show the very slow about munmap system call, there are tons of minflt in per secs(3000,000~8000,000),  issue in the hardware memory&lt;br&gt;upgrade firmware, disable ksm and disable powersave and export all numa cores and disable HT, reduce the minflt about 100x in the same applications&lt;br&gt;and I ‘m sure it ‘s not the memory fragment  &lt;/p&gt;
&lt;p&gt;Page faults can occur for a variety of reasons, as you can see above. Only one of them has to do with reading from the disk. If you try to allocate a block from the heap and the heap manager allocates new pages, then accesses those pages, you’ll get a demand-zero page fault. If you try to hook a function in kernel32 by writing to kernel32’s pages, you’ll get a copy-on-write fault because those pages are silently being copied so your changes don’t affect other processes.  &lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;禁止malloc调用mmap分配内存，禁止内存紧缩。&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;在进程启动时候，加入以下两行代码：&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;mallopt(M_MMAP_MAX, 0); // 禁止malloc调用mmap分配内存&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;mallopt(M_TRIM_THRESHOLD, -1); // 禁止内存紧缩&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;从操作系统角度来看，进程分配内存有两种方式，分别由两个系统调用完成：brk和mmap（不考虑共享内存）。brk是将数据段(.data)的最高地址指针_edata往高地址推，mmap是在进程的虚拟地址空间中（一般是堆和&amp;gt;栈中间）找一块空闲的。这两种方式分配的都是虚拟内存，没有分配物理内存。在第一次访问已分配的虚拟地址空间的时候，发生缺页中断，操作系统负责分配物理内存，然后建立虚拟内存和物理内存之间的映射关系&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;。在标准C库中，提供了malloc/free函数分配释放内存，这两个函数底层是由brk，mmap，munmap这些系统调用实现的。  &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ sar -B 2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:     pgpgin/s pgpgout/s   fault/s  majflt/s  pgfree/s pgscank/s pgscand/s pgsteal/s    %vmeff&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:        35.18     30.30 6259790.16    306.97 4470216.70      0.00      0.00      0.00      0.00&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ pidstat -r 2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:      UID       PID  minflt/s  majflt/s     VSZ    RSS   %MEM  Command&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:    1001429      4061      2.65      0.00 1062056 149640   0.01  node&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Average:        0      6313    672.57      0.00   44260  22652   0.00  condor_procd&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ ps -o majflt,minflt,pid,c,&lt;span class=&#34;built_in&#34;&gt;comm&lt;/span&gt; -C python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;MAJFLT MINFLT    PID  C COMMAND&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   231 5394232  4494  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;     6   8981   5305  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;     0   4387  10567  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   935  33308  36002  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;     8  18544  36005  0 python&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;       maj_flt     MAJFLT    The number of major page faults that have occurred with this process.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;       min_flt     MINFLT    The number of minor page faults that have occurred with this process.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;maj_flt low value, min_flt high value, check the python application code&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;Process                         kernel          Slab allocator    page allocator&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  |                               |                   |                |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Segments    libc allocator     ext4/scsi module-----caches-----------free lists-------DRAM(physical)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  |             |                                                      |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Heap----------memory----------------------------------------------------&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Application                    virtual mem&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   |                               |   ----------------------------------------physical mem&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;   |&amp;lt;-----------------------------||   |                                            ^&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Allocator (libc)                  ||   V                                            |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;1.malloc()--------2a blk---------&amp;gt;heap(from high to low)----&amp;gt;3.lookup--mmu--&amp;gt;4.page fault&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  free()       |                   |  |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  realloc()    |                   V  |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  calloc()     |                      ------------------5.page out------------&amp;gt;swap device&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;               |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;               -2b mmap/munmap---&amp;gt;mappings&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (processs address space)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;https://stackoverflow.com/questions/9819186/munmap-performance-on-linux&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;The actual reason it gets slow is that munmap() takes the mm-&amp;gt;mmap_sem lock &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; the entire duration of the syscall. &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Several other operations are liable to be blocked by this, &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; example (but not limited to) fork()/mmap(). This is especially important to note &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; architectures that &lt;span class=&#34;keyword&#34;&gt;do&lt;/span&gt; not implement a lockless get_user_pages_fast() operation &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; pages already in-memory, &lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;because a bunch of futex operations (that underpin pthread primitives) will call get_user_pages_fast() and the default implementation will try to take a &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; lock on mmap_sem.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;tcmalloc and jemalloc provide their own allocator along with garbage collection&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;38&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;page cache &amp;lt;--&amp;gt; filesystem              RAW block device&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                   |                        |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                   |                        |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                   --------------------------&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                     block device interface&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                     Volume manager (&lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; used)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                     Device Mapper (&lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; used)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                         Block layer&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;            classic scheduler  or  Multi queue scheduler&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                    Host Bus adaptor Driver(scsi)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              |&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                              V&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                        Disk devices&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;# blk_fill_rwbs&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;R: &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;W: Write&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;M: Metadata&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;S: Synchronous&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;A: Readahead&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;F: Flush or force unit access&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;D: Discard&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;E: Erase&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;N: None&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;IO scduler&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Noop/Dealine/CFQ&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;blk-mq (linux 3.13)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;In the event of a system failure while there are active writes, the parity of a stripe may become inconsistent with the data. If this is not detected and repaired before a disk or block fails, data loss may ensue as incorrect parity will be used to reconstruct the missing block in that stripe. This potential vulnerability is sometimes known as the write hole. Battery-backed cache and similar techniques are commonly used to reduce the window of opportunity for this to occur. The same issue occurs for RAID-6.&lt;/p&gt;
&lt;h4 id=&#34;slabtop&#34;&gt;&lt;a href=&#34;#slabtop&#34; class=&#34;headerlink&#34; title=&#34;slabtop&#34;&gt;&lt;/a&gt;slabtop&lt;/h4&gt;&lt;p&gt;In-kernel data structures cache&lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#slab &amp;gt; 100M object&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /proc/slabinfo |awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;if($3*$4/1024/1024 &amp;gt; 100)&amp;#123;print $1,$3*$4/1024/1024&amp;#125; &amp;#125;&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2554 x slabs&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;56 x obj &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; a slab&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;total 8128 = 254 x 32, obj sieze=1K&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8128 x 1K = cache size = 8128K&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;  OBJS ACTIVE  USE OBJ SIZE  SLABS OBJ/SLAB CACHE SIZE NAME&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt; 8128   7360  90%    1.00K    254        32      8128K kmalloc-1024&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;To free pagecache:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 1 &amp;gt; /proc/sys/vm/drop_caches&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;To free dentries and inodes:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 2 &amp;gt; /proc/sys/vm/drop_caches&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;To free pagecache, dentries and inodes:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 3 &amp;gt; /proc/sys/vm/drop_caches&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#Got the free pages from buddyinfo&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;sum=0;for(i=5;i&amp;lt;=NF;i++) sum+=$i*(2^(i-5))&amp;#125;;&amp;#123;total+=sum*4096/1024/1024&amp;#125;;&amp;#123;print $1 &amp;quot; &amp;quot; $2 &amp;quot; &amp;quot; $3 &amp;quot; &amp;quot; $4 &amp;quot;\t : &amp;quot; sum*4096/1024/1024 &amp;quot;M&amp;quot;&amp;#125; END &amp;#123;print &amp;quot;total\t\t\t : &amp;quot; total &amp;quot;M&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/buddyinfo&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;All processes mem= $(grep Pss /proc/[1-9]*/smaps | awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;total+=$2&amp;#125;; END &amp;#123;print total&amp;#125;&amp;#x27;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;RSS mem = awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;sum+=$2&amp;#125; END&amp;#123;print sum*4&amp;quot; KiB&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/[1-9]*/statm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#there are little diff result with python version, because awk and python is not the same application, malloc diff mem&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;slab size = awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;sum=sum+$3*$4;&amp;#125;END&amp;#123;print sum/1024&amp;quot; KiB&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/slabinfo&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;pagetable size = awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;$0~/PageTables/&amp;#123;print $2&amp;quot; KiB&amp;quot;&amp;#125;&amp;#x27;&lt;/span&gt; /proc/meminfo&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;The RSS contains some of duplicate part, some share lib, if you want the real value, get it from pmap    &lt;/p&gt;
&lt;h5 id=&#34;slab-exhaust-all-memory-because-a-lot-of-scan&#34;&gt;&lt;a href=&#34;#slab-exhaust-all-memory-because-a-lot-of-scan&#34; class=&#34;headerlink&#34; title=&#34;slab exhaust all memory because a lot of scan&#34;&gt;&lt;/a&gt;slab exhaust all memory because a lot of scan&lt;/h5&gt;&lt;p&gt;&lt;img src=&#34;/img/top-mem1.png&#34;&gt;&lt;br&gt;Why Slab&amp;#x3D;24021820 kB (24GB)&lt;br&gt;slabtop&lt;br&gt;&lt;img src=&#34;/img/slabtop1.png&#34;&gt;&lt;/p&gt;
&lt;h4 id=&#34;Single-process-memory&#34;&gt;&lt;a href=&#34;#Single-process-memory&#34; class=&#34;headerlink&#34; title=&#34;Single process memory&#34;&gt;&lt;/a&gt;Single process memory&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;       /proc/[pid]/statm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;              Provides information about memory usage, measured &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; pages.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;              The columns are:&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  size       (1) total program size&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (same as VmSize &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; /proc/[pid]/status)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  resident   (2) resident &lt;span class=&#34;built_in&#34;&gt;set&lt;/span&gt; size&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (same as VmRSS &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; /proc/[pid]/status)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  shared     (3) number of resident shared pages (i.e., backed by a file)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                             (same as RssFile+RssShmem &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; /proc/[pid]/status)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  text       (4) text (code)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  lib        (5) library (unused since Linux 2.6; always 0)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  data       (6) data + stack&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;                  dt         (7) dirty pages (unused since Linux 2.6; always 0)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /proc/3760/statm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;400865 96456 37653 27355 0 157019 0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Second field means res (resident)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;pmap $(pgrep bash)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;pmap &lt;span class=&#34;variable&#34;&gt;$pid&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;.....&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;ffffffffff600000      4K r-x--   [ anon ]&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt; total           795256K &amp;lt;--- &lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; the size too large, maybe the memory leak&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;There are some of share library &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt; each resident&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;If you want get share library memory consumption.&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;/proc/[pid]/smaps (since Linux 2.6.14)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;              This file shows memory consumption &lt;span class=&#34;keyword&#34;&gt;for&lt;/span&gt; each of the process&lt;span class=&#34;string&#34;&gt;&amp;#x27;s&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;              mappings.  (The pmap(1) command displays similar information,&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;              in a form that may be easier for parsing.)&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;


&lt;h4 id=&#34;Struct-page&#34;&gt;&lt;a href=&#34;#Struct-page&#34; class=&#34;headerlink&#34; title=&#34;Struct page&#34;&gt;&lt;/a&gt;Struct page&lt;/h4&gt;&lt;p&gt;page frame minimum unit. every page frame has a struct page to point&lt;br&gt;&lt;a href=&#34;https://stackoverflow.com/questions/34836806/how-to-get-physical-address-from-struct-page-in-linux-kernel&#34;&gt;struct page could mapping page frame to physical address&lt;/a&gt;&lt;br&gt;all page frame in the LUR list.&lt;br&gt;There are 2.3%(96&amp;#x2F;4096) usage in linux 2.6.32&lt;/p&gt;
&lt;h4 id=&#34;smem&#34;&gt;&lt;a href=&#34;#smem&#34; class=&#34;headerlink&#34; title=&#34;smem&#34;&gt;&lt;/a&gt;smem&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ smem -u&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;User     Count     Swap      USS      PSS      RSS&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;chrony       1        0      584      621     1316&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;rpc          1        0      604      633     1120&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;...&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -m&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -w&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -t&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem -c &lt;span class=&#34;string&#34;&gt;&amp;quot;name user pss&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ smem --bar &lt;span class=&#34;variable&#34;&gt;$pid&lt;/span&gt; -c &lt;span class=&#34;string&#34;&gt;&amp;quot;pss uss&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;Kernel-Memory-Leak-Detector&#34;&gt;&lt;a href=&#34;#Kernel-Memory-Leak-Detector&#34; class=&#34;headerlink&#34; title=&#34;Kernel Memory Leak Detector&#34;&gt;&lt;/a&gt;&lt;a href=&#34;https://www.kernel.org/doc/html/latest/dev-tools/kmemleak.html&#34;&gt;Kernel Memory Leak Detector&lt;/a&gt;&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ modprobe kmemleak-test&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; scan &amp;gt; /sys/kernel/debug/kmemleak&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /sys/kernel/debug/kmemleak&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;## centos not enabled by defualt&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ grep CONFIG_DEBUG_KMEMLEAK /boot/config-4.18.0-305.el8.x86_64&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ CONFIG_DEBUG_KMEMLEAK is not &lt;span class=&#34;built_in&#34;&gt;set&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;zram&#34;&gt;&lt;a href=&#34;#zram&#34; class=&#34;headerlink&#34; title=&#34;zram&#34;&gt;&lt;/a&gt;&lt;a href=&#34;http://www.wowotech.net/memory_management/zram.html&#34;&gt;zram&lt;/a&gt;&lt;/h4&gt;&lt;p&gt;CentOS 7 only work for lzo and the performance was not good enough  &lt;/p&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;20&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;21&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;22&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;23&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;24&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;25&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;26&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;27&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;28&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;29&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;30&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;31&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;32&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;33&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;34&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;35&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;36&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;37&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;38&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;39&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;40&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;41&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;42&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;43&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;44&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;45&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;46&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;47&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;48&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;49&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;50&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;51&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;52&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;53&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;54&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;55&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;56&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;57&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;58&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;59&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;60&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;61&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;62&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;63&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;64&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;65&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;66&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;67&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;68&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;69&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;70&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;71&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;72&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;73&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;74&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;75&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;76&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;77&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; lz4 &amp;gt; /sys/block/zram0/comp_algorithm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;-bash: &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt;: write error: Invalid argument&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;cat&lt;/span&gt; /sys/block/zram0/comp_algorithm&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;[lzo]&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ modprobe zram num_devices=2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 8589934592 &amp;gt; /sys/block/zram0/disksize&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 8589934592 &amp;gt; /sys/block/zram1/disksize&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ swapoff -a&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ mkswap /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ mkswap /dev/zram1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ swapon /dev/zram0 /dev/zram1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;or&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ modprobe zram num_devices=1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; 68719476736 &amp;gt; /sys/block/zram0/disksize&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ mkfs.ext4 /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ tune2fs -m0 /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;tune2fs 1.42.9 (28-Dec-2013)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Setting reserved blocks percentage to 0% (0 blocks)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;## no compression function&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ zpool create -o ashift=0 -o multihost=off -O compression=zstd -O primarycache=none -O secondarycache=none tank /dev/zram0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ sh zram.info&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Physical memory:     134,928,179,200 bytes&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Buffers and cache:       812,810,240 bytes /  0.6% of physical memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Unallocated:             825,503,744 bytes /  0.6% of physical memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Compressed:           19,495,145,472 bytes / 14.4% of physical memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Decompressed size:    33,168,183,296 bytes / 22.3% of decompressed memory&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;Compression ratio: 1.701&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;#!/bin/sh -eu&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;TOTAL_DATA=0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;TOTAL_COMP=0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;HAS_ZRAM=0&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;# Iterate through swap devices searching for compressed ones&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;while&lt;/span&gt; &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; NAME _; &lt;span class=&#34;keyword&#34;&gt;do&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;comment&#34;&gt;# Filter zram swaps and let&amp;#x27;s hope your ordinary swap doesn&amp;#x27;t have&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;comment&#34;&gt;# &amp;quot;zram&amp;quot; in its name :D&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;keyword&#34;&gt;case&lt;/span&gt; &lt;span class=&#34;variable&#34;&gt;$NAME&lt;/span&gt; &lt;span class=&#34;keyword&#34;&gt;in&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        *zram*) ;;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        *) &lt;span class=&#34;built_in&#34;&gt;continue&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;keyword&#34;&gt;esac&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    DIR=/sys`udevadm info --query=path --name=&lt;span class=&#34;variable&#34;&gt;$NAME&lt;/span&gt;`&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    DATA=$(awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;print $1&amp;#125;&amp;#x27;&lt;/span&gt; &lt;span class=&#34;variable&#34;&gt;$DIR&lt;/span&gt;/mm_stat)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    COMP=$(awk &lt;span class=&#34;string&#34;&gt;&amp;#x27;&amp;#123;print $3&amp;#125;&amp;#x27;&lt;/span&gt; &lt;span class=&#34;variable&#34;&gt;$DIR&lt;/span&gt;/mm_stat)&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    TOTAL_DATA=$((TOTAL_DATA + DATA))&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    TOTAL_COMP=$((TOTAL_COMP + COMP))&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    HAS_ZRAM=1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;done&lt;/span&gt; &amp;lt;/proc/swaps&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;comment&#34;&gt;# Extract physical memory in kibibytes and scale back to bytes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ MEM_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ FREE_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ BUF_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;read&lt;/span&gt; _ CACHE_KIB _&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&amp;#125; &amp;lt;/proc/meminfo&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;/usr/bin/printf &lt;span class=&#34;string&#34;&gt;&amp;quot;\&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Physical memory:   %&amp;#x27;17d bytes&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Buffers and cache: %&amp;#x27;17d bytes / %4.1f%% of physical memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Unallocated:       %&amp;#x27;17d bytes / %4.1f%% of physical memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;&amp;quot;&lt;/span&gt; `&lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;scale=6; 1024*&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;; 1024*(&lt;span class=&#34;variable&#34;&gt;$BUF_KIB&lt;/span&gt;+&lt;span class=&#34;variable&#34;&gt;$CACHE_KIB&lt;/span&gt;); 100*(&lt;span class=&#34;variable&#34;&gt;$BUF_KIB&lt;/span&gt;+&lt;span class=&#34;variable&#34;&gt;$CACHE_KIB&lt;/span&gt;)/&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;; 1024*&lt;span class=&#34;variable&#34;&gt;$FREE_KIB&lt;/span&gt;; 100*&lt;span class=&#34;variable&#34;&gt;$FREE_KIB&lt;/span&gt;/&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;&amp;quot;&lt;/span&gt;|bc`&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;if&lt;/span&gt; &lt;span class=&#34;built_in&#34;&gt;test&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;&lt;span class=&#34;variable&#34;&gt;$HAS_ZRAM&lt;/span&gt;&amp;quot;&lt;/span&gt;; &lt;span class=&#34;keyword&#34;&gt;then&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    /usr/bin/printf &lt;span class=&#34;string&#34;&gt;&amp;quot;\&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Compressed:        %&amp;#x27;17d bytes / %4.1f%% of physical memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Decompressed size: %&amp;#x27;17d bytes / %4.1f%% of decompressed memory&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;Compression ratio: %.3f&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;string&#34;&gt;&amp;quot;&lt;/span&gt; `&lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;scale=6; &lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;; 100*&lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;/&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;/1024; &lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;; 100*&lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;/(1024*&lt;span class=&#34;variable&#34;&gt;$MEM_KIB&lt;/span&gt;-&lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;+&lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;); &lt;span class=&#34;variable&#34;&gt;$TOTAL_DATA&lt;/span&gt;/&lt;span class=&#34;variable&#34;&gt;$TOTAL_COMP&lt;/span&gt;&amp;quot;&lt;/span&gt;|bc`&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;else&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;    &lt;span class=&#34;built_in&#34;&gt;echo&lt;/span&gt; &lt;span class=&#34;string&#34;&gt;&amp;quot;Compressed:                        0 bytes / zram not in use&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;&lt;span class=&#34;keyword&#34;&gt;fi&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;h4 id=&#34;Make-sure-the-ECC-work-in-Linux&#34;&gt;&lt;a href=&#34;#Make-sure-the-ECC-work-in-Linux&#34; class=&#34;headerlink&#34; title=&#34;Make sure the ECC work in Linux&#34;&gt;&lt;/a&gt;Make sure the ECC work in Linux&lt;/h4&gt;&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;4&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ dmidecode -t memory  | grep -Ei &lt;span class=&#34;string&#34;&gt;&amp;#x27;error correction type&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        Error Correction Type: Single-bit ECC&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;$ dmidecode -t memory  | grep -Ei &lt;span class=&#34;string&#34;&gt;&amp;#x27;error correction type&amp;#x27;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&#34;line&#34;&gt;        Error Correction Type: Multi-bit ECC&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;"><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/11/14/mem/&t=memory"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2025
    John Doe
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = '67e8c052/67e8c052.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'blog-comments';
      var utterances_theme = 'github-dark';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
