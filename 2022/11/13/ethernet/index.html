<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Ethernet cable Active The Active cable makes use of electronics for signal conditioning   Passive The Passive cable does not make use of electronics for signal conditioning Passive twinax cables are r">
<meta property="og:type" content="article">
<meta property="og:title" content="ethernet">
<meta property="og:url" content="http://example.com/2022/11/13/ethernet/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Ethernet cable Active The Active cable makes use of electronics for signal conditioning   Passive The Passive cable does not make use of electronics for signal conditioning Passive twinax cables are r">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://example.com/img/last-ack-reuse.svg">
<meta property="og:image" content="http://example.com/img/ring-buffer.png">
<meta property="og:image" content="http://example.com/img/2012110119582618-tcp-reception.png">
<meta property="og:image" content="http://example.com/img/2012110119593557-tcp-transmission.png">
<meta property="og:image" content="http://example.com/img/tshark-1.png">
<meta property="og:image" content="http://example.com/img/tshark-2.png">
<meta property="article:published_time" content="2022-11-13T08:51:06.000Z">
<meta property="article:modified_time" content="2024-05-20T03:56:04.594Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="network">
<meta property="article:tag" content="ethernet">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://example.com/img/last-ack-reuse.svg">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>ethernet</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2022/11/13/cpu/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2022/11/13/rdma/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/11/13/ethernet/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/11/13/ethernet/&text=ethernet"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/11/13/ethernet/&is_video=false&description=ethernet"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=ethernet&body=Check out this article: http://example.com/2022/11/13/ethernet/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/11/13/ethernet/&name=ethernet&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/11/13/ethernet/&t=ethernet"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ethernet-cable"><span class="toc-number">1.</span> <span class="toc-text">Ethernet cable</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#interface-Forward-Error-Correction"><span class="toc-number">1.0.1.</span> <span class="toc-text">interface Forward Error Correction</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TCP"><span class="toc-number">2.</span> <span class="toc-text">TCP</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#RTT-three-types"><span class="toc-number">2.1.</span> <span class="toc-text">RTT three types</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Retransmission-Timeout"><span class="toc-number">2.2.</span> <span class="toc-text">Retransmission Timeout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Delayed-ACK-Timer"><span class="toc-number">2.3.</span> <span class="toc-text">Delayed ACK Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Persist-Timer"><span class="toc-number">2.4.</span> <span class="toc-text">Persist Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Keepalive-Timer"><span class="toc-number">2.5.</span> <span class="toc-text">Keepalive Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FIN-WAIT-2-Timer"><span class="toc-number">2.6.</span> <span class="toc-text">FIN_WAIT_2 Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TIME-WAIT-Timer"><span class="toc-number">2.7.</span> <span class="toc-text">TIME_WAIT Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MSL-Maximum-Segment-Lifetime"><span class="toc-number">2.8.</span> <span class="toc-text">MSL (Maximum Segment Lifetime)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sysctl-conf"><span class="toc-number">2.9.</span> <span class="toc-text">sysctl.conf</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SLE-and-SER-in-SACK-RFC2018"><span class="toc-number">3.</span> <span class="toc-text">SLE and SER in SACK(RFC2018)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TOOLS-Tools-tools"><span class="toc-number">4.</span> <span class="toc-text">TOOLS Tools tools</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tcp-retrans"><span class="toc-number">5.</span> <span class="toc-text">tcp retrans</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tcpdump-802-3-header"><span class="toc-number">5.1.</span> <span class="toc-text">tcpdump 802.3 header</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tcp-netif-rx"><span class="toc-number">6.</span> <span class="toc-text">tcp netif_rx</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ethtool-cmd"><span class="toc-number">7.</span> <span class="toc-text">ethtool cmd</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#interrupt-moderation"><span class="toc-number">7.1.</span> <span class="toc-text">interrupt moderation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#show-NIC-model"><span class="toc-number">8.</span> <span class="toc-text">show NIC model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Linux-network-performance-PERFORMANCE"><span class="toc-number">8.1.</span> <span class="toc-text">Linux network performance PERFORMANCE</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Socket-receive-queues"><span class="toc-number">8.1.1.</span> <span class="toc-text">Socket receive queues</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#proc-net-softnet-stat"><span class="toc-number">8.1.2.</span> <span class="toc-text">&#x2F;proc&#x2F;net&#x2F;softnet_stat</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hardware-features"><span class="toc-number">8.2.</span> <span class="toc-text">Hardware features</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#busy-poll-Interrupt-Queues"><span class="toc-number">8.2.1.</span> <span class="toc-text">busy poll(Interrupt Queues)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ARFS-support"><span class="toc-number">8.2.2.</span> <span class="toc-text">ARFS support</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RSS-support"><span class="toc-number">8.2.3.</span> <span class="toc-text">RSS support</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#THEORETICAL-MAX-RATE"><span class="toc-number">8.2.4.</span> <span class="toc-text">THEORETICAL MAX RATE</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#CPU-setting"><span class="toc-number">8.2.5.</span> <span class="toc-text">CPU setting</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#dump-NIC-log"><span class="toc-number">8.2.6.</span> <span class="toc-text">dump NIC log</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#flow-control"><span class="toc-number">8.2.7.</span> <span class="toc-text">flow control</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#DRIVER-driver"><span class="toc-number">8.2.8.</span> <span class="toc-text">DRIVER driver</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hash-policy"><span class="toc-number">8.2.9.</span> <span class="toc-text">hash policy</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SYN-timeout-on-linux"><span class="toc-number">9.</span> <span class="toc-text">SYN timeout on linux</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ip-could-set-initrwnd"><span class="toc-number"></span> <span class="toc-text">ip could set initrwnd</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#net-ipv4-tcp-slow-start-after-idle-0-not-be-1-https-www-kawabangga-com-posts-5217"><span class="toc-number"></span> <span class="toc-text">net.ipv4.tcp_slow_start_after_idle &#x3D; 0 , not be 1  https:&#x2F;&#x2F;www.kawabangga.com&#x2F;posts&#x2F;5217</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E6%95%88%E6%9E%9C%E6%98%AF%EF%BC%8C%E5%8E%9F%E6%9D%A5%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81-30-%E5%A4%9A%E5%88%86%E9%92%9F%E4%B8%8B%E8%BD%BD%E5%AE%8C%E6%88%90%EF%BC%8C%E6%8D%A2%E6%88%90-BBR-%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81-2-%E5%88%86%E9%92%9F%E5%B0%B1%E4%B8%8B%E8%BD%BD%E5%AE%8C%E4%BA%86%E3%80%82"><span class="toc-number"></span> <span class="toc-text">实际效果是，原来数据需要 30 多分钟下载完成，换成 BBR 之后，只需要 2 分钟就下载完了。</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#wireshark"><span class="toc-number">1.</span> <span class="toc-text">wireshark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#detect-the-tcp-multipath-out-of-order"><span class="toc-number">2.</span> <span class="toc-text">detect the tcp multipath(out of order)</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        ethernet
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">John Doe</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2022-11-13T08:51:06.000Z" itemprop="datePublished">2022-11-13</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Network/">Network</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/ethernet/" rel="tag">ethernet</a>, <a class="tag-link-link" href="/tags/network/" rel="tag">network</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h3 id="Ethernet-cable"><a href="#Ethernet-cable" class="headerlink" title="Ethernet cable"></a><a target="_blank" rel="noopener" href="https://cshihong.github.io/2020/08/23/%E5%85%89%E7%BA%A4%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93/">Ethernet cable</a></h3><ul>
<li>Active<ul>
<li>The Active cable makes use of electronics for signal conditioning</li>
</ul>
</li>
<li>Passive<ul>
<li>The Passive cable does not make use of electronics for signal conditioning<ul>
<li>Passive twinax cables are rated for ranges up to 5m and provide a good working solutions at a great cost</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://www.arista.com/assets/data/pdf/Arista100G_TC_QA.pdf">Arista 100G cable</a><br><a target="_blank" rel="noopener" href="https://www.arista.com/assets/data/pdf/Arista100G_TC_QA.pdf">Mellanox 100G DAC cable</a><br><a target="_blank" rel="noopener" href="https://www.mellanox.com/sites/default/files/products/interconnect/pdf/Mellanox_Cable_Management_Guidelines_and_FAQs_Application_Note.pdf">Mellanox cable </a></p>
<ul>
<li><p>PCC (passive copper cable)</p>
<ul>
<li>A high-speed electrical cable with an SFP or QSFP connector in each end, but no active components in the RF connections. The term ‘passive’ means that there is no active processing of the electrical signal.</li>
</ul>
</li>
<li><p>Direct Attach Copper cable (DAC)</p>
<ul>
<li>direct electrical connection between cable ends</li>
<li>The DACs still have an EEPROM, a memory chip in each end, so the host system can read which type of cable is plugged in, and how much attenuation it should expect.</li>
</ul>
</li>
<li><p>Active Optical cable (AOC)</p>
<ul>
<li>Electrical conversion to optics transmitted over integrated optical fibers</li>
<li>An optical fiber cable iwth an optical transceiver with the fibers bonded inside and not removeable</li>
</ul>
</li>
<li><p>Optical Transceivers</p>
<ul>
<li>Electrical conversion to optics over fibers with optical connectors<ul>
<li>More expensive SR(short range),SR4(Short range 4 channels)</li>
<li>4 parallecl multi-mode fibers<ul>
<li>100m after which the signal degrades</li>
</ul>
</li>
<li>4 parallecl single-mode fibers<ul>
<li>Parallel single-mode transceivers(PSM4) are used from 500m-2km the ost of 8 fibers adds up with each meter</li>
<li>Multiplexing the four channels signals into two single fibers is more economical with CWDM4(Coarse Wavelength division multiplexer 4 channels) for up 2KM and LR4(long range 4 channels) up to 10km</li>
</ul>
</li>
<li>4 Channels multiplexed over 2 single-mode fibers</li>
</ul>
</li>
</ul>
</li>
<li><p>Connector Types</p>
<ul>
<li>SFP (Small Formfactor Pluggable)<ul>
<li>A transceiver or cable with a single lane(channel) in each direction</li>
</ul>
</li>
<li>SFP+ denotes the 10~14Gb&#x2F;s type of AOC&#x2F;transceivers</li>
<li>QSFP(Quad small Formfactor Pluggable)<ul>
<li>A bidirectional transceiver or cable with 4 lanes in each direction</li>
</ul>
</li>
<li>QSFP+ denotes cables&#x2F;transceivers for 4x (10<del>14)Gbps applications, while QSFP28 denotes the 4 x(24</del>28)&#x3D;100Gbps product range with QSFP form factor</li>
<li>QSFP-DD(double-density) support up to 400Gbps in aggregate over an 8x 50Gbps electrical interface.</li>
<li>Transceiver (transmitter and receiver) is a converter with an electrical connector in one end and optical connector in the other end</li>
</ul>
</li>
<li><p>Optical Transmission and Fiber Types</p>
<ul>
<li>MMF(multi-mode fiber)<ul>
<li>The type of fiber used for VCSEL (Vertical Cavity Surface<br>Emitting Laser) based transmission, normally operating at 850 nm wavelength. Its maximum reach is 100 m for 25 Gb&#x2F;s line rates.</li>
</ul>
</li>
<li>OM2&#x2F;3&#x2F;4(optical multi-mode) are classiffications of MMF for different reach.<ul>
<li>higher number indicates lower degradation of the optical signal, and longer reach. MMF cables usually have the following color scheme</li>
<li>OM2 orange used for data rates at 1~14 Gbps, 62.5 μm fiber core diameter</li>
<li>OM3 aqua or teal 70m reach for 25&#x2F;100Gbps transceivers, 50 μm fiber core diameter</li>
<li>OM4 aqua or purple 100m reach for 25&#x2F;100Gbps transceivers, 50-um core diameter</li>
<li>OM5 aqua green not commonly used</li>
</ul>
</li>
</ul>
</li>
<li><p>Optical connector type</p>
<ul>
<li>MPO (Multi-fiber plush on)<ul>
<li>is a connector standard supporting multiple rows with up to 12<br>fibers in each. A QSFP transceiver with MPO receptacle uses the outermost 4 positions on each side</li>
</ul>
</li>
<li>MTP<ul>
<li>are a vendor specific proprietary high-precision version of MPO connectors</li>
</ul>
</li>
<li>LC<ul>
<li>are used for both single-mode and multi-mode fibers and used in both SFP and QSFP MSA transceivers</li>
</ul>
</li>
</ul>
</li>
<li><p>diff 100GbE IB and Ethernet cable</p>
<ul>
<li>CDR (clock and data retimeing) default status</li>
<li>IB EDR CDR is bypsswd&#x2F;disabled this ensures backward compatibility to FDR devices</li>
<li>EThernet 100G: The CDR is default on, and must be disabled at lower data rates, eg: 40Gbps</li>
<li>Copper cables<ul>
<li>IB EDR: The link budget usually assumes that the signal attenuation is small enough to allow operation without Forward Error Correction (FEC).</li>
<li>Ethernet 100G: RS-FEC enabled by default</li>
</ul>
</li>
<li>Identifier<ul>
<li>IB EDR and QSFP+ backward compatibility to FDR devices</li>
<li>Ethernet 100G: QSFP28<ul>
<li>The EEPROM memory map of QSFP28 (100 Gb&#x2F;s cables&#x2F;transceivers) is defined in specification SFF-8636 and for SFP28 (25 Gb&#x2F;s cables&#x2F;transceivers) in SFF-8472</li>
</ul>
</li>
</ul>
</li>
<li>Bit error ratios<ul>
<li>IEEE defines the BER of 1E-5 for Ethernet that is corrected with FEC in the host to 1E-12</li>
<li>InfiniBand assumes no use of FEC in the host and requires 1E-15 BER.</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The Ethernet standards define FEC (Forward Error Correction) on by default for AOCs and optical transceivers. This implies that the Bit Error Rate (BER &#x3D; bit errors&#x2F;bits transmitted) maximum is 1E-12 after FEC (pre-FEC of maximum 1E-5). All LinkX products are tested in Mellanox end-to-end systems for pre-FEC BER of 1E-15 as part of our product qualification, more specifically the System Level Performance Quality Assurance (SLPQA) test.</p>
<table>
<thead>
<tr>
<th>Type</th>
<th>meters</th>
<th>Standard</th>
</tr>
</thead>
<tbody><tr>
<td>QSFP 100GbE Transceivers (OM3 70m&#x2F;4 100m)</td>
<td>70~100</td>
<td>100(100GBASE-SR10),70(100GBASE-SR4, SR&#x3D;short reach)</td>
</tr>
<tr>
<td></td>
<td>10k</td>
<td>100GBASE-LR4, LR&#x3D;long reach</td>
</tr>
<tr>
<td></td>
<td>80k</td>
<td>100GBASE-ZR, it is not an IEEE standard</td>
</tr>
<tr>
<td>QSFP 100GbE Transceivers</td>
<td>500,2k,10k,40k</td>
<td>10k(100GBASE-LR4),40k(100GBASE-ER4, ER&#x3D;Extended reach)</td>
</tr>
<tr>
<td>100GbE QSFP to QSFP Active Optical cable</td>
<td>3~30</td>
<td>Based on IEEE 100GBASE-SR4</td>
</tr>
<tr>
<td>QSFP+ to QSFP+ passive Twinax Copper Cables</td>
<td>0.5~5</td>
<td>100GBASE-ER4(The 802.3bj covers the electrical)</td>
</tr>
<tr>
<td>100GbE QSFP28 Direct Attach Copper Cable</td>
<td>3~7</td>
<td>5m(IEEE 802.3bj CA-25G-L),2m(IEEE 802.3bj CA-25G-N, 100BASE-CR4)</td>
</tr>
</tbody></table>
<p>AOC not support DDM(Digital Diagnostic Monitoring)&#x2F;DOM(Digital optical monitoring)<br>RGD means rugged transceivers, and can operate under more extreme conditions<br>Physical Media Devices (PMDs)</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">                            nTYPE-LLLm</span><br><span class="line">                            | |   ||||</span><br><span class="line">10 10Mb/s-------------------- |   ||||</span><br><span class="line">2.5G 2.5Gb/s                  |   |||-----------Trailing number</span><br><span class="line">10G  10Gb/s                   |   |||            1,4,8,10,16 pairs or lanes</span><br><span class="line">24/40/100/400 Gbps            |   ||------------Third letter</span><br><span class="line">                              |   ||            M multimode</span><br><span class="line">                              |   ||</span><br><span class="line">                              |   ||</span><br><span class="line">Modulation TYPE----------------   ||</span><br><span class="line">BASE    Baseband                  ||</span><br><span class="line">BROAD   Broadband                 |-----------------Second letter</span><br><span class="line">PASS    Passband                  |                 P PAM4</span><br><span class="line">                                  First letter      R Scrambled</span><br><span class="line">                                  B Bidirectional   W WAN coding</span><br><span class="line">                                  C Twin-ax copper  X External coding</span><br><span class="line">                                  E Extra long or 40km</span><br><span class="line">                                  F fiber or 2km</span><br><span class="line">                                  K Backplane</span><br><span class="line">                                  L long or 10km</span><br><span class="line">                                  P PON</span><br><span class="line">                                  S Short or 100</span><br><span class="line">                                  T twisted pair</span><br></pre></td></tr></table></figure>

<h5 id="interface-Forward-Error-Correction"><a href="#interface-Forward-Error-Correction" class="headerlink" title="interface Forward Error Correction"></a><a href="(https://solarflare.hammer-europe.com/assets/uploads/resources/QLogic%20-%20White%20Paper%20-%2025Gb%20Ethernet.pdf">interface Forward Error Correction</a></h5><table>
<thead>
<tr>
<th align="center">Phy layer</th>
<th align="center">Name</th>
<th align="center">error Correction</th>
</tr>
</thead>
<tbody><tr>
<td align="center">MMF Optics</td>
<td align="center">25GBASE-SR</td>
<td align="center">RS-FEC</td>
</tr>
<tr>
<td align="center">Direct Attach Copper</td>
<td align="center">25GBASE-CR</td>
<td align="center">BASE-R FEC or RS-FEC</td>
</tr>
<tr>
<td align="center">Direct Attach Copper</td>
<td align="center">25GBASE-CR-S</td>
<td align="center">BASE-R FEC or disabled</td>
</tr>
<tr>
<td align="center">Electrical Backplane</td>
<td align="center">25GBASE-KR</td>
<td align="center">BASE-R FEC or RS-FEC</td>
</tr>
<tr>
<td align="center">Electrical Backplane</td>
<td align="center">25GBASE-KR-S</td>
<td align="center">BASE-R FEC or disabled</td>
</tr>
<tr>
<td align="center">Twisted Pair</td>
<td align="center">25GBASE-T</td>
<td align="center">N&#x2F;A</td>
</tr>
</tbody></table>
<p>The IEEE standard specifies two backplane and copper interfaces. These have different goals, hence the different interface. The -S short reach interfaces aim to support high-quality cables without Forward Error Correction (FEC) to minimize latency. Full reach interfaces aimto support the lowest possible cable or backplane cost and the longest possible reach, which do require the use of FEC. FEC options include BASE-R FEC (also referred to as Fire Code) and RS-FEC (also referred to as Reed-Solomon). RS-FEC has been used for a range of applications including data storage satellite transmissions. BASE-R FEC is a newer technology that is particularly well suited for correction of the burst errors typical in a backplane channel resulting from error propagation in the receive equalizer.   </p>
<p>IEEE 标准指定了两个底板和铜接口。这些接口的目标不同，因此接口存在不同。-S 短距离接口旨在支持无需转发纠错（FEC）的高质量电缆，以最大限度地降低延迟。全距离接口旨在实现尽可能低的电缆或背板成本&gt;以及尽可能长的距离，这需要使用FEC。FEC 选项包括 BASE-R FEC（也称为 Fire Code）和 RS-FEC（也称为 Reed-Solomon）。RS-FEC 已用于一系列应用，包括数据存储卫星传播。BASE-R FEC 是一种新型技术，特别<br>适合纠正背板通道中由于接收均衡器的错误传播而导致的突发错误。   </p>
<h3 id="TCP"><a href="#TCP" class="headerlink" title="TCP"></a>TCP</h3><ul>
<li>SYN, FIN, ACK, PSH, RST, URG</li>
<li>SYN :  建立连接、发起包 </li>
<li>FIN :  关闭连接、结束包 </li>
<li>PSH :  DATA数据传输 </li>
<li>ACK :  ACK回应</li>
<li>RST : RESET连接重置</li>
<li>ACK是可能与SYN，FIN等同时使用的，比如SYN和ACK可能同时为1，它表示的就是建立连接之后的响应，如果只是单个的一个SYN，它表 示的只是建立连接</li>
<li>当出现FIN包或RST包时，我们便认为客户端与服务器端断开了连接</li>
<li>当出现SYN和SYN＋ACK包时，我们认为客户端与服务器建立了一个连接</li>
<li>抓保经常出现[Len] ：数据包长度 [Seq] ：数据包序列号</li>
<li>TCP而言，并不一定每个包都会ACK。TCP的ACK是一种累积的ACK，也就是表示在我这个ACK之前的所有其他ACK都已经确认收到了<ul>
<li>比如，97号包的ACK&#x3D;65701，96号包的Seq+Len&#x3D;64273+1428&#x3D;65701，那么就是表示97号的ACK是对96号的回应，也就是96号之前的其他没有被显示ACK的包，其实都已经通过97号包ACK了，这样发送方也就知道了在96号之前发出去的所有包对方都已经收到并ACK了。</li>
</ul>
</li>
<li>MSL,TTL,RTT</li>
<li>MSL Maximum Segment Lifetime<ul>
<li>报文最大生存时间, RFC 793中规定MSL为2分钟，实际应用中常用的是30秒，1分钟和2分钟等, 刊设定</li>
<li>2MSL即两倍的MSL，TCP的TIME_WAIT状态也称为2MSL等待状态</li>
</ul>
</li>
<li>TTL Time to live<ul>
<li>是ip头的一个域，生存时间是由源主机来设置一个初始值，但TTL不是存的具体时间，而是表示可以经过的最大路由数</li>
<li>根据RFC 1812，一个网络包的TTL每减去1就表示它经过一次路由。一般TTL的初始值为64，如果某个ACK包的TTL是62，则意味着是是距离此设备两跳的设备发出来的<ul>
<li>TTL在wireshark抓包中的形态如Time to live: 62</li>
</ul>
</li>
<li>TTL&#x3D;0则数据报将被丢弃，同时发送ICMP报文通知源主机</li>
<li>一般在缓存、连接心跳中也用到TTL这个，他们和TCP协议中的TTL是有区别的，缓存、连接心跳中的TTL表示的就是数据缓存or剩余的时间。</li>
</ul>
</li>
<li>RTT round-trip time<ul>
<li>表示客户到服务器往返所花时间，TCP含有动态估算RTT的算法</li>
<li>TCP会持续估算一个给定连接的RTT，因为RTT受网络传输拥塞程序的变化而变化</li>
</ul>
</li>
<li>MSS max segment size(data part)<ul>
<li>1500 MTU (Maximum Transmission Unit)<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">1500 MTU</span><br><span class="line">https://www.dasblinkenlichten.com/the-basics-mtu-mss-gre-and-pmtu/</span><br><span class="line"></span><br><span class="line">                                                  UDP header 8 Bytes</span><br><span class="line">Ethernet header 14 Bytes----IP header 20 Bytes----TCP header 20 Bytes----Payload 1460 Bytes----FCS 4 Bytes</span><br><span class="line">                            |----------------------ethernet 1500 mtu-----------------------|</span><br><span class="line">                            |------------------------ip mtu--------------------------------|</span><br><span class="line">                                                                         |-----TCP MSS-----|</span><br><span class="line"></span><br><span class="line">#This is not a problem at all. Fragmentation is performed on the sender following the link´s MTU. UDP rely on Fragment Offset, and More Fragments (MF)  present on IP header to organized the stream. </span><br><span class="line"></span><br><span class="line">#ICMP</span><br><span class="line">|--------------------------IP Dataram---------------------------|</span><br><span class="line">IP Header 20 Bytes----ICMP header 8 Bytes----ICMP data 1472 Bytes</span><br><span class="line"></span><br><span class="line">#The other feature cause the data be reduced</span><br><span class="line">#https://support.citrix.com/article/CTX115434/how-to-find-maximum-size-of-ip-data-payload-that-can-traverse-wan-environment-without-fragmentation</span><br><span class="line">Calculate the difference between the standard and reduced data size</span><br><span class="line">1472 bytes – 1172 bytes = 300 bytes</span><br><span class="line">If necessary, adjust the local CloudBridge MSS to fit into the data payload size, reducing the same offset found with the ping command. For example</span><br><span class="line">MSS = 1080 bytes (1380 – 300)</span><br><span class="line"></span><br><span class="line">#Sometimes the PDU at the network layer is called the IP datagram instead of packet. But these definitions could not be used in an equal way. Because a packet can either be a whole IP datagram or a fragment of an IP datagram, if fragmentation has occurred</span><br><span class="line"></span><br><span class="line">|--------------------------IP Dataram--------------------------|</span><br><span class="line">IP Header 20 Bytes----UDP Header 20 Bytes----UDP data 1490 Bytes = 1530 Bytes ?</span><br><span class="line">                                                     |</span><br><span class="line">                                                     |</span><br><span class="line">                                 1472 UDP data+20 Bytes IP header+UDP data 18 Bytes=1490 Bytes</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="http://blog.qiusuo.im/blog/2014/03/19/tcp-timeout/">TCP timer</a><br><a href="/img/Tcp_state_diagram.png"></a>   </p>
<h4 id="RTT-three-types"><a href="#RTT-three-types" class="headerlink" title="RTT three types"></a><a target="_blank" rel="noopener" href="https://www.geeksforgeeks.org/tcp-timers/">RTT three types</a></h4><p>Measured RTT(RTTm) – The measured round-trip time for a segment is the time required for the segment to reach the destination and be acknowledged, although the acknowledgment may include other segments.  </p>
<p>Smoothed RTT(RTTs) – It is the weighted average of RTTm. RTTm is likely to change and its fluctuation is so high that a single measurement cannot be used to calculate RTO.  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Initially -&gt; No value</span><br><span class="line">After the first measurement -&gt; RTTs=RTTm</span><br><span class="line">After each measurement -&gt; RTTs= (1-t)*RTTs + t*RTTm</span><br><span class="line">Note: t=1/8 (default <span class="keyword">if</span> not given)</span><br></pre></td></tr></table></figure>

<p>Deviated RTT(RTTd) – Most implementation do not use RTTs alone so RTT deviated is also calculated to find out RTO.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Initially -&gt; No value</span><br><span class="line">After the first measurement -&gt; RTTd=RTTm/2</span><br><span class="line">After each measurement -&gt; RTTd= (1-k)*RTTd + k*(RTTm-RTTs)</span><br><span class="line">Note: k=1/4 (default if not given)</span><br></pre></td></tr></table></figure>

<h4 id="Retransmission-Timeout"><a href="#Retransmission-Timeout" class="headerlink" title="Retransmission Timeout"></a>Retransmission Timeout</h4><p>RTO calculation – The value of RTO is based on the smoothed round-trip time and its deviation. Most implementations use the following formula to calculate the RTO:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Initial value -&gt; Original (given in question)</span><br><span class="line">After any measurement -&gt; RTO=RTTs + 4*RTTd</span><br><span class="line">#NOTE: At every retransmission the value of RTO doubles. ( RTO(new) = RTO(before retransmission) *2 )</span><br><span class="line"></span><br><span class="line">## in linux</span><br><span class="line">tcp_retries1 (integer; default: 3; since Linux 2.2)</span><br><span class="line"></span><br><span class="line">The number of times TCP will attempt to retransmit a packet on an established connection normally, without the extra effort of getting the network layers involved. Once we exceed this number of retransmits, we first have the network layer update the route if possible before each new retransmit. The default is the RFC specified minimum of 3.</span><br><span class="line"></span><br><span class="line">tcp_retries2 (integer; default: 15; since Linux 2.2)</span><br><span class="line"></span><br><span class="line">The maximum number of times a TCP packet is retransmitted in established state before giving up. The default value is 15, which corresponds to a duration of approxi‐mately between 13 to 30 minutes, depending on the retransmission timeout. The RFC 1122 specified minimum limit of 100 seconds is typically deemed too short.</span><br></pre></td></tr></table></figure>

<h4 id="Delayed-ACK-Timer"><a href="#Delayed-ACK-Timer" class="headerlink" title="Delayed ACK Timer"></a>Delayed ACK Timer</h4><p>If reach the timeout, reply the ACK, if in the timeout range, it will wait data to transmit, 200 ms in Microsoft Windows by default</p>
<p><a target="_blank" rel="noopener" href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/reducing_the_tcp_delayed_ack_timeout">In real time system</a></p>
<ul>
<li>This mode is used at the start of a TCP connection so that the congestion window can grow quickly.</li>
<li>The acknowledgment (ACK) timeout interval (ATO) is set to tcp_ato_min, the minimum timeout value.</li>
<li>To change the default TCP ACK timeout value, write the required value in milliseconds to the &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_ato_min file<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ echo 4 &gt; /proc/sys/net/ipv4/tcp_ato_min</span><br></pre></td></tr></table></figure></li>
</ul>
<p>Delayed ACK</p>
<ul>
<li><p>After the connection is established, TCP assumes this mode, in which ACKs for multiple received packets can be sent in a single packet.</p>
</li>
<li><p>ATO is set to tcp_delack_min to restart or reset the timer.</p>
</li>
<li><p>To change the default TCP Delayed ACK value, write the required value in milliseconds to the &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;tcp_delack_min file:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 4 &gt; /proc/sys/net/ipv4/tcp_delack_min</span><br></pre></td></tr></table></figure>
<p>TCP switches between the two modes depending on the current congestion.<br>Some applications that send small network packets could experience latencies due to the TCP quick and delayed acknowledgment timeouts, which previously were 40 ms by default. That means small packets from an application that seldom sends information through the network could experience a delay up to 40 ms to receive the acknowledgment that a packet has been received by the other side. To minimize this issue, both tcp_ato_min and tcp_delack_min timeouts are now 4 ms by default.</p>
</li>
<li><p>如果tcp对每个数据包都发送一个ack确认，那么只是一个单独的数据包为了发送一个ack代价比较高，所以tcp会延迟一段时间，如果这段时间内有数据发送到对端，则捎带发送ack，如果在延迟ack定时器触发时候，发现ack尚未发送，则立即单独发送  </p>
<ul>
<li>避免糊涂窗口综合症 Sliding Window<ul>
<li>TCP 通过滑动窗口来完成流量控制，当接收方发现自己跟不上发送的速度了，就缩小接收窗口大小，抑制发送方的发送速度，防止发送方发送太快</li>
<li>接收方的接收能力越来越差怎么办？甚至一次性能接收的数据量就几个字节</li>
<li>那只要窗口大小 &lt; 某个值（内核缓冲区大小的一半，也称为最大段长度 MSS）的时候，就直接将窗口大小设置为 0，防止发送方发送小数据</li>
<li>然后等到窗口大小 &gt;&#x3D; 内核缓冲区大小的一半 的时候，才打开窗口，通告发送方，告知其可以发送数据</li>
<li>另一个就是delay ack，立刻返回 ACK 应答，这时候返回的窗口可能比较小，更多时间来处理，希望能返回更大的窗口</li>
<li>对于后面的数据，协议栈会进行累计并等待，或者收到一个接收端发出一个ACK，或者累计到一个最大报文段，然后再发送数据</li>
</ul>
</li>
<li>发送数据的时候将ack捎带发送，不必单独发送ack</li>
<li>果延迟时间内有多个数据段到达，那么允许协议栈发送一个ack确认多个报文段</li>
</ul>
</li>
<li><p>Nagle</p>
<ul>
<li>是为了减少广域网的小分组数目，从而减小网络拥塞的出现</li>
<li>该算法要求一个tcp连接上最多只能有一个未被确认的未完成的小分组，在该分组ack到达之前不能发送其他的小分组，tcp需要收集这些少量的分组，并在ack到来时以一个分组的方式发送出去；其中小分组的定义是小于MSS的任何分组</li>
<li>该算法的优越之处在于它是自适应的，确认到达的越快，数据也就发哦送的越快；而在希望减少微小分组数目的低速广域网上，则会发送更少的分组</li>
</ul>
</li>
</ul>
<h4 id="Persist-Timer"><a href="#Persist-Timer" class="headerlink" title="Persist Timer"></a>Persist Timer</h4><p>When receive the zero window, stop to transmit data.<br>The RFC1122 suggest the window probe timer is RTO</p>
<h4 id="Keepalive-Timer"><a href="#Keepalive-Timer" class="headerlink" title="Keepalive Timer"></a>Keepalive Timer</h4><p>In the tcp_keepalive_intvl, the server will send the Probe Segment to make sure the other side is online ? if no respond, it will resend until timeout, if timeout that means the other side has crash.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tcp_keepalive_intvl (<span class="built_in">integer</span>; default: 75; since Linux 2.4)</span><br><span class="line">The number of seconds between TCP keep-alive probes.</span><br><span class="line"></span><br><span class="line">tcp_keepalive_probes (<span class="built_in">integer</span>; default: 9; since Linux 2.2)</span><br><span class="line">The maximum number of TCP keep-alive probes to send before giving up and killing the connection <span class="keyword">if</span> no response is obtained from the other end.</span><br><span class="line"></span><br><span class="line">tcp_keepalive_time (<span class="built_in">integer</span>; default: 7200; since Linux 2.2)</span><br><span class="line">The number of seconds a connection needs to be idle before TCP begins sending out keep-alive probes. Keep-alives are only sent when the SO_KEEPALIVE socket option is enabled. The default value is 7200 seconds (2 hours). An idle connection is terminated after approximately an additional 11 minutes (9 probes an interval of 75 sec‐onds apart) when keep-alive is enabled.</span><br></pre></td></tr></table></figure>

<h4 id="FIN-WAIT-2-Timer"><a href="#FIN-WAIT-2-Timer" class="headerlink" title="FIN_WAIT_2 Timer"></a>FIN_WAIT_2 Timer</h4><p>The activer try to disable the tcp connection, send the FIN and get the ACK, the activer to convert the FIN_WAIT_1 to FIN_WAIT_2 and it could not send any data, just wait the FIN from the client. if the client down or don’t send the FIN to the activer, same with Dos attack.<br>If timeout, give up the tcp connection.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tcp_fin_timeout (<span class="built_in">integer</span>; default: 60; since Linux 2.2)</span><br><span class="line">This specifies how many seconds to <span class="built_in">wait</span> <span class="keyword">for</span> a final FIN packet before the socket is forcibly closed. This is strictly a violation of the TCP specification, but required to prevent denial-of-service attacks. In Linux 2.2, the default value was 180.</span><br></pre></td></tr></table></figure>

<h4 id="TIME-WAIT-Timer"><a href="#TIME-WAIT-Timer" class="headerlink" title="TIME_WAIT Timer"></a>TIME_WAIT Timer</h4><p>timeout &#x3D; 2xMSL<br>Linux was net.ipv4.tcp_fin_timeout &#x3D; 60</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> TCP_TIMEWAIT_LEN (60*HZ) <span class="comment">/* how long to wait to destroy TIME-WAIT</span></span></span><br><span class="line"><span class="comment"><span class="meta">                              * state, about 60 seconds     */</span></span></span><br></pre></td></tr></table></figure>

<p>the server(FIN_WAIT_2) receive the FIN from the client, after the server reply the ACK,the server will convert FIN_WAIT_2 to TIME_WAIT and start the TIME_WAIT timer<br>and the ack loss, when the client will retrans the FIN to the server.</p>
<ol>
<li><p>if the server not in TIME_WAIT, if the port re-use for another application, maybe the application will go to the wrong state. if the connection in the TIME_WAIT state, it ‘s OK.<br>So the TIME_WAIT must be a long time enough, to avoid the wandering duplicates</p>
</li>
<li><p>because the ack package loss, if the activer has disable the A tcp connection, the client is waitting the ack. if the activer want create a new tcp connection to use the same port, the activer will send the SYN packet, the client is waitting the ack, the client will reply the RST cause the new connection interrupt</p>
</li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">When the persist timer expires, 1 byte of data is sent(segment 6). The receiving application has <span class="built_in">read</span> 256 bytes from the receivebuffer (at time 3.99), so the byte is accepted and acknowledged (segment 7).But the advertised window is still 0, since the receiver does not have room foreither one full-sized segment or one-half of its buffer. This is silly windowavoidance by the receiver.</span><br><span class="line"></span><br><span class="line">tcp_rcv_state_process</span><br><span class="line">    |--&gt;tcp_time_wait</span><br><span class="line">           |--&gt;inet_twsk_alloc</span><br><span class="line">           |      |--&gt;setup_pinned_timer(&amp;tw-&gt;tw_timer, tw_timer_handler,(unsigned long)tw);</span><br><span class="line">           |--&gt;__inet_twsk_schedule(tw, timeo, <span class="literal">false</span>);</span><br><span class="line">                  |--&gt;mod_timer(&amp;tw-&gt;tw_timer, jiffies + timeo);</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_reuse = 1</span><br><span class="line"><span class="comment"># TIME_WAIT port use the new tcp connection</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_tw_recycle = 1</span><br><span class="line"><span class="comment"># In some NAT/loading balancing env cause DROP the SYN packet and reply the RST</span></span><br><span class="line"><span class="comment"># remove from linux kernel 4.10</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_fin_timeout = 60</span><br><span class="line">net.ipv4.ip_local_port_range = 32768 60999</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 6000</span><br><span class="line"><span class="comment"># TIME_WAIT socket max number</span></span><br></pre></td></tr></table></figure>

<h4 id="MSL-Maximum-Segment-Lifetime"><a href="#MSL-Maximum-Segment-Lifetime" class="headerlink" title="MSL (Maximum Segment Lifetime)"></a>MSL (Maximum Segment Lifetime)</h4><ul>
<li><p>2MSL server&#x2F;client send the close request in the last ack(time wait status), timewait &#x3D; 2MSL ? after timeout ,send fin ? in this time, the port could not be used.</p>
<ul>
<li>if you set SO_REUSEPORT, don ‘t to wait 2MSL to use the port SO_REUSEPORT (since Linux 3.9)</li>
<li>Permits multiple AF_INET or AF_INET6 sockets to be bound to an identical socket address.  This option must be set on each socket (including the first socket) prior to calling bind(2) on the socket.  To prevent port hijacking, all of the processes binding to the same address must have the same effective UID.  This option can be employed with both TCP an UDP sockets.</li>
<li>the SO_REUSEPORT socket option support was implemented in a series of patches by Tom Herbert. The new socket option allows multiple sockets on the same host to bind to the same port, and is intended to improve the performance of multithreaded network server applications running on top of multicore systems.</li>
</ul>
</li>
<li><p>TTL (time to live) in ip head   </p>
</li>
<li><p>MSL(Maximum segment lifetime) &gt; TTL(Time-to-live)  </p>
</li>
<li><p>RTT (round-trip time)      </p>
</li>
<li><p>packets per second (pps)</p>
</li>
<li><p>bytes per second (bps)</p>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">tcp three handshake</span><br><span class="line">client ----SYN----&gt; server    [SYN] Seq=X win=xxx Len=0 MSS=xxx</span><br><span class="line">server --SYN-ACK--&gt; client    [SYN, ACK] Seq=Y，ACK=X+1 win=xxx Len=0 MSS=xxx</span><br><span class="line">client ----ACK----&gt; server    [ACK] Seq=X+1，Ack=Y+1 win=xxx Len=0</span><br><span class="line">client &lt;--established-with--&gt;server</span><br><span class="line">抓包数据，如何判断一个包是上一个包的回包呢？根据TCP协议，下一个包的Ack的值如果等于上一个包的Seq + Len，则表示是其回包</span><br><span class="line">上一个和下一个，很多情况下并不是连续的，也行下一个回包距离上一个包已经过了很多包了，因为重传、延迟的原因的</span><br><span class="line">三次握手的时候会相互声明各自的MSS</span><br><span class="line"></span><br><span class="line">#transmitter could be client or server, try to close, send the FIN</span><br><span class="line">transmitter ----FIN----&gt; receiver</span><br><span class="line"></span><br><span class="line">#close connection, receiver respond the FIN</span><br><span class="line">receiver ----ACK and FIN----&gt; transmitter</span><br><span class="line"></span><br><span class="line">#waves four times</span><br><span class="line">client-&gt;server : FIN Seq=X，ACK=Y</span><br><span class="line">server-&gt;client : Seq=Y，ACK=X+1</span><br><span class="line">server-&gt;client : FIN Seq=Y，ACK=X+1</span><br><span class="line">client-&gt;server : Seq=X+1，Ack=Y+1</span><br><span class="line">正常而言，都会有这样的四次挥手，但是如果有延迟确认，那么四次挥手就变成了3次挥手，省掉了四次挥手中的第二个包</span><br><span class="line">client-&gt;server : FIN Seq=X，ACK=Y</span><br><span class="line">server-&gt;client : FIN Seq=Y，ACK=X+1</span><br><span class="line">client-&gt;server : Seq=X+1，Ack=Y+1</span><br><span class="line"></span><br><span class="line">#TIME-WAIT</span><br><span class="line">After transmitter and receiver got all FIN, the transmitter and receiver go to &quot;TIME-WAIT&quot; status</span><br><span class="line">The &quot;TIME-WAIT&quot; is in the status of the tcp close , in general, about 2 x MSL(Maximum Segment Lifetime)</span><br><span class="line"></span><br><span class="line">#why TIME-WAIT</span><br><span class="line">- because the has the high latency in the connection, it make sure not mix with the new connection</span><br><span class="line">- If FIN lost, the tcp will retry the FIN package, make sure the close will be successful</span><br><span class="line"></span><br><span class="line">#optimize</span><br><span class="line">- Reduce 2x MSL time</span><br><span class="line">- Set the SO_REUSEADDR to reuse when the tcp connection in the &quot;TIME-WAIT&quot;</span><br></pre></td></tr></table></figure>
<p>net.ipv4.tcp_tw_reuse &#x3D; 1; &#x2F;&#x2F;Enabled, only work in the connect() function,  the client try to connect the server<br>Enable reuse of TIME-WAIT sockets for new connections when it is safe from protocol viewpoint.<br>    0 - disable<br>    1 - global enable<br>    2 - enable for loopback traffic only</p>
<p>net.ipv4.tcp_timestamp &#x3D; 1;<br>Enable timestamps as defined in RFC1323.<br>    0: Disabled.<br>    1: Enable timestamps as defined in RFC1323 and use random offset for each connection rather than only using the current time.<br>    2: Like 1, but without random offsets</p>
<p>net.ipv4.tcp_max_tw_buckets &#x3D; 18000 #default<br>    Maximal number of timewait sockets held by system simultaneously. If this number is exceeded time-wait socket is immediately destroyed and warning is printed. This limit exists only to prevent simple DoS attacks, you <em>must</em> not lower the limit artificially, but rather increase it (probably, after increasing installed memory), if network conditions require more than default value.</p>
<p>#BTW, tcp_tw_recycle was removed from Kernel 4.12</p>
<ul>
<li>When tcp_tw_recycle is enabled, kernel aggressively re-use sockets in the TIME_WAIT even before TCP_TIMEWAIT_LEN(60 seconds) got expired</li>
<li>tcp_tw_recycle relies on tcp_timestamps and for this reason RHEL 7.2 introduced upstream commit as explained <a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/2595471">KB 2595471</a><ul>
<li>RHEL 7.2 as per which when using tcp_tw_recycle, the SYNs without TCP Timestamps will now be ignored<ul>
<li>If using tcp_tw_recycle, all incoming connection requests should use TCP Timestamps</li>
</ul>
</li>
</ul>
</li>
<li>This commit breaks connection behind NAT&#x2F;LB when tcp_tw_recycle enabled server since it can’t guarantee monotonically increase tcp_timestamps from such a environment</li>
<li>From kernel 4.10, there was another change which breaks all types of connections [tcp: randomize tcp timestamp offsets for each connection](<a target="_blank" rel="noopener" href="https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=95a22caee396cef0bb2ca8fafdd82966a49367bb%5D">https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=95a22caee396cef0bb2ca8fafdd82966a49367bb]</a></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/mDyoyNFt-84Ud0fNfdafxw">https://mp.weixin.qq.com/s/mDyoyNFt-84Ud0fNfdafxw</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">#### [tcp_fin_timeout and tcp_max_tw_buckets](https://access.redhat.com/solutions/41776)</span><br></pre></td></tr></table></figure>
<p>If you set too large value to tcp_fin_timeout, the system may become out of port, file-descripter and memory. If you set too small value, the system may leak delayed packets.<br>If you set too large value to tcp_max_tw_buckets, the system may become out of port, file-descripter and memory. If you set too small value, the system may not communicate another host.</p>
<p>TCP(7)</p>
<pre><code>   tcp_fin_timeout (integer; default: 60)
          This  specifies  how many seconds to wait for a final FIN packet
          before the socket is forcibly closed.  This is strictly a viola-
          tion  of  the TCP specification, but required to prevent denial-
          of-service attacks.  In Linux 2.2, the default value was 180.
</code></pre>
<snip>
       tcp_max_tw_buckets (integer; default: see below)
              The  maximum number of sockets in TIME_WAIT state allowed in the
              system.  This limit exists only to prevent simple denial-of-ser-
              vice  attacks.   The  default  value  of  NR_FILE*2  is adjusted
              depending on the memory  in  the  system.   If  this  number  is
              exceeded, the socket is closed and a warning is printed.
<snip>
       TCP_LINGER2
              The  lifetime  of orphaned FIN_WAIT2 state sockets.  This option
              can be used to override the system wide  sysctl  tcp_fin_timeout
              on  this  socket.  This is not to be confused with the socket(7)
              level option SO_LINGER.  This option should not be used in  code
              intended to be portable.

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">#### [tcp_tw_reuse](https://vincent.bernat.ch/en/blog/2014-tcp-time-wait-state-linux)</span><br><span class="line">NOTE: net.ipv4.tcp_tw_recycle has been removed from Linux 4.12   </span><br><span class="line">Linux will randomize timestamp offsets for each connection, making this option completely broken, with or without NAT. It has been completely removed from Linux 4.12.  </span><br><span class="line">The LAST-ACK state is handled in the exact same way as for net.ipv4.tcp_tw_recycle.  </span><br><span class="line"></span><br><span class="line">By enabling net.ipv4.tcp_tw_reuse, Linux will reuse an existing connection in the TIME-WAIT state for a new outgoing connection if the new timestamp is strictly bigger than the most recent timestamp recorded for the previous connection: an outgoing connection in the TIME-WAIT state can be reused after just one second.   </span><br><span class="line">![](/img/tcp-state-diagram-v2.svg)   </span><br><span class="line">![](/img/duplicate-segment.svg)  </span><br><span class="line">Due to a shortened TIME-WAIT state, a delayed TCP segment has been accepted in an unrelated connection.  </span><br><span class="line">The other purpose is to ensure the remote end has closed the connection. When the last ACK is lost, the remote end stays in the LAST-ACK state. Without the TIME-WAIT state, a connection could be reopened while the remote end still thinks the previous connection is valid. When it receives a SYN segment (and the sequence number matches), it will answer with a RST as it is not expecting such a segment. The new connection will be aborted with an error    </span><br><span class="line">![](/img/last-ack.svg)   </span><br><span class="line">If the remote end stays in LAST-ACK state because the last ACK was lost, opening a new connection with the same quadruplet will not work.  </span><br><span class="line">RFC 793 requires the TIME-WAIT state to last twice the time of the MSL. On Linux, this duration is not tunable and is defined in include/net/tcp.h as one minute:  </span><br><span class="line">```bash</span><br><span class="line">#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT</span><br><span class="line">                                  * state, about 60 seconds     */</span><br></pre></td></tr></table></figure>
<p>If the FIN segments are received in a timely manner, the local end socket will still be in the TIME-WAIT state and the expected ACK segments will be sent.<br>Once a new connection replaces the TIME-WAIT entry, the SYN segment of the new connection is ignored (thanks to the timestamps) and won’t be answered by a RST but only by a retransmission of the FIN segment. The FIN segment will then be answered with a RST (because the local connection is in the SYN-SENT state) which will allow the transition out of the LAST-ACK state. The initial SYN segment will eventually be resent (after one second) because there was no answer and the connection will be established without apparent error, except a slight delay:<br><img src="/img/last-ack-reuse.svg">   </p>
<h4 id="sysctl-conf"><a href="#sysctl-conf" class="headerlink" title="sysctl.conf"></a>sysctl.conf</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br><span class="line">425</span><br><span class="line">426</span><br><span class="line">427</span><br><span class="line">428</span><br><span class="line">429</span><br><span class="line">430</span><br><span class="line">431</span><br><span class="line">432</span><br><span class="line">433</span><br><span class="line">434</span><br><span class="line">435</span><br><span class="line">436</span><br><span class="line">437</span><br><span class="line">438</span><br><span class="line">439</span><br><span class="line">440</span><br><span class="line">441</span><br><span class="line">442</span><br><span class="line">443</span><br><span class="line">444</span><br><span class="line">445</span><br><span class="line">446</span><br><span class="line">447</span><br><span class="line">448</span><br><span class="line">449</span><br><span class="line">450</span><br><span class="line">451</span><br><span class="line">452</span><br><span class="line">453</span><br><span class="line">454</span><br><span class="line">455</span><br><span class="line">456</span><br><span class="line">457</span><br><span class="line">458</span><br><span class="line">459</span><br><span class="line">460</span><br><span class="line">461</span><br><span class="line">462</span><br><span class="line">463</span><br><span class="line">464</span><br><span class="line">465</span><br><span class="line">466</span><br><span class="line">467</span><br><span class="line">468</span><br><span class="line">469</span><br><span class="line">470</span><br><span class="line">471</span><br><span class="line">472</span><br><span class="line">473</span><br><span class="line">474</span><br><span class="line">475</span><br><span class="line">476</span><br><span class="line">477</span><br><span class="line">478</span><br><span class="line">479</span><br><span class="line">480</span><br><span class="line">481</span><br><span class="line">482</span><br><span class="line">483</span><br><span class="line">484</span><br><span class="line">485</span><br><span class="line">486</span><br><span class="line">487</span><br><span class="line">488</span><br><span class="line">489</span><br><span class="line">490</span><br><span class="line">491</span><br><span class="line">492</span><br><span class="line">493</span><br><span class="line">494</span><br><span class="line">495</span><br><span class="line">496</span><br><span class="line">497</span><br><span class="line">498</span><br><span class="line">499</span><br><span class="line">500</span><br><span class="line">501</span><br><span class="line">502</span><br><span class="line">503</span><br><span class="line">504</span><br><span class="line">505</span><br><span class="line">506</span><br><span class="line">507</span><br><span class="line">508</span><br><span class="line">509</span><br><span class="line">510</span><br><span class="line">511</span><br><span class="line">512</span><br><span class="line">513</span><br><span class="line">514</span><br><span class="line">515</span><br><span class="line">516</span><br><span class="line">517</span><br><span class="line">518</span><br><span class="line">519</span><br><span class="line">520</span><br><span class="line">521</span><br><span class="line">522</span><br><span class="line">523</span><br><span class="line">524</span><br><span class="line">525</span><br><span class="line">526</span><br><span class="line">527</span><br><span class="line">528</span><br><span class="line">529</span><br><span class="line">530</span><br><span class="line">531</span><br><span class="line">532</span><br><span class="line">533</span><br><span class="line">534</span><br><span class="line">535</span><br><span class="line">536</span><br><span class="line">537</span><br><span class="line">538</span><br><span class="line">539</span><br><span class="line">540</span><br><span class="line">541</span><br><span class="line">542</span><br><span class="line">543</span><br><span class="line">544</span><br><span class="line">545</span><br><span class="line">546</span><br><span class="line">547</span><br><span class="line">548</span><br><span class="line">549</span><br><span class="line">550</span><br><span class="line">551</span><br><span class="line">552</span><br><span class="line">553</span><br><span class="line">554</span><br><span class="line">555</span><br><span class="line">556</span><br><span class="line">557</span><br><span class="line">558</span><br><span class="line">559</span><br><span class="line">560</span><br><span class="line">561</span><br><span class="line">562</span><br><span class="line">563</span><br><span class="line">564</span><br><span class="line">565</span><br><span class="line">566</span><br><span class="line">567</span><br><span class="line">568</span><br><span class="line">569</span><br><span class="line">570</span><br><span class="line">571</span><br><span class="line">572</span><br><span class="line">573</span><br><span class="line">574</span><br><span class="line">575</span><br><span class="line">576</span><br><span class="line">577</span><br><span class="line">578</span><br><span class="line">579</span><br><span class="line">580</span><br><span class="line">581</span><br><span class="line">582</span><br><span class="line">583</span><br><span class="line">584</span><br><span class="line">585</span><br><span class="line">586</span><br><span class="line">587</span><br><span class="line">588</span><br><span class="line">589</span><br><span class="line">590</span><br><span class="line">591</span><br><span class="line">592</span><br><span class="line">593</span><br><span class="line">594</span><br><span class="line">595</span><br><span class="line">596</span><br><span class="line">597</span><br><span class="line">598</span><br><span class="line">599</span><br><span class="line">600</span><br><span class="line">601</span><br><span class="line">602</span><br><span class="line">603</span><br><span class="line">604</span><br><span class="line">605</span><br><span class="line">606</span><br><span class="line">607</span><br><span class="line">608</span><br><span class="line">609</span><br><span class="line">610</span><br><span class="line">611</span><br><span class="line">612</span><br><span class="line">613</span><br><span class="line">614</span><br><span class="line">615</span><br><span class="line">616</span><br><span class="line">617</span><br><span class="line">618</span><br><span class="line">619</span><br><span class="line">620</span><br><span class="line">621</span><br><span class="line">622</span><br><span class="line">623</span><br><span class="line">624</span><br><span class="line">625</span><br><span class="line">626</span><br><span class="line">627</span><br><span class="line">628</span><br><span class="line">629</span><br><span class="line">630</span><br><span class="line">631</span><br><span class="line">632</span><br><span class="line">633</span><br><span class="line">634</span><br><span class="line">635</span><br><span class="line">636</span><br><span class="line">637</span><br><span class="line">638</span><br><span class="line">639</span><br><span class="line">640</span><br><span class="line">641</span><br><span class="line">642</span><br><span class="line">643</span><br><span class="line">644</span><br><span class="line">645</span><br><span class="line">646</span><br><span class="line">647</span><br><span class="line">648</span><br><span class="line">649</span><br><span class="line">650</span><br><span class="line">651</span><br><span class="line">652</span><br><span class="line">653</span><br><span class="line">654</span><br><span class="line">655</span><br><span class="line">656</span><br><span class="line">657</span><br><span class="line">658</span><br><span class="line">659</span><br><span class="line">660</span><br><span class="line">661</span><br><span class="line">662</span><br><span class="line">663</span><br><span class="line">664</span><br><span class="line">665</span><br><span class="line">666</span><br><span class="line">667</span><br><span class="line">668</span><br><span class="line">669</span><br><span class="line">670</span><br><span class="line">671</span><br><span class="line">672</span><br><span class="line">673</span><br><span class="line">674</span><br><span class="line">675</span><br><span class="line">676</span><br><span class="line">677</span><br><span class="line">678</span><br><span class="line">679</span><br><span class="line">680</span><br><span class="line">681</span><br><span class="line">682</span><br><span class="line">683</span><br><span class="line">684</span><br><span class="line">685</span><br><span class="line">686</span><br><span class="line">687</span><br><span class="line">688</span><br><span class="line">689</span><br><span class="line">690</span><br><span class="line">691</span><br><span class="line">692</span><br><span class="line">693</span><br><span class="line">694</span><br><span class="line">695</span><br><span class="line">696</span><br><span class="line">697</span><br><span class="line">698</span><br><span class="line">699</span><br><span class="line">700</span><br><span class="line">701</span><br><span class="line">702</span><br><span class="line">703</span><br><span class="line">704</span><br><span class="line">705</span><br><span class="line">706</span><br><span class="line">707</span><br><span class="line">708</span><br><span class="line">709</span><br><span class="line">710</span><br><span class="line">711</span><br><span class="line">712</span><br><span class="line">713</span><br><span class="line">714</span><br><span class="line">715</span><br><span class="line">716</span><br><span class="line">717</span><br><span class="line">718</span><br><span class="line">719</span><br><span class="line">720</span><br><span class="line">721</span><br><span class="line">722</span><br><span class="line">723</span><br><span class="line">724</span><br><span class="line">725</span><br><span class="line">726</span><br><span class="line">727</span><br><span class="line">728</span><br><span class="line">729</span><br><span class="line">730</span><br><span class="line">731</span><br><span class="line">732</span><br><span class="line">733</span><br><span class="line">734</span><br><span class="line">735</span><br><span class="line">736</span><br><span class="line">737</span><br><span class="line">738</span><br><span class="line">739</span><br><span class="line">740</span><br><span class="line">741</span><br><span class="line">742</span><br><span class="line">743</span><br><span class="line">744</span><br><span class="line">745</span><br><span class="line">746</span><br><span class="line">747</span><br><span class="line">748</span><br><span class="line">749</span><br><span class="line">750</span><br><span class="line">751</span><br><span class="line">752</span><br><span class="line">753</span><br><span class="line">754</span><br><span class="line">755</span><br><span class="line">756</span><br><span class="line">757</span><br><span class="line">758</span><br><span class="line">759</span><br><span class="line">760</span><br><span class="line">761</span><br><span class="line">762</span><br><span class="line">763</span><br><span class="line">764</span><br><span class="line">765</span><br><span class="line">766</span><br><span class="line">767</span><br><span class="line">768</span><br><span class="line">769</span><br><span class="line">770</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># each socket will use some fd</span></span><br><span class="line">fs.file-max = 1048576</span><br><span class="line"></span><br><span class="line"><span class="comment"># tcp short connection will cause a lot of TIME_WAIT</span></span><br><span class="line">net.ipv4.tcp_max_tw_buckets = 7000</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable tcp source trace</span></span><br><span class="line">net.ipv4.conf.default.accept_source_route = 0</span><br><span class="line"></span><br><span class="line"><span class="comment">#RPS driver call &quot;netif_rx()&quot; </span></span><br><span class="line">net.core.netdev_max_backlog=3000</span><br><span class="line"></span><br><span class="line"><span class="comment">#NAPI weight of the backlog poll loop</span></span><br><span class="line">net.core.dev_weight=600</span><br><span class="line"></span><br><span class="line"><span class="comment"># add flow hash table size #default 4096</span></span><br><span class="line">flow_limit_table_len = 16384 </span><br><span class="line"></span><br><span class="line"><span class="comment"># set timeout</span></span><br><span class="line">net.ipv4.tcp_fin_timeout = 40  <span class="comment">#default 60 </span></span><br><span class="line"><span class="comment">#define TCP_TIMEWAIT_LEN (60*HZ) /* how long to wait to destroy TIME-WAIT</span></span><br><span class="line"><span class="comment">#                              * state, about 60 seconds     */</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_syn_retries= 4     <span class="comment">#default 6</span></span><br><span class="line"><span class="comment">#[tcp_syn_retries 三次握手](https://ms2008.github.io/2017/04/14/tcp-timeout/)</span></span><br><span class="line"><span class="comment">#net.ipv4.tcp_syn_retries retrans interval was [1,3,7,15,31]s</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_retries2 = 10      <span class="comment">#default 15, 9 about </span></span><br><span class="line"><span class="comment"># https://access.redhat.com/solutions/726753</span></span><br><span class="line"><span class="comment"># In a High Availability (HA) situation consider decreasing the setting to 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#TIMEOUT socket re-use</span></span><br><span class="line">net.ipv4.tcp_tw_reuse= 1</span><br><span class="line">net.ipv4.tcp_timestamps = 1</span><br><span class="line">net.ipv4.tcp_syncookies = 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># socket buffer to 256MB</span></span><br><span class="line">net.core.rmem_max = 268435456</span><br><span class="line">net.core.wmem_max = 268435456</span><br><span class="line"><span class="comment"># allow tcp ipv4 buffer auto-tuning up to 256MB</span></span><br><span class="line"><span class="comment"># min,init,maxinum size</span></span><br><span class="line">net.ipv4.tcp_rmem = 65536 134217728 268435456</span><br><span class="line">net.ipv4.tcp_wmem = 65536 134217728 268435456</span><br><span class="line">net.ipv4.tcp_mem = 65536 134217728 268435456</span><br><span class="line"></span><br><span class="line">kernel.numa_balancing=0</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_frto=1</span><br><span class="line"><span class="comment">#Because packets out of order or another issue trigger the packet RTO, but the packet was not loss, it &#x27;s misjudging, that means Spurious retransmission timeouts.</span></span><br><span class="line"><span class="comment">#Active F-RTO</span></span><br><span class="line"><span class="comment">#net.ipv4.tcp_frto=2</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_no_metrics_save = 1</span><br><span class="line">net.ipv4.tcp_slow_start_after_idle = 0</span><br><span class="line">net.ipv4.tcp_mtu_probing = 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># works best with &gt;= 500 client computers ##</span></span><br><span class="line">net.ipv4.neigh.default.gc_interval = 3600</span><br><span class="line">net.ipv4.neigh.default.gc_stale_time = 3600</span><br><span class="line">net.ipv4.neigh.default.gc_thresh3 = 8192</span><br><span class="line">net.ipv4.neigh.default.gc_thresh2 = 4096</span><br><span class="line">net.ipv4.neigh.default.gc_thresh1 = 2048</span><br><span class="line"><span class="comment"># Keep alive</span></span><br><span class="line">net.ipv4.tcp_keepalive_intvl = 60</span><br><span class="line">net.ipv4.tcp_keepalive_probes = 3</span><br><span class="line">net.ipv4.tcp_keepalive_time = 7200</span><br><span class="line"></span><br><span class="line">dev.raid.speed_limit_max=8000000</span><br><span class="line">dev.raid.speed_limit_min=2000000</span><br><span class="line"></span><br><span class="line">kernel.unknown_nmi_panic = 1</span><br><span class="line">kernel.panic_on_unrecovered_nmi = 1</span><br><span class="line">kernel.panic_on_io_nmi = 1</span><br><span class="line">kernel.nmi_watchdog = 0</span><br><span class="line">kernel.numa_balancing=0</span><br><span class="line"></span><br><span class="line">Thin-stream; https://nnc3.com/mags/LJ_1994-2014/LJ/219/11180.html</span><br><span class="line"><span class="comment">#default eq 0</span></span><br><span class="line">sysctl -w net.ipv4.tcp_thin_linear_timeouts=1</span><br><span class="line">sysctl -w net.ipv4.tcp_thin_dupack=1</span><br><span class="line"><span class="built_in">echo</span> 1 &gt;  /proc/sys/net/ipv4/tcp_thin_linear_timeouts</span><br><span class="line"><span class="built_in">echo</span> 1 &gt;  /proc/sys/net/ipv4/tcp_thin_dupack</span><br><span class="line"></span><br><span class="line">vm.min_free_kbytes=700000</span><br><span class="line"><span class="comment">#the system&#x27;s emergency reserves - which is entirely unrelated to the system&#x27;s latency requirements. In order to get kswapd to maintain a 250M buffer of free memory. the emergency reserves need to be set to 1G. That is a lot of memory wasted for no good reason.  https://lore.kernel.org/patchwork/patch/648675/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#increase watermark[WMARK_MIN] via min_free_kbytes</span></span><br><span class="line"><span class="comment">#awk &#x27;$0~/min/&#123;sum+=$NF&#125; END&#123;print sum*4/1024 &quot;MiB&quot;&#125;&#x27; /proc/zoneinfo</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#In the intel i40e and ixgbe driver work in Jumbo frame network and enabled TSO feature will cause a lot of ENOMEM. In the 512G memory system, set it to 512M, the issue will gone else cause too many package dropped.</span></span><br><span class="line">ZONE_DMA                  normal DMA page</span><br><span class="line">ZONE_DMA32                normal DMA32 page</span><br><span class="line">ZONE_NORMAL/ZONE_HIGHMEM  64 bit OS means all mem, there is no HIGHMEM <span class="keyword">in</span> x86_64</span><br><span class="line"></span><br><span class="line">nstat -sz TcpExtPFMemallocDrop</span><br><span class="line">TcpExtPFMemallocDrop            183623                  0.0</span><br><span class="line"></span><br><span class="line">include/net/sock.h</span><br><span class="line">SOCK_MEMALLOC, /* VM depends on this socket <span class="keyword">for</span> swapping */</span><br><span class="line"></span><br><span class="line">./include/linux/mm_types.h:62</span><br><span class="line">        /* Second double word */</span><br><span class="line">        struct &#123;</span><br><span class="line">                union &#123;</span><br><span class="line">                        pgoff_t index;          /* Our offset within mapping. */</span><br><span class="line">                        void *freelist;         /* slub/slob first free object */</span><br><span class="line">                        RH_KABI_DEPRECATE(bool, pfmemalloc)</span><br><span class="line">                                                /* If <span class="built_in">set</span> by the page allocator,</span><br><span class="line">                                                 * ALLOC_NO_WATERMARKS was <span class="built_in">set</span></span><br><span class="line">                                                 * and the low watermark was not</span><br><span class="line">                                                 * met implying that the system</span><br><span class="line">                                                 * is under some pressure. The</span><br><span class="line">                                                 * <span class="built_in">caller</span> should try ensure</span><br><span class="line">                                                 * this page is only used to</span><br><span class="line">                                                 * free other pages.</span><br><span class="line">                                                 */</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Memory resource:https://access.redhat.com/solutions/4085851</span></span><br><span class="line"><span class="comment">#add LINUX_MIB_PFMEMALLOCDROP counter:https://git.kernel.org/pub/scm/linux/kernel/git/torvalds/linux.git/commit/?id=8fe809a992639b2013c0d8da2ba55cdea28a959a</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#if set by the page allocator ALLOC_NO_WATERMARKS was set and the low watermark was not met implying that the system is under some pressure. the caller should try ensure this page is only used to free other pages</span></span><br><span class="line"></span><br><span class="line">./include/linux/skbuff.h:634: * @pfmemalloc: skbuff was allocated from PFMEMALLOC reserves</span><br><span class="line"></span><br><span class="line">If the skb was allocated from pfmemalloc reserves, only allow SOCK_MEMALLOC sockets to use it as this socket is helping free memory</span><br><span class="line">int sk_filter_trim_cap(struct sock *sk, struct sk_buff *skb, unsigned int <span class="built_in">cap</span>)</span><br><span class="line">&#123;</span><br><span class="line">        int err;</span><br><span class="line">        struct sk_filter *filter;</span><br><span class="line"></span><br><span class="line">        /*</span><br><span class="line">         * If the skb was allocated from pfmemalloc reserves, only</span><br><span class="line">         * allow SOCK_MEMALLOC sockets to use it as this socket is</span><br><span class="line">         * helping free memory</span><br><span class="line">         */</span><br><span class="line">        <span class="keyword">if</span> (skb_pfmemalloc(skb) &amp;&amp; !sock_flag(sk, SOCK_MEMALLOC)) &#123;</span><br><span class="line">                NET_INC_STATS(sock_net(sk), LINUX_MIB_PFMEMALLOCDROP);</span><br><span class="line">                <span class="built_in">return</span> -ENOMEM;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">                        ip_rcv ----&gt; process the ip protocol</span><br><span class="line">                          ip_rcv_finish</span><br><span class="line">                            ip_local_deliver</span><br><span class="line">                              ip_local_deliver_finish</span><br><span class="line">                                -&gt; __skb_pull // remove data from the start of a buffer</span><br><span class="line">                                tcp_v4_rcv ----&gt; Layer 4 tcp_v4_rec</span><br><span class="line">                                  tcp_md5_do_lookup</span><br><span class="line">                                    tcp_parse_md5sig_option</span><br><span class="line">                                      tcp_filter</span><br><span class="line">                                        sk_filter_trim_cap (<span class="built_in">return</span> -12 ENOMEM out of memory) ----&gt; kmalloc or sock_kmalloc mem <span class="built_in">timeout</span></span><br><span class="line"></span><br><span class="line">linux-3.10.0-1127.el7/include/linux/gfp.h</span><br><span class="line">warn_alloc_failed: 776 callbacks suppressed</span><br><span class="line">page allocation failure: order:2, mode:0x4020</span><br><span class="line">                                          |</span><br><span class="line">                                          ---gpf_mask</span><br><span class="line"><span class="comment">#define ___GFP_HIGH             0x20u       //This allocation has high priority and may use emergency pools.</span></span><br><span class="line"><span class="comment">#define ___GFP_COMP             0x4000u     //https://stackoverflow.com/questions/62486827/about-the-usage-of-gfp-comp</span></span><br><span class="line"></span><br><span class="line">order:2 means 2^2=4 x 4Kpages = 16KB</span><br><span class="line">order:4 means 16x4pages = 64KB, order:10 means 1024 x 4K pages=4096KB</span><br><span class="line">mode:0x4020 - A bitmask flag</span><br><span class="line"><span class="comment"># In interrupt context it is not possible to reclaim memory, or wait for memory to be freed, to satisfy an allocation request. Therefore, if &quot;free&quot; privileged memory on a system is low, at the time of the allocation, the allocation will simply fail. Failed GFP_ATOMIC allocations by the network stack result in dropped packets which is likely to be received on a subsequent retransmit. They can also result in unexpected behaviour within various NIC drivers in the event that the calling codepath does not gracefully recover from the allocation failure. Note that a mode value like 0x4020 or 0x20 as reported in the &quot;page allocation failure&quot; message (a value ending in 0x20, the 1&lt;&lt;5 bit set) is a GFP_ATOMIC allocation. The order is the size of the allocation defined as: size = 2^order pages. For instance, an order 0 allocation request a single page (4k on most archs), an order 4 allocation request 16 pages (64k on most archs). The bigger the order, the more difficult the atomic allocation is going to be satisfied</span></span><br><span class="line"><span class="comment">#https://access.redhat.com/solutions/479983</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#linux-3.10.0-1127.el7/include/linux/gfp.h</span></span><br><span class="line">Two types:</span><br><span class="line">zone modifiers</span><br><span class="line">action modifiers</span><br><span class="line">alloc_page-------------------------------single page</span><br><span class="line">get_zeroed_page--&gt;__get_free_pages</span><br><span class="line">    alloc_pages--------------------------2^order</span><br><span class="line">        alloc_pages_node-----------------node <span class="built_in">id</span></span><br><span class="line">            __alloc_pages</span><br><span class="line">                __alloc_pages_node_mask--nodemaks</span><br><span class="line"></span><br><span class="line">__GFP_DMA         ((__force gfp_t)    <span class="number">0</span>x01u)</span><br><span class="line">__GFP_HIGHMEM     ((__force gfp_t)    <span class="number">0</span>x02u)</span><br><span class="line">__GFP_DMA32       ((__force gfp_t)    <span class="number">0</span>x04u)</span><br><span class="line">__GFP_MOVABLE     ((__force gfp_t)    <span class="number">0</span>x08u) /* Flag that this page will be movable by the */</span><br><span class="line">                                             /* page migration mechanism or reclaimed */</span><br><span class="line">__GFP_WAIT        ((__force gfp_t)    <span class="number">0</span>x10u) /* Can wait and reschedule? */</span><br><span class="line">__GFP_HIGH        ((__force gfp_t)    <span class="number">0</span>x20u) /* Should access emergency pools? */</span><br><span class="line">__GFP_IO          ((__force gfp_t)    <span class="number">0</span>x40u) /* Can start physical IO? */</span><br><span class="line">__GFP_FS          ((__force gfp_t)    <span class="number">0</span>x80u) /* Can call down to low-level FS? */</span><br><span class="line">__GFP_COLD        ((__force gfp_t)   <span class="number">0</span>x100u) /* Cache-cold page required */</span><br><span class="line">__GFP_NOWARN      ((__force gfp_t)   <span class="number">0</span>x200u) /* Suppress page allocation failure warning */</span><br><span class="line">__GFP_REPEAT      ((__force gfp_t)   <span class="number">0</span>x400u) /* Try hard to allocate the memory, but the */</span><br><span class="line">                                             /* allocation attempt _might_ fail.  This */</span><br><span class="line">                                             /* depends upon the particular VM implementation. */</span><br><span class="line">__GFP_NOFAIL      ((__force gfp_t)   <span class="number">0</span>x800u) /* The VM implementation _must_ retry infinitely: */</span><br><span class="line">                                             /* the caller cannot handle allocation failures. */</span><br><span class="line">__GFP_NORETRY     ((__force gfp_t)  <span class="number">0</span>x1000u) /* The VM implementation must not retry indefinitely */</span><br><span class="line">__GFP_COMP        ((__force gfp_t)  <span class="number">0</span>x4000u) /* Add compound page metadata */</span><br><span class="line">__GFP_ZERO        ((__force gfp_t)  <span class="number">0</span>x8000u) /* Return zeroed page on success */</span><br><span class="line">__GFP_NOMEMALLOC  ((__force gfp_t) <span class="number">0</span>x10000u) /* Don&#x27;t use emergency reserves */</span><br><span class="line">__GFP_HARDWALL    ((__force gfp_t) <span class="number">0</span>x20000u) /* Enforce hardwall cpuset memory allocs */</span><br><span class="line">__GFP_THISNODE    ((__force gfp_t) <span class="number">0</span>x40000u) /* No fallback, no policies */</span><br><span class="line">__GFP_RECLAIMABLE ((__force gfp_t) <span class="number">0</span>x80000u) /* Page is reclaimable */</span><br><span class="line">__GFP_NOTRACK     ((__force gfp_t)<span class="number">0</span>x200000u) /* Don&#x27;t track with kmemcheck */</span><br><span class="line">__GFP_NO_KSWAPD   ((__force gfp_t)<span class="number">0</span>x400000u)</span><br><span class="line">__GFP_OTHER_NODE  ((__force gfp_t)<span class="number">0</span>x800000u)</span><br><span class="line"></span><br><span class="line">GFP_ATOMIC        (__GFP_HIGH)               /* !wait (__GFP_WAIT not set) and use emergency pool *</span><br><span class="line"></span><br><span class="line">https://www.kernel.org/doc/htmldocs/kernel-api/API-kmalloc.html</span><br><span class="line">/* To avoid unnecessary overhead, we pass through large allocation requests</span><br><span class="line"> * directly to the page allocator. We use __GFP_COMP, because we will need to</span><br><span class="line"> * know the allocation order to free the pages properly in kfree. */</span><br><span class="line"></span><br><span class="line">当__alloc_pages分配标志gfp_flags指定了__GFP_COMP，那么内核必须将这些页组合成复合页compound page。第一个页称为head page,其余所有的页称为tail page。复合页的尺寸要远大于当前分&gt;页系统支持的页面&gt;大小。并且一定是<span class="number">2</span>^order * PAGE_SIZE大小。复合页主要用在HugeTLB相关的代码</span><br><span class="line">当页面分配函数使用GFP_COMP进行页面分配时，分配函数会为每一个增加标志PG_Compound，我们称复合页中的第一个<span class="number">4</span>KB页面为head page，后面的所有page 为tail page。每个page的private保存&gt;一个指针，head page的private指向本身，tail page的private指向head page</span><br><span class="line"></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.222</span>      <span class="number">814</span>   -&gt; <span class="number">192.168</span>.<span class="number">0.111</span>    <span class="number">2049</span> , retval -<span class="number">12</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.222</span>      <span class="number">814</span>   -&gt; <span class="number">192.168</span>.<span class="number">0.111</span>    <span class="number">2049</span> , retval -<span class="number">12</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.222</span>      <span class="number">814</span>   -&gt; <span class="number">192.168</span>.<span class="number">0.111</span>    <span class="number">2049</span> , retval -<span class="number">12</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.222</span>      <span class="number">814</span>   -&gt; <span class="number">192.168</span>.<span class="number">0.111</span>    <span class="number">2049</span> , retval -<span class="number">12</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.222</span>      <span class="number">814</span>   -&gt; <span class="number">192.168</span>.<span class="number">0.111</span>    <span class="number">2049</span> , retval -<span class="number">12</span></span><br><span class="line"><span class="number">192.168</span>.<span class="number">0.222</span>      <span class="number">814</span>   -&gt; <span class="number">192.168</span>.<span class="number">0.111</span>    <span class="number">2049</span> , retval -<span class="number">12</span></span><br><span class="line"></span><br><span class="line">/usr/include/asm-generic/errno-base.h</span><br><span class="line">#define ENOMEM          <span class="number">12</span>      /* Out of memory */</span><br><span class="line"></span><br><span class="line">#!/usr/bin/bpftrace</span><br><span class="line">#include &lt;net/sock.h&gt;</span><br><span class="line">#include &lt;linux/skbuff.h&gt;</span><br><span class="line"></span><br><span class="line">k:sk_filter_trim_cap</span><br><span class="line">&#123;</span><br><span class="line">        @sk[tid] = arg0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kr:sk_filter_trim_cap</span><br><span class="line">/@sk[tid]/</span><br><span class="line">&#123;</span><br><span class="line">        <span class="variable">$sk</span> = (struct sock *)@sk[tid];</span><br><span class="line"></span><br><span class="line">        <span class="variable">$af</span> = <span class="variable">$sk</span>-&gt;__sk_common.skc_family;</span><br><span class="line">        if (<span class="variable">$af</span> == AF_INET) &#123;</span><br><span class="line">            <span class="variable">$daddr</span> = ntop(<span class="variable">$af</span>, <span class="variable">$sk</span>-&gt;__sk_common.skc_daddr);</span><br><span class="line">            <span class="variable">$saddr</span> = ntop(<span class="variable">$af</span>, <span class="variable">$sk</span>-&gt;__sk_common.skc_rcv_saddr);</span><br><span class="line">            <span class="variable">$lport</span> = <span class="variable">$sk</span>-&gt;__sk_common.skc_num;</span><br><span class="line">            <span class="variable">$dport</span> = <span class="variable">$sk</span>-&gt;__sk_common.skc_dport;</span><br><span class="line"></span><br><span class="line">            <span class="variable">$dport</span> = (<span class="variable">$dport</span> &gt;&gt; <span class="number">8</span>) | ((<span class="variable">$dport</span> &lt;&lt; <span class="number">8</span>) &amp; <span class="number">0</span>xff00);</span><br><span class="line">            if (retval &gt; <span class="number">0</span>) &#123;</span><br><span class="line">                printf(&quot;%-<span class="number">15</span>s %-<span class="number">5</span>d -&gt; %-<span class="number">15</span>s %-<span class="number">5</span>d, retval %d\n&quot;,</span><br><span class="line">                <span class="variable">$saddr</span>, <span class="variable">$lport</span>, <span class="variable">$daddr</span>, <span class="variable">$dport</span>, retval);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        delete(@sk[tid]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">or you could</span><br><span class="line">$ perf probe -a &#x27;sk_filter_trim_cap%return return=<span class="variable">$retval</span>:s32&#x27;</span><br><span class="line">$ perf record -e probe:sk_filter_trim_cap -agR --filter &#x27;return &lt; <span class="number">0</span>&#x27; sleep <span class="number">10</span></span><br><span class="line"></span><br><span class="line">$ perf probe -d probe:sk_filter_trim_cap</span><br><span class="line">or</span><br><span class="line">// or for kernels &lt; <span class="number">3.10</span>.<span class="number">0</span>-<span class="number">1062</span>.el7 :</span><br><span class="line">$ perf record -e probe:sk_filter_trim_cap__return -agR --filter &#x27;return &lt; <span class="number">0</span>&#x27; sleep <span class="number">10</span></span><br><span class="line">$ perf report</span><br><span class="line"></span><br><span class="line">## ftrace</span><br><span class="line">$ echo <span class="number">0</span> &gt; /sys/kernel/debug/tracing/events/kprobes/sk_filter_trim_cap/enable</span><br><span class="line">$ echo  &gt; /sys/kernel/debug/tracing/kprobe_events</span><br><span class="line">$ echo &#x27;r:sk_filter_trim_cap sk_filter_trim_cap <span class="variable">$retval</span>&#x27; &gt; /sys/kernel/debug/tracing/kprobe_events</span><br><span class="line">$ echo <span class="number">1</span> &gt; /sys/kernel/debug/tracing/events/kprobes/sk_filter_trim_cap/enable</span><br><span class="line"></span><br><span class="line">cat /sys/kernel/debug/tracing/events/kprobes/sk_filter_trim_cap/format</span><br><span class="line">name: sk_filter_trim_cap</span><br><span class="line">ID: <span class="number">1548</span></span><br><span class="line">format:</span><br><span class="line">       field:unsigned short common_type;       offset:<span class="number">0</span>;       size:<span class="number">2</span>; signed:<span class="number">0</span>;</span><br><span class="line">       field:unsigned char common_flags;       offset:<span class="number">2</span>;       size:<span class="number">1</span>; signed:<span class="number">0</span>;</span><br><span class="line">       field:unsigned char common_preempt_count;       offset:<span class="number">3</span>;       size:<span class="number">1</span>; signed:<span class="number">0</span>;</span><br><span class="line">       field:int common_pid;   offset:<span class="number">4</span>;       size:<span class="number">4</span>; signed:<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">       field:unsigned long __probe_func;       offset:<span class="number">8</span>;       size:<span class="number">8</span>; signed:<span class="number">0</span>;</span><br><span class="line">       field:unsigned long __probe_ret_ip;     offset:<span class="number">16</span>;      size:<span class="number">8</span>; signed:<span class="number">0</span>;</span><br><span class="line">       field:u64 arg1; offset:<span class="number">24</span>;      size:<span class="number">8</span>; signed:<span class="number">0</span>;</span><br><span class="line">print fmt: &quot;(%lx &lt;- %lx) arg1=<span class="number">0</span>x%Lx&quot;, REC-&gt;__probe_func, REC-&gt;__probe_ret_ip, REC-&gt;arg1</span><br><span class="line"></span><br><span class="line">$ cat /sys/kernel/debug/tracing/trace</span><br><span class="line"># tracer: nop</span><br><span class="line">#</span><br><span class="line"># entries-in-buffer/entries-written: <span class="number">52403</span>/<span class="number">52403</span>   #P:<span class="number">96</span></span><br><span class="line">#</span><br><span class="line">#                              _-----=&gt; irqs-off</span><br><span class="line">#                             / _----=&gt; need-resched</span><br><span class="line">#                            | / _---=&gt; hardirq/softirq</span><br><span class="line">#                            || / _--=&gt; preempt-depth</span><br><span class="line">#                            ||| /     delay</span><br><span class="line">#           TASK-PID   CPU#  ||||    TIMESTAMP  FUNCTION</span><br><span class="line">#              | |       |   ||||       |         |</span><br><span class="line">          &lt;idle&gt;-<span class="number">0</span>     [<span class="number">006</span>] d.s. <span class="number">736525.098807</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line">           &lt;...&gt;-<span class="number">156973</span> [<span class="number">006</span>] d.s. <span class="number">736532.093611</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line">           &lt;...&gt;-<span class="number">156973</span> [<span class="number">006</span>] d.s. <span class="number">736532.093615</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line">           &lt;...&gt;-<span class="number">156973</span> [<span class="number">006</span>] d.s. <span class="number">736532.093618</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line">          &lt;idle&gt;-<span class="number">0</span>     [<span class="number">006</span>] dNs. <span class="number">736532.093644</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line">          &lt;idle&gt;-<span class="number">0</span>     [<span class="number">006</span>] dNs. <span class="number">736532.093653</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line">          &lt;idle&gt;-<span class="number">0</span>     [<span class="number">006</span>] dNs. <span class="number">736532.093655</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line">          &lt;idle&gt;-<span class="number">0</span>     [<span class="number">006</span>] dNs. <span class="number">736532.093656</span>: sk_filter_trim_cap: (tcp_filter+<span class="number">0</span>x2c/<span class="number">0</span>x40 &lt;- sk_filter_trim_cap) arg1=<span class="number">0</span>xfffffff4</span><br><span class="line"></span><br><span class="line">$ ./trace &#x27;r::sk_filter_trim_cap (retval != <span class="number">0</span>) &quot;%d&quot;, retval&#x27;</span><br><span class="line"><span class="number">281</span>     <span class="number">281</span>     ksoftirqd/<span class="number">54</span>    sk_filter_trim_cap -<span class="number">12</span></span><br><span class="line"><span class="number">281</span>     <span class="number">281</span>     ksoftirqd/<span class="number">54</span>    sk_filter_trim_cap -<span class="number">12</span></span><br><span class="line"><span class="number">20818</span>   <span class="number">81613</span>   user_app        sk_filter_trim_cap -<span class="number">12</span></span><br><span class="line"><span class="number">20818</span>   <span class="number">81613</span>   user_app        sk_filter_trim_cap -<span class="number">12</span></span><br><span class="line"><span class="number">314457</span>  <span class="number">314457</span>  kworker/<span class="number">54</span>:<span class="number">1</span>H   sk_filter_trim_cap -<span class="number">12</span></span><br><span class="line"><span class="number">314457</span>  <span class="number">314457</span>  kworker/<span class="number">54</span>:<span class="number">1</span>H   sk_filter_trim_cap -<span class="number">12</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#transmit</span><br><span class="line">$ echo <span class="number">1</span> &gt; /sys/kernel/debug/tracing/events/sock/sock_exceed_buf_limit/enable</span><br><span class="line">#too many tcp connection or tcp_mem limit the send packages</span><br><span class="line">$ cat /sys/kernel/debug/tracing/trace_pipe</span><br><span class="line"></span><br><span class="line">#https://sourceware.org/systemtap/examples/network/sk_stream_wait_memory.stp</span><br><span class="line"></span><br><span class="line">#lowmem_reserve_ratio</span><br><span class="line">vm.lowmem_reserve_ratio = <span class="number">256</span>   <span class="number">256</span>     <span class="number">32</span></span><br><span class="line"># The default values of lowmem_reserve_ratio[i] are <span class="number">256</span> (if zone[i] means DMA or DMA32 zone) <span class="number">32</span> (others). As above expression, they are reciprocal number of ratio. <span class="number">256</span> means <span class="number">1</span>/<span class="number">256</span>. # of protection pages becomes about “<span class="number">0.39</span>%” of total managed pages of higher zones on the node.</span><br><span class="line"></span><br><span class="line">### when high level zone not enough memory malloc the memory from the lower zone, the low zone free memory must has the enoguh free page, else failed</span><br><span class="line"></span><br><span class="line">    nr_free_cma  <span class="number">0</span></span><br><span class="line">        protection: (<span class="number">0</span>, <span class="number">0</span>, <span class="number">62431</span>, <span class="number">62431</span>)</span><br><span class="line">        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^-----------<span class="number">62431</span> x <span class="number">4</span>K page---------- /proc/zoneinfo</span><br><span class="line"></span><br><span class="line">### each zone size</span><br><span class="line">grep span /proc/zoneinfo</span><br><span class="line">        spanned  <span class="number">4095</span>  (<span class="number">4095</span>*<span class="number">4</span>K)</span><br><span class="line">        spanned  <span class="number">1044480</span></span><br><span class="line">        spanned  <span class="number">16252928</span></span><br><span class="line">        spanned  <span class="number">16777216</span></span><br><span class="line"></span><br><span class="line">if watermark + protection[j] &gt; free_page, refuse malloc the low zone memory</span><br><span class="line"></span><br><span class="line">zone[i]’s protection[j] is calculated by following expression.</span><br><span class="line">(i &lt; j):</span><br><span class="line">  zone[i]-&gt;protection[j]</span><br><span class="line">  = (total sums of managed_pages from zone[i+<span class="number">1</span>] to zone[j] on the node)</span><br><span class="line">    / lowmem_reserve_ratio[i];</span><br><span class="line">(i = j):</span><br><span class="line">   (should not be protected. = <span class="number">0</span>;</span><br><span class="line">(i &gt; j):</span><br><span class="line">   (not necessary, but looks <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">Node <span class="number">0</span> Normal free:<span class="number">36276</span>kB min:<span class="number">14608</span>kB low:<span class="number">18260</span>kB high:<span class="number">21912</span>kB dirty:<span class="number">0</span>kB shmem:<span class="number">4</span>kB slab_reclaimable:<span class="number">222264</span>kB slab_unreclaimable:<span class="number">27504</span>kB</span><br><span class="line">Node <span class="number">1</span> Normal free:<span class="number">19976</span>kB min:<span class="number">16260</span>kB low:<span class="number">20324</span>kB high:<span class="number">24388</span>kB dirty:<span class="number">0</span>kB shmem:<span class="number">0</span>kB slab_reclaimable:<span class="number">371916</span>kB slab_unreclaimable:<span class="number">22880</span>kB</span><br><span class="line">                     |                    |</span><br><span class="line">                     ---<span class="number">19976</span> &lt; <span class="number">20324</span>------</span><br><span class="line">kswap --&gt; reclaim ---&gt; check zone_reclaim_mode value --&gt;  if (zone_reclaim_mode == <span class="number">0</span>) this_zone_full; if <span class="number">1</span> reclaim local</span><br><span class="line">__alloc_pages_nodemask -&gt; __alloc_pages_slowpath -&gt; warn_alloc_failed</span><br><span class="line"></span><br><span class="line"># __alloc_pages_nodemask retry</span><br><span class="line">retry_cpuset:</span><br><span class="line"></span><br><span class="line">        if (unlikely(!page))</span><br><span class="line">                page = __alloc_pages_slowpath(...)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (unlikely(!put_mems_allowed(cpuset_mems_cookie) &amp;&amp; !page))</span><br><span class="line">                goto retry_cpuset;</span><br><span class="line"></span><br><span class="line">increase vm.min_free_kbytes or tune down vm.lowmem_reserve_ratio</span><br><span class="line">When the free memory not enough, the kernel will borrow the memory from the DMA32/DMA zone</span><br><span class="line"></span><br><span class="line"><span class="comment">#32 bit eg:</span></span><br><span class="line">vm.lowmem_reserve_ratio = 256   32     0</span><br><span class="line">DMA = 16MiB = 4096 pages</span><br><span class="line">NORMAL = 784MiB = 190464 pages</span><br><span class="line">HIGHMEM = 200MiB = 51200</span><br><span class="line"></span><br><span class="line">设ZONE_DMA, ZONE_NORMAL和ZONE_HIGHMEM的内存大小分别是A, B和C，那么在ZONE_DMA中，需要给自己预留的内存大小是<span class="string">&quot;B/256&quot;</span>加上<span class="string">&quot;(B+C)/256&quot;</span>。在ZONE_NORMAL中，需要预留的内存大小是<span class="string">&quot;C/32&quot;</span>。这里的分母来自&gt;于zone自己的<span class="string">&quot;lowmem_reserve_ratio&quot;</span>，而分子则来自于比它高位的zones。</span><br><span class="line">                               zone_DMA                zone_normal           zone_highmem</span><br><span class="line">lower_reserver[Zone_DMA]    =   0/256       ,           190464/256    ,      (190464+51200)/256</span><br><span class="line">lower_reserver[Zone_NORMAL] =   0/32        ,            0/32         ,         51200/32</span><br><span class="line">lower_reserver[Zone_HIGHMEM]=   0/0         ,            0/0          ,           0/0</span><br><span class="line"></span><br><span class="line">enough memory = watermark[WMARK_HIGH] + lowmem_reserve</span><br><span class="line"></span><br><span class="line">https://billtian.github.io/digoal.blog/2016/12/21/01.html</span><br><span class="line">https://blog.arstercz.com/linux-%E7%B3%BB%E7%BB%9F-page-allocation-failure-%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</span><br><span class="line">https://zhuanlan.zhihu.com/p/81961211</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">vm.zone_reclaim_mode=1</span><br><span class="line"><span class="comment">#Enabling the zone reclaimer actively frees memory from kernel zones, when the zone becomes full. The zone reclaimer can also be allowed to actively flush dirty pages, which also prevents a heavily-writing process from dirtying other NUMA nodes</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#It is also reported to help mitigate the issue to change vm.zone_reclaim_mode to 1 or 3 if it&#x27;s set to zero, so that the system can reclaim back memory from cached memory, so that the system can flush pages to disk to reclaim, and so that reclaimations occur within the same NUMA node for performance.</span></span><br><span class="line">0       = Disable Zone reclaim, could reclaim from remote node</span><br><span class="line"><span class="comment">### file cache will cause performance issue, I suggest set it to 1</span></span><br><span class="line">2^0 = 1       = Zone reclaim on <span class="comment">## only local node reclaim</span></span><br><span class="line">2^1 = 2       = Zone reclaim writes dirty pages out</span><br><span class="line">2^2 = 4       = Zone reclaim swaps pages</span><br><span class="line"></span><br><span class="line"><span class="comment">#Enabling the zone reclaimer actively frees memory from kernel zones, when the zone becomes full</span></span><br><span class="line"><span class="comment">#The zone reclaimer can also be allowed to actively flush dirty pages</span></span><br><span class="line"><span class="comment">#which also prevents a heavily-writing process from dirtying other NUMA nodes</span></span><br><span class="line"><span class="comment">#I think reclaim writes tirty pages out will cause too many sync IO cause the kernel tcp buff traffic jam and waste memory too</span></span><br><span class="line">vm.zone_reclaim_mode = 3 (11=3)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Enable all</span></span><br><span class="line">vm.zone_reclaim_mode = 7 (111=7)</span><br><span class="line"></span><br><span class="line"><span class="comment"># suggest set the vm.zone_reclaim_mode to 1 to avoid the remote node, in my env, it &#x27;s the good choice</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /proc/sys/vm/zone_reclaim_mode</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#Interrupt Coalescing (soft IRQ) and Ingress QDisc</span></span><br><span class="line">net.core.netdev_max_backlog=8192</span><br><span class="line"><span class="comment">#the maximum number of packets, queued on the INPUT side (the ingress qdisc), when the interface receives packets faster than kernel can process them</span></span><br><span class="line"><span class="comment"># Sets the maximum number of packets allowed to queue when a particular interface receives packets faster than the kernel can process them.</span></span><br><span class="line"><span class="comment"># Each CPU core can hold a number of packets in a ring buffer before the network stack is able to process them. If the buffer is filled faster than TCP stack can process them, a dropped packet counter is incremented and they will be dropped. The net.core.netdev_max_backlog setting should be increased to maximize the number of packets queued for processing on servers with high burst traffic.</span></span><br><span class="line"></span><br><span class="line">                                             softirq                              s o f t --------- i r q</span><br><span class="line">             |normal workload   | syscall  | handle 0|               | syscall |handle 1|handle 2|handle 3|</span><br><span class="line">---CPU core--------------------------------------------------------------------------------------------------</span><br><span class="line">             |                             |                                   |</span><br><span class="line">             |                             |                                   |</span><br><span class="line">       <span class="built_in">disable</span> irqs                      poll                                 poll</span><br><span class="line">             |                             |                                   |</span><br><span class="line">             |                             |                                   |</span><br><span class="line">---NIC------------------------------------------------------------------------------------------------------</span><br><span class="line">                                  pkg1         pkg1 pkg2                pkg3</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.core.somaxconn=4096 &lt;--After Linux 4.3------------before Linux 4.3-------&gt; tcp_max_syn_backlog</span><br><span class="line">              ^                             |</span><br><span class="line">recv SYN----  |     send SYN+ACK            |</span><br><span class="line">           |  |      ^                      |</span><br><span class="line">           V  |      |                      |</span><br><span class="line">        SYN queue---------recv ACK-----&gt; Accept queue------&gt; accept()------&gt; Application</span><br><span class="line"></span><br><span class="line">                                            tcp_v4_syn_recv_sock()</span><br><span class="line">---SYN received tcp_conn_request()---      ----ACK received-------       -----accept() client socket</span><br><span class="line">                                    |      |                      |       |</span><br><span class="line">                                    OOOOOOOO                      OOOOOOOOO</span><br><span class="line">license socket |----------------------------|------------------------------|</span><br><span class="line">                      SYN backlog                     Accept queue</span><br><span class="line">                  <span class="keyword">if</span> syncookies enabled:      min(net.core.somaxconn, &lt;backlog&gt;)</span><br><span class="line">             min(net.core.somaxconn, &lt;backlog&gt;)   global maximum <span class="built_in">limit</span>:</span><br><span class="line">              global <span class="built_in">limit</span> <span class="keyword">if</span> no syncookies:      net.core.somaxconn</span><br><span class="line">             net.ipv4.tcp_max_syn_backlog           per-socket <span class="built_in">limit</span>:</span><br><span class="line">                                                    listen(&lt;backlog&gt;)</span><br><span class="line"></span><br><span class="line">https://theojulienne.io/2020/07/03/scaling-linux-services-before-accepting-connections.html</span><br><span class="line">https://blog.cloudflare.com/syn-packet-handling-in-the-wild</span><br><span class="line"></span><br><span class="line"><span class="comment">#The tale of two queues</span></span><br><span class="line">https://github.com/torvalds/linux/commit/ef547f2ac16bd9d77a780a0e7c70857e69e8f23f<span class="comment">#diff-56ecfd3cd70d57cde321f395f0d8d743L43</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SYN Queue</span><br><span class="line">The SYN Queue stores inbound SYN packets[1] (specifically: struct inet_request_sock). It<span class="string">&#x27;s responsible for sending out SYN+ACK packets and retrying them on timeout.</span></span><br><span class="line"><span class="string">Accept Queue</span></span><br><span class="line"><span class="string">The Accept Queue contains fully established connections: ready to be picked up by the application. When a process calls accept(), the sockets are de-queued and passed to the application.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Each slot in SYN Queue uses some memory. During a SYN Flood it makes no sense to waste resources on storing attack packets. Each struct inet_request_sock entry in SYN Queue takes 256 bytes of memory on kernel 4.14.</span></span><br><span class="line"><span class="string">The NIC transfer data by the frame, and the frame has be splitted in sk_buff</span></span><br><span class="line"><span class="string">./drivers/net/ethernet/intel/ixgb/ixgb.h:99</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">/* wrapper around a pointer to a socket buffer,</span></span><br><span class="line"><span class="string"> * so a DMA handle can be stored along with the buffer */</span></span><br><span class="line"><span class="string">struct ixgb_buffer &#123;</span></span><br><span class="line"><span class="string">        struct sk_buff *skb;</span></span><br><span class="line"><span class="string">        dma_addr_t dma;</span></span><br><span class="line"><span class="string">        unsigned long time_stamp;</span></span><br><span class="line"><span class="string">        u16 length;</span></span><br><span class="line"><span class="string">        u16 next_to_watch;</span></span><br><span class="line"><span class="string">        u16 mapped_as_page;</span></span><br><span class="line"><span class="string">&#125;;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">struct ixgb_desc_ring &#123;</span></span><br><span class="line"><span class="string">        /* pointer to the descriptor ring memory */</span></span><br><span class="line"><span class="string">        void *desc;</span></span><br><span class="line"><span class="string">        /* physical address of the descriptor ring */</span></span><br><span class="line"><span class="string">        dma_addr_t dma;</span></span><br><span class="line"><span class="string">        /* length of descriptor ring in bytes */</span></span><br><span class="line"><span class="string">        unsigned int size;</span></span><br><span class="line"><span class="string">        /* number of descriptors in the ring */</span></span><br><span class="line"><span class="string">        unsigned int count;</span></span><br><span class="line"><span class="string">        /* next descriptor to associate a buffer with */</span></span><br><span class="line"><span class="string">        unsigned int next_to_use;</span></span><br><span class="line"><span class="string">        /* next descriptor to check for DD status bit */</span></span><br><span class="line"><span class="string">        unsigned int next_to_clean;</span></span><br><span class="line"><span class="string">        /* array of buffer information structs */</span></span><br><span class="line"><span class="string">        struct ixgb_buffer *buffer_info;</span></span><br><span class="line"><span class="string">&#125;;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">/**</span></span><br><span class="line"><span class="string"> * ixgb_setup_rx_resources - allocate Rx resources (Descriptors)</span></span><br><span class="line"><span class="string"> * @adapter: board private structure</span></span><br><span class="line"><span class="string"> *</span></span><br><span class="line"><span class="string"> * Returns 0 on success, negative on failure</span></span><br><span class="line"><span class="string"> **/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">int</span></span><br><span class="line"><span class="string">ixgb_setup_rx_resources(struct ixgb_adapter *adapter)</span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">i40e_clean_rx_irq_zc - Consumes Rx packets from the hardware ring</span></span><br><span class="line"><span class="string">  i40e_construct_skb_zc - Create skbufff from zero-copy Rx buffer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">i40e_add_rx_frag - Add contents of Rx buffer to sk_buff</span></span><br><span class="line"><span class="string">  skb_add_rx_frag</span></span><br><span class="line"><span class="string">    skb_fill_page_desc</span></span><br><span class="line"><span class="string">      __skb_fill_page_desc - initialise a paged fragment in an skb</span></span><br><span class="line"><span class="string">https://wsgzao.github.io/post/rps/</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">packet -&gt; NIC -&gt; internal hardware buffer/ring buffer in main memory -&gt; hardware interrupt request -&gt; software interrupt operation -&gt;</span></span><br><span class="line"><span class="string">from buffer to network stack -&gt; forwarded/discarded/rejected/passed to a socket receive queue for an application -&gt;</span></span><br><span class="line"><span class="string">remove from network stack until no packets left in NIC buffer or a certain number of packets are transferred (/proc/sys/net/core/dev_weight)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                        network (packages)</span></span><br><span class="line"><span class="string">                         |---&lt;&lt;&lt;---5. new packages</span></span><br><span class="line"><span class="string">                         ||</span></span><br><span class="line"><span class="string">                        NIC  (hardware)-------------------------------------------------</span></span><br><span class="line"><span class="string">                         |                                                             |</span></span><br><span class="line"><span class="string">                         |                                                             V</span></span><br><span class="line"><span class="string">           ----------rx ring buffer(linux mem, fifo queue, ready/used status)&lt;--DMA copy the package to ring bufffer(Fetch description)</span></span><br><span class="line"><span class="string">           |             |          ^                       |</span></span><br><span class="line"><span class="string">           |             |          |                       |</span></span><br><span class="line"><span class="string">6.write packs to SKB(DMA)|          |                       |</span></span><br><span class="line"><span class="string">           |             |          |  Each ready descriptor point an empty sk_buff(point sk_buff fail/timeout means something slow ? overflow/fifo err ???)</span></span><br><span class="line"><span class="string">           |             |          |  The kernel driver speed &lt; NIC receive packets = the ring buffer full cause packet loss(overrun/fifo err/ proc/net/dev)</span></span><br><span class="line"><span class="string">           |             |          |  No mem resource, The fifo overruns caused by the rate at which the buffer gets full and the kernel isn&#x27;</span>t to reclaim the buffer</span><br><span class="line">           |             |          |  No cpu resource, interrupt not balance and not affinity, eg: all nic interrupt <span class="keyword">in</span> core0</span><br><span class="line">           |             |          |</span><br><span class="line">           |             |          |  6.write packet to SKB by DMA(hardware to driver)</span><br><span class="line">           |             |          |    The process: NIC send a hardware interrupt</span><br><span class="line">           |             |          |      --&gt; CPU receive the interrupt from drivers--&gt;kernel interrupt handler, send a soft interrupt</span><br><span class="line">           |             |          |        --&gt; kernel soft interrrupt hander --&gt;copy the SKB data to tcp/ip stacks</span><br><span class="line">           |             |          |</span><br><span class="line">           |             |         3. tell NIC there are some new descriptons</span><br><span class="line">           V             |                       ^</span><br><span class="line">    SKB|SKB|SKB(mem)     |                       |</span><br><span class="line">           ^             |                       |</span><br><span class="line">           |             |  ------&gt;2. Write description to rx ring</span><br><span class="line">           |             |  |</span><br><span class="line">  1.alloc sk_buff&lt;-----Driver  (kernel)  trigger a hardware interrupt to kernel( got packages from ring buffer)</span><br><span class="line">                         |               The NIC from the driver raised the IRQ to the kernel -- &gt; the kernel runs IRQ handler --&gt; NAPI started</span><br><span class="line">                         |</span><br><span class="line">                         |  &lt;---driver call into NAPI to start a poll loop <span class="keyword">if</span> NAPI was not running already</span><br><span class="line">                       NAPI or backlog (ksoftirqd call NAPI poll <span class="keyword">function</span> got the package from ring buffer)</span><br><span class="line">                         |  &lt;---ring buffer unmapped the memory from the package</span><br><span class="line">                         |  &lt;---Data that was DMA<span class="string">&#x27;d into memory is passed up the networking layer as an &#x27;</span>skb<span class="string">&#x27; for more processing.</span></span><br><span class="line"><span class="string">                         |  NAPI poller is added to poll_list -&gt; softirq_pending bit set -&gt;  run_ksoftirqd checks softirq_pending bit</span></span><br><span class="line"><span class="string">                         |                                                                                   |</span></span><br><span class="line"><span class="string">                         |                                                                                   -&gt; Registered handler called from softirq_vec handlers</span></span><br><span class="line"><span class="string">                         |   (from softnet_data Poll list)       (to softirq_pending bits)   (ksoftirqd/0 if pending -&gt; __do_softirq() -&gt; net_rx_action())</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                         | poll_list entry received -&gt; Budget and Elapsed Time Checked -&gt; Driver poll function called -&gt;  Packet harvested from ring buffer</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                         |  Without NAPI: 1 interrupt per packet → high CPU load</span></span><br><span class="line"><span class="string">                         |  With NAPI: polling during high packet arrival times</span></span><br><span class="line"><span class="string">                         |  No work to drop packets if kernel is too busy (Ring buffer overwrite by NIC)</span></span><br><span class="line"><span class="string">                         |  the ringbuffer is the RAM, if there is no special design, each ringbuffer(ring buff) store the package descriptor and it point the sk_buff (ready/used status)</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                       packet steering (Incoming network data frames are distributed among multiple CPUs if packet steering support)</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                         |  continue by &quot;Driver poll function called&quot; -&gt; Packets passed for possible GRO -&gt; Packets coalesced or passed on toward  protocol stacks</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                         |  (net_rx_action() -&gt; softnet_data Poll list -&gt; mydrv_poll() -&gt; napi_gro_receive() ---------&gt; net_receive_skb)</span></span><br><span class="line"><span class="string">                         |                                                    | Packet harvested from ring buffer   |  Packets coalesced or passed on toward protocol stacks</span></span><br><span class="line"><span class="string">                         |                                                    |---&gt;ring buffer                      |---&gt;GRO list</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                       network protocol stacks(TCP/IP Ethernet) (kernel, the data frames are handed to the protocol layers from the queues)</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                       socket buffer (kernel)</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                         |</span></span><br><span class="line"><span class="string">                       Application  (user)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">-------------------------------------------&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&lt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;------------------------------------------</span></span><br><span class="line"><span class="string">                               ---ethtool -g</span></span><br><span class="line"><span class="string">                               |             --ethtool -c</span></span><br><span class="line"><span class="string">                           frame buff        |</span></span><br><span class="line"><span class="string">NIC hardaware  ---DMA--- &gt;  rx_ring -|-softirq/inter sched--- &gt; IP firewall/IP routing                                              |</span></span><br><span class="line"><span class="string">                                     |                                    |                                                         |</span></span><br><span class="line"><span class="string">                                     |                                    | tcp_v4_recv                                             |</span></span><br><span class="line"><span class="string">                                     |                                    |                                                         |</span></span><br><span class="line"><span class="string">                                     |                                    v                                                         |</span></span><br><span class="line"><span class="string">                                     |                                    |                                                         |</span></span><br><span class="line"><span class="string">                                     |                                    | sock_backlog                                            |</span></span><br><span class="line"><span class="string">                                     |                                    |                                                         |</span></span><br><span class="line"><span class="string">                                     |                                    |                                                         |</span></span><br><span class="line"><span class="string">                                     |                                    ---recv backlog---&gt; TCP process---kernel recv buff---read-|-&gt;Application</span></span><br><span class="line"><span class="string">               driver                |                                     kernel |                                                 |     User</span></span><br><span class="line"><span class="string">                                                                                  ---&gt; netdev_max_backlog</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">所有网络协议栈的收包队列，网卡收到的所有报文都在 netdev backlog 队列中等待软中断处理，和中断频率一起影响收包速度从而影响收包带宽，以 netdev_backlog=300, 中断频率=100HZ 为例</span></span><br><span class="line"><span class="string">packets(300)  x  HZ(Timeslice freq)(100)    =  30000 packets/s</span></span><br><span class="line"><span class="string">packets(30000) x average (Bytes/packet)(1000) = 30M  throughput Bytes/s</span></span><br><span class="line"><span class="string">可以通过 /proc/net/softnet_stat 的第二列来验证, 如果第二列有计数, 则说明出现过 backlog 不足导致丢包</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">在最传统的中断模式下，每个帧将产生一次硬中断，CPU 0 收到硬中断后产生一个软中断，内核切换上下文进行协议栈的处理，理论上这是延迟最低的方案，但大量的软中断会消耗 CPU 资源，导致其他外设来不及正&gt;常响应，因此可以启用中断聚合（Interrupt Coalesce），多帧产生一个中断，ethtool -c [nic] 可以查看中断聚合状态</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">NAPI 是一种更先进的处理方式，NAPI 模式下网卡收到帧后会进入 polling mode 此时网卡不再产生更多的硬中断，内核的 ksoftirqd 在软中断的上下文中调用 NAPI 的 poll 函数从 ring buffer 收包，直到 rx_ring 为空或执行超过一定时间（如 ixgbe 驱动中定义超时为2个 CPU 时钟）</span></span><br><span class="line"><span class="string">内核将收到的包复制到一块新的内存空间，组织成内核中定义的 skb 数据结构，交上层处理</span></span><br><span class="line"><span class="string">https://www.starduster.me/2020/03/02/linux-network-tuning-kernel-parameter/#netipv4tcp_max_tw_buckets</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">NAPI Exit</span></span><br><span class="line"><span class="string">  No more NAPI poll structures to process</span></span><br><span class="line"><span class="string">    netdev_budget Exceeded</span></span><br><span class="line"><span class="string">         Each driver hardcoded budget for one NAPI structure of 64</span></span><br><span class="line"><span class="string">         Default is 300</span></span><br><span class="line"><span class="string">         Approximately 5 driver poll calls</span></span><br><span class="line"><span class="string">    softirq Time Window Exceeded</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">If no structures remain, re-enable IRQ interrupt</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#trace stack</span></span><br><span class="line"><span class="string">do_IRQ ----&gt; driver trigger the softirq, set NET_RX_SOFTIRQ flag</span></span><br><span class="line"><span class="string">  irq_exit  ----&gt; not now ??</span></span><br><span class="line"><span class="string">     do_softirq ----&gt; the kernel think it need to do softirq</span></span><br><span class="line"><span class="string">       __do_softirq</span></span><br><span class="line"><span class="string">          net_rx_action</span></span><br><span class="line"><span class="string">            i40e_napi_poll ----&gt; NAPI poll (loop check the network device), util 300 frames (net.core.netdev_budget) or timeout(net.core.netdev_budget_usecs)</span></span><br><span class="line"><span class="string">              i40e_clean_rx_irq</span></span><br><span class="line"><span class="string">                napi_gro_receive</span></span><br><span class="line"><span class="string">                  netif_receive_skb(no RPS, only RPS need netdev_max_backlog)</span></span><br><span class="line"><span class="string">                    -&gt; netif_receive_skb_internal-&gt; malloc sk_buff, write metadata, remove the others, NET_RX_DROP for congestion control or protocol layer</span></span><br><span class="line"><span class="string">                    __netif_receive_skb ----&gt; set the skb to kernel tcp protocol stack</span></span><br><span class="line"><span class="string">                                    -&gt; if (sk_memalloc_socks() &amp;&amp; skb_pfmemalloc(skb)) ret = __netif_receive_skb_core(skb, true);</span></span><br><span class="line"><span class="string">                                                 --&gt;static_key_false</span></span><br><span class="line"><span class="string">                                    -&gt; tsk_restore_flags(current, pflags, PF_MEMALLOC); else ret = __netif_receive_skb_core(skb, false);</span></span><br><span class="line"><span class="string">                      __netif_receive_skb_core ----&gt; to protocol stack or tcpdump tap, goto qdisc (netdev_max_backlog)</span></span><br><span class="line"><span class="string">                               -&gt; net_timestamp_check</span></span><br><span class="line"><span class="string">                               -&gt; deliver_skb -&gt; ENOMEM DROP</span></span><br><span class="line"><span class="string">                               -&gt; static_key_false -&gt; sch_handle_ingress</span></span><br><span class="line"><span class="string">                               -&gt; pfmemalloc &amp;&amp; !skb_pfmemalloc_protocol -&gt; DROP</span></span><br><span class="line"><span class="string">                                   -&gt; Limit the use of PFMEMALLOC reserves to those protocols that implement the special handling of PFMEMALLOC skbs.</span></span><br><span class="line"><span class="string">                               -&gt; skb_orphan_frags orphan the frags contained in a buffer</span></span><br><span class="line"><span class="string">                                   For each frag in the SKB which needs a destructor (i.e. has an owner)</span></span><br><span class="line"><span class="string">                                   create a copy of that frag and release the original page by calling the destructor.</span></span><br><span class="line"><span class="string">                                   -&gt; skb_copy_ubufs to copy page and release the orgi page</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">net.core.dev_weight=128</span></span><br><span class="line"><span class="string"># the maximum number of packets that kernel can handle on a NAPI interrupt, it&#x27;</span>s a Per-CPU variable. For drivers that support LRO or GRO_HW, a hardware aggregated packet is counted as one packet <span class="keyword">in</span> this.</span><br><span class="line"></span><br><span class="line">net.core.netdev_budget=600</span><br><span class="line"><span class="comment">#is the maximum number of packets, queued on the INPUT side (the ingress qdisc), when the interface receives packets faster than kernel can process them.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## centos 7 not support, default value</span></span><br><span class="line"><span class="comment">#net.core.netdev_budget_usecs = 8000</span></span><br><span class="line"><span class="comment">#netdev_budget_usecs maximum number of microseconds in one NAPI polling cycle. Polling will exit when either netdev_budget_usecs have elapsed during the poll cycle or the number of packets processed reaches netdev_budget</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_reordering=128</span><br><span class="line"><span class="comment">## centos 7 not support, default value</span></span><br><span class="line"><span class="comment">#net.ipv4.tcp_max_reordering = 300</span></span><br><span class="line"><span class="comment">#Maximal reordering level of packets in a TCP stream. 300 is a fairly conservative value, but you might increase it if paths are using per packet load balancing (like bonding rr mode) Default: 300</span></span><br><span class="line"><span class="comment">## CentOS 7</span></span><br><span class="line"><span class="comment">#define TCP_MAX_REORDERING      127</span></span><br><span class="line">tcp_update_reordering -&gt; tp-&gt;reordering = min(TCP_MAX_REORDERING, metric);</span><br><span class="line"></span><br><span class="line">tcp_update_metrics-&gt;</span><br><span class="line">                /* Else slow start did not finish, cwnd is non-sense,</span><br><span class="line">                 * ssthresh may be also invalid.</span><br><span class="line">                 */</span><br><span class="line">-&gt;                             tp-&gt;reordering != sysctl_tcp_reordering)</span><br><span class="line">                                tcp_metric_set(tm, TCP_METRIC_REORDERING,</span><br><span class="line">                                               tp-&gt;reordering);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net.ipv4.neigh.default.base_reachable_time_ms=30000000</span><br><span class="line">net.ipv4.neigh.bond0.base_reachable_time_ms=30000000</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_slow_start_after_idle=0</span><br><span class="line"><span class="comment">#disable tcp slow start in the LAN</span></span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_fastopen=3</span><br><span class="line"></span><br><span class="line"><span class="comment">#avoid the bad udp package</span></span><br><span class="line">net.inet.udp.checksum=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#If set, TCP performs receive buffer auto-tuning, attempting to automatically size the buffer (no greater than tcp_rmem[2]) to match the size required by the path for full throughput. Enabled by default. TCP performs receive buffer auto-tuning, attempting to automatically size the buffer</span></span><br><span class="line">net.ipv4.tcp_moderate_rcvbuf = 1</span><br><span class="line"></span><br><span class="line"><span class="comment">#disable ipv6</span></span><br><span class="line">net.ipv6.conf.all.disable_ipv6=1</span><br><span class="line">net.ipv6.conf.default.disable_ipv6=1</span><br><span class="line"></span><br><span class="line"><span class="comment">#default buffer is good enough for 10/25GbE in the low latency env, don&#x27;t to reset them</span></span><br><span class="line"><span class="comment">#net.core.rmem_default=425984</span></span><br><span class="line"><span class="comment">#net.core.rmem_max=425984</span></span><br><span class="line"><span class="comment">#net.core.wmem_default=425984</span></span><br><span class="line"><span class="comment">#net.core.wmem_max=425984</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#calculate</span></span><br><span class="line"><span class="comment">#Buffer size = Bandwidth (bits/s) * RTT (seconds)  it &#x27;s not good for some cases. the large buffer will impact the latency</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#net.ipv4.tcp_mem=6170046 8226732 12340092</span></span><br><span class="line"><span class="comment">#net.ipv4.tcp_rmem=8192       164760   12582912</span></span><br><span class="line"><span class="comment">#net.ipv4.tcp_wmem=8192       32768   8388608</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># default value</span></span><br><span class="line"><span class="comment">#net.core.rmem_default=212992</span></span><br><span class="line"><span class="comment">#This parameter controls the maximum size in bytes of a socket&#x27;s receive buffer. Use getsockopt to get current value</span></span><br><span class="line"><span class="comment">#net.core.rmem_max=212992</span></span><br><span class="line"><span class="comment">#This parameter controls the default size of the receive buffer used by sockets. This value must be smaller than or equal to the value of /proc/sys/net/core/rmem_max.</span></span><br><span class="line"><span class="comment">#This parameter controls the maximum size in bytes of a socket&#x27;s receive buffer. Use the getsockopt system call to determine the current value of the buffer.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#net.core.wmem_default=212992</span></span><br><span class="line"><span class="comment">#net.core.wmem_max=212992</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#ubuntu 20.04 default</span></span><br><span class="line">net.core.wmem_max = 16154422</span><br><span class="line">net.core.rmem_max = 16154422</span><br><span class="line">                                           -----------$((<span class="number">2097152</span>*<span class="number">4096</span>/<span class="number">1024</span>/<span class="number">1024</span>/<span class="number">1024</span>))=8G</span><br><span class="line">                                           |</span><br><span class="line">net.ipv4.tcp_mem = 32768        131072  2097152</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_rmem = 32768       65536   1048576</span><br><span class="line">第一列表示每个 TCP socket 的最小收包缓冲，这个值用于系统内存紧张时保证最低限度的连接建立，注意指定过 SO_RCVBUF 的 socket 不受此参数限制。</span><br><span class="line">第二列表示每个 TCP socket 的默认收包缓冲，此数值将会覆盖全局参数 net.core.rmem_default （定义了所有协议的接收缓冲）</span><br><span class="line">第三列表示每个 TCP socket 最大收包缓冲，注意指定过 SO_RCVBUF 的 socket 不受此参数限制。此数值 不覆盖 全局参数 net.core.rmem_max，此数值的默认值由 max(87380, min(4 MB, tcp_mem[1]*PAGE_SIZE/128)) 得到</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_wmem = 32768       65536   1048576</span><br><span class="line">第一列，指定过 SO_SNDBUF 的 socket 不受此参数限制。</span><br><span class="line">第二列，此数值会覆盖全局参数 net.core.wmem_default （定义所有协议的发送缓冲</span><br><span class="line">第三列，此数值 不覆盖 全局参数 net.core.wmem_max ，此数值的默认值由 max(65536, min(4 MB, tcp_mem[1]*PAGE_SIZE/128)) 得到</span><br><span class="line"></span><br><span class="line"><span class="comment"># min (size used under memory pressure), default (initial size), max (maximum size) - size of receive buffer used by TCP sockets.</span></span><br><span class="line"><span class="comment"># default (initial size), max (maximum size) - size of send buffer used by TCP sockets.</span></span><br><span class="line"><span class="comment"># the number of pages(default 4K) allocated falls below the low, pressure, high mark.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#net.ipv4.tcp_early_retrans=3 (default)</span></span><br><span class="line">Probe Timeout(PTO), TCPLossProbes</span><br><span class="line"></span><br><span class="line"><span class="comment">#Egress qdisc</span></span><br><span class="line"><span class="comment">#default_qdisc is the default queuing discipline to use for network devices</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#default</span></span><br><span class="line">$ <span class="built_in">cat</span> /proc/sys/net/core/default_qdisc</span><br><span class="line">pfifo_fast</span><br><span class="line"></span><br><span class="line">$ sysctl net.core.default_qdisc</span><br><span class="line">pfifo_fast/fq_codel/fq</span><br><span class="line"></span><br><span class="line"><span class="comment">#This tunable is not present within RHEL kernels</span></span><br><span class="line"><span class="comment">#This tunable is present in Red Hat MRG Realtime kernels</span></span><br><span class="line"><span class="comment">#Control TCP delayed ACK and delayed sending</span></span><br><span class="line">https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_for_real_time/7/html/tuning_guide/reducing_the_tcp_delayed_ack_timeout</span><br><span class="line"></span><br><span class="line">Quick ACK</span><br><span class="line">This mode is used at the start of a TCP connection so that the congestion window can grow quickly.</span><br><span class="line">The acknowledgment (ACK) <span class="built_in">timeout</span> interval (ATO) is <span class="built_in">set</span> to tcp_ato_min, the minimum <span class="built_in">timeout</span> value.</span><br><span class="line"><span class="built_in">echo</span> 4 &gt; /proc/sys/net/ipv4/tcp_ato_min (milliseconds)</span><br><span class="line"></span><br><span class="line">Delayed ACK</span><br><span class="line">After the connection is established, TCP assumes this mode, <span class="keyword">in</span> <span class="built_in">which</span> ACKs <span class="keyword">for</span> multiple received packets can be sent <span class="keyword">in</span> a single packet.</span><br><span class="line">ATO is <span class="built_in">set</span> to tcp_delack_min to restart or reset the timer.</span><br><span class="line"><span class="built_in">echo</span> 4 &gt; /proc/sys/net/ipv4/tcp_delack_min</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line">setsockopt with TCP_NODELAY flag</span><br><span class="line"></span><br><span class="line">TCP_NODELAY: If <span class="built_in">set</span>, <span class="built_in">disable</span> the Nagle algorithm. This means that segments are always sent as soon as possible, even <span class="keyword">if</span> there is only a small amount of data. When not <span class="built_in">set</span>, data is buffered <span class="keyword">until</span> there is a sufficient amount to send out, thereby avoiding the frequent sending of small packets, <span class="built_in">which</span> results <span class="keyword">in</span> poor utilization of the network. This option is overridden by TCP_CORK; however, setting this option forces an explicit flush of pending output, even <span class="keyword">if</span> TCP_CORK is currently <span class="built_in">set</span>.</span><br></pre></td></tr></table></figure>

<p>sysctl2.conf</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kernel.unknown_nmi_panic = 1</span><br><span class="line">kernel.panic_on_unrecovered_nmi = 1</span><br><span class="line">kernel.panic_on_io_nmi = 1</span><br><span class="line">kernel.nmi_watchdog = 0</span><br><span class="line">dev.raid.speed_limit_max=8000000</span><br><span class="line">dev.raid.speed_limit_min=2000000</span><br><span class="line"></span><br><span class="line">net.ipv4.neigh.default.base_reachable_time_ms=30000000</span><br><span class="line">net.ipv4.neigh.bond0.base_reachable_time_ms=30000000</span><br><span class="line">net.ipv4.tcp_slow_start_after_idle=0</span><br><span class="line">net.ipv4.tcp_fastopen=3</span><br><span class="line">net.core.netdev_max_backlog=4096</span><br><span class="line">net.core.somaxconn=4096</span><br><span class="line">net.core.dev_weight=128</span><br><span class="line">net.core.netdev_budget=600</span><br><span class="line">flow_limit_table_len = 16384</span><br><span class="line">vm.min_free_kbytes=700000</span><br><span class="line"></span><br><span class="line">net.core.rmem_max = 851968</span><br><span class="line">net.core.wmem_max = 425984</span><br><span class="line">net.core.rmem_default = 212992</span><br><span class="line">net.core.wmem_default = 212992</span><br><span class="line">net.ipv4.tcp_rmem = 4096        87380   6291456</span><br><span class="line">net.ipv4.tcp_wmem = 4096        16384   4194304</span><br><span class="line">net.ipv4.tcp_mem =  1529979     2039975 3059958</span><br></pre></td></tr></table></figure>

<ul>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/47450231/what-is-the-relationship-of-dma-ring-buffer-and-tx-rx-ring-for-a-network-card">The Ring Buffer</a> Contains Start and End Address of Buffer in RAM. TX Ring will contain addresses of Buffer in RAM that contains data to be transmitted. RX Ring will contains address of Buffer in RAM where NIC will place data.These rings are present in RAM.TX buffer and RX buffer are in RAM pointed by TX&#x2F;RX rings. Now Network Card Register has Location of Rings Buffer in RAM.  </li>
<li><a target="_blank" rel="noopener" href="https://www.flamingbytes.com/2021/03/04/ring-buffer/">Receive ring buffers are shared between the device driver and NIC</a></li>
<li>The device driver drains the RX ring, typically via SoftIRQs which puts the incoming packets into a kernel data structure called an sk_buff or “skb” to begin its journey through the kernel and up to the application which owns the relevant socket.</li>
<li>The TX ring buffer is used to hold outgoing packets which are destined for the wire. When a NIC receives incoming data, it copies the data into kernel buffers using DMA. The NIC notifies the kernel of this data by raising a hard interrupt.  These interrupts are processed by interrupt handlers which do minimal work, as they have already interrupted another task and cannot be interrupted themselves. Hard interrupts can be expensive in terms of CPU usage, especially when holding kernel locks. The hard interrupt handler then leaves the majority of packet reception to a software interrupt, or SoftIRQ, process which can be scheduled more fairly.</li>
<li>High end ethernet cards(eg: Mellanox) will almost certainly have their own memory and microprocessors to offload work from the rest of the computer. BTW: if tcp stack offload by NIC and bypass kernel that means the ring buffer in NIC.</li>
<li>Low end ethernet cards may not have their own onboard memory or microprocessors, and will use the host system’s resources to handle network traffic.<ul>
<li>BTW: I think if you NIC adapter could not offload anything, that means all packages will process by linux, some low end ethernet adapters could bypass kernel too, could use direct memory access for some of IO</li>
</ul>
</li>
<li>Infiniband cards on the other hand will tend to have their own onboard processors but no onboard memory, and will use direct memory access for all IO.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#calculate CALCULATE</span></span><br><span class="line"><span class="comment">#Consider 9K jumbo frames, how long time will it take to empty 1024 jumbo frames on a 10G link:</span></span><br><span class="line">(9000*8)/(10000*10^6)*1000*1024 = 7.37ms</span><br><span class="line">But with 9K MTU and 512, we already have:</span><br><span class="line"> (9000*8)/(10000*10^6)*1000*512 = 3.69ms</span><br><span class="line">Guess the more normal use-case would be 1500+38 (Ethernet overhead)</span><br><span class="line"> (1538*8)/(10000*10^6)*1000*1024 = 1.25ms</span><br><span class="line">   |         |               |</span><br><span class="line">   |         |               -------1024 x Frames, the ring buffer</span><br><span class="line">   |         ---------10GbE</span><br><span class="line">   ----1500 + 38 (ethernet overhead)</span><br><span class="line"></span><br><span class="line">On top of V1 patchset:</span><br><span class="line"> - 6,747,016 pps - rx-usecs:  1 tx-ring: 1024 (irqs: 9492)</span><br><span class="line"> - 6,684,612 pps - rx-usecs: 10 tx-ring: 1024 (irqs:99322)</span><br><span class="line"> - 7,005,226 pps - rx-usecs: 20 tx-ring: 1024 (irqs:50444)</span><br><span class="line"> - 7,113,048 pps - rx-usecs: 30 tx-ring: 1024 (irqs:34004)</span><br><span class="line"> - 7,133,019 pps - rx-usecs: 40 tx-ring: 1024 (irqs:25845)</span><br><span class="line"> - 7,168,399 pps - rx-usecs: 50 tx-ring: 1024 (irqs:20896)</span><br><span class="line"></span><br><span class="line">Look same performance with 512 TX ring.</span><br><span class="line"></span><br><span class="line">Lowering TX ring size to (default) 512:</span><br><span class="line"> (On top of V1 patchset)</span><br><span class="line"> - 3,934,674 pps - rx-usecs:  1 tx-ring: 512 (irqs: 9602)</span><br><span class="line"> - 6,684,066 pps - rx-usecs: 10 tx-ring: 512 (irqs:99370)</span><br><span class="line"> - 7,001,235 pps - rx-usecs: 20 tx-ring: 512 (irqs:50567)</span><br><span class="line"> - 7,115,047 pps - rx-usecs: 30 tx-ring: 512 (irqs:34105)</span><br><span class="line"> - 7,130,250 pps - rx-usecs: 40 tx-ring: 512 (irqs:25741)</span><br><span class="line"> - 7,165,296 pps - rx-usecs: 50 tx-ring: 512 (irqs:20898)</span><br><span class="line"></span><br><span class="line">Look how even a 256 TX ring is enough, <span class="keyword">if</span> we cleanup the TX ring fast ennough, and how performance decrease <span class="keyword">if</span> we cleanup to slowly.</span><br><span class="line"></span><br><span class="line">Lowering TX ring size to (default) 256:</span><br><span class="line"> (On top of V1 patchset)</span><br><span class="line"> - 1,883,360 pps - rx-usecs:  1 tx-ring: 256 (irqs: 9800)</span><br><span class="line"> - 6,683,552 pps - rx-usecs: 10 tx-ring: 256 (irqs:99786)</span><br><span class="line"> - 7,005,004 pps - rx-usecs: 20 tx-ring: 256 (irqs:50749)</span><br><span class="line"> - 7,108,776 pps - rx-usecs: 30 tx-ring: 256 (irqs:34536)</span><br><span class="line"> - 5,734,301 pps - rx-usecs: 40 tx-ring: 256 (irqs:25909)</span><br><span class="line"> - 4,590,093 pps - rx-usecs: 50 tx-ring: 256 (irqs:21183)</span><br><span class="line"></span><br><span class="line"><span class="comment">## disable NAPI </span></span><br><span class="line">$ ethtool -C ethx rx-usecs 0 <span class="comment">##if disabled, tons of CPU interrupts</span></span><br><span class="line"></span><br><span class="line">1500 MTU</span><br><span class="line">Ethernet header 14 Bytes----IP header 20 Bytes----TCP header 20 Bytes----Payload 1460 Bytes----FCS 4 Bytes</span><br><span class="line">                            |-------------------- -ethernet 1500 mtu-----------------------|</span><br><span class="line">                            |------------------------ip mtu--------------------------------|</span><br><span class="line">                                                                         |-----TCP MSS-----|</span><br><span class="line"></span><br><span class="line">Larger MTU is associated with reduced overhead. Smaller MTU values can reduce network delay.</span><br><span class="line">Ethernet Frame 1500 + 14 Ethernet header + 8 VXLAN header  + 8 UDP header + 20 IP header = 1550 bytes</span><br><span class="line">capture show the total length = 1550 B</span><br><span class="line"></span><br><span class="line">网卡驱动知道 2 层的 MTU 是多少；</span><br><span class="line">3 层协议栈 IP 会问网卡驱动 MTU 是多少；</span><br><span class="line">4 层协议 TCP 会问 IP Max Datagram Data Size (MDDS) 是多少；</span><br><span class="line">在 TCP 的握手阶段， SYN 包里面的 TCP option 字段中，会带有 MSS，如果不带的话，default 是 536. 对方也会把 MSS 发送过来，这时候两端会比较 MSS 值，都是选择一个最小的值作为传输的 MSS.</span><br><span class="line"></span><br><span class="line"><span class="built_in">set</span> TCP MSS</span><br><span class="line">iptables -I OUTPUT -p tcp -m tcp --tcp-flags SYN,RST SYN -j TCPMSS --set-mss 48</span><br><span class="line">ip route change 192.168.11.0/24 dev ens33 proto kernel scope <span class="built_in">link</span> src 192.168.11.111 metric 100 advmss 48</span><br><span class="line"></span><br><span class="line">from scapy.all import *</span><br><span class="line">ip = IP(dst=<span class="string">&quot;192.168.11.112&quot;</span>)</span><br><span class="line">tcp = TCP(dport=80, flags=<span class="string">&quot;S&quot;</span>,options=[(<span class="string">&#x27;MSS&#x27;</span>,48),(<span class="string">&#x27;SAckOK&#x27;</span>, <span class="string">&#x27;&#x27;</span>)])</span><br><span class="line"></span><br><span class="line">ifconfig eth0 mtu 800 up</span><br><span class="line"></span><br><span class="line">Layer 2 二层协议一般都很简答，如果收到了超过 MTU 的包，一般会简单地 drop 掉</span><br><span class="line">Layer 3 所以 IP 支持一个 feature 叫做 IP Fragmentation.</span><br><span class="line">如果 IP Packet 超过了 1500bytes，IP 协议会将这个 packet 的 data 段拆分成多个，每一个分别上 IP header，以及 fragment header 标志这是拆分成的第几段。 接受端等收到所有的 IP 分片之后，再组装成完整的数据。</span><br><span class="line"></span><br><span class="line">IP Fragmentation is generally a BAD thing</span><br><span class="line">同上面提到的 MTU 为什么是 1500 一样的问题：假设拆分成了 3 个包，丢了一个包就相当于全丢了，丢包率直接变成（假设丢包率是 10%，那么3个包都不丢的概率就是 90%^3=72.9%）27%；</span><br><span class="line">导致 TCP 乱序：现在网络很多设备都是针对 TCP 做优化的，比如，根据 TCP 的 port number 去 <span class="built_in">hash</span> 到同一条路由上去，减少 TCP reorder 的概率。但是如果 IP fragmentation 发生的话，后续的 IP 包在路由器看来并不是 TCP 包，因为 TCP header 只在第一个 fragment 上才有，所以会导致 <span class="built_in">hash</span> 失效，从而更容易发生 TCP 乱序；另外，对段会等齐所有的 fragment 到达才会交给上层，这也导致了延迟增加和乱序的发生；</span><br><span class="line">产生一些比较难 debug 的问题；</span><br><span class="line">不是所有系统都能处理 IP Fragmentation，比如 Google GCE；</span><br><span class="line"></span><br><span class="line">DF(Don’t fragment bit)</span><br><span class="line">IP 协议的 header 中有一位 bit 叫做 DF，如果这个 bit 设置了，就是告诉中间的路由设备不要分片发送这个包，如果大于最大传输单元的大小，直接丢弃即可。丢弃这个包的设备会发回一个 ICMP 包，其中，<span class="built_in">type</span>=3 Destination Unreachable, Code=4 Fragmentation required, and DF flag <span class="built_in">set</span>. RFC 1191</span><br><span class="line">用 tcpdump 我们可以这么抓 ICMP 包：tcpdump -s0 -p -ni eth0 <span class="string">&#x27;icmp and icmp[0] == 3 and icmp[1] == 4&#x27;</span></span><br><span class="line"></span><br><span class="line">还可能有另一个问题，有些 DC 可能用了 ECMP 技术，简单来说，一个 IP 后面有多个服务器，ECMP 会根据 TCP 端口，和 IP 来做 <span class="built_in">hash</span>，这样可以根据 IP + Port 来保证路由到正确的 Server 上，即使 IP 一样。但是对于 ICMP 包来说就有问题了，ICMP error 包可能被路由到了错误的服务器上，导致 PMTUD 失败。</span><br><span class="line">https://blog.cloudflare.com/path-mtu-discovery-in-practice/</span><br><span class="line"></span><br><span class="line">明明看到MSS=1460, 实际抓包total length: 10188 Bytes</span><br><span class="line">ethtool -K eth0 tx off</span><br><span class="line"></span><br><span class="line">一种奇怪的“三角路由”，其中 Router 会添加 50bytes 的额外 header，然后发现 Router 这里发生了丢包。</span><br><span class="line">最后发现，原因是我们对 eth0 设置了 MTU = 1450，但是忘记设置 ip route，导致握手阶段的包从 eth1 出去了，eth1 的默认 MTU 是 1500，PC 发送的 MTU(MSS actually) 也是 1500，就导致双方一致认为 MTU=1500，MSS=1460. 但是实际上到 Rrouter 这里加了 50bytes 的 overhead，就造成了丢包。</span><br><span class="line"></span><br><span class="line">https://www.kawabangga.com/posts/4983</span><br></pre></td></tr></table></figure>

<ul>
<li>txqueuelen &#x3D; qdisc length &#x3D; default 1000 x skb</li>
<li><a target="_blank" rel="noopener" href="https://www.nas.nasa.gov/assets/pdf/papers/NAS_Technical_Report_NAS-2014-01.pdf">This queue parameter is mostly applicable for high-speed WAN transfers. For low-latency networks, the default setting of 1000 is sufficient. The receiving end is configured with the sysctl setting net.core.netdev_max_backlog. The default for this setting is also 1000 and does not need to be modified unless there is significant latency</a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#calculate</span></span><br><span class="line">$ ip <span class="built_in">link</span> <span class="built_in">set</span> dev bond0 txqueuelen 1500</span><br><span class="line">$ ip <span class="built_in">link</span> <span class="built_in">set</span> dev enp5s0 txqueuelen 1500</span><br><span class="line">$ ip <span class="built_in">link</span> <span class="built_in">set</span> dev enp5s0d1 txqueuelen 1500</span><br><span class="line"></span><br><span class="line">/* Default TSQ <span class="built_in">limit</span> of four TSO segments */</span><br><span class="line">net.ipv4.tcp_limit_output_bytes = 262144</span><br></pre></td></tr></table></figure></li>
</ul>
<p><a target="_blank" rel="noopener" href="https://patchwork.ozlabs.org/project/netdev/patch/20140514141748.20309.83121.stgit@dragon/">About rx,tx-usecs, here is the test, just for reference</a><br><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/17154759/how-much-memory-does-a-common-nic-have">how much memroy does a common nic have</a><br><a target="_blank" rel="noopener" href="https://ylgrgyq.github.io/2017/07/23/linux-receive-packet-1/">linux receive packet</a><br><a target="_blank" rel="noopener" href="https://people.redhat.com/pladd/MHVLUG_2017-04_Network_Receive_Stack.pdf">NIC Data Processing</a><br><img src="/img/ring-buffer.png"><br><img src="/img/2012110119582618-tcp-reception.png"><br><img src="/img/2012110119593557-tcp-transmission.png"></p>
<h3 id="SLE-and-SER-in-SACK-RFC2018"><a href="#SLE-and-SER-in-SACK-RFC2018" class="headerlink" title="SLE and SER in SACK(RFC2018)"></a>SLE and SER in SACK(RFC2018)</h3><ul>
<li><p>tcpdump will show some packet show “SLE&#x3D;20480 SER&#x3D;24576”</p>
<ul>
<li>SLE: Sequence Left Edge of already acknowledged data when Selective Acknowledgments are used</li>
<li>SRE: Sequence Right Edge of already acknowledged data when Selective Acknowledgments are used</li>
</ul>
</li>
<li><p>When server send the package length from 1~24576, the client just return ack&#x3D;$((seq+18736))</p>
<ul>
<li>The client receive 18736 bytes, and recevice the out of order packets from 20480<del>24576 bytes, and just loss from 18737</del>20479 Bytes</li>
<li>The server don ‘t need to retrans 20480 to 24576 bytes. if the system not enable sack, it will retrans from 18737~24576</li>
</ul>
</li>
</ul>
<h3 id="TOOLS-Tools-tools"><a href="#TOOLS-Tools-tools" class="headerlink" title="TOOLS Tools tools"></a>TOOLS Tools tools</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ ss -tan state time-wait</span><br><span class="line">$ dmesg | grep &quot;TCP established hash table&quot;</span><br><span class="line">[    1.041036] TCP established hash table entries: 524288 (order: 10, 4194304 bytes)</span><br><span class="line">$ dmesg | grep &quot;TCP bind hash table&quot;</span><br><span class="line">[    1.041724] TCP bind hash table entries: 65536 (order: 8, 1048576 bytes)</span><br><span class="line"></span><br><span class="line">$ slabtop -o | grep -E &#x27;(^  OBJS|tw_sock_TCP|tcp_bind_bucket)&#x27;</span><br><span class="line"></span><br><span class="line">$ ethtool -s p4p2 speed 1000 duplex full autoneg off</span><br><span class="line">$ grep ETHTOOL_OPTS /etc/sysconfig/network-scripts/ifcfg-p4p1</span><br><span class="line">ETHTOOL_OPTS=speed 1000 duplex full</span><br><span class="line">$ systemctl restart network</span><br><span class="line">$ ifconfig eth0 txqueuelen 3000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#promisc mode</span><br><span class="line">$ ip link set eth2 allmulticast on</span><br><span class="line"></span><br><span class="line">#add vlan to vf interface</span><br><span class="line">$ ip link add link eth2 name eth2.100 type vlan id 100</span><br><span class="line"></span><br><span class="line">$ ifconfig ethX txqueuelen 1500</span><br><span class="line">txqueuelen is the maximum number of packets, queued on the OUTPUT side.</span><br><span class="line">default it &#x27;s the 1000 x skb, if it too long cause the bufferbloat</span><br><span class="line">[如果没有一种机制公平地限定各个连接的发送数量，底层的qdisc/网卡队列就会被高发包率的应用占用，同时造成上层tcp计算RTT和cwnd的偏差，以及bufferbloat问题](http://www.cnhalo.net/2016/09/13/linux-tcp-small-queue/)</span><br><span class="line"></span><br><span class="line">http://blog.fengidri.me/2019/07/11/bbr/</span><br><span class="line">所以有了拥塞算法. 拥塞算法, 我认为就是做了一件事, 我一直发的话, 什么时候要停下来.</span><br><span class="line">这个值叫做发送窗口 cwnd.</span><br><span class="line">这个值的模型一直都是对于网络容量(BDP)进行估算到 BBR 也没有改变这一点. 也就是网络上可以放多少</span><br><span class="line">包.</span><br><span class="line">所以核心的问题就是测试 BDP 的大小. 应用这个值到发送窗口.</span><br><span class="line">BBR 之前的所有的算法都是当出现了丢包的时候认为网络已经满了. 在出现丢包之前一点点地增加发送窗口.</span><br><span class="line">对于丢包的认识 30 年前应该没有什么问题, 但是现在已经不一样的. 除了 BDP 满了, 还有一些错误包之类这些都会出现丢包.</span><br><span class="line">而 BBR 一个改动就是不再依赖丢包来判断 BDP. 它不再关心丢包这个事情. BBR 采用的方式是分别计算带宽和延迟来得到 BDP.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="tcp-retrans"><a href="#tcp-retrans" class="headerlink" title="tcp retrans"></a><a target="_blank" rel="noopener" href="http://perthcharles.github.io/2015/09/07/wiki-tcp-retries/">tcp retrans</a></h3><p>RTO(retransmission timeout)<br>RRT(round-trip time)<br>TCP keep the weight value SRTT(smoothed RRT)</p>
<p>The latest RRTS &#x3D; (1-a) * ( last RTTS) + a * (the lastest RRT)<br>In RFC2988, a &#x3D; 1&#x2F;8<br>RTTD was the RTT deviation weight value<br>First get RTTD, RTTD value &#x3D; RRT&#x2F;2<br>Next the RTTD&#x3D;(1-B)<em>(the last RTTD) + B(RTTS-the lastest RRT)<br>RFC B&#x3D;1&#x2F;4<br>RTO&#x3D; RTTS + 4</em>RTTD<br>The RFC suggest if the message retrans, this RRT value will be drop<br>When the trigger tcp retrans, the latest RTO * 2 &#x3D; the last RTO until the maximum</p>
<p>But RFC !&#x3D; the real implementation</p>
<p><a target="_blank" rel="noopener" href="https://ms2008.github.io/2017/04/14/tcp-timeout/">tcp_syn_retries 三次握手</a><br>net.ipv4.tcp_syn_retries retrans interval was [1,3,7,15,31]s   </p>
<p>tcp_syn_retries2  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line">#if tcp_retries2=15，the timeout was 924600ms</span><br><span class="line">#if 924.6s too long, application set the SocketTimeout</span><br><span class="line">#setsockopt(sock, IPPROTO_TCP, TCP_USER_TIMEOUT, (char *)&amp;timeout, sizeof(timeout)); &lt;---set timeout</span><br><span class="line">#</span><br><span class="line">#reset the value</span><br><span class="line">net.ipv4.tcp_retries2 = 10    #default 15, in the lower latency network , maybe 10 is the good choice</span><br><span class="line">$ awk &#x27;BEGIN&#123;print (1+2+4+8+16+32+64+128+256+512)*0.2+120&#125;&#x27; </span><br><span class="line">204.6</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_syn_retries = 4  #default 6</span><br><span class="line"></span><br><span class="line">#https://perthcharles.github.io/2015/09/07/wiki-tcp-retries/</span><br><span class="line"></span><br><span class="line">1. If RTT is tiny that the RTO init value was about 200ms</span><br><span class="line">   The total timeout was 924600ms，looks like retrans 15 times cause over the timeout value and end the tcp stream</span><br><span class="line"></span><br><span class="line">2. If RTT is large,the RTO calculate value was 1000ms</span><br><span class="line">   it doesn&#x27; t need retrans 15 times，the timeout value will over 924600ms.</span><br><span class="line">   if the RTT=400ms,set tcp_retries2=10, only 3 times the tcp stream will end</span><br><span class="line"></span><br><span class="line"># 在Linux3.10中，如果tcp_retres2设置为15。总重传超时周期应该在如下范围内</span><br><span class="line">        [924.6s, 1044.6s)</span><br><span class="line">如果某个RTO值导致，在已经重传了14次后，总重传间隔开销是924s 那么它还需要重传第15次，即使离924.6s只差0.6s。这就是发挥了lower bound的作用</span><br><span class="line">如果某个RTO值导致，在重传了10次后，总重传间隔开销是924s 重传第11次后，第12次超时触发时计算得到的总间隔变为1044s，超过924.6s, 那么此时会放弃第12次重传，这就是924.6s发挥了upper bound的作用</span><br><span class="line"></span><br><span class="line">                                    ---RTT 200ms, from 9th, 120s for each time, before 9th, total time: 204.6s</span><br><span class="line">                                    |</span><br><span class="line">$ awk &#x27;BEGIN&#123;print (lshift(2,9)-1)*0.2+(15-9)*120&#125;&#x27;</span><br><span class="line"></span><br><span class="line">#define TCP_RETR1       3   /*</span><br><span class="line">                             * This is how many retries it does before it</span><br><span class="line">                             * tries to figure out if the gateway is</span><br><span class="line">                             * down. Minimal RFC value is 3; it corresponds</span><br><span class="line">                             * to ~3sec-8min depending on RTO.</span><br><span class="line">                             */</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_syn_retries = 4  #default 6</span><br><span class="line"></span><br><span class="line">#https://perthcharles.github.io/2015/09/07/wiki-tcp-retries/</span><br><span class="line"></span><br><span class="line">1. If RTT is tiny that the RTO init value was about 200ms</span><br><span class="line">   The total timeout was 924600ms，looks like retrans 15 times cause over the timeout value and end the tcp stream</span><br><span class="line"></span><br><span class="line">2. If RTT is large,the RTO calculate value was 1000ms</span><br><span class="line">   it doesn&#x27; t need retrans 15 times，the timeout value will over 924600ms.</span><br><span class="line">   if the RTT=400ms,set tcp_retries2=10, only 3 times the tcp stream will end</span><br><span class="line"></span><br><span class="line"># 在Linux3.10中，如果tcp_retres2设置为15。总重传超时周期应该在如下范围内</span><br><span class="line">        [924.6s, 1044.6s)</span><br><span class="line">如果某个RTO值导致，在已经重传了14次后，总重传间隔开销是924s 那么它还需要重传第15次，即使离924.6s只差0.6s。这就是发挥了lower bound的作用</span><br><span class="line">如果某个RTO值导致，在重传了10次后，总重传间隔开销是924s 重传第11次后，第12次超时触发时计算得到的总间隔变为1044s，超过924.6s, 那么此时会放弃第12次重传，这就是924.6s发挥了upper bound的作用</span><br><span class="line"></span><br><span class="line">                                    ---RTT 200ms, from 9th, 120s for each time, before 9th, total time: 204.6s</span><br><span class="line">                                    |</span><br><span class="line">$ awk &#x27;BEGIN&#123;print (lshift(2,9)-1)*0.2+(15-9)*120&#125;&#x27;</span><br><span class="line">924.6</span><br><span class="line">$ awk &#x27;BEGIN&#123;print (lshift(2,9)-1)*0.2&#125;&#x27; &lt;---means (lshift(2,9)-1)=0x3ff= 0011 1111 1111, 2+4+8+16+32+64+128+256+512=1022 != 1023, means first bit is 1, not 0</span><br><span class="line">204.6</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/brendangregg/perf-tools">tcpretrans</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TCPWqueueTooBig</span></span><br><span class="line">https://github.com/torvalds/linux/commit/f070ef2ac66716357066b683fb0baf55f8191a2e</span><br><span class="line">TCP allows an application to queue up to sk_sndbuf bytes,so we need to give some allowance <span class="keyword">for</span> non malicious splitting of retransmit queue.</span><br><span class="line">CVE-2019-11478 : tcp_fragment, prevent fragmenting a packet when the socket is already using more than half the allowed space</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ nstat -rsz | grep -Ei <span class="string">&#x27;fail|crc|dis|drop|miss|err|over|timeout|jabb|toobig&#x27;</span></span><br><span class="line">TcpExtTCPSackFailures           559232             0.0</span><br><span class="line">TcpExtTCPLossFailures           44575              0.0</span><br><span class="line">TcpExtTCPRenoRecoveryFail       0                  0.0</span><br><span class="line">TcpExtTCPSackRecoveryFail       400237             0.0</span><br><span class="line">TcpExtTCPSchedulerFailed        0                  0.0</span><br><span class="line">TcpExtTCPAbortFailed            3                  0.0</span><br><span class="line">TcpExtTCPSACKDiscard            0                  0.0</span><br><span class="line">TcpExtTCPBacklogDrop            0                  0.0</span><br><span class="line">TcpExtPFMemallocDrop            0                  0.0</span><br><span class="line">TcpExtTCPMinTTLDrop             0                  0.0</span><br><span class="line">TcpExtTCPDeferAcceptDrop        0                  0.0</span><br><span class="line">TcpExtTCPReqQFullDrop           0                  0.0</span><br><span class="line">TcpExtTCPRetransFail            631                0.0</span><br><span class="line">TcpExtTCPOFODrop                15819              0.0</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">switch and cable package loss could cause the TcpExtTCPSackRecoveryFail increased </span><br><span class="line"></span><br><span class="line"><span class="comment">#check 10GbE SFP+ optic module status</span></span><br><span class="line">$ ethtool -m <span class="variable">$NIC_NAME</span>  | grep <span class="string">&quot;Laser output power   &quot;</span></span><br><span class="line">	Laser output power                        : 0.5273 mW / -2.78 dBm &lt;-----------worked</span><br><span class="line">	Laser output power                        : 0.0000 mW / -inf dBm  &lt;-----------failed</span><br><span class="line"></span><br><span class="line">Laser output power(Not available <span class="keyword">for</span> QSFP+ transceivers)  Displays the laser output power, <span class="keyword">in</span> milliwatts (mW) and decibels referred to 1.0 mW (dBm).</span><br><span class="line"></span><br><span class="line">	Laser bias current                        : 9.298 mA &lt;-----------worked</span><br><span class="line">	Laser bias current                        : 0.184 mA &lt;-----------failed</span><br><span class="line">	Laser bias current high alarm threshold   : 13.000 mA</span><br><span class="line">	Laser bias current low alarm threshold    : 4.000 mA</span><br><span class="line">	Laser bias current high warning threshold : 12.500 mA</span><br><span class="line">	Laser bias current low warning threshold  : 5.000 mA</span><br><span class="line"></span><br><span class="line"><span class="comment">#100GbE QSFP+ #default</span></span><br><span class="line">	Laser tx bias current (Channel 1)         : 6.152 mA &lt;----------worked</span><br><span class="line">	Laser tx bias current (Channel 2)         : 6.152 mA &lt;----------worked</span><br><span class="line">	Laser tx bias current (Channel 3)         : 6.152 mA &lt;----------worked</span><br><span class="line">	Laser tx bias current (Channel 4)         : 6.152 mA &lt;----------worked</span><br><span class="line"></span><br><span class="line">	Laser bias current high alarm threshold   : 13.000 mA</span><br><span class="line">	Laser bias current low alarm threshold    : 4.000 mA</span><br><span class="line">	Laser bias current high alarm threshold   : 13.000 mA</span><br><span class="line">	Laser bias current low alarm threshold    : 4.000 mA</span><br><span class="line"></span><br><span class="line">	Transmit avg optical power (Channel 1)    : 1.2916 mW / 1.11 dBm &lt;---avg value ,any <span class="built_in">help</span> ?</span><br><span class="line">	Transmit avg optical power (Channel 2)    : 1.3471 mW / 1.29 dBm </span><br><span class="line">	Transmit avg optical power (Channel 3)    : 1.3778 mW / 1.39 dBm</span><br><span class="line">	Transmit avg optical power (Channel 4)    : 1.3553 mW / 1.32 dBm</span><br><span class="line">	Rcvr signal avg optical power(Channel 1)  : 1.2702 mW / 1.04 dBm</span><br><span class="line">	Rcvr signal avg optical power(Channel 2)  : 1.1145 mW / 0.47 dBm</span><br><span class="line">	Rcvr signal avg optical power(Channel 3)  : 1.2505 mW / 0.97 dBm</span><br><span class="line">	Rcvr signal avg optical power(Channel 4)  : 1.2294 mW / 0.90 dBm</span><br><span class="line"></span><br><span class="line">	Laser bias current high alarm threshold   : 15.000 mA</span><br><span class="line">	Laser bias current high warning threshold : 12.000 mA</span><br><span class="line">	Laser bias current low alarm threshold    : 1.000 mA</span><br><span class="line">	Laser bias current low warning threshold  : 2.000 mA</span><br><span class="line">	Laser output power high alarm threshold   : 2.7542 mW / 4.40 dBm</span><br><span class="line">	Laser output power high warning threshold : 2.1878 mW / 3.40 dBm</span><br><span class="line">	Laser output power low alarm threshold    : 0.0912 mW / -10.40 dBm</span><br><span class="line">	Laser output power low warning threshold  : 0.1148 mW / -9.40 dBm</span><br><span class="line">	Laser rx power high alarm threshold       : 2.7542 mW / 4.40 dBm</span><br><span class="line">	Laser rx power high warning threshold     : 2.1878 mW / 3.40 dBm</span><br><span class="line">	Laser rx power low alarm threshold        : 0.0589 mW / -12.30 dBm</span><br><span class="line">	Laser rx power low warning threshold      : 0.0741 mW / -11.30 dBm</span><br><span class="line">	Module temperature high alarm threshold   : 80.00 degrees C / 176.00 degrees F</span><br><span class="line">	Module temperature high warning threshold : 75.00 degrees C / 167.00 degrees F</span><br><span class="line">	Module temperature low alarm threshold    : -10.00 degrees C / 14.00 degrees F</span><br><span class="line">	Module temperature low warning threshold  : -5.00 degrees C / 23.00 degrees F</span><br><span class="line">	Module voltage high alarm threshold       : 3.6300 V</span><br><span class="line">	Module voltage high warning threshold     : 3.4600 V</span><br><span class="line">	Module voltage low alarm threshold        : 2.9700 V</span><br><span class="line">	Module voltage low warning threshold      : 3.1300 V</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ethtool -S ens3 | grep -Ei <span class="string">&#x27;pause|flow_con|fail|crc|dis|drop|miss|err|over|timeout|jabb|toobig&#x27;</span> | awk <span class="string">&#x27;$2&gt;0 &#123;print $0&#125;&#x27;</span></span><br><span class="line">     rx_steer_missed_packets: 1064834</span><br><span class="line">     tx_pause_ctrl_phy: 9584492</span><br><span class="line">     rx_discards_phy: 8232288</span><br><span class="line">     rx_err_lane_2_phy: 2</span><br><span class="line">     rx_prio0_discards: 8232288</span><br><span class="line">     tx_global_pause: 9584489</span><br><span class="line">     tx_global_pause_duration: 858502067</span><br><span class="line"></span><br><span class="line">ethtool -S ens3 | grep -Ei <span class="string">&#x27;pause|flow_con|fail|crc|dis|drop|miss|err|over|timeout|jabb|toobig&#x27;</span> | awk <span class="string">&#x27;$2&gt;0 &#123;print $0&#125;&#x27;</span></span><br><span class="line">     rx_steer_missed_packets: 1065061</span><br><span class="line">     Number of packets that was received by the NIC, however was discarded because it did not match any flow <span class="keyword">in</span> the NIC flow table. supported from kernel 4.16( error </span><br><span class="line">)</span><br><span class="line">     tx_pause_ctrl_phy: 11459398</span><br><span class="line">     The number of <span class="built_in">link</span> layer pause packets transmitted on a physical port. If this counter is increasing, it implies that the NIC is congested and cannot absorb the traffic coming from the network. (info,not error)</span><br><span class="line"></span><br><span class="line">     rx_discards_phy: 9851318</span><br><span class="line">The number of received packets dropped due to lack of buffers on a physical port. If this counter is increasing, it implies that the adapter is congested and cannot absorb the traffic coming from the network.(error)</span><br><span class="line"></span><br><span class="line">     rx_err_lane_2_phy: 2</span><br><span class="line">     rx_prio0_discards: 9851318</span><br><span class="line"></span><br><span class="line">     tx_global_pause: 11459390</span><br><span class="line">The number of pause packets transmitted on a physical port. If this counter is increasing, it implies that the adapter is congested and cannot absorb the traffic coming from the network. Note: This counter is only enabled when global pause mode is enabled.</span><br><span class="line"></span><br><span class="line">     tx_global_pause_duration: 1057651436</span><br><span class="line">The duration of pause transmitter (<span class="keyword">in</span> microSec) on the physical port. Note: This counter is only enabled when global pause mode is enabled.</span><br><span class="line"></span><br><span class="line">TcpExtTCPSackRecovery(many) ---&gt; TcpExtTCPSackRecoveryFail(litt)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ netstat -s | grep -i retrans</span><br><span class="line">    36900 segments retransmited</span><br><span class="line">    TCPLostRetransmit: 135</span><br><span class="line">    36854 fast retransmits</span><br><span class="line">    32 forward retransmits</span><br><span class="line">    9 retransmits <span class="keyword">in</span> slow start</span><br><span class="line">    5 SACK retransmits failed</span><br><span class="line">$ netstat -s | grep -i retrans</span><br><span class="line">    36963 segments retransmited</span><br><span class="line">    TCPLostRetransmit: 135</span><br><span class="line">    36917 fast retransmits</span><br><span class="line">    32 forward retransmits</span><br><span class="line">    9 retransmits <span class="keyword">in</span> slow start</span><br><span class="line">    5 SACK retransmits failed</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ tshark -r first.pcapng <span class="string">&quot;tcp.analysis.retransmission&quot;</span></span><br><span class="line">no output</span><br><span class="line"></span><br><span class="line">$ tshark -r first.pcapng <span class="string">&quot;tcp.analysis.duplicate_ack&quot;</span></span><br><span class="line"></span><br><span class="line">21801 0.710632534   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#1] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794721 TSecr=61820560 SLE=777178578 SRE=777375434</span></span><br><span class="line">21804 0.710733730   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#2] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794721 TSecr=61820560 SLE=777178578 SRE=777500706</span></span><br><span class="line">21811 0.711034519   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#3] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794722 TSecr=61820560 SLE=777178578 SRE=777867574</span></span><br><span class="line">21814 0.711153496   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#4] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794722 TSecr=61820560 SLE=777178578 SRE=778010742</span></span><br><span class="line">21816 0.711206308   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#5] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794722 TSecr=61820560 SLE=777178578 SRE=778082326</span></span><br><span class="line">21818 0.711259113   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#6] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794722 TSecr=61820560 SLE=777178578 SRE=778144962</span></span><br><span class="line">21820 0.711369837   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#7] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794722 TSecr=61820560 SLE=777178578 SRE=778279182</span></span><br><span class="line">21823 0.711477980   10.53.19.1 -&gt; 10.53.19.2   TCP 78 [TCP Dup ACK 21799<span class="comment">#8] targus-getdata1 &gt; 40746 [ACK] Seq=1 Ack=777160682 Win=5263360 Len=0 TSval=2597794722 TSecr=61820560 SLE=777178578 SRE=778413402</span></span><br></pre></td></tr></table></figure>

<p><img src="/img/tshark-1.png"><br><img src="/img/tshark-2.png">  </p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://osqa-ask.wireshark.org/questions/21006/tcp-dup-acktcp-retransmission">TCP has this inherent mechanism of recovery. In tcp stream eq 8 of your trace there was a condition of retransmission generated due to timing but not because of drops. Here is the snippet of your trace</a></p>
</li>
<li><p>Server send a packet to client(Let us call it packet-A){Packet.no-150}</p>
</li>
<li><p>Client acknowledged the packet (Let us call it ack-A){Packet.no-192}</p>
</li>
<li><p>Somehow packet-A was retransmitted by Server.The reason might be the delay in receiving ack-A from client and ack timer got out and retransmission timer got kicked in.(Dup of Packet-A){Packet.no-194}</p>
</li>
<li><p>Client generated a duplicate ack for the retransmitted packet.(Dup of ack-A){Packet.no-195}</p>
</li>
<li><p>TCPAttemptFails</p>
<ul>
<li>The number of times that TCP connections have made a direct transition to the CLOSED state from either the SYN-SENT state or the SYN-RCVD state, plus the number of times that TCP connections have made a direct transition to the LISTEN state from the SYN-RCVD state.</li>
<li>there is a mis-configured application on the system that is either missing a process that should be listening on port 9411, or is trying to connect to the ‘wrong’ port number.</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ tcpdump -ieno2 -n -v &#x27;tcp[tcpflags] &amp; (tcp-rst) != 0&#x27;</span><br></pre></td></tr></table></figure>

<h4 id="tcpdump-802-3-header"><a href="#tcpdump-802-3-header" class="headerlink" title="tcpdump 802.3 header"></a><a target="_blank" rel="noopener" href="https://support.f5.com/csp/article/K2289">tcpdump 802.3 header</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">LACP is handled by the switch fabric, therefore packets must be captured directly on the physical interface, <span class="keyword">for</span> example, 1.1, as opposed to the VLAN or 0.0.</span><br><span class="line"></span><br><span class="line">To capture only LACP packets, use the following syntax:</span><br><span class="line"></span><br><span class="line">tcpdump -ni &lt;interface&gt; -e ether proto 0x8809</span><br><span class="line"></span><br><span class="line">Note: The -e switch prints the link-level header on each line.</span><br><span class="line"></span><br><span class="line">For example, to capture LACP packets on interface 1.1 of a Link Aggregation Group (LAG), <span class="built_in">type</span> the following <span class="built_in">command</span>:</span><br><span class="line"></span><br><span class="line">tcpdump -ni 1.1 -e ether proto 0x8809</span><br><span class="line"></span><br><span class="line">The output appears similar to the following example:</span><br><span class="line"></span><br><span class="line">15:21:03.445333 00:50:56:92:2a:82 &gt; 01:80:c2:00:00:02, ethertype 802.1Q (0x8100), length 128: vlan 0, p 0, ethertype Slow Protocols, LACPv1, length: 110</span><br><span class="line"></span><br><span class="line">Ethernet header (basically IEEE 802.3)</span><br><span class="line"></span><br><span class="line">0                   1                   2                   3</span><br><span class="line">0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">|                         Preamble (8)                          |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">|                      Preamble cont.                           |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">|                    Destination address (6)                    |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">|   Destination address cont.   |         Source address (6)    |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">|                    Source address cont.                       |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">|            Type (2)           | Pri |C|     802.1q tag        |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">| <span class="keyword">if</span> <span class="built_in">type</span>!=0x8100, prev. 2 bytes are part of this field  (46 -  |</span><br><span class="line">| 1500 bytes of data) ...                                       |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">|                    Frame Check Sequence, FCS (4)              |</span><br><span class="line">+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+</span><br><span class="line">Note: Although the previous diagram correctly shows both Preamble and Frame Check Sequence, tcpdump does not include these <span class="keyword">in</span> the totals <span class="keyword">for</span> header bytes (presumably because the data is fixed, so there is no reason to match against it.) As a result, <span class="keyword">if</span> you are basing ether expressions on this chart, you must subtract 12 (as <span class="keyword">if</span> the Preamble and FCS sections <span class="keyword">do</span> not exist.)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">To view the SYN packets and the SYN and ACK packets, you would create the following filter that accepts either value <span class="keyword">for</span> the flag byte:</span><br><span class="line">tcpdump -ni internal <span class="string">&#x27;tcp[13] == 18&#x27;</span> or <span class="string">&#x27;tcp[13] == 2&#x27;</span></span><br><span class="line">tcpdump -ni internal <span class="string">&#x27;tcp[13] == 18&#x27;</span> or <span class="string">&#x27;tcp[13] == 2&#x27;</span></span><br><span class="line"></span><br><span class="line">You can also create a filter that looks <span class="keyword">for</span> the <span class="built_in">set</span> SYN bit and ignores the rest of the flags <span class="keyword">in</span> the header. You must <span class="built_in">set</span> the filter to perform a logic AND (&amp;) to remove all but the value of the SYN bit and <span class="keyword">then</span> <span class="built_in">test</span> it.</span><br><span class="line"></span><br><span class="line">For example, <span class="keyword">if</span> the TCP flags are 00010010 and the mask <span class="keyword">for</span> Syn is 00000010(2 <span class="keyword">in</span> binary) <span class="keyword">then</span> 00010010 + 00000010 = 00000010. You can <span class="keyword">then</span> <span class="built_in">test</span> the resulting value against the SYN flag, by setting the filter as follows:</span><br><span class="line">tcpdump -ni internal <span class="string">&#x27;tcp[13] &amp; 2 == 2&#x27;</span></span><br></pre></td></tr></table></figure>

<h3 id="tcp-netif-rx"><a href="#tcp-netif-rx" class="headerlink" title="tcp netif_rx"></a><a target="_blank" rel="noopener" href="https://tldp.org/HOWTO/KernelAnalysis-HOWTO-8.html">tcp netif_rx</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line">net_rx_action</span><br><span class="line">   |skb = __skb_dequeue (the exact opposite of __skb_queue_tail)</span><br><span class="line">   |<span class="keyword">for</span> (ptype = first_protocol; ptype &lt; max_protocol; ptype++) // Determine </span><br><span class="line">      |<span class="keyword">if</span> (skb-&gt;protocol == ptype)                               // what is the network protocol</span><br><span class="line">         |ptype-&gt;func -&gt; ip_rcv // according to <span class="string">&#x27;&#x27;</span>struct ip_packet_type [net/ipv4/ip_output.c]<span class="string">&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line">    **** NOW WE KNOW THAT PACKET IS IP ****</span><br><span class="line">         |ip_rcv</span><br><span class="line">            |NF_HOOK (ip_rcv_finish)</span><br><span class="line">               |ip_route_input // search from routing table to determine <span class="keyword">function</span> to call</span><br><span class="line">                  |skb-&gt;dst-&gt;input -&gt; ip_local_deliver // according to previous routing table check, destination is <span class="built_in">local</span> machine</span><br><span class="line">                     |ip_defrag // reassembles IP fragments</span><br><span class="line">                        |NF_HOOK (ip_local_deliver_finish)</span><br><span class="line">                           |ipprot-&gt;handler -&gt; tcp_v4_rcv // according to <span class="string">&#x27;&#x27;</span>tcp_protocol [include/net/protocol.c]<span class="string">&#x27;&#x27;</span></span><br><span class="line"> </span><br><span class="line">     **** NOW WE KNOW THAT PACKET IS TCP ****</span><br><span class="line">                           |tcp_v4_rcv   </span><br><span class="line">                              |sk = __tcp_v4_lookup </span><br><span class="line">                              |tcp_v4_do_rcv</span><br><span class="line">                                 |switch(sk-&gt;state) </span><br><span class="line"></span><br><span class="line">     *** Packet can be sent to the task <span class="built_in">which</span> uses relative socket ***</span><br><span class="line">                                 |<span class="keyword">case</span> TCP_ESTABLISHED:</span><br><span class="line">                                    |tcp_rcv_established</span><br><span class="line">                                       |__skb_queue_tail // enqueue packet to socket</span><br><span class="line">                                       |sk-&gt;data_ready -&gt; sock_def_readable </span><br><span class="line">                                          |wake_up_interruptible</span><br><span class="line">                                </span><br><span class="line"></span><br><span class="line">     *** Packet has still to be handshaked by 3-way TCP handshake ***</span><br><span class="line">                                 |<span class="keyword">case</span> TCP_LISTEN:</span><br><span class="line">                                    |tcp_v4_hnd_req</span><br><span class="line">                                       |tcp_v4_search_req</span><br><span class="line">                                       |tcp_check_req</span><br><span class="line">                                          |syn_recv_sock -&gt; tcp_v4_syn_recv_sock</span><br><span class="line">                                       |__tcp_v4_lookup_established</span><br><span class="line">                                 |tcp_rcv_state_process</span><br><span class="line"></span><br><span class="line">                    *** 3-Way TCP Handshake ***</span><br><span class="line">                                    |switch(sk-&gt;state)</span><br><span class="line">                                    |<span class="keyword">case</span> TCP_LISTEN: // We received SYN</span><br><span class="line">                                       |conn_request -&gt; tcp_v4_conn_request</span><br><span class="line">                                          |tcp_v4_send_synack // Send SYN + ACK</span><br><span class="line">                                             |tcp_v4_synq_add // <span class="built_in">set</span> SYN state</span><br><span class="line">                                    |<span class="keyword">case</span> TCP_SYN_SENT: // we received SYN + ACK</span><br><span class="line">                                       |tcp_rcv_synsent_state_process</span><br><span class="line">                                          tcp_set_state(TCP_ESTABLISHED)</span><br><span class="line">                                             |tcp_send_ack</span><br><span class="line">                                                |tcp_transmit_skb</span><br><span class="line">                                                   |queue_xmit -&gt; ip_queue_xmit</span><br><span class="line">                                                      |ip_queue_xmit2</span><br><span class="line">                                                         |skb-&gt;dst-&gt;output</span><br><span class="line">                                    |<span class="keyword">case</span> TCP_SYN_RECV: // We received ACK</span><br><span class="line">                                       |<span class="keyword">if</span> (ACK)</span><br><span class="line">                                          |tcp_set_state(TCP_ESTABLISHED)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">net_rx_action [net/core/dev.c]</span><br><span class="line">__skb_dequeue [include/linux/skbuff.h]</span><br><span class="line">ip_rcv [net/ipv4/ip_input.c]</span><br><span class="line">NF_HOOK -&gt; nf_hook_slow [net/core/netfilter.c]</span><br><span class="line">ip_rcv_finish [net/ipv4/ip_input.c]</span><br><span class="line">ip_route_input [net/ipv4/route.c]</span><br><span class="line">ip_local_deliver [net/ipv4/ip_input.c]</span><br><span class="line">ip_defrag [net/ipv4/ip_fragment.c]</span><br><span class="line">ip_local_deliver_finish [net/ipv4/ip_input.c]</span><br><span class="line">tcp_v4_rcv [net/ipv4/tcp_ipv4.c]</span><br><span class="line">__tcp_v4_lookup</span><br><span class="line">tcp_v4_do_rcv</span><br><span class="line">tcp_rcv_established [net/ipv4/tcp_input.c]</span><br><span class="line">__skb_queue_tail [include/linux/skbuff.h]</span><br><span class="line">sock_def_readable [net/core/sock.c]</span><br><span class="line">wake_up_interruptible [include/linux/sched.h]</span><br><span class="line">tcp_v4_hnd_req [net/ipv4/tcp_ipv4.c]</span><br><span class="line">tcp_v4_search_req</span><br><span class="line">tcp_check_req</span><br><span class="line">tcp_v4_syn_recv_sock</span><br><span class="line">__tcp_v4_lookup_established</span><br><span class="line">tcp_rcv_state_process [net/ipv4/tcp_input.c]</span><br><span class="line">tcp_v4_conn_request [net/ipv4/tcp_ipv4.c]</span><br><span class="line">tcp_v4_send_synack</span><br><span class="line">tcp_v4_synq_add</span><br><span class="line">tcp_rcv_synsent_state_process [net/ipv4/tcp_input.c]</span><br><span class="line">tcp_set_state [include/net/tcp.h]</span><br><span class="line">tcp_send_ack [net/ipv4/tcp_output.c]</span><br><span class="line"></span><br><span class="line">SERVER (LISTENING)                       CLIENT (CONNECTING)</span><br><span class="line">                           SYN </span><br><span class="line">                   &lt;-------------------</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">                        SYN + ACK</span><br><span class="line">                   -------------------&gt;</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">                           ACK </span><br><span class="line">                   &lt;-------------------</span><br><span class="line"></span><br><span class="line">                    3-Way TCP handshake</span><br></pre></td></tr></table></figure>


<h3 id="ethtool-cmd"><a href="#ethtool-cmd" class="headerlink" title="ethtool cmd"></a>ethtool cmd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Enable merge IRQ; else tons of CPU overhead</span></span><br><span class="line">$ ethtool -C <span class="variable">$ETH_NAME</span> adaptive-tx on adaptive-rx on</span><br><span class="line">$ ethtool -L <span class="variable">$ETH_NAME</span> combined <span class="variable">$com_num</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#if ringbuff not overflow, the large means more latency</span></span><br><span class="line"><span class="comment">#1x descriptor = 1x package</span></span><br><span class="line">$ ethtool -G <span class="variable">$ETH_NAME</span> rx 2000 tx 2000</span><br><span class="line"></span><br><span class="line">$ ethtool --set-perqueue-command <span class="variable">$ETH_NAME</span> queue_mask <span class="variable">$MASK</span> --coalesce <span class="variable">$OPTIONS</span></span><br><span class="line">$ ethtool -K <span class="variable">$nic_dev</span> rx on tx on sg on gro on lro off gso on tso off ntuple on <span class="comment">#i40e not enable ntuple</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#blinking one or more LEDs on the specific network port</span></span><br><span class="line">$ ethtool -p eth2</span><br><span class="line"><span class="comment">#300 secs</span></span><br><span class="line">$ ethtool -p eth2 300</span><br><span class="line"></span><br><span class="line">$ vi /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">ETHTOOL_OPTS=<span class="string">&quot;-G <span class="variable">$&#123;ifname&#125;</span> &#123;parm&#125; &#123;value&#125;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##Link delay</span></span><br><span class="line">LINKDELAY=30 <span class="keyword">in</span> /etc/sysconfig/network-scripts/ifcfg-eth0</span><br><span class="line">or</span><br><span class="line">kernel parameter: rd.net.timeout.carrier=30</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> /etc/NetworkManager/dispatcher.d/</span><br><span class="line">01-ifupdown  99tlp-rdw-nm  no-wait.d  pre-down.d  pre-up.d</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">&quot;<span class="variable">$1</span>&quot;</span> = <span class="string">&quot;eth0&quot;</span> ] &amp;&amp; [ <span class="string">&quot;<span class="variable">$2</span>&quot;</span> = <span class="string">&quot;up&quot;</span> ]; <span class="keyword">then</span></span><br><span class="line"> ethtool -K <span class="string">&quot;<span class="variable">$1</span>&quot;</span> rx off gro off lro off</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line">$ ethtool -L <span class="variable">$nic_dev</span> combined <span class="variable">$num</span> <span class="comment">## CPU cores &lt; 8 , set num = cpu cores else set 16~32, not the more the better</span></span><br><span class="line">$ <span class="built_in">cat</span> /proc/interrupts  | grep <span class="variable">$nic_dev</span></span><br><span class="line">$ grep -E <span class="string">&quot;CPU0|enp1s0f0&quot;</span> /proc/interrupts</span><br><span class="line">           CPU0       CPU1       CPU2       CPU3       CPU4       CPU5       CPU6       CPU7</span><br><span class="line"> 48:          0          0          1          0         73          0        154          0   PCI-MSI 524289-edge      i40e-enp1s0f0-TxRx-0</span><br><span class="line"> 49:         70          0         51         45          0        112          0        121   PCI-MSI 524290-edge      i40e-enp1s0f0-TxRx-1</span><br><span class="line"> 50:         34          0        273         90         33         53          0          7   PCI-MSI 524291-edge      i40e-enp1s0f0-TxRx-2</span><br><span class="line"> 51:      83286          0          0        229          0          1          0          0   PCI-MSI 524292-edge      i40e-enp1s0f0-TxRx-3</span><br><span class="line"> 52:          0        226          0          0          5          0          1          0   PCI-MSI 524293-edge      i40e-enp1s0f0-TxRx-4</span><br><span class="line"> 53:         29          0        192         72          0          0          0          1   PCI-MSI 524294-edge      i40e-enp1s0f0-TxRx-5</span><br><span class="line"> 54:         16          0          0          0          0          5        438         11   PCI-MSI 524295-edge      i40e-enp1s0f0-TxRx-6</span><br><span class="line"> 55:         78          1          0          0          0       6430          0      13438   PCI-MSI 524296-edge      i40e-enp1s0f0-TxRx-7</span><br><span class="line"></span><br><span class="line"><span class="comment"># driver support get_channels()</span></span><br><span class="line">$ ethtool -l ens3</span><br><span class="line">Channel parameters <span class="keyword">for</span> ens3:</span><br><span class="line">Pre-<span class="built_in">set</span> maximums:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          0</span><br><span class="line">Combined:       36</span><br><span class="line">Current hardware settings:</span><br><span class="line">RX:             0</span><br><span class="line">TX:             0</span><br><span class="line">Other:          0</span><br><span class="line">Combined:       36</span><br><span class="line"></span><br><span class="line">$ numactl --hardware  | grep <span class="string">&quot;node <span class="subst">$(cat /sys/class/net/ens3/device/numa_node)</span> cpu&quot;</span> | awk -F: <span class="string">&#x27;&#123;print $2&#125;&#x27;</span></span><br><span class="line"> 0 1 2 5 6 9 10 14 15</span><br><span class="line"></span><br><span class="line">$ ethtool -set-priv-flags eth2 vf-true-promisc-support on</span><br><span class="line"></span><br><span class="line"><span class="comment">#In case “tx-nocache-copy” is enabled, (this is the case for some kernels, e.g. kernel 3.10, which is the default for RH7.0) “tx-nocache-copy” should be disabled.</span></span><br><span class="line">$ ethtool -K ethX tx-nocache-copy off</span><br><span class="line"><span class="comment"># tx-nocache-copy is a feature which bypasses local cache and writes user-space data directly into memory.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># support PTP(IEEE 1588)精密时间协议, 支持PTP</span></span><br><span class="line">$ ethtool -T ethX</span><br><span class="line">make CFLAGS_EXTRA=<span class="string">&quot;-DIGB_PTP&quot;</span> install</span><br><span class="line"></span><br><span class="line"><span class="comment"># vxlan offloading</span></span><br><span class="line">$ ethtool -K ethX tx-udp_tnl-segmentation [off|on]</span><br></pre></td></tr></table></figure>


<p>This driver supports an adaptive interrupt throttle rate (ITR) mechanism that is tuned for general workloads. The user can customize the interrupt rate control for specific workloads, via ethtool, adjusting the number of microseconds between interrupts    </p>
<h4 id="interrupt-moderation"><a href="#interrupt-moderation" class="headerlink" title="interrupt moderation"></a><a target="_blank" rel="noopener" href="https://01.org/linux-interrupt-moderation">interrupt moderation</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#it &#x27;s intel policies, need to test in Mellanox and broadcom</span></span><br><span class="line">POLICIES</span><br><span class="line">For different application requirements and characteristics, we designed three different policies <span class="keyword">for</span> queues:</span><br><span class="line">LATENCY policy</span><br><span class="line">BULK policy</span><br><span class="line">CPU policy</span><br><span class="line"></span><br><span class="line">LATENCY POLICY</span><br><span class="line">The LATENCY policy is designed to achieve the lowest latency. Under this policy, the queues are assigned the highest interrupt rate so that the packets are processed immediately. Usually this policy applies to small packets. Small packets can be processed quickly without introducing a lot of CPU overhead.</span><br><span class="line"></span><br><span class="line">According to the <span class="built_in">test</span> results, the optimal interrupt moderation <span class="keyword">for</span> the LATENCY policy is rx-usecs 5 and tx-usecs 10.</span><br><span class="line"></span><br><span class="line">BULK POLICY</span><br><span class="line">The BULK policy is designed to achieve the highest throughput. Under this policy, the queues are assigned an intermediate interrupt rate so that the packets can be expediently processed <span class="keyword">in</span> <span class="built_in">groups</span>. The drawback is that CPU utilization can be higher due to the larger number of interrupts, <span class="built_in">which</span> leaves fewer CPU resources <span class="keyword">for</span> applications. Usually this policy applies to arbitrary packet sizes on a lightly loaded system.</span><br><span class="line"></span><br><span class="line">According to the <span class="built_in">test</span> results, the optimal interrupt moderation <span class="keyword">for</span> BULK policy is rx-usecs 62 and tx-usecs 122.</span><br><span class="line"></span><br><span class="line">CPU POLICY</span><br><span class="line">The CPU policy is designed to achieve a high throughput with reasonable CPU utilization. It tries to decrease CPU utilization on the network and leaves more resources <span class="keyword">for</span> applications. Under this policy, the queues are assigned a low interrupt rate to reduce the interrupt overhead <span class="keyword">while</span> still maintaining the highest available throughput. Usually this policy applies to arbitrary packets sizes on a heavily loaded system.</span><br><span class="line"><span class="built_in">disable</span> irqbalance</span><br><span class="line">Throughput queues: 8-11, 20-47</span><br><span class="line">Latency queues: 0-7, 12-19</span><br><span class="line"></span><br><span class="line"><span class="comment">#https://docs.kernel.org/networking/device_drivers/ethernet/intel/ice.html</span></span><br><span class="line"></span><br><span class="line">$ ethtool --set-perqueue-command eth5 queue_mask 0x1 --coalesce rx-usecs 10 tx-usecs 5</span><br><span class="line">$ ethtool --set-perqueue-command eth5 queue_mask 0x1 --show-coalesce</span><br><span class="line"></span><br><span class="line"><span class="comment">#To disable Rx adaptive ITR and set static Rx ITR to 10 microseconds(not ms,µs) or about 100,000 interrupts/second, for queues 1 and 3</span></span><br><span class="line">$ ethtool --per-queue &lt;ethX&gt; queue_mask 0xa --coalesce adaptive-rx off rx-usecs 10</span><br><span class="line"></span><br><span class="line"><span class="comment">#To show the current coalesce settings for queues 1 and 3</span></span><br><span class="line">$ ethtool --per-queue &lt;ethX&gt; queue_mask 0xa --show-coalesce</span><br><span class="line"></span><br><span class="line"><span class="comment">#The following command would disable adaptive interrupt moderation, and allow a maximum of 5 microseconds before indicating a receive or transmit was complete. However, instead of resulting in as many as 200,000 interrupts per second, it limits total interrupts per second to 50,000 via the rx-usecs-high parameter.</span></span><br><span class="line">$ ethtool -C &lt;ethX&gt; adaptive-rx off adaptive-tx off rx-usecs-high 20 rx-usecs 5 tx-usecs 5</span><br><span class="line"></span><br><span class="line">According to the <span class="built_in">test</span> results, the optimal interrupt moderation <span class="keyword">for</span> CPU policy is rx-usecs 125 and tx-usecs 250.</span><br><span class="line"></span><br><span class="line">If there are mixed workloads (a mixture of applications that require high throughput and applications that require low latency) running on the system, we need to divide the queues into different policy <span class="built_in">groups</span>. For example, we can divide the queues into latency queues (<span class="keyword">for</span> applications requiring low latency) and bulk queues (<span class="keyword">for</span> applications requiring high throughput). For latency queues, the LATENCY policy should be applied. For bulk queues, either the BULK policy or the CPU policy should be applied, depending on the load of the system.</span><br><span class="line"></span><br><span class="line">If HyperThreading is on, there are logical cores sharing the same physical cores. Since the applications that require high throughput usually utilize more CPU resources, try to avoid letting two applications that require high throughput occupy the same physical core.</span><br><span class="line"></span><br><span class="line">pin app <span class="keyword">in</span> <span class="built_in">local</span> numa</span><br></pre></td></tr></table></figure>

<h3 id="show-NIC-model"><a href="#show-NIC-model" class="headerlink" title="show NIC model"></a><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/2898381">show NIC model</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#qede</span></span><br><span class="line">2f:00.0 Ethernet controller [0200]: QLogic Corp. FastLinQ QL41000 Series 10/25/40/50GbE Controller [1077:8070] (rev 02)</span><br><span class="line">2f:00.1 Ethernet controller [0200]: QLogic Corp. FastLinQ QL41000 Series 10/25/40/50GbE Controller [1077:8070] (rev 02)</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> ~/fastlinq-8.50.25.0/qede-8.50.25.0/src/qede_main.c</span><br><span class="line"></span><br><span class="line"><span class="comment">#define CHIP_NUM_57980S_IOV             0x1664</span></span><br><span class="line"><span class="comment">#define CHIP_NUM_AH                     0x8070</span></span><br><span class="line"><span class="comment">#define CHIP_NUM_AH_IOV                 0x8090</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#define PCI_DEVICE_ID_57980S_IOV        CHIP_NUM_57980S_IOV</span></span><br><span class="line"><span class="comment">#define PCI_DEVICE_ID_AH                CHIP_NUM_AH</span></span><br><span class="line"><span class="comment">#define PCI_DEVICE_ID_AH_IOV            CHIP_NUM_AH_IOV</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#show PCIE status</span></span><br><span class="line">05:00.0 Ethernet controller: Intel Corporation Ethernet Controller XXV710 <span class="keyword">for</span> 25GbE SFP28 (rev 02)</span><br><span class="line"></span><br><span class="line">$ lspci -s 05:00.0 -vvv | grep -Ei <span class="string">&#x27;8G|MSI-X&#x27;</span></span><br><span class="line">      Capabilities: [70] MSI-X: Enable+ Count=129 Masked-</span><br><span class="line">              LnkCap: Port <span class="comment">#0, Speed 8GT/s, Width x8, ASPM L1, Latency L0 &lt;2us, L1 &lt;16us</span></span><br><span class="line">              LnkSta: Speed 8GT/s, Width x8, TrErr- Train- SlotClk+ DLActive- BWMgmt- ABWMgmt-</span><br></pre></td></tr></table></figure>

<h4 id="Linux-network-performance-PERFORMANCE"><a href="#Linux-network-performance-PERFORMANCE" class="headerlink" title="Linux network performance PERFORMANCE"></a>Linux network performance PERFORMANCE</h4><p><a target="_blank" rel="noopener" href="https://lwn.net/Articles/629155/">The problem, Jesper said, is that the kernel developers have focused on scaling out to large numbers of cores. In the process, they have been able to hide regressions in per-core efficiency. The networking stack, as a result, works well for many workloads, but workloads that are especially latency-sensitive have suffered. The kernel, today, can only forward something between 1M and 2M packets per core every second, while some of the bypass alternatives approach a rate of 15M packets per core per second. Then there is the cost of performing a system call. On a system with SELinux and auditing enabled, that cost is just over 75ns — over the time budget on its own. Disabling auditing and SELinux reduces the time required to just under 42ns, which is better, but that is still a big part of the time budget. There are ways of amortizing that cost over multiple packets; they include system calls like sendmmsg(), recvmmsg(), sendfile(), and splice().</a></p>
<h5 id="Socket-receive-queues"><a href="#Socket-receive-queues" class="headerlink" title="Socket receive queues"></a>Socket receive queues</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## disable NAPI</span></span><br><span class="line">$ ethtool -C ethx rx-usecs 0 <span class="comment">##if disabled, tons of CPU interrupts</span></span><br><span class="line"></span><br><span class="line">seq_printf(<span class="built_in">seq</span>,</span><br><span class="line">           <span class="string">&quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&quot;</span>,</span><br><span class="line">           sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, 0,</span><br><span class="line">           0, 0, 0, 0, /* was fastroute */</span><br><span class="line">           sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);</span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> /proc/net/softnet_stat</span><br><span class="line">00119a6b 00000000 00000095</span><br><span class="line">00000000 00000000 00000000</span><br><span class="line">0007cb46 00000000 00000291</span><br><span class="line">                      |</span><br><span class="line">                      |time_squeeze parameters</span><br><span class="line">                      V</span><br><span class="line">$ <span class="built_in">cat</span> /proc/sys/net/core/dev_weight</span><br><span class="line">64</span><br><span class="line">Maximum number of packets the driver can receive during a NAPI interrupt, per CPU</span><br><span class="line">NAPI polling 时每核每次软中断最多处理的帧数量</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/sys/net/core/core.netdev_budget</span><br><span class="line">300</span><br><span class="line">Maximum number of packets received <span class="keyword">in</span> one NAPI polling cycle, total <span class="keyword">for</span> all interfaces/CPUs. Cannot exceed</span><br><span class="line">是所有网卡每次软中断最多处理的总帧数量, 在时延和吞吐之间平衡</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/sys/net/core/netdev_budget_usecs</span><br><span class="line">8000</span><br><span class="line">Time <span class="keyword">in</span> microseconds of one NAPI polling cycle</span><br></pre></td></tr></table></figure>
<ul>
<li><p>The maximum number of packets that kernel can handle on a NAPI interrupt, it’s a Per-CPU variable. For drivers that support LRO HW or GRO_HW, a hardware aggregated packet is counted as one packet in this context</p>
</li>
<li><p>Altering the drain rate of a queue is usually the simplest way to mitigate poor network performance. However, increasing the number of packets that a device can receive at one time uses additional processor time, during which no other processes can be scheduled, so this can cause other performance problems</p>
</li>
<li><p>netdev_budget is the maximum number of packets taken from all interfaces in one polling cycle (NAPI poll). In one polling cycle interfaces which are registered to polling are probed in a round-robin manner. Also, a polling cycle may not exceed netdev_budget_usecs microseconds, even if netdev_budget has not been exhausted.<br>Maximum number of packets received in one NAPI polling cycle, total for all interfaces&#x2F;CPUs. Cannot exceed</p>
</li>
<li><p>The maximum number of packets that kernel can handle on a NAPI interrupt, it’s a Per-CPU variable. For drivers that support LRO or GRO_HW, a hardware aggregated packet is counted as one packet in this context.</p>
</li>
<li><p>Maximum number of packets taken from all interfaces in one polling cycle (NAPI poll).</p>
<ul>
<li>In one polling cycle interfaces which are registered to polling are probed in a round-robin manner. Also, a polling cycle may not exceed netdev_budget_usecs microseconds, even if netdev_budget has not been exhausted</li>
</ul>
</li>
</ul>
<h5 id="proc-net-softnet-stat"><a href="#proc-net-softnet-stat" class="headerlink" title="&#x2F;proc&#x2F;net&#x2F;softnet_stat"></a>&#x2F;proc&#x2F;net&#x2F;softnet_stat</h5><p><a target="_blank" rel="noopener" href="https://blog.cloudflare.com/how-to-achieve-low-latency/">achieve low lat</a><br><a target="_blank" rel="noopener" href="https://github.com/majek/dump/blob/master/how-to-receive-a-packet/softnet.sh">softnet.sh</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /proc/net/softnet_stat</span><br><span class="line"><span class="comment"># 4 cores, each line means a &quot;struct softnet_data&quot;, each line each core</span></span><br><span class="line">000cc282 00000000 000004e7 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">0000b828 00000000 00000053 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">0000945b 00000000 00000046 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">0003d716 00000000 000001f7 00000000 00000000 00000000 00000000 00000000 00000000 00000000 00000000</span><br><span class="line">-----    -----    --------                                              -------  -------</span><br><span class="line">|         \             \                                                  \        \-flow <span class="built_in">limit</span> count</span><br><span class="line">- sd-&gt;processe frame num \                                                  \-- cpu collision, transmit packages lock collision</span><br><span class="line">            \             \- time sequeeze, <span class="keyword">if</span> the count increase, increase the net.core.netdev_budget/net.core.dev_weight</span><br><span class="line">             \             \- and keep more cpu/mem resource, the budget or time <span class="built_in">limit</span> exhausted, no enough time to softirq</span><br><span class="line">              \----sd-&gt;dropped, The netdev_max_backlog is a queue within the Linux kernel <span class="built_in">where</span> traffic is stored after reception from the NIC, one backlog queue per CPU core.</span><br><span class="line"></span><br><span class="line"><span class="comment">#Each line of /proc/net/softnet_stat corresponds to a struct softnet_data structure, of which there is 1 per CPU.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#The values are separated by a single space and are displayed in hexadecimal</span></span><br><span class="line"><span class="comment">#The first value, sd-&gt;processed, is the number of network frames processed. This can be more than the total number of network frames received if you are using ethernet bonding. There are cases where the ethernet bonding driver will trigger network data to be re-processed, which would increment the sd-&gt;processed count more than once for the same packet.</span></span><br><span class="line"></span><br><span class="line">The second value, sd-&gt;dropped, is the number of network frames dropped because there was no room on the processing queue. More on this later. If second values are gorwing, improve sysctl -w net.core.netdev_max_backlog = 2000, A value over 10000 is unlikely to be very helpful.</span><br><span class="line">The second colume number of frames dropped due to netdev_max_backlog being exceeded</span><br><span class="line"></span><br><span class="line">The netdev_max_backlog is a queue within the Linux kernel <span class="built_in">where</span> traffic is stored after reception from the NIC, but before processing by the protocol stacks (IP, TCP, etc). There is one backlog queue per CPU core.</span><br><span class="line">In CentOS 7.8 the default is 1000</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#The third value, sd-&gt;time_squeeze, is (as we saw) the number of times the net_rx_action loop terminated because the budget was consumed or the time limit was reached, but more work could have been. Increasing the budget as explained earlier can help reduce this.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#If 3rd column is growing,improve sysctl -w net.core.netdev_budget=600</span></span><br><span class="line">Be careful of increasing this value unless there is a very good reason. A value exceeding 1000 is unlikely to be very helpful. In fact increasing this value too much can have detrimental effect and <span class="keyword">in</span> the worse <span class="keyword">case</span> scenario lead to softirq hangs or performance problems, as the softirqs can run <span class="keyword">for</span> too long and starve other processes of CPU</span><br><span class="line">3rd colume is number of <span class="built_in">times</span> ksoftirqd ran out of netdev_budget or CPU time when there was still work to be <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#This is much faster, but brings up another problem. What happens if we have so many packets to process that we spend all our time processing packets from the NIC, but we never have time to let the userspace processes actually drain those queues (read from TCP connections, etc.)? Eventually the queues would fill up, and we&#x27;d start dropping packets. To try and make this fair, the kernel limits the amount of packets processed in a given softirq context to a certain budget. Once this budget is exceeded, it wakes up a separate thread called ksoftirqd (you&#x27;ll see one of these in ps for each core) which processes these softirqs outside of the normal syscall/interrupt path. This thread is scheduled using the standard process scheduler, which already tries to be fair.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#net.core.netdev_budget=300 (default), This will cause the SoftIRQ process to drain 300 messages from the NIC before getting off the CPU, let the CPU do the others job</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#The next 5 values are always 0.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#The ninth value, sd-&gt;cpu_collision, is a count of the number of times a collision occurred when trying to obtain a device lock when transmitting packets. This article is about receive, so this statistic will not be seen below.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#The tenth value, sd-&gt;received_rps, is a count of the number of times this CPU has been woken up to process packets via an Inter-processor Interrupt</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#The last value, flow_limit_count, is a count of the number of times the flow limit has been reached. Flow limiting is an optional Receive Packet Steering feature that will be examined shortly.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#If you decide to monitor this file and graph the results, you must be extremely careful that the ordering of these fields hasn&#x27;t changed and that the meaning of each field has been preserved. You will need to read the kernel source to verify this.</span></span><br><span class="line"></span><br><span class="line">第 1 列是 processed 网络帧的计数，第 2 列是 dropped 计数也就是因 input_pkt_queue 不能处理导致的丢包数（和 ring buffer 满导致的丢包是两个问题），第 3 列是 NAPI 中由于 budget 或 time <span class="built_in">limit</span> 用&gt;完而退出 net_rx_action 循环的次数，4-8 列没有意义因此全是0，第 9 列是 CPU 为了发送包而获取锁的时候有冲突的次数，第 10 列是 CPU 被其他 CPU 唤醒来处理 backlog 数据的次数，第 11 列是触发 flow_limit 限制的次数</span><br><span class="line"></span><br><span class="line">seq_printf(<span class="built_in">seq</span>,</span><br><span class="line">       <span class="string">&quot;%08x %08x %08x %08x %08x %08x %08x %08x %08x %08x %08x\n&quot;</span>,</span><br><span class="line">       sd-&gt;processed, sd-&gt;dropped, sd-&gt;time_squeeze, 0,</span><br><span class="line">       0, 0, 0, 0, /* was fastroute */</span><br><span class="line">       sd-&gt;cpu_collision, sd-&gt;received_rps, flow_limit_count);</span><br><span class="line"></span><br><span class="line"><span class="comment">#convert these data by bash</span></span><br><span class="line"><span class="built_in">cat</span> ./softnet_stat.sh</span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN &#123;</span></span><br><span class="line"><span class="string">     t[1]=&quot;sd-&gt;processed&quot;</span></span><br><span class="line"><span class="string">     t[2]=&quot;sd-&gt;dropped&quot;</span></span><br><span class="line"><span class="string">     t[3]=&quot;sd-&gt;time_squeeze&quot;</span></span><br><span class="line"><span class="string">     t[9]=&quot;sd-&gt;cpu_collision&quot;</span></span><br><span class="line"><span class="string">     t[10]=&quot;sd-&gt;received_rps&quot;</span></span><br><span class="line"><span class="string">     printf &quot;%s %s %s %s %s\n&quot;,t[1],t[2],t[3],t[9],t[10];</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">&#123;</span></span><br><span class="line"><span class="string">   printf &quot;%d %d %d %d %d\n&quot;, strtonum( &quot;0x&quot;$1 ),strtonum( &quot;0x&quot;$2 ),strtonum( &quot;0x&quot;$3 ),strtonum( &quot;0x&quot;$9 ),strtonum( &quot;0x&quot;$10 )</span></span><br><span class="line"><span class="string">&#125;&#x27;</span>  /proc/net/softnet_stat | column -t</span><br><span class="line"></span><br><span class="line">$ watch -d sh ./softnet_stat.sh</span><br><span class="line">Every 2.0s: sh ./softnet_stat.sh                                                                                                  Mon Feb 17 00:23:15 2020</span><br><span class="line"></span><br><span class="line">sd-&gt;processed  sd-&gt;dropped  sd-&gt;time_squeeze  sd-&gt;cpu_collision  sd-&gt;received_rps</span><br><span class="line">46897674       0            15                0                  0</span><br><span class="line">33150875       0            4                 0                  0</span><br><span class="line">46371491       0            10                0                  0</span><br><span class="line">64446426       0            7                 0                  0</span><br><span class="line">57994272       0            11                0                  0</span><br><span class="line">34935375       0            90                0                  0</span><br><span class="line"></span><br><span class="line"><span class="comment">#when I was watch it, the time_squeeze not increased. only slowly increasing at rx_steer_missed_packets: 1623</span></span><br><span class="line"><span class="comment">#Herer is mellanox doc: Number of packets that was received by the NIC, however was discarded because it did not match any flow in the NIC flow table. supported from kernel 4.16</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># here is 2 v 1 test case,  node 1 and node 2 send and receive to node 3, there is no tcp retrans in node(little , 0.5/s), but it was full throughput (bond, dual port, tx:2.4GB/s tx 2.4GB/s), only a lot of tcp retrans in node 1 and node 2 (190~200/s)</span></span><br><span class="line"></span><br><span class="line">If you follow the softnet_break label you stumble upon something interesting. From net/core/dev.c:</span><br><span class="line">softnet_break:</span><br><span class="line">  sd-&gt;time_squeeze++;</span><br><span class="line">  __raise_softirq_irqoff(NET_RX_SOFTIRQ);</span><br><span class="line">  goto out;</span><br><span class="line"></span><br><span class="line"><span class="comment">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">cmd=<span class="string">&quot;<span class="variable">$&#123;0##*/&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">usage</span></span>() &#123;</span><br><span class="line"><span class="built_in">cat</span> &gt;&amp;2 &lt;&lt;<span class="string">EOI</span></span><br><span class="line"><span class="string">usage: $cmd [ -h ]</span></span><br><span class="line"><span class="string">Output column definitions:</span></span><br><span class="line"><span class="string">      cpu  # of the cpu</span></span><br><span class="line"><span class="string">    total  # of packets (not including netpoll) received by the interrupt handler</span></span><br><span class="line"><span class="string">             There might be some double counting going on:</span></span><br><span class="line"><span class="string">                net/core/dev.c:1643: __get_cpu_var(netdev_rx_stat).total++;</span></span><br><span class="line"><span class="string">                net/core/dev.c:1836: __get_cpu_var(netdev_rx_stat).total++;</span></span><br><span class="line"><span class="string">             I think the intention was that these were originally on separate</span></span><br><span class="line"><span class="string">             receive paths ...</span></span><br><span class="line"><span class="string">  dropped  # of packets that were dropped because netdev_max_backlog was exceeded</span></span><br><span class="line"><span class="string"> squeezed  # of times ksoftirq ran out of netdev_budget or time slice with work</span></span><br><span class="line"><span class="string">             remaining</span></span><br><span class="line"><span class="string">collision  # of times that two cpus collided trying to get the device queue lock.</span></span><br><span class="line"><span class="string">EOI</span></span><br><span class="line">        <span class="built_in">exit</span> 1</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">softnet_stats_header</span></span>() &#123;</span><br><span class="line">        <span class="built_in">printf</span> <span class="string">&quot;%3s %10s %10s %10s %10s %10s %10s\n&quot;</span> cpu total dropped squeezed collision rps flow_limit</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">softnet_stats_format</span></span>() &#123;</span><br><span class="line">        <span class="built_in">printf</span> <span class="string">&quot;%3u %10lu %10lu %10lu %10lu %10lu %10lu\n&quot;</span> <span class="string">&quot;<span class="variable">$1</span>&quot;</span> <span class="string">&quot;0x<span class="variable">$2</span>&quot;</span> <span class="string">&quot;0x<span class="variable">$3</span>&quot;</span> <span class="string">&quot;0x<span class="variable">$4</span>&quot;</span> <span class="string">&quot;0x<span class="variable">$5</span>&quot;</span> <span class="string">&quot;0x<span class="variable">$6</span>&quot;</span> <span class="string">&quot;0x<span class="variable">$7</span>&quot;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">getopts</span> h flag &amp;&amp; usage</span><br><span class="line"></span><br><span class="line">cpu=0</span><br><span class="line">softnet_stats_header</span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> total dropped squeezed j1 j2 j3 j4 j5 collision rps flow_limit_count</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">        <span class="comment"># the last field does not appear on older kernels</span></span><br><span class="line">        <span class="comment"># https://github.com/torvalds/linux/commit/99bbc70741903c063b3ccad90a3e06fc55df9245#diff-5dd540e75b320a50866267e9c52b3289R165</span></span><br><span class="line">        softnet_stats_format $((cpu++)) <span class="string">&quot;<span class="variable">$total</span>&quot;</span> <span class="string">&quot;<span class="variable">$dropped</span>&quot;</span> <span class="string">&quot;<span class="variable">$squeezed</span>&quot;</span> <span class="string">&quot;<span class="variable">$collision</span>&quot;</span> <span class="string">&quot;<span class="variable">$rps</span>&quot;</span> <span class="string">&quot;<span class="variable">$&#123;flow_limit_count:-0&#125;</span>&quot;</span></span><br><span class="line"><span class="keyword">done</span> &lt; /proc/net/softnet_stat</span><br></pre></td></tr></table></figure>
<p>print lock-owned   </p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">lock_sock_nested</span><span class="params">(<span class="keyword">struct</span> sock *sk, <span class="type">int</span> subclass)</span> &#123;</span><br><span class="line">...</span><br><span class="line">  <span class="keyword">if</span> (sk-&gt;sk_lock.owned)</span><br><span class="line">        __lock_sock(sk);</span><br><span class="line">    <span class="comment">//上面__lock_sock()返回后现场已经被还原，即持有锁并且已经关闭下半部。</span></span><br><span class="line">    <span class="comment">//将owned设置为1，表示本进程现在持有该传输控制块</span></span><br><span class="line">    sk-&gt;sk_lock.owned = <span class="number">1</span>;</span><br><span class="line">    <span class="comment">//释放锁但是没有开启下半部-----还是关闭了 软中断</span></span><br><span class="line">    spin_unlock(&amp;sk-&gt;sk_lock.slock);</span><br><span class="line"></span><br><span class="line">owned为<span class="number">1</span>之后不再持有自旋锁，也已经开启软中断。-----作用是协议栈的处理并非立刻就能结束，如果只是简单的在开始起持有自旋锁并关闭下半部，在处理结束时释放自旋锁并打开下半部，会降低系统性能，同时&gt;长时间关闭软中断，还可能使得网卡接收软中断得不到及时调用，导致丢包</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#!/usr/bin/env bpftrace</span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;net/sock.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">k:tcp_delack_timer</span><br><span class="line">&#123;</span><br><span class="line">        @sk[tid] = arg0;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">kr:tcp_delack_timer</span><br><span class="line">/@sk[tid]/</span><br><span class="line">&#123;</span><br><span class="line">        $sk = (<span class="keyword">struct</span> sock *)@sk[tid];</span><br><span class="line"></span><br><span class="line">        $af = $sk-&gt;__sk_common.skc_family;</span><br><span class="line">        $ulock = $sk-&gt;sk_lock.owned;</span><br><span class="line">        <span class="comment">//if ($af == AF_INET) &#123;</span></span><br><span class="line">        <span class="keyword">if</span> ($ulock != <span class="number">0</span>) &#123;</span><br><span class="line">            $daddr = ntop($af, $sk-&gt;__sk_common.skc_daddr);</span><br><span class="line">            $saddr = ntop($af, $sk-&gt;__sk_common.skc_rcv_saddr);</span><br><span class="line">            $lport = $sk-&gt;__sk_common.skc_num;</span><br><span class="line">            $dport = $sk-&gt;__sk_common.skc_dport;</span><br><span class="line">            $dport = ($dport &gt;&gt; <span class="number">8</span>) | (($dport &lt;&lt; <span class="number">8</span>) &amp; <span class="number">0xff00</span>);</span><br><span class="line">            <span class="built_in">printf</span>(<span class="string">&quot;%s %d %-15s %-5d -&gt; %-15s %-5d, lock-owned: %d retval: %d\n&quot;</span>,</span><br><span class="line">                comm, tid, $saddr, $lport, $daddr, $dport, $ulock, retval);</span><br><span class="line">        &#125;</span><br><span class="line">        delete(@sk[tid]);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">END &#123;</span><br><span class="line">   clear(@sk);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h4 id="Hardware-features"><a href="#Hardware-features" class="headerlink" title="Hardware features"></a>Hardware features</h4><table>
<thead>
<tr>
<th>Features</th>
<th>status</th>
</tr>
</thead>
<tbody><tr>
<td>TSO TCP Segmentation Offload</td>
<td>hardware off, outdated</td>
</tr>
<tr>
<td>GSO Generic Segmentation Offload,hard TSO</td>
<td>GSO replaced TSO</td>
</tr>
<tr>
<td>GRO Generic Receive Offload&#x2F;LRO</td>
<td>GRO replaced LRO; options ixgbe LRO&#x3D;1,  LRO hard outdated</td>
</tr>
<tr>
<td>UFO UDP Fragmentation Offload</td>
<td>hardware off, outdated</td>
</tr>
<tr>
<td>rx-checksumming</td>
<td>hardware on</td>
</tr>
<tr>
<td>tx-checksumming</td>
<td>hardware on</td>
</tr>
<tr>
<td>scatter-gather</td>
<td>hardware on</td>
</tr>
<tr>
<td>RSS Receive side scaling</td>
<td>hardware on (hardware support)</td>
</tr>
<tr>
<td>RPS software RSS</td>
<td>software off, if not support, enable it, disable it in RSS adapter: echo 0 &gt; &#x2F;sys&#x2F;class&#x2F;net&#x2F;<dev>&#x2F;queues&#x2F;rx-<n>&#x2F;rps_cpus</td>
</tr>
<tr>
<td>RFS Receive Flow Streering,UDP support ?</td>
<td>software off</td>
</tr>
<tr>
<td>ACCELERATED RFS</td>
<td>hardware on, looks like deps software RFS ?</td>
</tr>
<tr>
<td>XPS Transmit Packet Steering</td>
<td>software on</td>
</tr>
<tr>
<td>TOE tcp offloading engine</td>
<td>some said not support linux, <a target="_blank" rel="noopener" href="https://wiki.linuxfoundation.org/networking/toe">why not support</a>, I found the Chelsio iWarp support linux</td>
</tr>
<tr>
<td>busy-poll: on fixed</td>
<td>hw and sw, work with sysctl.net.core.busy_poll &gt; 0</td>
</tr>
<tr>
<td>tx-udp_tnl-segmentation</td>
<td>hardware on</td>
</tr>
<tr>
<td>tx-udp_tnl-csum-segmentation</td>
<td>hardware on</td>
</tr>
</tbody></table>
<ul>
<li>TSO &#x3D; LSO (also called large segmentation offload)   <ul>
<li>if TSO was on, disable GSO, same with RSS and RPS, generally, the GSO will be enabled, TSO has outted</li>
</ul>
</li>
<li>RFS and XPS has the same function from receive or transmit, avoid cache miss, numa overhead   <ul>
<li>XPS setting &#x2F;sys&#x2F;class&#x2F;net&#x2F;enp2s0f0&#x2F;queues&#x2F;tx-*&#x2F;xps_cpus</li>
</ul>
</li>
<li>Receive Packet Steering (RPS) is logically a software implementation of RSS    <ul>
<li>&#x2F;proc&#x2F;sys&#x2F;net&#x2F;core&#x2F;rps_sock_flow_entries</li>
<li>&#x2F;sys&#x2F;class&#x2F;net&#x2F;<dev>&#x2F;queues&#x2F;rx-<n>&#x2F;rps_flow_cnt</li>
</ul>
</li>
<li>Generic segmentation offload (GSO) is logically a software implementation of TSO   </li>
<li>Enabling the RFS requires enabling the ‘ntuple’ flag via the ethtool,RFS requires the kernel to be compiled with the CONFIG_RFS_ACCEL option. This options is available in kernels 2.6.39 and above. Furthermore, RFS requires Device Managed Flow Steering support.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">+---------------+</span><br><span class="line">|  Application  |</span><br><span class="line">+---------------+</span><br><span class="line">|    TCP/IP     |</span><br><span class="line">+---------------+</span><br><span class="line">|  nit_if/PCAP  | &lt;--- tcpdump / wireshark</span><br><span class="line">+---------------+</span><br><span class="line">|      Nic      | GSO Split the big package into separate packets</span><br><span class="line">+---------------+</span><br><span class="line">        |</span><br><span class="line">        |         Network(Packets size less than MTU)</span><br><span class="line">        +----------------------------------------------------&gt;`</span><br></pre></td></tr></table></figure>

<h5 id="busy-poll-Interrupt-Queues"><a href="#busy-poll-Interrupt-Queues" class="headerlink" title="busy poll(Interrupt Queues)"></a>busy poll(Interrupt Queues)</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Busy polling behavior is supported by the following drivers. These drivers are also supported on Red Hat Enterprise Linux 7.1</span><br><span class="line">driver support: bnx2x,be2net,ixgbe,mlx4,myri10ge</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /boot/config-3.10.0-1127.el7.x86_64 | grep BUSY_POLL</span><br><span class="line">CONFIG_NET_RX_BUSY_POLL=y</span><br><span class="line"></span><br><span class="line">$ ethtool -k em1  |grep busy</span><br><span class="line">busy-poll: off [fixed]</span><br><span class="line">$ ethtool -K <span class="variable">$nic_dev</span> busy-poll on</span><br></pre></td></tr></table></figure>
<ul>
<li>The busy polling helps reduce latency in the network receive path by allowing socket layer code to poll the receive queue of a network device, and disabling network interrupts</li>
<li><code>This removes delays caused by the interrupt and the resultant context switch.</code></li>
<li>However, <code>it also increases CPU utilization</code>. Busy polling also prevents the CPU from sleeping, which can incur additional power consumption.</li>
<li>single queue map single cpu core, avoid lock or race cpu resource</li>
</ul>
<p>kernel 3.11 support SO_BUSY_POLL</p>
<ul>
<li>Busy polling is disabled by default (sysctl.net.core.busy_poll &#x3D; 0)<ul>
<li>This parameter controls the number of microseconds to wait for packets on the device queue for socket reads.<br>This parameter controls the number of microseconds to wait for packets on the device queue for socket poll and selects. Red Hat recommends a value of 50<br>Red Hat recommends a value of 50 for a small number of sockets, and a value of 100 for large numbers of sockets. For extremely large numbers of sockets (more than several hundred), use epoll(kernel 4.12) instead.<br><a target="_blank" rel="noopener" href="https://oxnz.github.io/2016/05/03/performance-tuning-networking/">Busy polling helps reduce latency in the network receive path by</a><br>it allowing socket layer code to poll the receive queue of a network device and disable network interrupts</li>
</ul>
</li>
</ul>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">#sw</span><br><span class="line">#if the NIC firmware support busy poll and enough CPU resource</span><br><span class="line">net.core.busy_poll = 50 # default 0</span><br><span class="line">#This parameter controls the number of microseconds to wait for packets on the device queue for socket poll and selects</span><br><span class="line"></span><br><span class="line">net.core.busy_read = 50/100 # default 0</span><br><span class="line"># This parameter controls the number of microseconds to wait for packets on the device queue for socket reads</span><br><span class="line">#bnx2x</span><br><span class="line">#be2net</span><br><span class="line">#ixgbe</span><br><span class="line">#mlx4</span><br><span class="line">#myri10ge</span><br><span class="line"></span><br><span class="line">#hw</span><br><span class="line">#mlx4 driver support the busy polling</span><br><span class="line">$ ethtool -k device | grep &quot;busy-poll&quot;</span><br><span class="line">busy-poll: on [fixed]</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>rx-frames[-irq] rx-usecs[-irq] tx-frames[-irq] tx-usecs[-irq]<ul>
<li>The range of 0-235 microseconds provides an effective range of 4,310 to 250,000 interrupts per second. The value of rx-µsecs-high can be set independent of rx-µsecs and tx-µsecs in the same ethtool command, and is also independent of the adaptive interrupt moderation algorithm. The underlying hardware supports granularity in 2-microsecond intervals, so adjacent values might result in the same interrupt rate.</li>
</ul>
</li>
<li>rx-usecs This is the number of microseconds to wait before raising an RX interrupt after a packet has been received. When rx-usecs is set to 0 rx-frames is used</li>
<li>rx-frames     This is the number of frames to queue up before raising an RX interrupt.</li>
<li>adaptive-tx Dynamic control to decrease TX latency at low packet rates and increase throughput at high packet rates</li>
<li>tx-usecs This is the number of microseconds to wait before raising an TX interrupt after a packet has been sent. When tx-usecs is set to 0 tx-frames is used</li>
<li>tx-frames This is the number of frames to queue up before raising an TX interrupt<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#To turn on adaptive interrupt moderation, recommand</span></span><br><span class="line">$ ethtool -C ethX adaptive-rx on adaptive-tx on</span><br><span class="line"></span><br><span class="line"><span class="comment">#To turn off adaptive interrupt moderation</span></span><br><span class="line">$ ethtool -C ethX adaptive-rx off adaptive-tx off</span><br><span class="line"></span><br><span class="line">$ ethtool -c enp4s0f1</span><br><span class="line">Coalesce parameters <span class="keyword">for</span> enp4s0d1:</span><br><span class="line">Adaptive RX: off  TX: off</span><br><span class="line">stats-block-usecs: 0</span><br><span class="line">sample-interval: 0</span><br><span class="line">pkt-rate-low: 400000</span><br><span class="line">pkt-rate-high: 450000</span><br><span class="line"></span><br><span class="line">rx-usecs: 0</span><br><span class="line">rx-frames: 0</span><br><span class="line">rx-usecs-irq: 0</span><br><span class="line">rx-frames-irq: 0</span><br><span class="line"></span><br><span class="line">tx-usecs: 8</span><br><span class="line">tx-frames: 16</span><br><span class="line">tx-usecs-irq: 0</span><br><span class="line">tx-frames-irq: 256</span><br><span class="line"></span><br><span class="line">rx-usecs-low: 0</span><br><span class="line">rx-frame-low: 0</span><br><span class="line">tx-usecs-low: 0</span><br><span class="line">tx-frame-low: 0</span><br><span class="line"></span><br><span class="line">rx-usecs-high: 128</span><br><span class="line">rx-frame-high: 0</span><br><span class="line">tx-usecs-high: 0</span><br><span class="line"></span><br><span class="line">tx-usecs: 28</span><br><span class="line">tx-frames: 30</span><br><span class="line">tx-usecs-irq: 2</span><br><span class="line">tx-frames-irq: 2</span><br><span class="line"></span><br><span class="line">direct connect mellanox 5 by DAC, there is no any retrans <span class="keyword">in</span> mellanox adapter, the BCM57508 NetXtreme-E</span><br><span class="line"></span><br><span class="line"><span class="comment">#set it to BCM 57508, the rx tcp retrans and bandwidth descased in centos 7 with bnxt_en (1.10.2-223.0.162.0)</span></span><br><span class="line"></span><br><span class="line">                         (28 to 56)     (30 to 60)</span><br><span class="line">$ ethtool -C ens1f0 tx-usecs 56 tx-frames 60 tx-usecs-irq 2 tx-frames-irq 2</span><br></pre></td></tr></table></figure></li>
</ul>
<p>If you require low latency performance and&#x2F;or have plenty of CPU to devote to network processing, you can disable interrupt moderation entirely, which enables the interrupts to fire as fast as possible     </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -C ethX adaptive-rx off adaptive-tx off rx-usecs 0 tx-usecs 0</span><br></pre></td></tr></table></figure>
<p>Totaly, tx,rx-usecs value the lower means the smaller frame to interrupt moderation. the more cpu overhead and the lower latency<br>increase tx,tx-usecs that means high latency and get the biger frame to interrupt moderation, the lower cpu overhead and high latency and high througput, it will impact tcp performance, so increase tcp_tso_win_divisor to 30    </p>
<p><a target="_blank" rel="noopener" href="http://www.cnhalo.net/2017/07/24/linux-busy-poll/">netperf TCP_RR 1 byte payload each way, 3.11 kernel,default 17500tps, busy poll could reach 63000 tps</a>   </p>
<h5 id="ARFS-support"><a href="#ARFS-support" class="headerlink" title="ARFS support"></a>ARFS support</h5><ul>
<li>CPU to queue mapping is deduced based on the IRQ affinities configured by the driver for each receive queue.<ul>
<li>RFS enabled</li>
<li>CONFIG_RFS_ACCEL enabled</li>
<li>Enable ntuple</li>
<li>Configure your IRQ settings to ensure each RX queue is handled by one of your desired network processing CPUs</li>
</ul>
</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ grep RFS /boot/config-4.18.0-305.el8.x86_64</span><br><span class="line">CONFIG_RFS_ACCEL=y</span><br><span class="line">CONFIG_MLX5_EN_ARFS=y</span><br><span class="line"></span><br><span class="line">$ ethtool -K ens6 ntuple on</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/sys/net/core/rps_sock_flow_entries</span><br><span class="line">$ <span class="built_in">cat</span> /sys/class/net/&lt;dev&gt;/queues/rx-&lt;n&gt;/rps_flow_cnt</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 32768 &gt; /proc/sys/net/core/rps_sock_flow_entries</span><br><span class="line">$ <span class="keyword">for</span> f <span class="keyword">in</span> /sys/class/net/ens6/queues/rx-*/rps_flow_cnt; <span class="keyword">do</span> <span class="built_in">echo</span> 4096 &gt; <span class="variable">$f</span>; <span class="keyword">done</span></span><br><span class="line"></span><br><span class="line">Set the value of the net.core.rps_sock_flow_entries kernel value to the maximum expected number of concurrently active connections</span><br><span class="line"></span><br><span class="line">Set the value of the sys/class/net/device/queues/rx-queue/rps_flow_cnt file to the value of the (rps_sock_flow_entries/N)</span><br><span class="line"><span class="built_in">where</span> N is the number of receive queues on a device</span><br><span class="line"></span><br><span class="line">Replace N with the number of configured receive queues. For example, <span class="keyword">if</span> the rps_flow_entries is <span class="built_in">set</span> to 32768 and there are 16 configured receive queues, the rps_flow_cnt = 32786/16= 2048 (that is, rps_flow_cnt = rps_flow_enties/N ).</span><br></pre></td></tr></table></figure>

<ul>
<li><a target="_blank" rel="noopener" href="https://community.mellanox.com/s/article/howto-configure-arfs-on-connectx-4">Test on the Mellanox</a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ ethtool -S eno1 | egrep rx.*pack</span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">$ ethtool -K ens6 ntuple off</span><br><span class="line"></span><br><span class="line">$ taskset -c 5 netserver &amp;</span><br><span class="line">$ netperf -H <span class="variable">$ipaddr</span> -l 200 -t TCP_STREAM &amp;</span><br><span class="line"></span><br><span class="line">$ ethtool -S ens6 | egrep rx.*pack</span><br><span class="line"><span class="comment"># only rx8 improved</span></span><br><span class="line">rx7_packets: 0</span><br><span class="line">rx7_lro_packets: 0</span><br><span class="line">rx8_packets: 6296748</span><br><span class="line">rx8_lro_packets: 0</span><br><span class="line">rx9_packets: 0</span><br><span class="line"></span><br><span class="line">$ ethtool -K ens6 ntuple on</span><br><span class="line">$ taskset -c 5 netserver &amp;</span><br><span class="line">$ netperf -H 11.134.201.5 -l 200 -t TCP_STREAM &amp;</span><br><span class="line">$ ethtool -S ens6 | egrep rx.*pack</span><br><span class="line">rx5_packets: 234532 <span class="comment"># only rx5 increase, it &#x27;s worked</span></span><br><span class="line">rx5_lro_packets: 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># mellanox enable the driver arfs</span></span><br><span class="line">$ enable_arfs.sh ens6</span><br></pre></td></tr></table></figure></li>
</ul>
<h5 id="RSS-support"><a href="#RSS-support" class="headerlink" title="RSS support"></a>RSS support</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool -n enp2s0f0</span><br><span class="line">8 RX rings available</span><br><span class="line">Total 0 rules</span><br><span class="line"></span><br><span class="line">$ lspci -v -s 83:00.0 | grep &quot;MSI-X: Enable+&quot;</span><br><span class="line">83:00.0 Ethernet controller: Intel Corporation 82599ES 10-Gigabit SFI/SFP+ Network Connection (rev 01)</span><br><span class="line">        Flags: bus master, fast devsel, latency 0, IRQ 247, NUMA node 1</span><br><span class="line">        I/O ports at d020 [size=32]</span><br><span class="line">        Capabilities: [50] MSI: Enable- Count=1/1 Maskable+ 64bit+</span><br><span class="line">        Capabilities: [70] MSI-X: Enable+ Count=64 Masked-</span><br></pre></td></tr></table></figure>
<p>Some devices have the ability to write incoming packets to several different regions of RAM simultaneously; each region is a separate queue. This allows the OS to use multiple CPUs to process incoming data in parallel, starting at the hardware level  </p>
<p><a target="_blank" rel="noopener" href="https://access.redhat.com/solutions/2144921">RSS IRQ Affinity</a>   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /sys/class/net/[interface]/device/numa_node</span><br><span class="line">$ <span class="built_in">cat</span> /sys/devices/[PCI root]/[PCIe <span class="keyword">function</span>]/numa_node</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>CPU</th>
<th>0</th>
<th>1</th>
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
</tr>
</thead>
<tbody><tr>
<td>bin</td>
<td>0001</td>
<td>0010</td>
<td>0100</td>
<td>1000</td>
<td>10000</td>
<td></td>
<td></td>
<td>10000000</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>Deci</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>8</td>
<td>16</td>
<td>32</td>
<td>64</td>
<td>128</td>
<td>256</td>
<td>512</td>
<td>1024</td>
<td>2048</td>
<td>4096</td>
</tr>
<tr>
<td>Hex</td>
<td>1</td>
<td>2</td>
<td>4</td>
<td>8</td>
<td>10</td>
<td>20</td>
<td>40</td>
<td>80</td>
<td>100</td>
<td>200</td>
<td>400</td>
<td>800</td>
<td>1000</td>
</tr>
</tbody></table>
<p><a target="_blank" rel="noopener" href="https://bitsum.com/tools/cpu-affinity-calculator/">cpu affinity calculator</a><br><a target="_blank" rel="noopener" href="https://unix.stackexchange.com/questions/649013/why-does-awk-print-0xffffffffbb6002e0-as-ffffffffbb600000-using-printf">awk calculate limit</a>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">For calculations, this will default to the same 53 bits of precision as available with IEEE-754 doubles, but the PREC variable can be used to control that. See the manual linked above <span class="keyword">for</span> extensive details.</span><br><span class="line">There is a difference <span class="keyword">in</span> handling <span class="keyword">for</span> large integers and floating-point values requiring more than the default precision, <span class="built_in">which</span> can result <span class="keyword">in</span> surprising behaviour; large integers are parsed correctly with -M and its default settings (only subsequent calculations are affected by PREC), whereas floating-point values are stored with the precision defined at the time they are parsed</span><br><span class="line"></span><br><span class="line">                   ----------- core63 (0 to 63), the 64th cpu core</span><br><span class="line">                   |</span><br><span class="line">$ awk <span class="string">&#x27;BEGIN&#123;cnum=63; printf 2^(cnum%4);  for (i=1;i&lt;=(cnum/4);i++) &#123;printf 0&#125;; print&#125;&#x27;</span></span><br><span class="line">8000000000000000</span><br><span class="line"></span><br><span class="line"><span class="comment">### awk caculate limit</span></span><br><span class="line">nic_dev=enp129s0f0</span><br><span class="line">           -------------------CPU cores, 7 15 46 47 48 49 50 51 52 53 54 62</span><br><span class="line">           |</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> $(numactl --hardware  | grep <span class="string">&quot;node <span class="subst">$(cat /sys/class/net/$&#123;nic_dev&#125;/device/numa_node)</span> cpu&quot;</span> | awk -F: <span class="string">&#x27;&#123;print $2&#125;&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 112 113 114 115 240 241 242 243</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">   <span class="built_in">echo</span> $(awk -v cnum=<span class="variable">$i</span> <span class="string">&#x27;BEGIN&#123;printf &quot;0x&quot;2^(cnum%4);  for (i=1;i&lt;=(cnum/4);i++) &#123;printf 0&#125;; print&#125;&#x27;</span>);</span><br><span class="line"><span class="keyword">done</span> | awk -M -vPREC=500  --non-decimal-data <span class="string">&#x27;&#123;sum=sum+$1&#125; END&#123;printf &quot;%x\n&quot;,sum&#125;&#x27;</span></span><br><span class="line">f0000000000000000000000000000000f0000000000000000000000000000</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> f0000000000000000000000000000000f0000000000000000000000000000 &gt; /proc/irq/404/smp_affinity</span><br><span class="line">-bash: <span class="built_in">echo</span>: write error: Value too large <span class="keyword">for</span> defined data <span class="built_in">type</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> f000000,00000000,00000000,0000000,00f00000,00000000,0000000,00000000 &gt; /proc/irq/404/smp_affinity</span><br><span class="line">$ <span class="built_in">echo</span> $?</span><br><span class="line">0</span><br><span class="line"><span class="comment">#I test it ok in gawk 5.1, not work in gawk 4.0.2</span></span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /sys/devices/pci0000:17/0000:17:02.0/0000:18:00.0</span><br><span class="line">$ <span class="built_in">ls</span> /sys/devices/pci0000:17/0000:17:02.0/0000:18:00.0/msi_irqs</span><br><span class="line">56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85</span><br><span class="line">$ <span class="built_in">ls</span> -l /sys/devices/pci0000:17/0000:17:02.0/0000:18:00.0/msi_irqs</span><br><span class="line"></span><br><span class="line"><span class="comment">#scripts in driver</span></span><br><span class="line"><span class="comment">#intel</span></span><br><span class="line">$ set_irq_affinity -x all ethX</span><br><span class="line">$ set_irq_affinity -x <span class="built_in">local</span> ethX</span><br><span class="line">$ set_irq_affinity 1-2 ethX</span><br><span class="line"></span><br><span class="line"><span class="comment">#mellanox</span></span><br><span class="line">set_irq_affinity_bynode.sh 0 p7p1</span><br><span class="line">$ <span class="built_in">printf</span> %0.2x<span class="string">&#x27;\n&#x27;</span> 1024</span><br><span class="line">400</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">printf</span> %0.2x<span class="string">&#x27;\n&#x27;</span> 4096</span><br><span class="line">1000</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">echo</span> 400 &gt; /proc/irq/66/smp_affinity</span><br><span class="line">$ <span class="built_in">echo</span> 400 &gt; /proc/irq/67/smp_affinity</span><br><span class="line">$ <span class="built_in">echo</span> 1000 &gt; /proc/irq/69/smp_affinity</span><br><span class="line">$ <span class="built_in">echo</span> 1000 &gt; /proc/irq/70/smp_affinity</span><br></pre></td></tr></table></figure>
<ul>
<li>When configuring RSS, Red Hat recommends <code>limiting the number of queues to one per physical CPU core</code>    </li>
<li>Hyper-threads are often represented as separate cores in analysis tools, but configuring queues for all cores including logical cores such as <code>hyper-threads has not proven beneficial to network performance</code></li>
<li>When enabled, RSS distributes network processing equally between available CPUs based on the amount of processing each CPU has queued. However, you can use the ethtool –show-rxfh-indir and –set-rxfh-indir parameters to modify how network activity is distributed, and weight certain types of network activity as more important than others<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool --set-rxfh-indir enp1s0f0 equal 4</span><br><span class="line"><span class="comment">#-X = --set-rxfh-indir  Sets the receive flow hash indirection table to spread flows evenly between the first N receive queues</span></span><br><span class="line"></span><br><span class="line">$ watch -d  -n 1 <span class="string">&quot;cat /proc/interrupts | grep enp1s0f0&quot;</span></span><br><span class="line"><span class="comment"># You could see only 4 interrupt number increase a lot</span></span><br><span class="line"></span><br><span class="line">$ ethtool --show-rxfh-indir enp1s0f0</span><br><span class="line"></span><br><span class="line"><span class="comment">#get_rxfh_indir_size() + get_rxfh_indir() in the NIC driver</span></span><br><span class="line">$ ethtool -x enp1s0f0</span><br><span class="line"><span class="comment">## You can set priority for each queue.</span></span><br><span class="line">RX flow <span class="built_in">hash</span> indirection table <span class="keyword">for</span> p4p1 with 48 RX ring(s):</span><br><span class="line">    0:      0     1     2     3     4     5     6     7</span><br><span class="line">    8:      8     9    10    11    12    13    14    15</span><br><span class="line">   16:     16    17    18    19    20    21    22    23</span><br><span class="line">   24:     24    25    26    27    28    29    30    31</span><br><span class="line">   32:     32    33    34    35    36    37    38    39</span><br><span class="line">   40:     40    41    42    43    44    45    46    47</span><br><span class="line">   48:      0     1     2     3     4     5     6     7</span><br><span class="line">   56:      8     9    10    11    12    13    14    15</span><br><span class="line">   64:     16    17    18    19    20    21    22    23</span><br><span class="line">   72:     24    25    26    27    28    29    30    31</span><br><span class="line">   80:     32    33    34    35    36    37    38    39</span><br><span class="line">   88:     40    41    42    43    44    45    46    47</span><br><span class="line">   96:      0     1     2     3     4     5     6     7</span><br><span class="line">  104:      8     9    10    11    12    13    14    15</span><br><span class="line">  112:     16    17    18    19    20    21    22    23</span><br><span class="line">  120:     24    25    26    27    28    29    30    31</span><br><span class="line"><span class="comment">#16 (line) x 8 (column) = 128, total 128 value, that means indirection table = 128, total 48 RX rings</span></span><br><span class="line"><span class="comment">#eg: line &quot;96:&quot; second field(1), means 98th hash data equal 2, the data will go to second queue</span></span><br><span class="line"></span><br><span class="line">The first two queue receive packages averagely</span><br><span class="line">$ ethtool -X eth0 equal 2</span><br><span class="line"></span><br><span class="line">$ ethtool -X eth0 weight 6 5 4 3 2 1 .... <span class="comment">#you can set 48 x weights, 48 data sum will not large than 128</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,20,22</span><br><span class="line">NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,21,23</span><br><span class="line"></span><br><span class="line">                        c0 c1 c2 c3 c4 c5 c6 c7 ---cpu core</span><br><span class="line">$ ethtool -X em3 weight  0  1  0  1  0  1  0  1</span><br><span class="line">$ ethtool -x em3</span><br><span class="line">RX flow <span class="built_in">hash</span> indirection table <span class="keyword">for</span> em3 with 8 RX ring(s):</span><br><span class="line">    0:      1     1     1     1     1     1     1     1</span><br><span class="line">    8:      1     1     1     1     1     1     1     1</span><br><span class="line">   16:      1     1     1     1     1     1     1     1</span><br><span class="line">   24:      1     1     1     1     1     1     1     1</span><br><span class="line">   32:      3     3     3     3     3     3     3     3</span><br><span class="line">   40:      3     3     3     3     3     3     3     3</span><br><span class="line">   48:      3     3     3     3     3     3     3     3</span><br><span class="line">   56:      3     3     3     3     3     3     3     3</span><br><span class="line">   64:      5     5     5     5     5     5     5     5</span><br><span class="line">   72:      5     5     5     5     5     5     5     5</span><br><span class="line">   80:      5     5     5     5     5     5     5     5</span><br><span class="line">   88:      5     5     5     5     5     5     5     5</span><br><span class="line">   96:      7     7     7     7     7     7     7     7</span><br><span class="line">  104:      7     7     7     7     7     7     7     7</span><br><span class="line">  112:      7     7     7     7     7     7     7     7</span><br><span class="line">  120:      7     7     7     7     7     7     7     7</span><br><span class="line"></span><br><span class="line">                          -------high value, more interrupt</span><br><span class="line">                          |</span><br><span class="line">$ ethtool -X em3 weight 0 3 0 1 0 1 0 1</span><br><span class="line">$ ethtool -x em3</span><br><span class="line">RX flow <span class="built_in">hash</span> indirection table <span class="keyword">for</span> em3 with 8 RX ring(s):</span><br><span class="line">    0:      1     1     1     1     1     1     1     1</span><br><span class="line">    8:      1     1     1     1     1     1     1     1</span><br><span class="line">   16:      1     1     1     1     1     1     1     1</span><br><span class="line">   24:      1     1     1     1     1     1     1     1</span><br><span class="line">   32:      1     1     1     1     1     1     1     1</span><br><span class="line">   40:      1     1     1     1     1     1     1     1</span><br><span class="line">   48:      1     1     1     1     1     1     1     1</span><br><span class="line">   56:      1     1     1     1     1     1     1     1</span><br><span class="line">   64:      3     3     3     3     3     3     3     3</span><br><span class="line">   72:      3     3     3     3     3     3     3     3</span><br><span class="line">   80:      3     3     3     3     3     5     5     5</span><br><span class="line">   88:      5     5     5     5     5     5     5     5</span><br><span class="line">   96:      5     5     5     5     5     5     5     5</span><br><span class="line">  104:      5     5     7     7     7     7     7     7</span><br><span class="line">  112:      7     7     7     7     7     7     7     7</span><br><span class="line">  120:      7     7     7     7     7     7     7     7</span><br><span class="line"></span><br><span class="line"><span class="comment">#set weight for the queue</span></span><br><span class="line">$ ethtool --set-rxfh-indir eth3 weight 6 2</span><br><span class="line">$ ethtool --set-rxfh-indir eth3 weight 1 2</span><br></pre></td></tr></table></figure></li>
<li><code>The irqbalance daemon can be used in conjunction with RSS to reduce the likelihood of cross-node memory transfers and cache line bouncing. This lowers the latency of processing network packets.</code></li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl <span class="built_in">enable</span> irqbalance</span><br><span class="line">$ systemctl start irqbalance</span><br><span class="line"></span><br><span class="line"><span class="comment"># debug irqblaance</span></span><br><span class="line">$ irqbalance -d -f</span><br><span class="line"></span><br><span class="line"><span class="comment">#disable PCI-E powersave</span></span><br><span class="line">$ grubby --update-kernel=ALL --args=<span class="string">&#x27;pcie_aspm=off scsi_mod.use_blk_mq=y dm_mod.use_blk_mq=y consoleblank=0&#x27;</span></span><br></pre></td></tr></table></figure>

<h5 id="THEORETICAL-MAX-RATE"><a href="#THEORETICAL-MAX-RATE" class="headerlink" title="THEORETICAL MAX RATE"></a><a target="_blank" rel="noopener" href="https://support-kb.spirent.com/resources/sites/SPIRENT/content/live/FAQS/10000/FAQ10597/en_US/How_to_Test_10G_Ethernet_WhitePaper_RevB.PDF">THEORETICAL MAX RATE</a></h5><p>How to calculate CALCULATE</p>
<ul>
<li>There are two important concepts related to 10GbE performance: <ul>
<li>frame rate and throughput. </li>
<li>The MAC bit rate of 10GbE, defined in the IEEE standard 802.3ae, is 10 billion bits per second. </li>
<li>Frame rate is a simple arithmetic calculation based on the bit rate and frame format definitions. </li>
<li>Throughput, defined in IETF RFC 1242, is the highest rate at which the system under test can forward the offered load, without loss. Manufacturers can claim line-rate throughput only if their switch forwards all the traffic offered at the 10Gb&#x2F;s line rate for the entire duration of the test. </li>
<li>The bit rate at which 10GbE Media Access Layer (MAC) operates, 10 billion bits per second, is only one of the parameters in defining the transmission rate for this important new technology. The usual description of true network performance is frame rate, which indicates how many Ethernet frames are moving across the network. The maximum frame rate for 10GbE is determined by a formula that divides the 10 billion bits per second by the preamble, frame length, and inter-frame gap fields, expressed in bits. The maximum frame rate is calculated using the minimum values of the following parameters, as described in the IEEE 802.3ae standard:<ul>
<li>Preamble - 8 bytes * 8 &#x3D; 64 bits</li>
<li>Frame length - 64 bytes (minimum) * 8 &#x3D; 512 bits</li>
<li>Inter-frame gap - 12 bytes (minimum) * 8 &#x3D; 96 bits<br>Therefore   <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Maximum Frame Rate =MAC Transmit Bit Rate/(Preamble + Frame Length + Inter-frame Gap)</span><br><span class="line">= 10,000,000,000 / (64 + 512 + 96)</span><br><span class="line">= 10,000,000,000 / 672</span><br><span class="line">= 14,880,952.38 frame per second (fps)</span><br><span class="line"></span><br><span class="line">#calculate buffer</span><br><span class="line">#Buffer size = Bandwidth (bits/s) * RTT (seconds)  it &#x27;s not good for some cases. the large buffer will impact the latency</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
</li>
</ul>
<h5 id="CPU-setting"><a href="#CPU-setting" class="headerlink" title="CPU setting"></a>CPU setting</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">$ cpupower idle-set -d 3</span><br><span class="line">$ cpupower idle-set -d 2</span><br><span class="line">$ cpupower idle-set -d 1</span><br><span class="line">$ cpupower idle-set -d 0</span><br><span class="line"></span><br><span class="line">or</span><br><span class="line"></span><br><span class="line">$ cpupower idle-set -D 133 <span class="comment">##enable 0,1,2,3 and disable 4</span></span><br><span class="line">$ cpupower idle-set -D 33  <span class="comment">##enable 0,1,2 and disable 4</span></span><br><span class="line"></span><br><span class="line">$ cpupower  frequency-set -g performance</span><br><span class="line">$ <span class="built_in">echo</span> performance &gt; /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_max_freq</span><br><span class="line">$ <span class="built_in">cat</span> /sys/devices/system/cpu/cpu*/cpufreq/cpuinfo_cur_freq</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get local cpus</span></span><br><span class="line">$ <span class="built_in">cat</span> /sys/class/net/eth7/device/local_cpus</span><br><span class="line">0000,000fffc0,00000000,0fffc000</span><br><span class="line"></span><br><span class="line"><span class="comment">#if you are mellanox NIC</span></span><br><span class="line">$ show_irq_affinity.sh ens6</span><br></pre></td></tr></table></figure>


<h5 id="dump-NIC-log"><a href="#dump-NIC-log" class="headerlink" title="dump NIC log"></a>dump NIC log</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># set interface messages level</span></span><br><span class="line">Old level   Name   Bit position</span><br><span class="line">    0    NETIF_MSG_DRV          0x0001</span><br><span class="line">    1    NETIF_MSG_PROBE        0x0002</span><br><span class="line">    2    NETIF_MSG_LINK         0x0004</span><br><span class="line">    2    NETIF_MSG_TIMER        0x0004</span><br><span class="line">    3    NETIF_MSG_IFDOWN       0x0008</span><br><span class="line">    3    NETIF_MSG_IFUP         0x0008</span><br><span class="line">    4    NETIF_MSG_RX_ERR       0x0010</span><br><span class="line">    4    NETIF_MSG_TX_ERR       0x0010</span><br><span class="line">    5    NETIF_MSG_TX_QUEUED    0x0020</span><br><span class="line">    5    NETIF_MSG_INTR         0x0020</span><br><span class="line">    6    NETIF_MSG_TX_DONE      0x0040</span><br><span class="line">    6    NETIF_MSG_RX_STATUS    0x0040</span><br><span class="line">    7    NETIF_MSG_PKTDATA      0x0080</span><br><span class="line"></span><br><span class="line">$ ethtool -s em1 msglvl 0x0020000</span><br><span class="line">$ ethtool -d em3 | <span class="built_in">head</span> -n 20</span><br><span class="line">Offset          Values</span><br><span class="line">------          ------</span><br><span class="line">0x0000:         03 00 00 00 11 11 11 61 ff 1f 00 00 10 02 00 00</span><br><span class="line">0x0010:         00 00 00 00 00 20 00 00 c0 31 00 00 00 00 00 00</span><br><span class="line">0x0020:         00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">0x0030:         00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">0x0040:         00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line">0x0050:         00 00 08 00 00 00 00 00 00 00 00 00 00 00 00 00</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h5 id="flow-control"><a href="#flow-control" class="headerlink" title="flow control"></a>flow control</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Layer 2 flow control can impact TCP performance considerably and is recommended to be disabled for most workloads</span></span><br><span class="line"></span><br><span class="line">The question answer</span><br><span class="line">https://blog.cloudflare.com/how-to-achieve-low-latency/</span><br><span class="line">Finally, some network cards have Ethernet flow control enabled by default. In our experiment we won<span class="string">&#x27;t push too many packets, so it shouldn&#x27;</span>t matter. In any <span class="keyword">case</span> - it<span class="string">&#x27;s unlikely the average user actually wants flow control - it can introduce unpredictable latency spikes during higher load</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">client$ sudo ethtool -A eth2 autoneg off rx off tx off</span></span><br><span class="line"><span class="string">server$ sudo ethtool -A eth3 autoneg off rx off tx off</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#diable autoneg and flow control</span></span><br><span class="line"><span class="string">$ ethtool -A eth2 autoneg off rx off</span></span><br><span class="line"><span class="string">$ ethtool -S eth2 | grep -Ei &#x27;</span>pause|flow_con<span class="string">&#x27;</span></span><br><span class="line"><span class="string">     rx_pause_frames: 0</span></span><br><span class="line"><span class="string">     rx_constant_pause_events: 0</span></span><br><span class="line"><span class="string">     tx_pause_frames: 489</span></span><br></pre></td></tr></table></figure>

<h5 id="DRIVER-driver"><a href="#DRIVER-driver" class="headerlink" title="DRIVER driver"></a>DRIVER driver</h5><p>82599 ixgbe    </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># unsupport optical module</span></span><br><span class="line">$ modprobe ixgbe allow_unsupported_sfp=1,1</span><br><span class="line"></span><br><span class="line">ixgbe InterruptThrottleRate=0,0</span><br><span class="line">- 在基于 82598 的适配器上，禁用InterruptThrottleRate 还会导致禁用 LRO</span><br><span class="line">当 igbvf 以默认设置加载并同时使用多个适配器时，CPU 利用率可能呈非线性增大。要限制 CPU 的利用率</span><br><span class="line">而不影响总体吞吐量，建议按以下所述加载驱动程序：</span><br><span class="line">modprobe igbvf InterruptThrottleRate=3000,3000,3000</span><br><span class="line">此命令为驱动程序的第一个、第二个和第三个实例设定 InterruptThrottleRate 为 3000 中断、秒。每秒</span><br><span class="line">2000 到 3000 中断的范围在大多数系统上有效，而且是一个良好的起点，但是最佳值则应根据平台而具体</span><br><span class="line">设置。如果 CPU 利用率不是问题的话，则使用默认驱动程序设置。</span><br><span class="line"></span><br><span class="line"><span class="built_in">disable</span> LRO</span><br><span class="line">$ make CFLAGS_EXTRA=<span class="string">&quot;-DIGB_LRO&quot;</span> install</span><br><span class="line"></span><br><span class="line"><span class="built_in">disable</span> MSI-X interrupt</span><br><span class="line">$ make CFLAGS_EXTRA=-DDISABLE_PCI_MSI install</span><br><span class="line"></span><br><span class="line"><span class="comment"># support PTP(IEEE 1588)精密时间协议, 支持PTP</span></span><br><span class="line">$ ethtool -T ethX</span><br><span class="line">make CFLAGS_EXTRA=<span class="string">&quot;-DIGB_PTP&quot;</span> install</span><br><span class="line"></span><br><span class="line"><span class="comment"># vxlan offloading</span></span><br><span class="line">$ ethtool -K ethX tx-udp_tnl-segmentation [off|on]</span><br><span class="line"></span><br><span class="line"><span class="comment"># disable hw rsc</span></span><br><span class="line">$ make CFLAGS_EXTRA=<span class="string">&quot;-DIXGBE_NO_HW_RSC&quot;</span> install</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://www.ichenfu.com/2020/05/07/intel-x700-i40e-do-not-receive-LLDP-frames/">i40e disable lldp</a>    </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ ethtool --set-priv-flags eth0 disable-fw-lldp on</span><br><span class="line">or</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;lldp stop&quot;</span> &gt; /sys/kernel/debug/i40e/&lt;pci bus address&gt;/command</span><br><span class="line"></span><br><span class="line"><span class="comment">#check status</span></span><br><span class="line">$ ethtool --show-priv-flags eth0|grep disable-fw-lldp</span><br><span class="line">$ lldpctl</span><br></pre></td></tr></table></figure>

<h5 id="hash-policy"><a href="#hash-policy" class="headerlink" title="hash policy"></a>hash policy</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br></pre></td><td class="code"><pre><span class="line">-N to config</span><br><span class="line">rx-flow-hash tcp4|udp4|ah4|esp4|sctp4|tcp6|udp6|ah6|esp6|sctp6 m|v|t|s|d|f|n|r...</span><br><span class="line"><span class="comment">## hash algorithms</span></span><br><span class="line">udp4 UDP over IPv4</span><br><span class="line">udp6 UDP over IPv6</span><br><span class="line">     f   on the <span class="built_in">hash</span> bytes 0 and 1 of the Layer 4 header of the rx packet.</span><br><span class="line">     n   on the <span class="built_in">hash</span> bytes 2 and 3 of the Layer 4 header of the rx packet.</span><br><span class="line">     s   on the <span class="built_in">hash</span> src ipaddr</span><br><span class="line">     d   on the <span class="built_in">hash</span> dst ipaddr</span><br><span class="line"></span><br><span class="line"><span class="comment">#RSS rx hash value</span></span><br><span class="line">$ ethtool -n em1 rx-flow-hash tcp4</span><br><span class="line">TCP over IPV4 flows use these fields <span class="keyword">for</span> computing Hash flow key:</span><br><span class="line">IP SA  &lt;---<span class="built_in">source</span> addr</span><br><span class="line">IP DA  &lt;---destination</span><br><span class="line">L4 bytes 0 &amp; 1 [TCP/UDP src port]</span><br><span class="line">L4 bytes 2 &amp; 3 [TCP/UDP dst port]</span><br><span class="line"></span><br><span class="line">https://downloadmirror.intel.com/22919/eng/README.txt</span><br><span class="line"><span class="comment">#To include UDP port numbers in RSS hashing run</span></span><br><span class="line">$ ethtool -N ethX rx-flow-hash udp4 sdfn</span><br><span class="line"></span><br><span class="line"><span class="comment">#To exclude UDP port numbers from RSS hashing run</span></span><br><span class="line">$ ethtool -N ethX rx-flow-hash udp4 sd</span><br><span class="line"></span><br><span class="line"><span class="comment">#To display UDP hashing current configuration run</span></span><br><span class="line">$ ethtool -n ethX rx-flow-hash udp4</span><br><span class="line"></span><br><span class="line"><span class="comment">#modify</span></span><br><span class="line">$ ethtool -N em1 rx-flow-hash udp4 sdfn</span><br><span class="line"></span><br><span class="line"><span class="comment">#policy</span></span><br><span class="line">$ ethtool -N ethX flow-type tcp4 src-ip 192.168.10.1 dst-ip 192.168.10.2 src-port 2000 dst-port 2001 action 2 [loc 1]</span><br><span class="line">$ ethtool -N ethX flow-type tcp4 src-ip 192.168.10.1 dst-ip 192.168.10.2 action 2 [loc 1]</span><br><span class="line">$ ethtool -N ethX flow-type tcp4 src-ip 192.168.10.1 dst-ip 192.168.10.2 user-def 0xffffffff00000001 m 0x40 action 2 [loc 1]</span><br><span class="line"><span class="comment"># 0xffffffff00000001 mode, offset: 0x40</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Custom policy, EP(Externally Programed) mode, when the ntuple on</span></span><br><span class="line"></span><br><span class="line">$ ethtool -u eth2</span><br><span class="line">63 RX rings available</span><br><span class="line">Total 0 rules</span><br><span class="line"></span><br><span class="line">$ ethtool -U &lt;device&gt; flow-type &lt;<span class="built_in">type</span>&gt; src-ip &lt;ip&gt; dstip &lt;ip&gt; src-port &lt;port&gt; dst-port &lt;port&gt; action &lt;queue&gt;</span><br><span class="line">$ ethtool -u eth2</span><br><span class="line">$ ethtool -U eth2 flow-type tcp4 src-ip 192.168.0.1 dst-ip 192.168.0.5 src-port 5300 dst-port 80 action 7</span><br><span class="line"><span class="comment">#from 192.168.0.1:5300 -----packages to queue 7-------&gt; 192.168.0.5:80</span></span><br><span class="line"></span><br><span class="line">$ ethtool -U eth2 flow-type ip4 src-ip 192.168.0.1 src-port 5300 action 7</span><br><span class="line">$ ethtool -U eth2 flow-type ip4 src-ip 192.168.0.5 src-port 55 action 10</span><br><span class="line"></span><br><span class="line"><span class="comment">#dest port 80 go to rx queue 2</span></span><br><span class="line">$ ethtool -U eth2 flow-type tcp4 dst-port 80 action 2</span><br><span class="line"></span><br><span class="line">$ ethtool --config-ntuple eth2 flow-type ip4 src-ip 192.168.100.1  action -1</span><br><span class="line">$ ethtool --config-ntuple eth2 flow-type tcp4 src-port 80  action 2</span><br><span class="line">$ ethtool --config-ntuple eth2 flow-type udp4 src-port 80  action 2</span><br><span class="line"></span><br><span class="line"><span class="comment">#To specify that all traffic from 10.23.4.6 to 10.23.4.18 be placed in queue 4, issue this command:</span></span><br><span class="line">$ ethtool --config-ntuple flow-type tcp4 src-ip 10.23.4.6 dst-ip 10.23.4.18 action 4</span><br><span class="line"></span><br><span class="line"><span class="comment">#Forwards to queue 2 all IPv4 TCP traffic from 192.168.10.1:2000 that is going to 192.168.10.2:2001, placing the filter at position 33 of the Perfect-Match filter table (and overwriting any rule currently in that position):</span></span><br><span class="line">$ ethtool --config-ntuple &lt;interface name&gt; flow-type tcp4 src-ip 192.168.10.1 dst-ip 192.168.10.2 src-port 2000 dst-port 2001 action 2 loc 33</span><br><span class="line"></span><br><span class="line"><span class="comment">#Drops all UDP packets from 10.4.83.2:</span></span><br><span class="line">$ ethtool --config-ntuple flow-type udp4 src-ip 10.4.82.2 action -1</span><br><span class="line"><span class="comment">#Note: The VLAN field is not a supported filter with the i40e driver (Intel Ethernet Controller XL710 and Intel Ethernet Controller X710 NICs).</span></span><br><span class="line"><span class="comment">#For more information and options, see the ethtool man page documentation on the -U, -N, or --config-ntuple option.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#List</span></span><br><span class="line">$ ethtool --show-ntuple p2p1</span><br><span class="line">10 RX rings available</span><br><span class="line">Total 0 rules</span><br><span class="line"></span><br><span class="line"><span class="comment">#Remove</span></span><br><span class="line">$ ethtool --config-ntuple &lt;interface name&gt; delete N</span><br><span class="line"></span><br><span class="line"><span class="comment">#There&#x27;s generic flow steering rule configuration interface on ethtool, called RX NFC.</span></span><br><span class="line">$ ethtool --config-nfc ix00 flow-type tcp4 src-ip 10.0.0.1 dst-ip 10.0.0.2 src-port 10000 dst-port 10001 action 6</span><br><span class="line">Added rule with ID 2045</span><br><span class="line">$ ethtool --show-nfc ix00</span><br><span class="line">12 RX rings available</span><br><span class="line">Total 1 rules</span><br><span class="line">Filter: 2045</span><br><span class="line">     Rule Type: TCP over IPv4</span><br><span class="line">     Src IP addr: 10.0.0.1 mask: 0.0.0.0</span><br><span class="line">     Dest IP addr: 10.0.0.2 mask: 0.0.0.0</span><br><span class="line">     TOS: 0x0 mask: 0xff</span><br><span class="line">     Src port: 10000 mask: 0x0</span><br><span class="line">     Dest port: 10001 mask: 0x0</span><br><span class="line">     VLAN EtherType: 0x0 mask: 0xffff</span><br><span class="line">     VLAN: 0x0 mask: 0xffff</span><br><span class="line">     User-defined: 0x0 mask: 0xffffffffffffffff</span><br><span class="line">     Action: Direct to queue 6</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/YWzuKBK3TMclejeN2ziAvQ">TCP rst</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Q：TCP数据交互过程中，在一方发了RST以后，连接一定会终止么?</span><br><span class="line">A：不一定会终止，需要看这个RST的Seq是否在接收方的接收窗口之内，如上例中就因为Seq号较小，所以不是一个合法的RST被Linux内核无视了。</span><br><span class="line">Q：连接会立即终止么，还是会等10s?</span><br><span class="line">A：连接会立即终止，上面的例子中过了10s终止，正是因为，linux内核对RFC严格实现，无视了RST报文，但是客户端和数据库之间经过的SLB（云负载均衡设备），却处理了RST报文，导致10s（SLB 10s 后清理session）之后关闭了TCP连接。</span><br></pre></td></tr></table></figure>

<h3 id="SYN-timeout-on-linux"><a href="#SYN-timeout-on-linux" class="headerlink" title="SYN timeout on linux"></a><a target="_blank" rel="noopener" href="https://serverfault.com/questions/1043758/initial-syn-timeout-on-linux">SYN timeout on linux</a></h3><p>if SYN not timeout, some server of application not release the socket in the kernel</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#define TCP_TIMEOUT_INIT ((unsigned)(1*HZ)) /* RFC6298 2.1 initial RTO value</span><br><span class="line">https://elixir.bootlin.com/linux/v5.9.11/source/include/net/tcp.h#L142</span><br><span class="line"></span><br><span class="line">it is hardcoded in the kernel. So change the kernel and recompile.</span><br><span class="line">#define TCP_TIMEOUT_INIT ((unsigned)(3*HZ))     /* RFC 1122 initial RTO value   */</span><br><span class="line">https://serverfault.com/questions/295390/how-can-i-tune-the-initial-tcp-retransmit-timeout</span><br><span class="line"></span><br><span class="line">#reduce retry count</span><br><span class="line">net.ipv4.tcp_synack_retries = 3 #default is 5 or 6</span><br><span class="line">#Number of times initial SYNs for an active TCP connection attempt will be retransmitted. Should not be higher than 127</span><br><span class="line"># which corresponds to 63 seconds till the last retransmission with the current initial RTO of 1 second</span><br><span class="line"># With this the final timeout for an active TCP connection attempt will happen after 127 seconds.</span><br><span class="line"></span><br><span class="line">### iperf3</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>Connecting to host 10.53.128.235, port 5202<br>[  5] local 10.53.128.236 port 45294 connected to 10.53.128.235 port 5202<br>[ ID] Interval           Transfer     Bitrate         Retr  Cwnd<br>[  5]   2.00-3.00   sec  1.16 GBytes  9.97 Gbits&#x2F;sec    0   1.17 MBytes<br>[  5]   3.00-4.00   sec  1.11 GBytes  9.55 Gbits&#x2F;sec   36   1.11 MBytes<br>[  5]   4.00-5.00   sec  1.03 GBytes  8.87 Gbits&#x2F;sec  127    628 KBytes &lt;——TCP 默认的拥塞控制算法 cubic 很容易被影响，一旦发生丢包，cwnd 会快速下降<br>[  5]   5.00-6.00   sec  1.15 GBytes  9.91 Gbits&#x2F;sec    0    686 KBytes &lt;——这里没有太冲击吞吐<br>[  5]   6.00-7.00   sec  1.15 GBytes  9.92 Gbits&#x2F;sec    0    721 KBytes<br>[  5]   7.00-8.00   sec  1.15 GBytes  9.87 Gbits&#x2F;sec    0    730 KBytes<br>[  5]   8.00-9.00   sec  1.12 GBytes  9.59 Gbits&#x2F;sec    0    761 KBytes<br>[  5]   9.00-10.00  sec  1.04 GBytes  8.98 Gbits&#x2F;sec    0    771 KBytes<br>[  5]  10.00-11.01  sec  1.09 GBytes  9.31 Gbits&#x2F;sec    0    795 KBytes<br>[  5]  11.01-12.00  sec  1.03 GBytes  8.92 Gbits&#x2F;sec    0    834 KBytes<br>[  5]  12.00-13.00  sec  1.16 GBytes  9.98 Gbits&#x2F;sec    0    880 KBytes<br>[  5]  13.00-14.00  sec  1.14 GBytes  9.76 Gbits&#x2F;sec    0    956 KBytes<br>[  5]  14.00-15.00  sec  1.08 GBytes  9.24 Gbits&#x2F;sec    0   1.12 MBytes<br>[  5]  15.00-16.00  sec  1.16 GBytes  9.96 Gbits&#x2F;sec    0   1.22 MBytes</p>
<p>#cubic sysctl -w net.core.default_qdisc&#x3D;fq_codel net.ipv4.tcp_congestion_control&#x3D;cubic<br>[ ID] Interval           Transfer     Bitrate         Retr  Cwnd<br>[  5]   0.00-1.00   sec   876 MBytes  7.35 Gbits&#x2F;sec    0    769 KBytes<br>[  5]   1.00-2.00   sec   648 MBytes  5.43 Gbits&#x2F;sec    0    769 KBytes<br>[  5]   2.00-3.00   sec   646 MBytes  5.42 Gbits&#x2F;sec    0    769 KBytes<br>[  5]   3.00-4.00   sec   648 MBytes  5.43 Gbits&#x2F;sec    0    769 KBytes<br>[  5]   4.00-5.00   sec   648 MBytes  5.43 Gbits&#x2F;sec    0    809 KBytes<br>[  5]   5.00-6.00   sec   644 MBytes  5.40 Gbits&#x2F;sec    0    863 KBytes<br>[  5]   6.00-7.00   sec   648 MBytes  5.43 Gbits&#x2F;sec    0    863 KBytes<br>[  5]   7.00-8.00   sec   646 MBytes  5.42 Gbits&#x2F;sec    0    863 KBytes<br>[  5]   8.00-9.00   sec   648 MBytes  5.43 Gbits&#x2F;sec    0    863 KBytes<br>[  5]   9.00-10.00  sec   646 MBytes  5.42 Gbits&#x2F;sec    0    863 KBytes<br>[  5]  10.00-11.00  sec   648 MBytes  5.43 Gbits&#x2F;sec    0    863 KBytes<br>[  5]  11.00-12.00  sec   645 MBytes  5.41 Gbits&#x2F;sec    0    863 KBytes<br>[  5]  12.00-13.00  sec   640 MBytes  5.37 Gbits&#x2F;sec    0    863 KBytes<br>[  5]  13.00-14.00  sec   641 MBytes  5.38 Gbits&#x2F;sec    0    863 KBytes &lt;——- slow start, if the Retr increased,  little impact the throughput of the cubic,<br>[  5]  38.00-39.00  sec   642 MBytes  5.39 Gbits&#x2F;sec    0   1.28 MBytes  the bbr average BW higher than cubic, not too much<br>[  5]  39.00-40.00  sec   639 MBytes  5.36 Gbits&#x2F;sec    0   1.28 MBytes<br>…..<br>[  5]  40.00-41.00  sec   641 MBytes  5.38 Gbits&#x2F;sec    0   1.93 MBytes<br>[  5]  41.00-42.00  sec   642 MBytes  5.39 Gbits&#x2F;sec    0   1.93 MBytes<br>…..<br>[  5] 118.00-119.00 sec   646 MBytes  5.42 Gbits&#x2F;sec    0   3.09 MBytes<br>[  5] 119.00-120.00 sec   869 MBytes  7.29 Gbits&#x2F;sec    0   3.09 MBytes  </p>
<p>[  5]  20.00-21.00  sec   638 MBytes  5.35 Gbits&#x2F;sec    0   2.47 MBytes<br>[  5]  21.00-22.00  sec   639 MBytes  5.36 Gbits&#x2F;sec    0   2.47 MBytes<br>[  5]  22.00-23.00  sec   639 MBytes  5.36 Gbits&#x2F;sec    0   2.47 MBytes<br>[  5]  23.00-24.00  sec   608 MBytes  5.10 Gbits&#x2F;sec  363   1.79 MBytes &lt;—– retr impact the cwnd size and throughput<br>[  5]  24.00-25.00  sec   639 MBytes  5.36 Gbits&#x2F;sec    0   1.85 MBytes<br>[  5]  25.00-26.00  sec   634 MBytes  5.32 Gbits&#x2F;sec    0   1.85 MBytes<br>……<br>[  5]  69.00-70.00  sec   632 MBytes  5.31 Gbits&#x2F;sec    0   1.67 MBytes<br>[  5]  70.00-71.00  sec   632 MBytes  5.31 Gbits&#x2F;sec    0   1.67 MBytes<br>[  5]  71.00-72.00  sec   634 MBytes  5.32 Gbits&#x2F;sec    0   1.67 MBytes<br>[  5]  72.00-73.00  sec   636 MBytes  5.34 Gbits&#x2F;sec    0   1.67 MBytes<br>[  5]  73.00-74.00  sec   618 MBytes  5.18 Gbits&#x2F;sec  925   1.22 MBytes&lt;—–<br>[  5]  74.00-75.00  sec   635 MBytes  5.33 Gbits&#x2F;sec    0   1.22 MBytes<br>[  5]  75.00-76.00  sec   636 MBytes  5.34 Gbits&#x2F;sec    0   1.33 MBytes<br>[  5]  76.00-77.00  sec   639 MBytes  5.36 Gbits&#x2F;sec    0   1.34 MBytes<br>[  5]  77.00-78.00  sec   638 MBytes  5.35 Gbits&#x2F;sec    0   1.34 MBytes<br>[  5]  78.00-79.00  sec   619 MBytes  5.19 Gbits&#x2F;sec   11   1.02 MBytes&lt;—–<br>[  5]  79.00-80.00  sec   628 MBytes  5.26 Gbits&#x2F;sec    0   1.10 MBytes<br>[  5]  80.00-81.00  sec   636 MBytes  5.34 Gbits&#x2F;sec    0   1.10 MBytes       </p>
<hr>
<p>[ ID] Interval           Transfer     Bitrate         Retr<br>[  5]   0.00-138.21 sec  82.2 GBytes  5.11 Gbits&#x2F;sec  3698             sender<br>[  5]   0.00-138.21 sec  0.00 Bytes  0.00 bits&#x2F;sec                  receiver<br>iperf3: interrupt - the client has terminated</p>
<hr>
<p>[ ID] Interval           Transfer     Bitrate         Retr<br>[  5]   0.00-138.35 sec  82.7 GBytes  5.13 Gbits&#x2F;sec  2433             sender<br>[  5]   0.00-138.35 sec  0.00 Bytes  0.00 bits&#x2F;sec                  receiver</p>
<p>#sysctl -w net.core.default_qdisc&#x3D;fq net.ipv4.tcp_congestion_control&#x3D;bbr<br>[  5]  66.00-67.00  sec   500 MBytes  4.19 Gbits&#x2F;sec    0    359 KBytes<br>[  5]  67.00-68.00  sec   641 MBytes  5.38 Gbits&#x2F;sec    0    339 KBytes<br>[  5]  68.00-69.00  sec   636 MBytes  5.34 Gbits&#x2F;sec    0    365 KBytes<br>[  5]  69.00-70.00  sec   631 MBytes  5.30 Gbits&#x2F;sec    0    464 KBytes<br>[  5]  70.00-71.00  sec   631 MBytes  5.30 Gbits&#x2F;sec    0    450 KBytes<br>[  5]  71.00-72.00  sec   604 MBytes  5.06 Gbits&#x2F;sec    0    351 KBytes<br>[  5]  72.00-73.00  sec   636 MBytes  5.34 Gbits&#x2F;sec    0    339 KBytes<br>[  5]  73.00-74.00  sec   635 MBytes  5.33 Gbits&#x2F;sec    0    337 KBytes<br>[  5]  74.00-75.00  sec   638 MBytes  5.35 Gbits&#x2F;sec    0    334 KBytes<br>[  5]  75.00-76.00  sec   979 MBytes  8.21 Gbits&#x2F;sec    0    402 KBytes       </p>
<hr>
<p>[ ID] Interval           Transfer     Bitrate         Retr<br>[  5]   0.00-76.01  sec  46.5 GBytes  5.26 Gbits&#x2F;sec    1             sender &lt;—–bbr has the lower cwnd, and lower Retr and higher BW than cubic<br>[  5]   0.00-76.01  sec  0.00 Bytes  0.00 bits&#x2F;sec                  receiver</p>
<p>[  5]  72.00-73.00  sec   588 MBytes  4.93 Gbits&#x2F;sec    0    600 KBytes<br>[  5]  73.00-74.00  sec   628 MBytes  5.26 Gbits&#x2F;sec    0    419 KBytes<br>[  5]  74.00-75.00  sec   639 MBytes  5.36 Gbits&#x2F;sec    0    416 KBytes<br>[  5]  75.00-76.00  sec   636 MBytes  5.34 Gbits&#x2F;sec    0    455 KBytes       </p>
<hr>
<p>[ ID] Interval           Transfer     Bitrate         Retr<br>[  5]   0.00-76.60  sec  47.1 GBytes  5.29 Gbits&#x2F;sec    0             sender<br>[  5]   0.00-76.60  sec  0.00 Bytes  0.00 bits&#x2F;sec                  receiver</p>
<h1 id="ip-could-set-initrwnd"><a href="#ip-could-set-initrwnd" class="headerlink" title="ip could set initrwnd"></a>ip could set initrwnd</h1><h1 id="net-ipv4-tcp-slow-start-after-idle-0-not-be-1-https-www-kawabangga-com-posts-5217"><a href="#net-ipv4-tcp-slow-start-after-idle-0-not-be-1-https-www-kawabangga-com-posts-5217" class="headerlink" title="net.ipv4.tcp_slow_start_after_idle &#x3D; 0 , not be 1  https://www.kawabangga.com/posts/5217"></a>net.ipv4.tcp_slow_start_after_idle &#x3D; 0 , not be 1  <a target="_blank" rel="noopener" href="https://www.kawabangga.com/posts/5217">https://www.kawabangga.com/posts/5217</a></h1><h1 id="实际效果是，原来数据需要-30-多分钟下载完成，换成-BBR-之后，只需要-2-分钟就下载完了。"><a href="#实际效果是，原来数据需要-30-多分钟下载完成，换成-BBR-之后，只需要-2-分钟就下载完了。" class="headerlink" title="实际效果是，原来数据需要 30 多分钟下载完成，换成 BBR 之后，只需要 2 分钟就下载完了。"></a>实际效果是，原来数据需要 30 多分钟下载完成，换成 BBR 之后，只需要 2 分钟就下载完了。</h1><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">### el8 2x 100GbE full duplex</span><br><span class="line">4 clients vs 1 server</span><br><span class="line">```bash</span><br><span class="line">net.ipv4.neigh.default.base_reachable_time_ms=30000000</span><br><span class="line">net.ipv4.neigh.ens3f0np0.base_reachable_time_ms = 30000000</span><br><span class="line">net.ipv4.neigh.ens6f0np0.base_reachable_time_ms = 30000000</span><br><span class="line"></span><br><span class="line">net.ipv4.tcp_slow_start_after_idle=0</span><br><span class="line">net.ipv4.tcp_fastopen=3</span><br><span class="line">net.core.netdev_max_backlog=4096</span><br><span class="line">net.core.somaxconn=4096</span><br><span class="line">net.core.dev_weight=128</span><br><span class="line">net.core.netdev_budget=600</span><br><span class="line">vm.min_free_kbytes=700000</span><br><span class="line"></span><br><span class="line">net.core.default_qdisc=fq</span><br><span class="line">net.ipv4.tcp_congestion_control = bbr &lt;--in the bbr v1 many tcp retrans decreased.</span><br><span class="line"></span><br><span class="line">All throughput reach the 45~47GB/s in few hours</span><br><span class="line">07:22:52 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</span><br><span class="line">07:22:53 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:53 PM      eno1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:53 PM      eno2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:53 PM ens3f0np0 8297642.00 7716608.00 12013015.47 11153677.50      0.00      0.00      5.00     98.41  &lt;---numa 0, if not pin numa, the port bandwidth flip under 8~12GB/s</span><br><span class="line">07:22:53 PM ens3f1np1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00                    after pin numa node, ens3f0np0 10.5~12GB/s, ens6f0np0 11~12GB/s</span><br><span class="line">07:22:53 PM ens6f0np0 8339354.00 8291743.00 12009014.41 12011816.63      0.00      0.00      0.00     98.40  &lt;---numa 1, all iperf3(server and client) pin the numa node</span><br><span class="line">07:22:53 PM ens6f1np1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">07:22:52 PM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line">07:22:53 PM      0.00      0.00    664.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">07:22:53 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</span><br><span class="line">07:22:54 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:54 PM      eno1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:54 PM      eno2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:54 PM ens3f0np0 8295383.00 7681823.00 12013346.46 11102835.87      0.00      0.00      5.00     98.41</span><br><span class="line">07:22:54 PM ens3f1np1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:54 PM ens6f0np0 8340322.00 8287155.00 12009335.57 12010374.54      0.00      0.00      0.00     98.39</span><br><span class="line">07:22:54 PM ens6f1np1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">07:22:53 PM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line">07:22:54 PM      0.00      0.00    708.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">07:22:54 PM     IFACE   rxpck/s   txpck/s    rxkB/s    txkB/s   rxcmp/s   txcmp/s  rxmcst/s   %ifutil</span><br><span class="line">07:22:55 PM        lo      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:55 PM      eno1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:55 PM      eno2      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:55 PM ens3f0np0 8301854.00 7733813.00 12012592.85 11180779.04      0.00      0.00      5.00     98.41</span><br><span class="line">07:22:55 PM ens3f1np1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line">07:22:55 PM ens6f0np0 8345981.00 8297684.00 12009556.08 12012134.93      0.00      0.00      0.00     98.40</span><br><span class="line">07:22:55 PM ens6f1np1      0.00      0.00      0.00      0.00      0.00      0.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">07:22:54 PM  atmptf/s  estres/s retrans/s isegerr/s   orsts/s</span><br><span class="line">07:22:55 PM      0.00      0.00    797.00      0.00      0.00</span><br><span class="line"></span><br><span class="line">cpu is Intel(R) Xeon(R) Platinum 8358P CPU @ 2.60GHz</span><br><span class="line"></span><br><span class="line">top - 19:27:03 up 5 days,  5:12,  2 users,  load average: 7.30, 7.95, 7.18</span><br><span class="line">Tasks: 1025 total,   9 running, 1012 sleeping,   4 stopped,   0 zombie</span><br><span class="line">%Cpu(s):  0.2 us, 11.5 sy,  0.0 ni, 73.5 id,  0.0 wa,  1.2 hi, 13.6 si,  0.0 st</span><br><span class="line">MiB Mem : 256142.6 total, 250264.3 free,   4350.4 used,   1528.0 buff/cache</span><br><span class="line">MiB Swap:   4096.0 total,   4096.0 free,      0.0 used. 249311.8 avail Mem </span><br><span class="line"></span><br><span class="line">    PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND                                                                                                                </span><br><span class="line">  29424 root      20   0   26840   2252   1508 R  64.9   0.0  16:14.99 iperf3 -s -D -p 5203</span><br><span class="line">  29426 root      20   0   26840   2200   1448 S  48.0   0.0  13:24.35 iperf3 -s -D -p 5204                                                                                                   </span><br><span class="line">  29438 root      20   0   26840   2204   1452 S  46.4   0.0  11:28.60 iperf3 -s -D -p 5210                                                                                                   </span><br><span class="line">  29440 root      20   0   26840   2364   1628 S  42.4   0.0  11:17.89 iperf3 -s -D -p 5211                                                                                                   </span><br><span class="line">  29434 root      20   0   26840   2304   1560 R  42.1   0.0  11:24.49 iperf3 -s -D -p 5208                                                                                                   </span><br><span class="line">  29444 root      20   0   26840   2232   1484 S  41.7   0.0  10:55.77 iperf3 -s -D -p 5213                                                                                                   </span><br><span class="line">  29428 root      20   0   26840   2232   1484 R  41.4   0.0  11:52.43 iperf3 -s -D -p 5205                                                                                                   </span><br><span class="line">  29432 root      20   0   26840   2232   1484 S  41.1   0.0  11:20.98 iperf3 -s -D -p 5207                                                                                                   </span><br><span class="line">  29430 root      20   0   26840   2204   1452 S  40.1   0.0  11:35.57 iperf3 -s -D -p 5206                                                                                                   </span><br><span class="line">  29442 root      20   0   26840   2256   1508 S  39.7   0.0  10:39.59 iperf3 -s -D -p 5212                                                                                                   </span><br><span class="line">  29420 root      20   0   26840   2228   1484 S  38.7   0.0  13:37.14 iperf3 -s -D -p 5201                                                                                                   </span><br><span class="line">  29422 root      20   0   26840   2236   1488 S  38.7   0.0  13:03.86 iperf3 -s -D -p 5202                                                                                                   </span><br><span class="line">  29664 root      20   0   26840   3972   3176 R  18.5   0.0   5:20.92 iperf3 -c 10.53.128.74 -P 1 -t 36000 -p 5202                                                                           </span><br><span class="line">  29490 root      20   0   26840   4096   3296 S  17.9   0.0   5:17.28 iperf3 -c 10.53.128.74 -P 1 -t 36000 -p 5201                                                                           </span><br><span class="line">  29759 root      20   0   26840   3948   3148 S  17.9   0.0   4:40.53 iperf3 -c 10.53.128.72 -P 1 -t 36000 -p 5203                                                                           </span><br><span class="line">  30043 root      20   0   26708   4060   3268 S  16.9   0.0   3:51.19 iperf3 -c 10.53.128.72 -P 1 -t 36000 -p 5208                                                                           </span><br><span class="line">  29663 root      20   0   26840   3896   3084 S  16.6   0.0   5:03.21 iperf3 -c 10.53.128.72 -P 1 -t 36000 -p 5202                                                                           </span><br><span class="line">  30004 root      20   0   26708   3904   3116 S  16.2   0.0   3:54.09 iperf3 -c 10.53.128.72 -P 1 -t 36000 -p 5207                                                                           </span><br><span class="line">  29760 root      20   0   26840   4180   3400 S  15.6   0.0   4:33.37 iperf3 -c 10.53.128.74 -P 1 -t 36000 -p 5203                                                                           </span><br><span class="line">  29986 root      20   0   26708   3976   3176 S  13.6   0.0   3:18.78 iperf3 -c 10.53.129.75 -P 1 -t 36000 -p 5205                                                                           </span><br><span class="line">  29984 root      20   0   26708   4160   3376 S   9.9   0.0   2:22.86 iperf3 -c 10.53.129.72 -P 1 -t 36000 -p 5205                                                                           </span><br><span class="line">  29990 root      20   0   26708   4080   3284 S   9.9   0.0   2:27.84 iperf3 -c 10.53.129.72 -P 1 -t 36000 -p 5206                                                                           </span><br><span class="line">  29991 root      20   0   26708   4076   3280 S   9.9   0.0   2:22.08 iperf3 -c 10.53.129.74 -P 1 -t 36000 -p 5206                                                                           </span><br><span class="line">  29985 root      20   0   26708   4100   3300 S   9.6   0.0   2:22.43 iperf3 -c 10.53.129.74 -P 1 -t 36000 -p 5205                                                                           </span><br><span class="line">  29992 root      20   0   26708   3976   3176 R   8.6   0.0   1:56.89 iperf3 -c 10.53.129.75 -P 1 -t 36000 -p 5206                                                                           </span><br><span class="line">  29489 root      20   0   26840   4080   3280 S   7.9   0.0   3:13.61 iperf3 -c 10.53.128.72 -P 1 -t 36000 -p 5201                                                                           </span><br><span class="line">  29993 root      20   0   26708   4044   3248 S   7.9   0.0   1:56.49 iperf3 -c 10.53.129.76 -P 1 -t 36000 -p 5206                                                                           </span><br><span class="line">  29987 root      20   0   26708   4000   3196 S   7.6   0.0   1:51.31 iperf3 -c 10.53.129.76 -P 1 -t 36000 -p 5205                                                                           </span><br><span class="line">    389 root      20   0       0      0      0 R   7.3   0.0   7:05.75 [ksoftirqd/62]                                                                                                         </span><br><span class="line">  30005 root      20   0   26708   4040   3252 S   7.3   0.0   1:46.58 iperf3 -c 10.53.128.74 -P 1 -t 36000 -p 5207                                                                           </span><br><span class="line">  30044 root      20   0   26708   4012   3216 S   7.3   0.0   1:43.41 iperf3 -c 10.53.128.74 -P 1 -t 36000 -p 5208                                                                           </span><br><span class="line">    371 root      20   0       0      0      0 R   2.0   0.0   5:20.96 [ksoftirqd/59]                                                                                                         </span><br><span class="line">  30212 root      20   0       0      0      0 I   0.7   0.0   0:03.52 [kworker/u128:2-mlx5_cmd_0000:31:00.0]                                                                                 </span><br><span class="line">  30344 root      20   0       0      0      0 I   0.7   0.0   0:00.43 [kworker/u128:0-mlx5_cmd_0000:98:00.0]                    </span><br></pre></td></tr></table></figure>


<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">__dev_queue_xmit  </span><br><span class="line">   validate_xmit_skb</span><br><span class="line">      netif_skb_features</span><br><span class="line">           ndo_features_check</span><br><span class="line">      validate_xmit_vlan</span><br><span class="line">          vlan_hw_offload_capable   网卡是否支持vlan 加速 </span><br><span class="line">              __vlan_hwaccel_push_inside  如果网卡不支持 “tx-vlan-offload” 把vlan直接插入包头，</span><br><span class="line">      netif_needs_gso</span><br><span class="line">          skb_gso_segment</span><br><span class="line">      skb_needs_linearize</span><br><span class="line">          __skb_linearize</span><br></pre></td></tr></table></figure>

<h3 id="wireshark"><a href="#wireshark" class="headerlink" title="wireshark"></a><a target="_blank" rel="noopener" href="https://juejin.cn/post/6844903710640832526">wireshark</a></h3><ul>
<li><p>Statistics -&gt; Summary</p>
</li>
<li><p>Wireshark -&gt;Analyze -&gt; Expert Infos -&gt; Notes</p>
</li>
<li><p>statistics -&gt; Service Response Time -&gt; xxxxx</p>
</li>
<li><p>Edit-&gt;Preferences-&gt;Protocols-&gt;TCP-&gt; Relative Sequence Numbers</p>
</li>
<li><p><code>Statistics -&gt; TCP StreamGraph -&gt; TCP Sequence Graph/Througput/RTT/Window Scaling</code></p>
</li>
<li><p>Analyze -&gt; Expert Info -&gt; Notes菜单可以看到重传统计</p>
</li>
<li><p>Packer size limited during caputre</p>
<ul>
<li>如果某个包被标记提示[Packer size limited during caputre]，说明这个包没有抓全，可以进一步查看下面的frame信息。一般这个情况是抓包的姿势不对。某些操作系统中，tcpdump默认只抓取每个帧的前96个字节，因此tcpdump抓包的时候，可以通过 -s参数指定要抓取的字节数</li>
</ul>
</li>
<li><p>TCP ACKed unseen segment</p>
<ul>
<li>如果wireshark发现被Ack的那个包没有抓到，就会提示[TCP ACKed unseen segment]，不过这个提示大部分情况都可以忽略。因为大都情况下，刚开始抓包的时候，都是只抓到了后面的Ack而没有抓到前面的ACK</li>
</ul>
</li>
<li><p>TCP Previous segment not captured</p>
<ul>
<li>TCP数据传输中，除了三次握手和四次握手之外，同一台机器发出的数据段应该是连续的，即后一个包的Seq等于前一个包的Seq+Len，正确情况都应该是这样；如果发现后一个包的Seq大于前一个包的Seq+Len，那么就说明中间丢了一段数据，如果丢失的数据在整个网络包中都找不到，wireshark就会提示[TCP Previous segment not captured]</li>
<li>数据包真的丢了</li>
<li>数据包并没有真丢，只是抓包工具漏掉了,如果确认Ack包中包含了没有抓到的包，那就是抓包工具漏掉了，否则就是真丢了</li>
</ul>
</li>
<li><p>TCP Out-of-Order</p>
<ul>
<li>TCP数据传输中，除了三次握手和四次握手之外，同一台机器发出的数据段应该是连续的，即后一个包的Seq等于前一个包的Seq+Len，正确情况都应该是这样；或者说后一个包的Seq应该会大于等于前一个包的Seq+Len，如果wireshark发现后一个包的Seq小于前一个包的Seq+Len，那么就认为是乱序了，就会提示[TCP Out-of-Order]。</li>
<li>一般而言，小跨度的乱序影响不大，如果是大跨度的乱序则会导致快速重传。举例如下，如果一个包的顺序是1、2、3、4、5被打乱成2、1、3、4、5则属于小跨度乱序，影响不大；如果被打乱成2、3、4、5、1，则会触发足够多的Dup ACK，从而导致1号包的重传。</li>
</ul>
</li>
<li><p>TCP Dup ACK</p>
<ul>
<li>当乱序或者丢包发生时，接收方就会收到一些Seq号比期望值大的包，TCP协议每收到一个这种包就会ACK一次期望的Seq值，通过这个方式告知发送方，因此就产生了一些重复的Ack。Wireshark抓到这些重复的Ack就会提示</li>
</ul>
</li>
<li><p>TCP Fast Retransmission</p>
<ul>
<li>当发送方连续收到3个或者以上[TCP Dup ACK]时，就意识到之前发的包可能丢了，于是根据RFC的规定就会开始快速重传。[TCP Dup ACK]是接收方回应给发送方的，因此发送方就能够感知到并当连续收到3个以上的时候就开启快速重传。</li>
<li>快重传算法规定，发送方只要一连收到3个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计数器时间到期。</li>
</ul>
</li>
<li><p>TCP Retransmission</p>
<ul>
<li>如果一个包真的丢了，又没有后续包可以在接收方触发[Dup Ack]，那么就不会开启快速重传，这种情况发送方只能等到超时后再发送重传，超时重传的包就会被wireshark标记并提示[TCP Retransmission]</li>
<li>TCP 超时与重传应该是 TCP 最复杂的部分之一，超时重传是 TCP 保证可靠传输的基础。当 TCP 在发送数据时，数据和 ack 都有可能会丢失，因此，TCP 通过在发送时设置一个定时器来解决这种问题。如果定时器溢出还没有收到确认，它就重传数据。关键之处就在于超时和重传的策略，需要考虑两方面, 超时时间设置和重传的频率（次数）</li>
<li>Linux 3.15 中，已经有了至少 9 个定时器：超时重传定时器，持续定时器，ER延迟定时器，PTO定时器，ACK延迟定时器，SYNACK定时器，保活定时器，FIN_WAIT2定时器，TIME_WAIT定时器。</li>
</ul>
</li>
<li><p>TCP zerowindow</p>
<ul>
<li>TCP包中“win&#x3D;xxx”代表接收窗口的大小，表示这个包的发送方当前还有多少缓冲区可以接受数据。当wireshark发行一个包中的“win&#x3D;0”时，就会标记提示[TCP zerowindow]，表示缓冲区已经满了，无法再接收数据了。一般的，在缓冲区满之前，窗口大小应该是逐渐减小的过程。</li>
</ul>
</li>
<li><p>TCP window Full</p>
<ul>
<li>如果一个包的发送方已经把对方所声明的接收窗口大小耗尽了，就会被wireshark标记为[TCP window Full]。比如某一端在握手时声明自己的接收窗口只有65535，也就意味着对端最多只能给他发送65535字节的数据而无需确认，即“在途字节数”最多只能是65535，当wireshark计算出对端已经有65535字节未被确认时，就会发生这个提示</li>
<li>[TCP window Full]和上面的[TCP zerowindow]比较容易混淆，前者表示这个包的发送方暂时没有办法再发送数据了；后者表示这个包的发送方没有办法再接收数据了；两者都会意味着要暂停数据传输</li>
</ul>
</li>
<li><p>TCP segment of reassembled PDU</p>
<ul>
<li>只有在Edit-&gt;Preferences-&gt;Protocols-&gt;TCP菜单里启用了Allow sub dissector to reassemble TCP streams后，才有可能收到这个提示。这个表示可以把属于同一个应用层PDU的TCP包虚拟的集中起来</li>
</ul>
</li>
<li><p>Continuation to #</p>
<ul>
<li>只有在Edit-&gt;Preferences-&gt;Protocols-&gt;TCP菜单里关闭了Allow sub dissector to reassemble TCP streams后，才有可能收到这个提示</li>
</ul>
</li>
<li><p>Time-to-live-exceeded(Fragment reasembly time execeeded)</p>
<ul>
<li>（Fragment reasembly time execeeded）表示这个包的发送方之前收到了一些分片，但是由于某些原因导致迟迟无法组装起来。</li>
<li>比如传输过程中有一些分片被丢包了，那么接收方就无法组装起来，然后就通过这个ICMP的方式告知发送方</li>
<li>ICMP是（Internet Control Message Protocol）Internet控制报文协议。它是TCP&#x2F;IP协议族的一个子协议，用于在IP主机、路由器之间传递控制消息。控制消息是指网络通不通、主机是否可达、路由是否可用等网络本身的消息。这些控制消息虽然并不传输用户数据，但是对于用户数据的传递起着重要的作用。</li>
</ul>
</li>
<li><p>抓包数据，如何判断一个包是上一个包的回包呢？根据TCP协议，下一个包的Ack的值如果等于上一个包的Seq + Len，则表示是其回包</p>
<ul>
<li>上一个和下一个，很多情况下并不是连续的，也行下一个回包距离上一个包已经过了很多包了，因为重传、延迟的原因的</li>
</ul>
</li>
<li><p>网络拥塞 network congestion</p>
<ul>
<li>超过网络承载量而导致的网络拥塞。发生拥塞时的在途字节数就是该时刻的网络拥塞点，估算网络拥塞点只需要简单找到拥塞时的在途字节数即可</li>
<li>拥塞的特征就是连串的丢包、丢包后又会重传；wireshark、tcpdump等抓包工具可以标识出重传包</li>
<li>根据抓包工具，找到一连串重传包中的第一个包，然后根据该包重传的Seq值找到对应的原始包，最后，计算该原始包发送时刻的在途字节数，这个就标识当前的拥塞点<ul>
<li>假如Seq+Len-Ack &#x3D; 103122，这个单位表示字节，也就是100KB，这个100KB就是最大的发送窗口，因此需要设置Linux系统中发送窗口的值</li>
<li>Wireshark -&gt;Analyze -&gt; Expert Info -&gt; Notes菜单可以看到重传统计</li>
<li>通过在途字节数只是估算 网络拥塞点，并不一定很精确，可以采样多次然后找到合适的值；多次采样后的数据，其实保守的话应该取最小值而不是平均值</li>
<li>取最小值是因为这样最保守，这样才能真正保证网络不会拥塞；取完之后要设置Linux的发送窗口</li>
<li>前面有说的，数据包的Len最大不应该超过1460(1388)，但是实际抓包分析过程中，却会发现实际的Len可能是1460(1388)的两倍或者N倍，这个是什么原因呢？ 这是因为有一个所谓的LSO。</li>
<li>一般网络工作方式是：应用层把产生的数据交给TCP层，TCP再根据MSS大小进行分段，分段由CPU负责进行，最后再交给网卡</li>
<li>如果启用了LSO：TCP层就把大于MSS的数据块直接交给了网卡，让网卡去负责分段工作<ul>
<li>从发送方的视角，相当于是站在了CPU视角，这样抓包看到的应该是分段前的打包。</li>
<li>从接收方视角，相当于是站在了网卡视角，那么看到的就应该是分段后的多个小包</li>
</ul>
</li>
<li>一些实战经验告诉我们，Wireshark -&gt;Analyze -&gt; Expert Info -&gt; Notes统计中的重传率如果超过了0.1%，就需要采取一些措施了。但是现实网络环境下，要低于0.01%的重传是基本不可能的。</li>
<li>发送窗口<ul>
<li>网络上的拥塞窗口（cwnd）<ul>
<li>cwnd的增长方式是：先“慢启动”、再进入“拥塞避免”，前者起点低但是能够快速增长</li>
</ul>
</li>
<li>服务器上的接收窗口<ul>
<li>后者起点高但是每一个RTT只能增加一个MSS（RTT指往返时间，也就是到了下一个从同一个方向传输的包）</li>
</ul>
</li>
<li>根据抓包的数据，点开详情，查看其“Bytes in flight”值，可以简单等同与cwnd</li>
<li>如果是“慢启动”阶段，那么下一个RTT的包的cwnd应该要远远大于上一个包的cwnd</li>
<li>如果是“拥塞避免”阶段，那么下一个RTT的包的cwnd应该要增加一个MSS（以太网中的MSS约为1460字节）。</li>
<li>如果不符合上述两种情况，比如cwnd增长的非常慢，那么就需要根据cwnd的计算方式去分析了</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="detect-the-tcp-multipath-out-of-order"><a href="#detect-the-tcp-multipath-out-of-order" class="headerlink" title="detect the tcp multipath(out of order)"></a><a target="_blank" rel="noopener" href="https://www.kawabangga.com/posts/5876">detect the tcp multipath(out of order)</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">traceroute 的原理很精妙。它利用了 IP 协议本身的特性：每一个 IP 包都有一个 TTL 字段，表示这个包还能在网络中被转发多少次，每次路由器转发一个 IP 包就将其 -1，如果一个路由器发现 IP 包 -1 之后是 0 了，就直接丢弃，并且给 IP 包的 Source IP 发送一个 ICMP 包（包含此 hop 自己的 IP），说这个包气数已尽，无法送达目的地。</span><br><span class="line"></span><br><span class="line">traceroute 想要知道去往另一个 IP，中间都会经过哪些 IP 节点。它发送一个 TTL=1 的 ping 包出去，包会挂在第一个 hop 上，第一个 hop 发回去 ICMP，traceroute 就知道了第一个 hop 的 IP 地址；它再发送一个 TTL=2 的 ping 包，就知道了第二个 hop 的 IP 地址…… 直到收到正确的 ping 响应，就算到头了。</span><br><span class="line"></span><br><span class="line"><span class="comment">#这个说法，个人觉得不太一定，大部分场景确实是的</span></span><br><span class="line">回到乱序的问题，最常见的问题是，中间某一跳到下一跳有多个节点可以选择。这时候，正确的情况下，对于 (src ip, src port, dst ip, dst port) 这样的四元组，应该固定走一条线路。</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ </span><br><span class="line">from scapy.all import *</span><br><span class="line"> </span><br><span class="line">hostname = <span class="string">&quot;10.0.0.1&quot;</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># add timeout</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(1, 20):</span><br><span class="line">    pkt = IP(dst=hostname, ttl=i) / TCP(sport=22334, dport=9100)</span><br><span class="line">    <span class="comment"># Send the packet and get a reply</span></span><br><span class="line">    reply = sr1(pkt, verbose=0, <span class="built_in">timeout</span>=5)</span><br><span class="line">    <span class="keyword">if</span> reply is None:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%d hop ??&quot;</span> % i)</span><br><span class="line">        <span class="built_in">continue</span></span><br><span class="line">    <span class="keyword">elif</span> TCP <span class="keyword">in</span> reply:</span><br><span class="line">        <span class="comment"># We&#x27;ve reached our destination</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Done!&quot;</span>, reply.src)</span><br><span class="line">        <span class="built_in">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># We&#x27;re in the middle somewhere</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%d hops away: &quot;</span> % i, reply.src)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ <span class="keyword">while</span> <span class="literal">true</span>; <span class="keyword">do</span> sudo python3 tra.py  &gt;&gt; traceroute.log; <span class="keyword">done</span></span><br><span class="line">$ grep -v <span class="string">&#x27;??&#x27;</span> traceroute.log | grep -v Done | <span class="built_in">sort</span> | <span class="built_in">uniq</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#如上输出，在 hop 5 出现了两个 IP，说明是 hop 4 到 hop 5 的时候出现了 multipath。</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#mtcp就是让tcp走多路径，但是没有看到生产使用的</span></span><br></pre></td></tr></table></figure>

  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Ethernet-cable"><span class="toc-number">1.</span> <span class="toc-text">Ethernet cable</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#interface-Forward-Error-Correction"><span class="toc-number">1.0.1.</span> <span class="toc-text">interface Forward Error Correction</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TCP"><span class="toc-number">2.</span> <span class="toc-text">TCP</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#RTT-three-types"><span class="toc-number">2.1.</span> <span class="toc-text">RTT three types</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Retransmission-Timeout"><span class="toc-number">2.2.</span> <span class="toc-text">Retransmission Timeout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Delayed-ACK-Timer"><span class="toc-number">2.3.</span> <span class="toc-text">Delayed ACK Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Persist-Timer"><span class="toc-number">2.4.</span> <span class="toc-text">Persist Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Keepalive-Timer"><span class="toc-number">2.5.</span> <span class="toc-text">Keepalive Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#FIN-WAIT-2-Timer"><span class="toc-number">2.6.</span> <span class="toc-text">FIN_WAIT_2 Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#TIME-WAIT-Timer"><span class="toc-number">2.7.</span> <span class="toc-text">TIME_WAIT Timer</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MSL-Maximum-Segment-Lifetime"><span class="toc-number">2.8.</span> <span class="toc-text">MSL (Maximum Segment Lifetime)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sysctl-conf"><span class="toc-number">2.9.</span> <span class="toc-text">sysctl.conf</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SLE-and-SER-in-SACK-RFC2018"><span class="toc-number">3.</span> <span class="toc-text">SLE and SER in SACK(RFC2018)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TOOLS-Tools-tools"><span class="toc-number">4.</span> <span class="toc-text">TOOLS Tools tools</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tcp-retrans"><span class="toc-number">5.</span> <span class="toc-text">tcp retrans</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#tcpdump-802-3-header"><span class="toc-number">5.1.</span> <span class="toc-text">tcpdump 802.3 header</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#tcp-netif-rx"><span class="toc-number">6.</span> <span class="toc-text">tcp netif_rx</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ethtool-cmd"><span class="toc-number">7.</span> <span class="toc-text">ethtool cmd</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#interrupt-moderation"><span class="toc-number">7.1.</span> <span class="toc-text">interrupt moderation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#show-NIC-model"><span class="toc-number">8.</span> <span class="toc-text">show NIC model</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Linux-network-performance-PERFORMANCE"><span class="toc-number">8.1.</span> <span class="toc-text">Linux network performance PERFORMANCE</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#Socket-receive-queues"><span class="toc-number">8.1.1.</span> <span class="toc-text">Socket receive queues</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#proc-net-softnet-stat"><span class="toc-number">8.1.2.</span> <span class="toc-text">&#x2F;proc&#x2F;net&#x2F;softnet_stat</span></a></li></ol></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Hardware-features"><span class="toc-number">8.2.</span> <span class="toc-text">Hardware features</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#busy-poll-Interrupt-Queues"><span class="toc-number">8.2.1.</span> <span class="toc-text">busy poll(Interrupt Queues)</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#ARFS-support"><span class="toc-number">8.2.2.</span> <span class="toc-text">ARFS support</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#RSS-support"><span class="toc-number">8.2.3.</span> <span class="toc-text">RSS support</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#THEORETICAL-MAX-RATE"><span class="toc-number">8.2.4.</span> <span class="toc-text">THEORETICAL MAX RATE</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#CPU-setting"><span class="toc-number">8.2.5.</span> <span class="toc-text">CPU setting</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#dump-NIC-log"><span class="toc-number">8.2.6.</span> <span class="toc-text">dump NIC log</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#flow-control"><span class="toc-number">8.2.7.</span> <span class="toc-text">flow control</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#DRIVER-driver"><span class="toc-number">8.2.8.</span> <span class="toc-text">DRIVER driver</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#hash-policy"><span class="toc-number">8.2.9.</span> <span class="toc-text">hash policy</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SYN-timeout-on-linux"><span class="toc-number">9.</span> <span class="toc-text">SYN timeout on linux</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#ip-could-set-initrwnd"><span class="toc-number"></span> <span class="toc-text">ip could set initrwnd</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#net-ipv4-tcp-slow-start-after-idle-0-not-be-1-https-www-kawabangga-com-posts-5217"><span class="toc-number"></span> <span class="toc-text">net.ipv4.tcp_slow_start_after_idle &#x3D; 0 , not be 1  https:&#x2F;&#x2F;www.kawabangga.com&#x2F;posts&#x2F;5217</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E6%95%88%E6%9E%9C%E6%98%AF%EF%BC%8C%E5%8E%9F%E6%9D%A5%E6%95%B0%E6%8D%AE%E9%9C%80%E8%A6%81-30-%E5%A4%9A%E5%88%86%E9%92%9F%E4%B8%8B%E8%BD%BD%E5%AE%8C%E6%88%90%EF%BC%8C%E6%8D%A2%E6%88%90-BBR-%E4%B9%8B%E5%90%8E%EF%BC%8C%E5%8F%AA%E9%9C%80%E8%A6%81-2-%E5%88%86%E9%92%9F%E5%B0%B1%E4%B8%8B%E8%BD%BD%E5%AE%8C%E4%BA%86%E3%80%82"><span class="toc-number"></span> <span class="toc-text">实际效果是，原来数据需要 30 多分钟下载完成，换成 BBR 之后，只需要 2 分钟就下载完了。</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#wireshark"><span class="toc-number">1.</span> <span class="toc-text">wireshark</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#detect-the-tcp-multipath-out-of-order"><span class="toc-number">2.</span> <span class="toc-text">detect the tcp multipath(out of order)</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2022/11/13/ethernet/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2022/11/13/ethernet/&text=ethernet"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2022/11/13/ethernet/&is_video=false&description=ethernet"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=ethernet&body=Check out this article: http://example.com/2022/11/13/ethernet/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2022/11/13/ethernet/&title=ethernet"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2022/11/13/ethernet/&name=ethernet&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2022/11/13/ethernet/&t=ethernet"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2024
    John Doe
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = '67e8c052/67e8c052.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'blog-comments';
      var utterances_theme = 'github-dark';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
