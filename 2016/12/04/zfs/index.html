<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Why ZFS Transaction file system(Data consistency guaranteed)  - Manages writing of data with copy-on-write method  - Does not overwrite original data  - Commits or ignores sequential processing comple">
<meta property="og:type" content="article">
<meta property="og:title" content="openzfs in the production">
<meta property="og:url" content="http://example.com/2016/12/04/zfs/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="Why ZFS Transaction file system(Data consistency guaranteed)  - Manages writing of data with copy-on-write method  - Does not overwrite original data  - Commits or ignores sequential processing comple">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2016-12-04T07:06:13.000Z">
<meta property="article:modified_time" content="2023-01-29T09:51:50.042Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="scsi">
<meta property="article:tag" content="zfs">
<meta name="twitter:card" content="summary">
    
    
      
        
          <link rel="shortcut icon" href="/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>openzfs in the production</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
<meta name="generator" content="Hexo 6.3.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" aria-label="Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post" href="/2016/12/18/sparse/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post" href="/2016/09/04/kcptun/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2016/12/04/zfs/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2016/12/04/zfs/&text=openzfs in the production"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2016/12/04/zfs/&is_video=false&description=openzfs in the production"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=openzfs in the production&body=Check out this article: http://example.com/2016/12/04/zfs/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2016/12/04/zfs/&name=openzfs in the production&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2016/12/04/zfs/&t=openzfs in the production"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-ZFS"><span class="toc-number">1.</span> <span class="toc-text">Why ZFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Create-zpool"><span class="toc-number">2.</span> <span class="toc-text">Create zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameters"><span class="toc-number">3.</span> <span class="toc-text">parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#intel-%E2%80%98s-zfs-solution-just-record-Ashamed"><span class="toc-number">4.</span> <span class="toc-text">intel ‘s zfs solution, just record ,Ashamed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#udev-setting"><span class="toc-number">5.</span> <span class="toc-text">udev setting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NVME-zpool-throughput-not-test"><span class="toc-number">6.</span> <span class="toc-text">NVME zpool throughput, not test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-0-7-import-slow"><span class="toc-number">7.</span> <span class="toc-text">[ zfs 0.7 import slow]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-schduler"><span class="toc-number">8.</span> <span class="toc-text">zfs schduler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Monitor"><span class="toc-number">9.</span> <span class="toc-text">Monitor</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L2ARC-and-slog"><span class="toc-number">9.1.</span> <span class="toc-text">L2ARC and slog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#txg-timeout"><span class="toc-number">9.2.</span> <span class="toc-text">txg_timeout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zpool-sync-mode"><span class="toc-number">9.3.</span> <span class="toc-text">zpool sync mode</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-import-failed"><span class="toc-number">10.</span> <span class="toc-text">zpool import failed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-command"><span class="toc-number">11.</span> <span class="toc-text">zfs command</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#special-class-vdev"><span class="toc-number">11.1.</span> <span class="toc-text">special class vdev</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#single-device-no-raid-too-a-mirror"><span class="toc-number">12.</span> <span class="toc-text">single device(no raid) too a mirror</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Add-special-device"><span class="toc-number">12.1.</span> <span class="toc-text">Add special device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zpool-replace"><span class="toc-number">12.2.</span> <span class="toc-text">zpool replace</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Online-upgrade-SAS-device-firmware"><span class="toc-number">12.3.</span> <span class="toc-text">Online upgrade SAS device firmware</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Custom-packages"><span class="toc-number">12.4.</span> <span class="toc-text">Custom packages</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Replication-zpool"><span class="toc-number">13.</span> <span class="toc-text">Replication zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checkpoint"><span class="toc-number">14.</span> <span class="toc-text">checkpoint</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-test"><span class="toc-number">15.</span> <span class="toc-text">zfs test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Work-with-gdb"><span class="toc-number">16.</span> <span class="toc-text">Work with gdb</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#snapshot"><span class="toc-number">16.1.</span> <span class="toc-text">snapshot</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-vol"><span class="toc-number">16.2.</span> <span class="toc-text">zfs vol</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-event"><span class="toc-number">16.3.</span> <span class="toc-text">zfs event</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Compression"><span class="toc-number">16.4.</span> <span class="toc-text">Compression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Xattr"><span class="toc-number">16.5.</span> <span class="toc-text">Xattr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ACL"><span class="toc-number">16.6.</span> <span class="toc-text">ACL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rmount"><span class="toc-number">16.7.</span> <span class="toc-text">Rmount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Zpool-rename"><span class="toc-number">16.8.</span> <span class="toc-text">Zpool rename</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Detach-hdd-from-mirror"><span class="toc-number">16.9.</span> <span class="toc-text">Detach hdd from mirror</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Attach-one-hdd-to-mirror"><span class="toc-number">16.10.</span> <span class="toc-text">Attach one hdd to mirror</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sub-zpool"><span class="toc-number">16.11.</span> <span class="toc-text">sub zpool</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Quota"><span class="toc-number">16.12.</span> <span class="toc-text">Quota</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#user-quota"><span class="toc-number">16.13.</span> <span class="toc-text">user quota</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zpool-Add"><span class="toc-number">16.14.</span> <span class="toc-text">zpool Add</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Clear-zfs-label"><span class="toc-number">16.15.</span> <span class="toc-text">Clear zfs label</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#iostat"><span class="toc-number">16.16.</span> <span class="toc-text">iostat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Custom-script-show-temperature"><span class="toc-number">16.17.</span> <span class="toc-text">Custom script ,show temperature</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pin-CPU"><span class="toc-number">17.</span> <span class="toc-text">pin CPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx"><span class="toc-number">18.</span> <span class="toc-text">Disable AVX512 for scalable Xeon silver and gold 5xxx</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-dracut-cause-brain-split"><span class="toc-number">19.</span> <span class="toc-text">zfs-dracut cause brain split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Remove-slog"><span class="toc-number">20.</span> <span class="toc-text">Remove slog</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-SMR-HDD"><span class="toc-number">21.</span> <span class="toc-text">About SMR HDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-silent-error"><span class="toc-number">22.</span> <span class="toc-text">About silent error</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Import-zpool-hang-single-dev-just-respond-very-slow-cause-all-zpool-command-hang"><span class="toc-number">22.1.</span> <span class="toc-text">Import zpool hang, single dev just respond very slow cause all zpool command hang</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SAS-signal-about-Invalid-DWORD-count"><span class="toc-number">23.</span> <span class="toc-text">SAS signal (about Invalid DWORD count)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#direct-IO"><span class="toc-number">24.</span> <span class="toc-text">direct IO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Performance-tuning"><span class="toc-number">25.</span> <span class="toc-text">Performance tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#single-HDD-cause-the-zpool-import-failed"><span class="toc-number">26.</span> <span class="toc-text">single HDD cause the zpool import failed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Error-ICRC-ABRT-at-LBA-in-SATA-device"><span class="toc-number">27.</span> <span class="toc-text">Error: ICRC, ABRT at LBA in SATA device</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MMP-mmp-timeout-calculate"><span class="toc-number">28.</span> <span class="toc-text">MMP(mmp) timeout calculate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-block"><span class="toc-number">29.</span> <span class="toc-text">zpool block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#case-2246"><span class="toc-number">30.</span> <span class="toc-text">case 2246</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#case-reslivering-error-restart-dead-loop"><span class="toc-number">31.</span> <span class="toc-text">case reslivering error restart dead loop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-uberblock-info"><span class="toc-number">32.</span> <span class="toc-text">zfs uberblock info</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-status-show-the-read-x2F-write-x2F-chsum-error"><span class="toc-number">33.</span> <span class="toc-text">zpool status show the read&#x2F;write&#x2F;chsum error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enable-debug"><span class="toc-number">34.</span> <span class="toc-text">enable-debug</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-read-perfromance"><span class="toc-number">35.</span> <span class="toc-text">zfs read perfromance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#upgrade-SCSI-device-firmware-online"><span class="toc-number">36.</span> <span class="toc-text">upgrade SCSI device firmware online</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-write-full"><span class="toc-number">37.</span> <span class="toc-text">zpool write full</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-the-cksum-error"><span class="toc-number">38.</span> <span class="toc-text">About the cksum error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-receive-stalls-x2F-slowdown-with-dnodesize-x3D-legacy-8458"><span class="toc-number">39.</span> <span class="toc-text">zfs receive stalls&#x2F;slowdown with dnodesize!&#x3D;legacy #8458</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-0-7-X-zfs-receive-slow"><span class="toc-number">40.</span> <span class="toc-text">zfs 0.7.X zfs receive slow</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        openzfs in the production
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">John Doe</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2016-12-04T07:06:13.000Z" itemprop="datePublished">2016-12-04</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/categories/Storage/">Storage</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/tags/scsi/" rel="tag">scsi</a>, <a class="tag-link-link" href="/tags/zfs/" rel="tag">zfs</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h3 id="Why-ZFS"><a href="#Why-ZFS" class="headerlink" title="Why ZFS"></a><a target="_blank" rel="noopener" href="https://www.fujitsu.com/global/Images/ZFS%20Overview%20and%20Design%20Guide.pdf">Why ZFS</a></h3><ul>
<li>Transaction file system(Data consistency guaranteed)<br>  - Manages writing of data with copy-on-write method<br>  - Does not overwrite original data<br>  - Commits or ignores sequential processing completely<br>  - No data inconsistency<br>  - Consistency check not required for the file system<br>  - Asynchronous writing to disks<br>          - The system writes to the disk (I&#x2F;O) after the end of sequential processing, so a check of the actual disk capacity (by a command such as df or du) may find that it is different.</li>
<li>End-to-end checksum(Invalid data detected Data self-corrected)<br>  - Stores checksum of data block in its parent block<br>  - Restores data from redundant block when error is detected<br>  - Also detects and corrects logical inconsistencies (software bugs)<br>  - Data cannot be recovered when the data corruption range includes the checksum<br>  - The data and checksum are physically separated and read individually. The checksum itself can be recovered from the higher-level block.<br>  - ZFS not only detects the read errors caused by a disk fault, but also detects and corrects logical inconsistencies caused by a software bug<br>  - When read or written, invalid data is detected by the checksum. When invalid data is detected, if the system is in a redundant configuration (RAID 1 (mirroring), RAID-Z, RAID-Z2, or RAID-Z3), the data is automatically recovered (selfhealing)</li>
</ul>
<h3 id="Create-zpool"><a href="#Create-zpool" class="headerlink" title="Create zpool"></a>Create zpool</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create raid</span></span><br><span class="line">$ lsscsi -gis &gt; HDDs</span><br><span class="line">$ awk <span class="string">&#x27;BEGIN&#123;count=1; printf &quot;zpool create ost_0 -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=12 -o multihost=on &quot;&#125;;$0~/sd/ &amp;&amp; $0~/3500/ &#123;if(count%30!=0) &#123;if(count%3==1) &#123;printf &quot; mirror &quot;$(NF-3)&quot; &quot;&#125; else &#123;printf $(NF-3)&quot; &quot;&#125;&#125; else &#123;print $(NF-3); printf &quot;zpool create ost_0 -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=12 -o multihost=on &quot;&#125;; count++&#125;&#x27;</span> HDDs </span><br><span class="line">$ awk <span class="string">&#x27;BEGIN&#123;count=1&#125;;$0~/sd/ &amp;&amp; $0~/3500/ &#123;if(count%30!=0) &#123;printf $(NF-3)&quot; &quot;&#125; else &#123;print $(NF-3)&#125;; count++&#125;&#x27;</span> HDDs  | awk <span class="string">&#x27;&#123;print NF&#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">$ zpool create -f tank01 -O dnodesize=auto -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=12 -O recordsize=64k -O secondarycache=none -O logbias=throughput -o multihost=on -O autoreplace=on raidz /dev/sd&#123;b..d&#125;</span><br><span class="line">$ zpool create tank01 -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=9 -o multihost=on -o autoreplace=on raidz /dev/sd&#123;b..d&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">##raidz3 15xHDDs</span></span><br><span class="line">lsscsi -gis | awk <span class="string">&#x27;BEGIN&#123;</span></span><br><span class="line"><span class="string">  count=1;begin_var=0;zc=0</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">  $0~/sd/ &amp;&amp; $0~/3500/ &#123;</span></span><br><span class="line"><span class="string">      if(count%15!=0) &#123;</span></span><br><span class="line"><span class="string">          if(begin_var==0) &#123;</span></span><br><span class="line"><span class="string">             printf &quot;zpool create ost_&quot;zc&quot; -O canmount=on -O xattr=sa -O acltype=posixacl -o ashift=9 -o multihost=on  raidz3 &quot;$(NF-3)&quot; &quot;; begin_var=1</span></span><br><span class="line"><span class="string">          &#125; else &#123;</span></span><br><span class="line"><span class="string">            printf $(NF-3)&quot; &quot;</span></span><br><span class="line"><span class="string">          &#125;</span></span><br><span class="line"><span class="string">      &#125; else &#123;</span></span><br><span class="line"><span class="string">          print $(NF-3); begin_var=0; zc++</span></span><br><span class="line"><span class="string">      &#125;</span></span><br><span class="line"><span class="string">      count++</span></span><br><span class="line"><span class="string">  &#125;&#x27;</span></span><br><span class="line"></span><br><span class="line">0.7.13 ashift=9 16x16T raidz3 69%</span><br><span class="line">2.0.7  ashift=9 16x16T raidz3 73%, 15x16T 72%, 14x16T 70.9%</span><br></pre></td></tr></table></figure>

<h3 id="parameters"><a href="#parameters" class="headerlink" title="parameters"></a>parameters</h3><ul>
<li>xattr&#x3D;on stores extended attributes in hidden sub directories, which can require multiple lookups when accessing a file</li>
<li>xattr&#x3D;sa, stores extended attributes in inodes, resulting in less IO requests when extended attributes are in use</li>
<li><a target="_blank" rel="noopener" href="https://www.reddit.com/r/zfs/comments/azt8sz/logbiasthroughput_without_a_slog/eiilwoi">Logbias&#x3D;throughput</a><ul>
<li>The throughout setting just means that during txg_sync, the blocks written to disk are assigned their Txn in place rather than being allocated fresh.on the assumption that you’re writing big blocks that don’t need to be aggregated by the SPA.</li>
<li>Logbias&#x3D;throughput with no SLOG will likely improve performance if your workload is lots of big block writes, which is a workload that usually isn’t suffering from performance issues much in the first place.</li>
<li>Logbias&#x3D;throughput with no SLOG and small block writes will result in the most horrific fragmentation imaginable, which will penalize you both in the initial writes AND when you re read that data from metal later.</li>
<li>Terminology note: logbias&#x3D;throughput writes the blocks in “indirect mode” to the ZIL where the data is written to the pool and a pointer to the data is written to the ZIL.</li>
<li>zfs_immediate_write_sz: If a pool does not have a log device, data blocks equal to or larger than zfs_immediate_write_sz are treated as if the dataset being written to had the property setting logbias&#x3D;throughput<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line">options zfs zfs_multihost_history=300</span><br><span class="line">options zfs zfs_txg_history=300</span><br><span class="line">options zfs zfs_prefetch_disable=0  <span class="comment">## Good for HDD and sequential read/write; 0=prefetch enabled, 1=prefetch disabled</span></span><br><span class="line">options zfs metaslab_debug_unload=1 <span class="comment">## echo 512 &gt; /sys/module/zfs/parameters/zfs_flags #zfs_flags = debug level</span></span><br><span class="line"><span class="comment"># https://openzfs.github.io/openzfs-docs/Performance%20and%20Tuning/Module%20Parameters.html#zfs-flags</span></span><br><span class="line">options zfs zfs_multihost_history=300 <span class="comment">## /proc/spl/kstat/zfs/tank_0/multihost</span></span><br><span class="line">options zfs zfs_arc_max=49928994816 <span class="comment"># eg: 64GB mem  ##echo $(awk &#x27;$0~/MemTotal/&#123;printf &quot;%.0f \n&quot;, $2*1024*0.75&#125;&#x27; /proc/meminfo) #75%</span></span><br><span class="line">options zfs zio_dva_throttle_enabled=0</span><br><span class="line"><span class="comment">#zfs_vdev_queue_depth_pct - Queue depth percentage for each top-level vdev increase the default value for high throuhput system  or disable zio_dva_throttle_enabled</span></span><br><span class="line">options zfs zfs_dirty_data_sync=1073741824</span><br><span class="line">options zfs zfs_vdev_scheduler=deadline</span><br><span class="line">options zfs zfs_dbgmsg_enable=1  <span class="comment"># 0=do not log debug messages, 1=log debug messages, /proc/spl/kstat/zfs/dbgmsg</span></span><br><span class="line"><span class="comment"># echo 0xfffffbfe &gt; /sys/module/zfs/parameters/zfs_flags #for more debuginfo</span></span><br><span class="line"></span><br><span class="line">options zfs zio_taskq_batch_pct=50 <span class="comment"># Depends your cpu core numbers and disable compression</span></span><br><span class="line">options zfs zfs_multihost_fail_intervals=0</span><br><span class="line"><span class="comment">## multihost write failures are ignored. The write failures are reported to the ZFS event daemon (zed) which can take action such as suspending the pool or offlining a device, and MMP will work under the pair node duplicate force import the zpool, and it may go to brain split although only a little chance.</span></span><br><span class="line"><span class="comment">#options zfs zfetch_max_distance=67108864 ## If there are many sequential read</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#zfs_commit_timeout_pct (int)</span></span><br><span class="line"><span class="comment">#This controls the amount of time that a ZIL block (lwb) will remain &quot;open&quot; when it isn&#x27;t &quot;full&quot;, and it has a thread waiting for it to be committed to stable storage. The timeout is scaled based on a percentage of the last lwb latency to avoid significantly impacting the latency of each individual transaction record (itx).</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#zfs_vdev_max_active</span></span><br><span class="line"><span class="comment">#zfs_commit_timeout_pct</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## if you want MMP go to suspend</span></span><br><span class="line">options zfs zfs_multihost_fail_intervals=100</span><br><span class="line">options zfs zfs_multihost_import_intervals=200</span><br><span class="line">options zfs zfs_multihost_interval=6000</span><br><span class="line"><span class="comment"># Don&#x27;t allow fail_intervals larger than import_intervals</span></span><br><span class="line"><span class="comment"># When zfs_multihost_fail_intervals &gt; 0 then sequential mul‐tihost write failures will cause the pool to be  suspended. This occurs when zfs_multihost_fail_intervals * zfs_multi‐htank_interval milliseconds have passed since the last successful multihost write.  This guarantees the activity test will see multihost writes if the pool is imported.</span></span><br><span class="line"><span class="comment">#zfs_multihost_fail_intervals &gt;= zfs_multihost_import_intervals which could allow a pool to be imported on two nodes without any warnings.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Tuning for Scrubs and Resilvers http://broken.net/uncategorized/zfs-performance-tuning-for-scrubs-and-resilvers</span></span><br><span class="line">options zfs zfs_resilver_delay=0 <span class="comment">#default 2</span></span><br><span class="line">options zfs zfs_scrub_delay=0  <span class="comment">#default 4</span></span><br><span class="line"><span class="comment">#Maximum number of scrub I/O per top-level vdev, by default 32. Increases zfs scrub speed. (at what cost, no idea), and it will impact the zpool performance for application</span></span><br><span class="line">options zfs zfs_top_maxinflight=512 <span class="comment"># in 2.0.7, no this parameter, increase these 2 parameters, zfs_vdev_queue_depth_pct and zfs_scan_vdev_limit</span></span><br><span class="line"><span class="comment">#zfs_scan_min_time_ms:Min millisecs to scrub per txg (int)</span></span><br><span class="line"><span class="comment">#resilver for five seconds per TXG</span></span><br><span class="line">options zfs zfs_resilver_min_time_ms=5000</span><br><span class="line"><span class="comment">#The resilvers or scrub speed increase from 50MB/s to 800MB/s, increase a lot</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#In some of emergency case, skip the scan error</span></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/module/zfs/parameters/zfs_scan_ignore_errors</span><br><span class="line"></span><br><span class="line"><span class="comment">#Get better performance in the resilvering</span></span><br><span class="line"><span class="comment">#https://github.com/openzfs/zfs/issues/4825</span></span><br><span class="line"><span class="comment">#Speed up to resilvering</span></span><br><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/zfs/parameters/zfs_resilver_delay</span><br><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/zfs/parameters/zfs_scrub_delay</span><br><span class="line"><span class="built_in">echo</span> 1024 &gt; /sys/module/zfs/parameters/zfs_top_maxinflight</span><br><span class="line"><span class="built_in">echo</span> 8000 &gt; /sys/module/zfs/parameters/zfs_resilver_min_time_ms</span><br><span class="line"></span><br><span class="line"><span class="comment">#Get better performance in the resilvering</span></span><br><span class="line"><span class="comment">#Speed up on pool</span></span><br><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/zfs/parameters/zfs_scan_idle</span><br><span class="line"><span class="built_in">echo</span> 24 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_min_active</span><br><span class="line"><span class="built_in">echo</span> 64 &gt; /sys/module/zfs/parameters/zfs_vdev_scrub_max_active</span><br><span class="line"><span class="built_in">echo</span> 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_min_active</span><br><span class="line"><span class="built_in">echo</span> 64 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_write_max_active</span><br><span class="line"><span class="built_in">echo</span> 8 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_min_active</span><br><span class="line"><span class="built_in">echo</span> 32 &gt; /sys/module/zfs/parameters/zfs_vdev_sync_read_max_active</span><br><span class="line"><span class="built_in">echo</span> 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_min_active</span><br><span class="line"><span class="built_in">echo</span> 32 &gt; /sys/module/zfs/parameters/zfs_vdev_async_read_max_active</span><br><span class="line"><span class="built_in">echo</span> 8 &gt; /sys/module/zfs/parameters/zfs_vdev_async_write_min_active</span><br><span class="line"></span><br><span class="line"><span class="comment">#l2arc_write_max</span></span><br><span class="line">options zfs l2arc_write_max=536870912</span><br><span class="line"></span><br><span class="line"><span class="comment"># number of max device writes to precache, How far through the ARC lists to search for L2ARC cacheable content</span></span><br><span class="line"><span class="comment"># If the rate of change in the ARC is faster than the overall L2ARC feed rate, then increasing l2arc_headroom can increase L2ARC efficiency. Setting the value too large can cause the L2ARC feed thread to consume more CPU time looking for data to feed.</span></span><br><span class="line"><span class="comment"># default : 2</span></span><br><span class="line">options zfs l2arc_headroom=12</span><br><span class="line"></span><br><span class="line">l2arc_write_boost</span><br><span class="line"><span class="built_in">echo</span> 536870912 &gt; /sys/module/zfs/parameters/l2arc_write_boost</span><br><span class="line">Cold L2ARC devices will have l2arc_write_max increased by this amount <span class="keyword">while</span> they remain cold.</span><br><span class="line"><span class="comment">#If the cache devices can sustain the write workload, increasing the rate of cache device fill when workloads generate new data at a rate higher than l2arc_write_max can increase L2ARC hit rate</span></span><br><span class="line"><span class="comment"># Maximum number of bytes to be written to each cache device for each L2ARC feed thread interval</span></span><br><span class="line"><span class="comment"># tradeoff between write/read and durability of ssd (?)</span></span><br><span class="line"><span class="comment"># default : 8388608 Bytes</span></span><br><span class="line"><span class="comment"># setting here : 512 * 1024 * 1024 = 512MiB</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 16 &gt; /sys/module/zfs/parameters/l2arc_headroom_boost</span><br><span class="line">Scales l2arc_headroom by this percentage when L2ARC contents are being successfully compressed before writing. A value of 100 disables this feature.</span><br><span class="line">Default value: 200%.</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/zfs/parameters/l2arc_norw</span><br><span class="line">No reads during writes.</span><br><span class="line">Use 1 <span class="keyword">for</span> <span class="built_in">yes</span> and 0 <span class="keyword">for</span> no (default).</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/module/zfs/parameters/l2arc_feed_again</span><br><span class="line"><span class="built_in">echo</span> 5 &gt; /sys/module/zfs/parameters/l2arc_feed_min_ms</span><br><span class="line">Turbo L2ARC warm-up. When the L2ARC is cold the fill interval will be <span class="built_in">set</span> as fast as possible.</span><br><span class="line">Use 1 <span class="keyword">for</span> <span class="built_in">yes</span> (default) and 0 to <span class="built_in">disable</span>.</span><br><span class="line">Min feed interval <span class="keyword">in</span> milliseconds. Requires l2arc_feed_again=1 and only applicable <span class="keyword">in</span> related situations.</span><br><span class="line">Default value: 200.</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/module/zfs/parameters/l2arc_noprefetch</span><br><span class="line">l2arc_noprefetch (int)</span><br><span class="line">Do not write buffers to L2ARC <span class="keyword">if</span> they were prefetched but not used by applications. In <span class="keyword">case</span> there are prefetched buffers <span class="keyword">in</span> L2ARC and this option is later <span class="built_in">set</span> to 1, we <span class="keyword">do</span> not <span class="built_in">read</span> the prefetched buffers from L2ARC. Setting this option to 0 is useful <span class="keyword">for</span> caching sequential reads from the disks to L2ARC and serve those reads from L2ARC later on. This may be beneficial <span class="keyword">in</span> <span class="keyword">case</span> the L2ARC device is significantly faster <span class="keyword">in</span> sequential reads than the disks of the pool.</span><br><span class="line">Use 1 to <span class="built_in">disable</span> (default) and 0 to <span class="built_in">enable</span> caching/reading prefetches to/from L2ARC..</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="intel-‘s-zfs-solution-just-record-Ashamed"><a href="#intel-‘s-zfs-solution-just-record-Ashamed" class="headerlink" title="intel ‘s zfs solution, just record ,Ashamed"></a><a target="_blank" rel="noopener" href="//wiki.lustre.org/images/4/48/ZFS-As-Backend-File-System_Paciucci.pdf">intel ‘s zfs solution, just record ,Ashamed</a></h3><h3 id="udev-setting"><a href="#udev-setting" class="headerlink" title="udev setting"></a>udev setting</h3><p>When the power loss, the zfs could be OK when the WCE enabled<br>If the device from the zpool from first to last, reset the device one by one, if the </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /etc/udev/rules.d/50-zfs-udev.rules</span><br><span class="line"><span class="comment"># Reset some HDD vendor default value</span></span><br><span class="line">ACTION==<span class="string">&quot;add&quot;</span>, KERNEL==<span class="string">&quot;sd*[!0-9]&quot;</span>, RUN+=<span class="string">&quot;/bin/sh -c &#x27;sdparm -p ca -s WCE=0,RCD=0 /dev/%k &amp;&amp; sdparm -p bc -s EN_BMS=0 /dev/%k&#x27;&quot;</span></span><br><span class="line"><span class="comment"># Ban the model device</span></span><br><span class="line">ACTION==<span class="string">&quot;add|change&quot;</span>, KERNEL==<span class="string">&quot;sd*[!0-9]&quot;</span>, ATTRS&#123;model&#125;==<span class="string">&quot;ST6000NM0034     &quot;</span>, RUN+=<span class="string">&quot;/bin/sh -c &#x27;echo 1 &gt; /sys/block/%k/device/delete&#x27;&quot;</span></span><br><span class="line"></span><br><span class="line">$ udevadm control --reload-rules &amp;&amp; udevadm trigger</span><br></pre></td></tr></table></figure>


<h3 id="NVME-zpool-throughput-not-test"><a href="#NVME-zpool-throughput-not-test" class="headerlink" title="NVME zpool throughput, not test"></a><a target="_blank" rel="noopener" href="https://zfsonlinux.topicbox.com/groups/zfs-discuss/Tdec42b46ad7dd0e7-Mb6a047a880b62d2664335ef3/zfs-write-performance-stuck-at-11gb-with-nvme-ssd-pools">NVME zpool throughput, not test</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># PERFORMANCE performance</span></span><br><span class="line">We have changed the following tuning parameters to reach 11-12GB.</span><br><span class="line"></span><br><span class="line"><span class="comment">#cat /etc/modprobe.d/zfs.conf</span></span><br><span class="line"><span class="comment"># sync write</span></span><br><span class="line">options zfs zfs_vdev_sync_write_min_active=5</span><br><span class="line">options zfs zfs_vdev_sync_write_max_active=16</span><br><span class="line"><span class="comment"># sync reads (normal)</span></span><br><span class="line">options zfs zfs_vdev_sync_read_min_active=5</span><br><span class="line">options zfs zfs_vdev_sync_read_max_active=16</span><br><span class="line"><span class="comment"># async reads : prefetcher</span></span><br><span class="line">options zfs zfs_vdev_async_read_min_active=5</span><br><span class="line">options zfs zfs_vdev_async_read_max_active=16</span><br><span class="line"><span class="comment"># async write : bulk writes</span></span><br><span class="line">options zfs zfs_vdev_async_write_min_active=5</span><br><span class="line">options zfs zfs_vdev_async_write_max_active=24</span><br><span class="line">options zfs zfs_vdev_max_active=1600      &lt;--- default 1000</span><br><span class="line">options zfs zfs_vdev_queue_depth_pct=2000 &lt;--- increase more ?</span><br><span class="line">options zfs zfs_top_maxinflight=1600      &lt;--- not work <span class="keyword">in</span> 2.0.7</span><br><span class="line">options zfs zfs_vdev_scheduler=deadline</span><br><span class="line">options zfs zfs_vdev_aggregation_limit=0  &lt;--- default 1048576</span><br><span class="line"><span class="comment">#Setting zfs_vdev_aggregation_limit = 0 effectively disables aggregation by ZFS. However, the block device scheduler can still merge (aggregate) I/Os. Also, many devices, such as modern HDDs, contain schedulers that can aggregate I/Os</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#To reduce IOPs, small, adjacent I/Os can be aggregated (coalesced) into a large I/O. For reads, aggregations occur across small adjacency gaps. For writes, aggregation can occur at the ZFS or disk level. zfs_vdev_aggregation_limit is the upper bound on the size of the larger, aggregated I/O.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#write throttles</span></span><br><span class="line">options zfs zfs_delay_min_dirty_percent=98 &lt;--- default 60</span><br><span class="line">options zfs zfs_delay_scale=50000          &lt;--- default 500000</span><br><span class="line"><span class="comment">#zfs_delay_scale controls how quickly the ZFS write throttle transaction delay approaches infinity. Larger values cause longer delays for a given amount of dirty data</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#options zfs zfs_dirty_data_sync=2147483648</span></span><br><span class="line"><span class="comment">#options zfs zfs_dirty_data_max=8589934592</span></span><br><span class="line"><span class="comment">#options zfs zfs_dirty_data_max_max=8589934592</span></span><br><span class="line">options zfs zfs_dirty_data_max=12884901888</span><br><span class="line">options zfs zfs_dirty_data_max_max=12884901888</span><br><span class="line">options zfs zfs_dirty_data_max_percent=29</span><br><span class="line">options zfs zfs_txg_timeout=10           &lt;--- default = 5</span><br><span class="line">options zfs zfs_vdev_write_gap_limit=0   &lt;--- default = 4096</span><br><span class="line"><span class="comment">#To reduce IOPs, small, adjacent I/Os are aggregated (coalesced) into into a large I/O. For writes, aggregations occur across small adjacency gaps where the gap is less than zfs_vdev_write_gap_limit</span></span><br></pre></td></tr></table></figure>

<h3 id="zfs-0-7-import-slow"><a href="#zfs-0-7-import-slow" class="headerlink" title="[ zfs 0.7 import slow]"></a>[ zfs 0.7 import slow]</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 0 &gt; /sys/module/zfs/parameters/zfs_multihost_fail_intervals</span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/module/zfs/parameters/zfs_multihost_import_intervals</span><br><span class="line">$ zpool import -a</span><br></pre></td></tr></table></figure>

<h3 id="zfs-schduler"><a href="#zfs-schduler" class="headerlink" title="zfs schduler"></a>zfs schduler</h3><p>zfs_vdev_scheduler not support mq-deadline<br>since ZFS has its own I&#x2F;O scheduler, using a simple scheduler can result in more consistent performance<br>expected: noop, cfq, bfq, and deadline    </p>
<h3 id="Monitor"><a href="#Monitor" class="headerlink" title="Monitor"></a>Monitor</h3><p><a target="_blank" rel="noopener" href="http://lfs.ornl.gov/ecosystem-2016/documents/tutorials/Stearman-LLNL-ZFS.pdf">Tutorial: How to install, tune and Monitor a ZFS based Lustre file system</a>   </p>
<h4 id="L2ARC-and-slog"><a href="#L2ARC-and-slog" class="headerlink" title="L2ARC and slog"></a>L2ARC and slog</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ arcstat.py -f <span class="string">&quot;time,read,hit%,hits,miss%,miss,arcsz,c&quot;</span> 1</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/spl/kstat/zfs/arcstats</span><br><span class="line">$ arcstat.py 2</span><br><span class="line">$ arc_summary.py | grep <span class="string">&quot;L2 ARC Breakdown&quot;</span> -A 2</span><br><span class="line">L2 ARC Breakdown:                               846.25m</span><br><span class="line">        Hit Ratio:                      0.47%   3.98m</span><br><span class="line">        Miss Ratio:                     99.53%  842.27m</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h4 id="txg-timeout"><a href="#txg-timeout" class="headerlink" title="txg_timeout"></a>txg_timeout</h4><p>The last zfs_txg_history txg commits are available in &#x2F;proc&#x2F;spl&#x2F;kstat&#x2F;zfs&#x2F;POOL_NAME&#x2F;txgs</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cat</span> /sys/module/zfs/parameters/zfs_txg_timeout</span><br><span class="line">5</span><br><span class="line"></span><br><span class="line">every 5s will <span class="keyword">do</span> transaction <span class="built_in">sync</span></span><br><span class="line"></span><br><span class="line">$ zdb -lu /dev/sdc1 | grep <span class="string">&quot;txg =&quot;</span> -c</span><br><span class="line">128</span><br><span class="line"></span><br><span class="line">128 x 5s= 640s</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cat</span> /proc/spl/kstat/zfs/tank/txgs</span><br><span class="line">25 0 0x01 83 9296 11320470741 2382694537989353</span><br><span class="line">txg      birth            state ndirty       nread        nwritten     reads    writes   otime        qtime        wtime        stime</span><br><span class="line">13974499 2382281791630216 C     1949696      0            1433600      0        526      4999917952   44691        34328        382573295</span><br><span class="line">13974500 2382286791548168 C     1998848      0            1453056      0        552      5000003695   46223        44384        386415831</span><br><span class="line">13974501 2382291791551863 C     1458176      0            1245696      0        539      4999926718   45704        33119        386636075</span><br><span class="line">13974502 2382296791478581 C     950272       0            887296       0        492      4999981264   44784        34896        336542457</span><br><span class="line">13974503 2382301791459845 C     1146880      0            1244672      0        546      4999967135   67884        46074        346304560</span><br><span class="line">13974504 2382306791426980 C     4177920      0            2828288      0        600      4999946876   46380        45891        423776127</span><br></pre></td></tr></table></figure>

<h4 id="zpool-sync-mode"><a href="#zpool-sync-mode" class="headerlink" title="zpool sync mode"></a>zpool sync mode</h4><p>sync&#x3D;standard : sync writes are written 2 times (first to LOG, second as normal write every ~5 seconds), async write are written only once (every ~5 seconds)<br>sync&#x3D;always : sync writes and async writes are written 2 times (first to LOG, second as normal write every ~5 seconds).<br>sync&#x3D;disabled : sync writes and async writes are written only once (every ~5 seconds)<br>Of course ZFS can flush it write cache between 5 second period but the biggest flush is every ~5 seconds.   </p>
<h3 id="zpool-import-failed"><a href="#zpool-import-failed" class="headerlink" title="zpool import failed"></a>zpool import failed</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line">$ zpool <span class="built_in">set</span> cachefile=none tank</span><br><span class="line">$ zpool import -N -o cachefile=none -d /dev/disk/by-id tank01</span><br><span class="line">$ zfs mount -o ro tank01</span><br><span class="line"></span><br><span class="line">$ zpool import -Nm tank</span><br><span class="line"></span><br><span class="line"><span class="comment"># very dangerours, please backup all data</span></span><br><span class="line">$ zpool import -fFX -R /tmp/tank tank_11</span><br><span class="line"></span><br><span class="line"><span class="comment"># Security script</span></span><br><span class="line"></span><br><span class="line"><span class="comment">### Just check, not take action</span></span><br><span class="line">pool=<span class="string">&quot;tank&quot;</span></span><br><span class="line"><span class="keyword">while</span> <span class="built_in">read</span> line1</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> -------- <span class="variable">$line1</span></span><br><span class="line">  zdb -lu /dev/disk/by-id/<span class="variable">$&#123;line1&#125;</span>-part1 |grep <span class="string">&quot;txg &quot;</span> |<span class="built_in">cut</span> -d <span class="string">&quot; &quot;</span> -f 3 | <span class="built_in">tee</span> /tmp/txg</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> <span class="built_in">read</span> LINE;</span><br><span class="line">  <span class="keyword">do</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">&quot;<span class="variable">$pool</span> <span class="variable">$LINE</span>&quot;</span></span><br><span class="line">    zdb -e <span class="variable">$pool</span> -d -t <span class="variable">$LINE</span> &gt; /dev/null || <span class="built_in">echo</span> ---------error error-----------</span><br><span class="line">  <span class="keyword">done</span> &lt; /tmp/txg</span><br><span class="line">  <span class="built_in">echo</span> <span class="variable">$line1</span><span class="string">&#x27;----------------finish-----&#x27;</span></span><br><span class="line"><span class="keyword">done</span> &lt; zpool_devs</span><br><span class="line"></span><br><span class="line"><span class="comment">#in my test env, the -p not work</span></span><br><span class="line">$ zdb -dep /files -G -o zfs_max_missing_tvds=1 datapool</span><br><span class="line">$ zdb -ep /home/pzakharov/dsks/ -d -t 25 tank</span><br><span class="line"></span><br><span class="line"><span class="comment"># In openzfs 2.0+</span></span><br><span class="line"><span class="comment"># parm:           zfs_max_missing_tvds:Allow importing pool with up to this number of missing top-level vdevs (in read-only mode) (ulong)</span></span><br><span class="line"><span class="comment"># which defines how many missing top level vdevs we can tolerate before marking a pool as unopenable, default is 0</span></span><br><span class="line">$ zpool import -d /files -o <span class="built_in">readonly</span>=on datapool</span><br><span class="line">$ zdb -dep /files -G -o zfs_max_missing_tvds=1 datapool</span><br><span class="line"></span><br><span class="line">$ zdb -dep /files -G -o vdev_validate_skip=1 datapool</span><br><span class="line"></span><br><span class="line"><span class="comment">#after force re-create in some devs</span></span><br><span class="line">$ zdb -dep /files -G -o vdev_validate_skip=1 datapool</span><br><span class="line">Dataset mos [META], ID 0, …</span><br><span class="line"></span><br><span class="line">ZFS_DBGMSG(zdb):</span><br><span class="line">spa_import: importing datapool</span><br><span class="line">spa_load(datapool, config untrusted): LOADING</span><br><span class="line">file vdev <span class="string">&#x27;/files/dsk1&#x27;</span>: best uberblock found <span class="keyword">for</span> spa datapool. txg 68</span><br><span class="line">spa_load(datapool, config untrusted): using uberblock with txg=68</span><br><span class="line">spa_load(datapool, config trusted): LOADED</span><br><span class="line"></span><br><span class="line"><span class="comment">#vdev_validate_skip disables label validation steps during pool import. Changing is not recommended unless you know what you are doing and are recovering a damaged label.</span></span><br><span class="line"><span class="comment">#0=validate labels during pool import, 1=do not validate vdev labels during pool import</span></span><br><span class="line">spa_load_print_vdev_tree [boolean, default = 0]: Always <span class="built_in">print</span> the whole vdev tree <span class="keyword">in</span> the zfs_dbgmsg <span class="built_in">log</span> when loading a pool.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">https://www.delphix.com/blog/openzfs-pool-import-recovery</span><br><span class="line"></span><br><span class="line"><span class="comment">## got the txg error</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#If you want to loss all data, please run -X</span></span><br><span class="line">-F      Attempt to make an unreadable pool readable by trying progressively older transactions.</span><br><span class="line">-X      Attempt <span class="string">&quot;extreme&quot;</span> transaction rewind, that is attempt the same recovery as -F but <span class="built_in">read</span> transactions otherwise deemed too old.</span><br><span class="line"></span><br><span class="line">$ zdb -c -e -G tank</span><br><span class="line"></span><br><span class="line">Traversing all blocks to verify metadata checksums and verify nothing leaked ...</span><br><span class="line"></span><br><span class="line">loading concrete vdev 0, metaslab 3492 of 3493 ...</span><br><span class="line"></span><br><span class="line">        No leaks (block <span class="built_in">sum</span> matches space maps exactly)</span><br><span class="line"></span><br><span class="line">        bp count:                    71</span><br><span class="line">        ganged count:                 0</span><br><span class="line">        bp logical:             1236480      avg:  17415</span><br><span class="line">        bp physical:             137728      avg:   1939     compression:   8.98</span><br><span class="line">        bp allocated:           1253376      avg:  17653     compression:   0.99</span><br><span class="line">        bp deduped:                   0    ref&gt;1:      0   deduplication:   1.00</span><br><span class="line">        Normal class:           1253376     used:  0.00%</span><br><span class="line"></span><br><span class="line">        additional, non-pointer bps of <span class="built_in">type</span> 0:         34</span><br><span class="line">        Dittoed blocks on same vdev: 37</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ZFS_DBGMSG(zdb) START:</span><br><span class="line">spa.c:5490:spa_import(): spa_import: importing tank</span><br><span class="line">spa_misc.c:408:spa_load_note(): spa_load(tank, config trusted): LOADING</span><br><span class="line">vdev.c:125:vdev_dbgmsg(): disk vdev <span class="string">&#x27;/dev/sdb1&#x27;</span>: best uberblock found <span class="keyword">for</span> spa tank. txg 26</span><br><span class="line">spa_misc.c:408:spa_load_note(): spa_load(tank, config untrusted): using uberblock with txg=26</span><br><span class="line">spa_misc.c:408:spa_load_note(): spa_load(tank, config trusted): LOADED</span><br><span class="line">spa.c:7592:spa_async_request(): spa=tank async request task=32</span><br><span class="line">ZFS_DBGMSG(zdb) END</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/2831">Try zpool -T parameter</a><br>You’ll need to make this one line change and rebuild the module. After which you’ll be able to use the -T option. This effectively disables the logic which prevents you from using uberblocks which are older than the label.</p>
<p>import from txg, like import snapshot timestamp<br><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/6497">issue-6497</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ modprobe zfs zfs_rescue=1</span><br><span class="line"></span><br><span class="line">diff --git a/module/zfs/vdev_label.c b/module/zfs/vdev_label.c</span><br><span class="line">index 1c2f00f..509e812 100644</span><br><span class="line">--- a/module/zfs/vdev_label.c</span><br><span class="line">+++ b/module/zfs/vdev_label.c</span><br><span class="line">@@ -471,7 +471,7 @@ retry:</span><br><span class="line">                        <span class="keyword">if</span> ((error || label_txg == 0) &amp;&amp; !config) &#123;</span><br><span class="line">                                config = label;</span><br><span class="line">                                <span class="built_in">break</span>;</span><br><span class="line">-                       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (label_txg &lt;= txg &amp;&amp; label_txg &gt; best_txg) &#123;</span><br><span class="line">+                       &#125; <span class="keyword">else</span> <span class="keyword">if</span> (label_txg &gt; best_txg) &#123;</span><br><span class="line">                                best_txg = label_txg;</span><br><span class="line">                                nvlist_free(config);</span><br><span class="line">                                config = fnvlist_dup(label);</span><br></pre></td></tr></table></figure>
<p>How to I know the compile successful? the dkms could make sure the source code</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">$ zdb -e -p /dev/disk/by-id -F -t old_txg tank &gt; old_txg.log 2&gt;&amp;1 </span><br><span class="line">The source code  change the &quot;F&quot; parameter behavior and got the old txg</span><br><span class="line"></span><br><span class="line">if could not detect the zpool info, zdb core or input/output error, try to get info from single devs</span><br><span class="line">pool=&quot;tank&quot;</span><br><span class="line">while read line1</span><br><span class="line">do</span><br><span class="line">  echo -------- $line1</span><br><span class="line">  zdb -lu /dev/disk/by-id/$&#123;line1&#125;-part1 |grep &quot;txg &quot; |cut -d &quot; &quot; -f 3 | tee /tmp/txg</span><br><span class="line"></span><br><span class="line">  while read LINE;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;$pool $LINE&quot;</span><br><span class="line">    zdb -e $pool -d -t $LINE &gt; /dev/null || echo ---------error error-----------</span><br><span class="line">  done &lt; /tmp/txg</span><br><span class="line">  echo $line1&#x27;----------------finish-----&#x27;</span><br><span class="line">done &lt; zpool_devs</span><br><span class="line"></span><br><span class="line">pool=&quot;ost_9&quot;</span><br><span class="line">for ((line1=0;line1&lt;=19;line1++))</span><br><span class="line">do</span><br><span class="line">  echo -------- $line1</span><br><span class="line">  zdb -lu /dev/loop$&#123;line1&#125;p1 |grep &quot;txg &quot; |cut -d &quot; &quot; -f 3 | tee /tmp/txg</span><br><span class="line"></span><br><span class="line">  while read LINE;</span><br><span class="line">  do</span><br><span class="line">    echo &quot;$pool $LINE&quot;</span><br><span class="line">    zdb -e $pool -d -t $LINE &gt; /dev/null || echo ---------error error-----------</span><br><span class="line">  done &lt; /tmp/txg</span><br><span class="line">  echo $line1&#x27;----------------finish-----&#x27;</span><br><span class="line"></span><br><span class="line">done &lt; ost_9</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># Which txg is OK and some txgs are get the input/output error, In the 8+2 raidz2, it need 8 x devices could got the same txg</span><br><span class="line"></span><br><span class="line">$ echo 0 &gt; /sys/module/zfs/parameters/spa_load_verify_metadata</span><br><span class="line">$ zpool import -o readonly=on -d /dev/disk/by-id -F -T $old_txg tank</span><br></pre></td></tr></table></figure>

<p>spa_load_verify_data<br>At the risk of data integrity, to speed extreme import of large pool<br>If this parameter is set to 0, the traversal skips non-metadata blocks. It can be toggled once the import has started to stop or start the traversal of non-metadata blocks.  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br></pre></td><td class="code"><pre><span class="line">zdb -eul -p /dev/mapper/ tank <span class="comment">#not work for 0.7.13</span></span><br><span class="line">$ zdb -eh tank <span class="comment">#show history</span></span><br><span class="line">$ zdb -ei test_raidz3</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 1.06M, 69 objects</span><br><span class="line">Dataset test_raidz3/ost_test [ZPL], ID 144, cr_txg 60, 2.36M, 10 objects</span><br><span class="line">Dataset test_raidz3 [ZPL], ID 51, cr_txg 1, 302K, 7 objects</span><br><span class="line">Verified large_blocks feature refcount of 0 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line"></span><br><span class="line">$ zdb -ei -t 82 test_raidz3</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 942K, 66 objects</span><br><span class="line">Dataset test_raidz3/ost_test@20220506 [ZPL], ID 256, cr_txg 71, 1.31M, 8 objects</span><br><span class="line">Dataset test_raidz3/ost_test [ZPL], ID 144, cr_txg 60, 1.31M, 8 objects</span><br><span class="line">Dataset test_raidz3 [ZPL], ID 51, cr_txg 1, 302K, 7 objects</span><br><span class="line">Verified large_blocks feature refcount of 0 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 3 is correct</span><br><span class="line"></span><br><span class="line">$ zdb -AAA -L -t 82 -bcdmue -p /dev/disk/by-id test_raidz3</span><br><span class="line">Uberblock:</span><br><span class="line">        magic = 0000000000bab10c</span><br><span class="line">        version = 5000</span><br><span class="line">        txg = 82</span><br><span class="line">        guid_sum = 9835001288681724680</span><br><span class="line">        timestamp = 1651806901 UTC = Fri May  6 11:15:01 2022</span><br><span class="line">        mmp_magic = 00000000a11cea11</span><br><span class="line">        mmp_delay = 0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Metaslabs:</span><br><span class="line">        vdev          0</span><br><span class="line">        metaslabs   160   offset                spacemap          free</span><br><span class="line">        ---------------   -------------------   ---------------   -------------</span><br><span class="line">        metaslab      0   offset            0   spacemap     68   free    1.00T</span><br><span class="line">        metaslab      1   offset  10000000000   spacemap      4   free    1.00T</span><br><span class="line">        metaslab      2   offset  20000000000   spacemap      0   free       1T</span><br><span class="line">        metaslab      3   offset  30000000000   spacemap      0   free       1T</span><br><span class="line">        metaslab      4   offset  40000000000   spacemap      0   free       1T</span><br><span class="line">        metaslab      5   offset  50000000000   spacemap      0   free       1T</span><br><span class="line"></span><br><span class="line"><span class="comment">### show info ,it &#x27;s better</span></span><br><span class="line">$ zdb -AAA -F -e test_raidz3</span><br><span class="line"></span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 4636677940281163615</span><br><span class="line">        name: <span class="string">&#x27;test_raidz3&#x27;</span></span><br><span class="line">        state: 1</span><br><span class="line">        <span class="built_in">hostid</span>: 2307543505</span><br><span class="line">        hostname: <span class="string">&#x27;centos7-oss-zfs-test&#x27;</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">...</span><br><span class="line">Uberblock:</span><br><span class="line">        magic = 0000000000bab10c</span><br><span class="line">        version = 5000</span><br><span class="line">        txg = 3929</span><br><span class="line">        guid_sum = 9835001288681724680</span><br><span class="line">        timestamp = 1651826932 UTC = Fri May  6 16:48:52 2022</span><br><span class="line">        mmp_magic = 00000000a11cea11</span><br><span class="line">        mmp_delay = 0</span><br><span class="line"></span><br><span class="line">All DDTs are empty</span><br><span class="line">Metaslabs:</span><br><span class="line">        vdev          0</span><br><span class="line">        metaslabs   160   offset                spacemap          free</span><br><span class="line">        ---------------   -------------------   ---------------   -------------</span><br><span class="line">        metaslab      0   offset            0   spacemap     68   free    1.00T</span><br><span class="line">        On-disk histogram:              fragmentation 0</span><br><span class="line">                         14:     10 **********</span><br><span class="line">                         15:      5 *****</span><br><span class="line">                         16:      4 ****</span><br><span class="line">                         17:      3 ***</span><br><span class="line">                         18:      0</span><br><span class="line">                         19:      0</span><br><span class="line">                         20:      0</span><br><span class="line">                         21:      0</span><br><span class="line">                         22:      0</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### it &#x27;s dangerous, don &#x27;t use FX</span></span><br><span class="line">$ zpool import -N -o <span class="built_in">readonly</span>=on -f -R /mnt/usbstick -d /dev/disk/by-id -FX rpool</span><br><span class="line">cannot import <span class="string">&#x27;rpool&#x27;</span>: one or more devices is currently unavailable</span><br><span class="line"></span><br><span class="line"><span class="comment"># check the zpool</span></span><br><span class="line"><span class="comment">## export the zpool or you have antoher server connect this zpool</span></span><br><span class="line">$ zdb -ec <span class="variable">$poolname</span></span><br><span class="line">Traversing all blocks to verify metadata checksums and verify nothing leaked ...</span><br><span class="line"></span><br><span class="line">loading space map <span class="keyword">for</span> vdev 0 of 1, metaslab 1 of 139 ...</span><br><span class="line"></span><br><span class="line">$ zdb -ec -AAA <span class="variable">$poolname</span></span><br><span class="line"><span class="comment"># if there is &quot;assertion failure&quot;...</span></span><br><span class="line"><span class="comment"># -AAA could not abort</span></span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 1548875728334022114</span><br><span class="line">        name: <span class="string">&#x27;mdt_0&#x27;</span></span><br><span class="line">        state: 0</span><br><span class="line">        <span class="built_in">hostid</span>: 4281985155</span><br><span class="line">        hostname: <span class="string">&#x27;cngb-mds-m20-1&#x27;</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">&#x27;root&#x27;</span></span><br><span class="line">            <span class="built_in">id</span>: 0</span><br><span class="line">            guid: 1548875728334022114</span><br><span class="line">            children[0]:</span><br><span class="line">                <span class="built_in">type</span>: <span class="string">&#x27;raidz&#x27;</span></span><br><span class="line">                <span class="built_in">id</span>: 0</span><br><span class="line">                guid: 1680148767042285697</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ zdb -e -AAA <span class="variable">$poolname</span> &gt; <span class="variable">$poolnam</span></span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 1548875728334022114</span><br><span class="line">        name: <span class="string">&#x27;mdt_0&#x27;</span></span><br><span class="line">        state: 0</span><br><span class="line">        <span class="built_in">hostid</span>: 4281985155</span><br><span class="line">        hostname: <span class="string">&#x27;cngb-mds-m20-1&#x27;</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">&#x27;root&#x27;</span></span><br><span class="line">            <span class="built_in">id</span>: 0</span><br><span class="line">            guid: 1548875728334022114</span><br><span class="line">            children[0]:</span><br><span class="line">                <span class="built_in">type</span>: <span class="string">&#x27;raidz&#x27;</span></span><br><span class="line">                <span class="built_in">id</span>: 0</span><br><span class="line">                guid: 1680148767042285697</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#metaslab check</span></span><br><span class="line">$ zdb -em -AAA <span class="variable">$poolname</span> <span class="comment"># to check metalab</span></span><br><span class="line">Metaslabs:</span><br><span class="line">        vdev          0</span><br><span class="line">        metaslabs   139   offset                spacemap          free</span><br><span class="line">        ---------------   -------------------   ---------------   -------------</span><br><span class="line">        metaslab      0   offset            0   spacemap     39   free    27.6G</span><br><span class="line">        metaslab      1   offset    800000000   spacemap     55   free    31.1G</span><br><span class="line">        metaslab      2   offset   1000000000   spacemap     58   free    30.3G</span><br><span class="line">        metaslab      3   offset   1800000000   spacemap     42   free    29.4G</span><br><span class="line">        metaslab      4   offset   2000000000   spacemap     45   free    28.3G</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>zpool -T could not reslove all issues, some critical case will cause zfs panic.    </p>
<p><a target="_blank" rel="noopener" href="https://gist.github.com/mkhon/34d979c78077a20648456272d7f2cc15">zfs_revert.py</a>    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$zfs_revert.py -bs=512 -tb=$(blockdev --getsize64 /dev/sda) /dev/sda</span><br></pre></td></tr></table></figure>
<p>for reverting ZFS transactions by destroying uberblocks    </p>
<h3 id="zfs-command"><a href="#zfs-command" class="headerlink" title="zfs command"></a>zfs command</h3><h4 id="special-class-vdev"><a href="#special-class-vdev" class="headerlink" title="special class vdev"></a>special class vdev</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zpool add tank -o ashift=12 special mirror /dev/sdc /dev/sdd</span><br><span class="line">$ zfs set special_small_blocks=128K tank</span><br><span class="line">$ zfs set special_small_blocks=128K tank/sub1</span><br></pre></td></tr></table></figure>

<h3 id="single-device-no-raid-too-a-mirror"><a href="#single-device-no-raid-too-a-mirror" class="headerlink" title="single device(no raid) too a mirror"></a>single device(no raid) too a mirror</h3><p>scsi-35001173d028c8c08 not in the mirror, it ‘s dangerous</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">$ zpool status</span><br><span class="line">  pool: mdt_0</span><br><span class="line"> state: ONLINE</span><br><span class="line">  scan: scrub repaired 0B <span class="keyword">in</span> 1h35m with 0 errors on Tue Oct  1 01:35:20 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">        NAME                        STATE     READ WRITE CKSUM</span><br><span class="line">        mdt_0                       ONLINE       0     0     0</span><br><span class="line">          mirror-0                  ONLINE       0     0     0</span><br><span class="line">            sdb                     ONLINE       0     0     0</span><br><span class="line">            sdc                     ONLINE       0     0     0</span><br><span class="line">          scsi-35001173d028c8c08    ONLINE       0     0     0 &lt;----------- </span><br><span class="line">          mirror-2                  ONLINE       0     0     0</span><br><span class="line">            scsi-35001173d028c8740  ONLINE       0     0     0</span><br><span class="line">            scsi-35001173d028cb334  ONLINE       0     0     0</span><br><span class="line"></span><br><span class="line">errors: No known data errors</span><br><span class="line">$ zpool attach -f mdt_0 scsi-35001173d028c8c08 /dev/sdh</span><br><span class="line">$ zpool status -x</span><br><span class="line">  pool: mdt_0</span><br><span class="line"> state: ONLINE</span><br><span class="line">status: One or more devices is currently being resilvered.  The pool will</span><br><span class="line">        <span class="built_in">continue</span> to <span class="keyword">function</span>, possibly <span class="keyword">in</span> a degraded state.</span><br><span class="line">action: Wait <span class="keyword">for</span> the resilver to complete.</span><br><span class="line">  scan: resilver <span class="keyword">in</span> progress since Sun Jan 19 12:46:57 2020</span><br><span class="line">        176M scanned out of 465G at 29.3M/s, 4h30m to go</span><br><span class="line">        58.8M resilvered, 0.04% <span class="keyword">done</span></span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">        NAME                        STATE     READ WRITE CKSUM</span><br><span class="line">        mdt_0                       ONLINE       0     0     0</span><br><span class="line">          mirror-0                  ONLINE       0     0     0</span><br><span class="line">            sdb                     ONLINE       0     0     0</span><br><span class="line">            sdc                     ONLINE       0     0     0</span><br><span class="line">          mirror-1                  ONLINE       0     0     0</span><br><span class="line">            scsi-35001173d028c8c08  ONLINE       0     0     0</span><br><span class="line">            sdh                     ONLINE       0     0     0  (resilvering)</span><br><span class="line">          mirror-2                  ONLINE       0     0     0</span><br><span class="line">            scsi-35001173d028c8740  ONLINE       0     0     0</span><br><span class="line">            scsi-35001173d028cb334  ONLINE       0     0     0</span><br><span class="line"></span><br><span class="line">errors: No known data errors</span><br></pre></td></tr></table></figure>

<h4 id="Add-special-device"><a href="#Add-special-device" class="headerlink" title="Add special device"></a>Add special device</h4><p>special<br>A device dedicated solely for allocating various kinds of internal metadata, and optionally small file blocks. The redundancy of this device should match the redundancy of the other normal devices in the pool. If more than one special device is specified, then allocations are load-balanced between those devices.<br>For more information on special allocations, see the Sx Special Allocation Class section.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zpool add <span class="variable">$POOLNAME</span> special mirror <span class="variable">$TMPDIR</span>/zpool&#123;4,5&#125;.dat</span><br><span class="line"></span><br><span class="line">$ zpool add -f test_ost0 special mirror /dev/sdcc4 /dev/sdcd4</span><br></pre></td></tr></table></figure>
<p>In my benchmark ,it could be increate the throughput and remove speed(5~10x), create&#x2F;rename&#x2F;move&#x2F;chown no more because cache, looks like good for every meta request(not hit cache)<br>the 2x 240 GB SSD(mirror) could store 251043119 files(52 TiB) in the ashfit&#x3D;12 config for this zpool</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mirror    222G   222G     0        -         -     1%  100.0%</span><br><span class="line">#after 100%, the cow filesystem could be deleted.</span><br></pre></td></tr></table></figure>

<h4 id="zpool-replace"><a href="#zpool-replace" class="headerlink" title="zpool replace"></a>zpool replace</h4><p>Plug out the wrong device, and re-plugin the device, you can ‘t replace it , just online it</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ zpool replace -f tank_82 6957378459605677274 sden</span><br><span class="line">invalid vdev specification</span><br><span class="line">the following errors must be manually repaired:</span><br><span class="line">/dev/sden1 is part of active pool <span class="string">&#x27;tank_82&#x27;</span></span><br><span class="line"></span><br><span class="line">$ zpool online tank_82 6957378459605677274</span><br></pre></td></tr></table></figure>

<h4 id="Online-upgrade-SAS-device-firmware"><a href="#Online-upgrade-SAS-device-firmware" class="headerlink" title="Online upgrade SAS device firmware"></a>Online upgrade SAS device firmware</h4><p>unzip Dell XXX.EXE firmware, you could got TT53.fwh</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">$ zpool offline tank scsi-35000c500c3f5c9f2</span><br><span class="line">$ lsscsi -gis | grep 35000c500c3f5c9f2</span><br><span class="line">[6:0:76:0]   disk    SEAGATE  ST10000NM0256    TT53  /dev/sdbu  35000c500c3f5c9f2  /dev/sg76  9.79TB</span><br><span class="line"></span><br><span class="line">$ shmlic list drives -enc = xxxxx -verbose</span><br><span class="line">5000c50003c9cd9d /dev/sg76                 SEAGATE                            ST10000NM0256            XXXXXXX     TT55      8.91TB          50050cc1628e1043 (EN-8435A-E6EBD)  TH0YF87JSGT0087401M5A00          26   00 / 26   6G   2018    NO   5000c500dd9da3b8, 0000000000000000</span><br><span class="line"></span><br><span class="line">$ shmcli update drive -d = 5000c50003c9cd9d -file = ./TT54.fwh</span><br><span class="line"></span><br><span class="line">update drive - Executing <span class="built_in">command</span>..</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        (SEAGATE / ST10000NM0256 / ZA27NCHH) - File firmware version = TT54</span><br><span class="line">        (SEAGATE / ST10000NM0256 / ZA27NCHH) - Current drive firmware version = TT53</span><br><span class="line">        (SEAGATE / ST10000NM0256 / ZA27NCHH) - Sending firmware file to device.</span><br><span class="line">        (SEAGATE / ST10000NM0256 / ZA27NCHH) - Firmware successfully sent to drive.</span><br><span class="line">        (SEAGATE / ST10000NM0256 / ZA27NCHH) - After update drive firmware version = TT54</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">Drive Update Statistics:</span><br><span class="line">        Total Drives Attempted                                   : 1</span><br><span class="line">        Successful Drive update count                            : 1</span><br><span class="line">        FW File transfer failures                                : 0</span><br><span class="line">        FW Update failures despite successful FW file transfer   : 0</span><br><span class="line">        Unsupported Drive count (Non-Dell Drives)                : 0</span><br><span class="line">        Drives Incompatible with Firmware File                   : 0</span><br><span class="line">        Drives skipped because drive is same version             : 0</span><br><span class="line">        Drives skipped because drive is newer version            : 0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l | grep -v ata</span><br><span class="line">total 0</span><br><span class="line">lrwxrwxrwx 1 root root 0 Apr 25 14:22 host6 -&gt; ../../devices/pci0000:b2/0000:b2:00.0/0000:b3:00.0/host6/scsi_host/host6</span><br><span class="line">$ <span class="built_in">echo</span> 1 &gt; /sys/block/sdbu/device/delete</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">&quot;0 76 0&quot;</span> &gt;  /sys/class/scsi_host/host6/scan</span><br><span class="line">$ lsscsi -gis | grep 35000c500c3f5c9f2</span><br><span class="line">[6:0:76:0]   disk    SEAGATE  ST10000NM0256    TT54  /dev/sdbu  35000c500c3f5c9f2  /dev/sg76  9.79TB</span><br><span class="line">$ zpool online tank  scsi-35000c500c3f5c9f2</span><br></pre></td></tr></table></figure>

<h4 id="Custom-packages"><a href="#Custom-packages" class="headerlink" title="Custom packages"></a><a target="_blank" rel="noopener" href="https://openzfs.github.io/openzfs-docs/Developer%20Rebackup/Custom%20Packages.html">Custom packages</a></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ sudo yum install epel-release gcc make autoconf automake libtool rpm-build dkms libtirpc-devel libblkid-devel libuuid-devel libudev-devel openssl-devel zlib-devel libaio-devel libattr-devel elfutils-libelf-devel kernel-devel-$(<span class="built_in">uname</span> -r) python python2-devel python-setuptools python-cffi libffi-devel</span><br><span class="line"></span><br><span class="line">$ sudo dnf install gcc make autoconf automake libtool rpm-build kernel-rpm-macros dkms libtirpc-devel libblkid-devel libuuid-devel libudev-devel openssl-devel zlib-devel libaio-devel libattr-devel elfutils-libelf-devel kernel-devel-$(<span class="built_in">uname</span> -r) python3 python3-devel python3-setuptools python3-cffi libffi-devel</span><br><span class="line"></span><br><span class="line"><span class="comment">### DKMS</span></span><br><span class="line">$ <span class="built_in">cd</span> zfs</span><br><span class="line">$ ./configure</span><br><span class="line">$ make -j1 rpm-utils rpm-dkms</span><br><span class="line">$ sudo yum localinstall *.$(<span class="built_in">uname</span> -p).rpm *.noarch.rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">### kmod</span></span><br><span class="line">$ <span class="built_in">cd</span> zfs</span><br><span class="line">$ ./configure</span><br><span class="line">$ make -j1 rpm-utils rpm-kmod</span><br><span class="line">$ sudo yum localinstall *.$(<span class="built_in">uname</span> -p).rpm</span><br><span class="line"></span><br><span class="line"><span class="comment">### kABI-tracking kmod</span></span><br><span class="line">$ <span class="built_in">cd</span> zfs</span><br><span class="line">$ ./configure --with-spec=redhat</span><br><span class="line">$ make -j1 rpm-utils rpm-kmod</span><br><span class="line">$ sudo yum localinstall *.$(<span class="built_in">uname</span> -p).rpm</span><br></pre></td></tr></table></figure>
<h3 id="Replication-zpool"><a href="#Replication-zpool" class="headerlink" title="Replication zpool"></a>Replication zpool</h3><p><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/8458">send recv slow issue</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">-L   Generate  a  stream <span class="built_in">which</span> may contain blocks larger than 128KB.  This flag has no effect <span class="keyword">if</span> the large_blocks pool feature is disabled, or <span class="keyword">if</span> the recordsize property of this filesystem has never been <span class="built_in">set</span> above 128KB.  The receiving system must have the large_blocks pool feature enabled as well.  See zpool-features(5) <span class="keyword">for</span> details on ZFS feature flags and the large_blocks feature.</span><br><span class="line"></span><br><span class="line">A $ umount /lfs/test_mgt /lfs/test_mdt</span><br><span class="line">A $ zfs list -t snapshot</span><br><span class="line"></span><br><span class="line">no datasets available</span><br><span class="line">A $ zfs snapshot test0_mdt/test_mgt@20200529-1</span><br><span class="line">A $ zfs snapshot test0_mdt/test_mdt@20200529-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#increase transfer speed</span></span><br><span class="line">A $ zfs send -Rp test0_mdt/test_mdt@20200529-1 | mbuffer -s 128k -m 1G -o mdt_0-20200529-1</span><br><span class="line"></span><br><span class="line"><span class="comment">#send</span></span><br><span class="line">zfs send tank/nfs/data@today | mbuffer -s 128k -m 1G -O 192.168.1.10:50001</span><br><span class="line"><span class="comment">#receive</span></span><br><span class="line">mbuffer -s 128k -m 1G -I 192.168.1.100:50001 | zfs receive tank/backup/data</span><br><span class="line"></span><br><span class="line">A $ zfs send -Rp test0_mdt/test_mdt@20200529-1 &gt; <span class="string">&quot;/nfshare/test0_mdt_test_mdt@20200529-1&quot;</span></span><br><span class="line">A $ zfs send -Rp test0_mdt/test_mgt@20200529-1 &gt; <span class="string">&quot;/nfshare/test0_mdt_test_mgt@20200529-1&quot;</span></span><br><span class="line"></span><br><span class="line">B $ zfs create test0_backup/test_mgt</span><br><span class="line">B $ zfs create test0_backup/test_mdt</span><br><span class="line">B $ zfs receive -F test0_backup/test_mgt &lt; test0_mdt_test_mgt@20200529-1</span><br><span class="line">B $ <span class="built_in">dd</span> <span class="keyword">if</span>=test0_mdt_test_mdt\@20200529-1 bs=1M | zfs recv -F test0_backup/test_mdt</span><br><span class="line">577+1 records <span class="keyword">in</span></span><br><span class="line">577+1 records out</span><br><span class="line">605602396 bytes (606 MB) copied, 2340.37 s, 259 kB/s</span><br><span class="line"></span><br><span class="line">B $ mount.lfs test0_backup/test_mgt /lfs/test_mgt</span><br><span class="line">mount.lfs: test0_backup/test_mgt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mgt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mgt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">B $ mount.lfs test0_backup/test_mdt /lfs/test_mdt</span><br><span class="line">mount.lfs: test0_backup/test_mdt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mdt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line">mount.lfs: test0_backup/test_mdt is configured <span class="keyword">for</span> failover but zpool does not have multihost enabled</span><br><span class="line"></span><br><span class="line">test0_backup/test_mgt      lfs    287G  2.8M  287G   1% /lfs/test_mgt</span><br><span class="line">test0_backup/test_mdt      lfs    287G  196M  287G   1% /lfs/test_mdt</span><br><span class="line"></span><br><span class="line"><span class="comment"># the server B create a snapshot, if not you could create it</span></span><br><span class="line">B $ zfs list -t snapshot</span><br><span class="line">NAME                               USED  AVAIL  REFER  MOUNTPOINT</span><br><span class="line">test0_backup/test_mdt@20200529-1   488K      -   174M  -</span><br><span class="line"></span><br><span class="line">-n -v -P</span><br><span class="line">       -P, --parsable</span><br><span class="line">           Print machine-parsable verbose information about the stream package generated.</span><br><span class="line">       -n, --dryrun</span><br><span class="line">           Do a dry-run (<span class="string">&quot;No-op&quot;</span>) send.</span><br><span class="line">       -v, --verbose</span><br><span class="line">           Print verbose information about the stream package generated</span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/xai/scripts/blob/master/dobackup">backup script</a><br>write new data from the client</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">B $ zfs snapshot test0_backup/test_mdt@20200529-2</span><br><span class="line">B $ zfs  list -t snapshot</span><br><span class="line">NAME                               USED  AVAIL  REFER  MOUNTPOINT</span><br><span class="line">test0_backup/test_mdt@20200529-1   488K      -   174M  -</span><br><span class="line">test0_backup/test_mdt@20200529-2     0B      -   305M  -</span><br><span class="line"></span><br><span class="line">B $ zfs send -p -i test0_backup/test_mdt@20200529-1 test0_backup/test_mdt@20200529-2 &gt; /nfshare/test0_mdt_test_mdt\@20200529-from-1-to-2</span><br><span class="line"></span><br><span class="line">B $ zfs send -RI home@20180826 home@20180827 | zfs recv -Fu backup/homezfs</span><br><span class="line">#what the different -I and -i</span><br><span class="line">-I snapshot</span><br><span class="line">Generate a stream package that sends all intermediary snapshots from the first snapshot to the second snapshot.</span><br><span class="line">For example, -I @a fs@d is similar to -i @a fs@b; -i @b fs@c; -i @c fs@d.  The incremental source may be specified as with the -i option.</span><br><span class="line"></span><br><span class="line">-R, --replicate</span><br><span class="line">Generate a replication stream package, which will replicate the specified file system, and all descendent file systems, up to the named snapshot.  When received, all properties, snapshots, descendent file systems, and clones are preserved.</span><br><span class="line"></span><br><span class="line">If the -i or -I flags are used in conjunction with the -R flag, an incremental replication stream is generated.</span><br><span class="line">The current values of properties, and current snapshot and file system names are set when the stream is received.  If the -F flag is specified when this stream is received, snapshots and file systems that do not exist on the sending side are destroyed. If the -R flag is used to send encrypted datasets, then -w must also be specified.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># import to server A</span><br><span class="line">A $ zfs receive -dF test0_mdt/test_mdt &lt; test0_mdt_test_mdt\@20200529-from-1-to-2</span><br><span class="line">A $ zfs list -t snapshot</span><br><span class="line">NAME                            USED  AVAIL  REFER  MOUNTPOINT</span><br><span class="line">test0_mdt/test_mdt@20200529-1   498K      -   174M  -</span><br><span class="line">test0_mdt/test_mdt@20200529-2     0B      -   305M  -</span><br><span class="line">test0_mdt/test_mgt@20200529-1    62K      -  2.72M  -</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">### hold, release,destroy snapshot</span><br><span class="line">$ zfs holds -r tank/home@now</span><br><span class="line">$ zfs release -r keep tank/home@now</span><br><span class="line">$ zfs destroy -r tank/home@now</span><br><span class="line">$ zfs rename tank/home/cindys@083006 tank/home/cindys@today</span><br></pre></td></tr></table></figure>
<p>In the client, you could see the modified files   </p>
<h3 id="checkpoint"><a href="#checkpoint" class="headerlink" title="checkpoint"></a><a target="_blank" rel="noopener" href="https://oshogbo.vexillium.org/blog/46/">checkpoint</a></h3><p>The problem with snapshot is that it works only on a single dataset. If we added some dataset, we wouldn’t then be able to create the snapshot which would rollback that operation. The same with changing the attributes of a dataset. If we change the compression on the dataset, we cannot rollback it. We would need to change that manually.</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ zpool create testpool c1t1d0</span><br><span class="line">$ zfs create testpool/testfs0</span><br><span class="line">$ zfs create testpool/testfs1</span><br><span class="line"></span><br><span class="line">$ zpool checkpoint testpool</span><br><span class="line"></span><br><span class="line">$ zfs destroy testpool/testfs0</span><br><span class="line">$ zfs rename testpool/testfs1 testpool/testfs2</span><br><span class="line">$ zfs list -r testpool</span><br><span class="line"></span><br><span class="line">$ zpool <span class="built_in">export</span> testpool</span><br><span class="line">$ zpool import -o <span class="built_in">readonly</span>=on --rewind-to-checkpoint testpool</span><br><span class="line">$ zfs list -r testpool</span><br><span class="line"></span><br><span class="line">$ zpool <span class="built_in">export</span> testpool</span><br><span class="line">$ zpool import  testpool</span><br><span class="line">$ zfs list -r testpool</span><br><span class="line"></span><br><span class="line">$ zpool <span class="built_in">export</span> testpool</span><br><span class="line">$ zpool import --rewind-to-checkpoint testpool</span><br><span class="line">$ zfs list -r testpool</span><br><span class="line"></span><br><span class="line">$ zpool checkpoint --discard testpool</span><br><span class="line">$ zpool list testpool</span><br></pre></td></tr></table></figure>

<h3 id="zfs-test"><a href="#zfs-test" class="headerlink" title="zfs test"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/blob/master/tests/README.md">zfs test</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/usr/share/zfs/zfs-tests.sh -v -d /test_tank</span><br><span class="line">/usr/share/zfs/zfs-tests/tests/functional</span><br></pre></td></tr></table></figure>

<h3 id="Work-with-gdb"><a href="#Work-with-gdb" class="headerlink" title="Work with gdb"></a>Work with gdb</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ gdb zdb -c xxxx.core</span><br></pre></td></tr></table></figure>

<h4 id="snapshot"><a href="#snapshot" class="headerlink" title="snapshot"></a>snapshot</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ zfs snapshot tank@ver1</span><br><span class="line">$ zfs list -t snapshot</span><br><span class="line">$ zfs snapshot tank@ver2</span><br><span class="line">$ zfs rollback tank@ver1</span><br><span class="line">cannot rollback to <span class="string">&#x27;tank@ver1&#x27;</span>: more recent snapshots or bookmarks exist</span><br><span class="line">use <span class="string">&#x27;-r&#x27;</span> to force deletion of the following snapshots and bookmarks:</span><br><span class="line">tank@ver2</span><br><span class="line"></span><br><span class="line">$ zfs rollback -r tank@ver1 <span class="comment">## rollback to ver1, the ver2 will be delete</span></span><br><span class="line">$ zfs destroy tank/@ver1</span><br></pre></td></tr></table></figure>

<h4 id="zfs-vol"><a href="#zfs-vol" class="headerlink" title="zfs vol"></a>zfs vol</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">zfs create -V 40T -o volblocksize=128k tank/zvol</span><br><span class="line"><span class="built_in">ls</span> -l /dev/zvol/tank/zvol</span><br><span class="line">``</span><br><span class="line"></span><br><span class="line"><span class="comment">### mount zfs</span></span><br><span class="line">```bash</span><br><span class="line">pool=tank</span><br><span class="line">zfs <span class="built_in">set</span> canmount=on <span class="variable">$pool</span></span><br><span class="line">zfs <span class="built_in">set</span> mountpoint=/mnt <span class="variable">$pool</span></span><br><span class="line">zfs mount <span class="variable">$pool</span></span><br><span class="line"><span class="built_in">rm</span> -f /mnt/xxx/xxx/xxx</span><br><span class="line">zfs umount <span class="variable">$pool</span></span><br><span class="line">zfs <span class="built_in">set</span> canmount=off</span><br></pre></td></tr></table></figure>

<h4 id="zfs-event"><a href="#zfs-event" class="headerlink" title="zfs event"></a>zfs event</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zpool events -c <span class="comment"># clean events</span></span><br><span class="line">$ zpool import tank</span><br><span class="line">$ zpool events -v</span><br><span class="line">$ zpool events -v &gt; zpool_import_tank</span><br></pre></td></tr></table></figure>

<h4 id="Compression"><a href="#Compression" class="headerlink" title="Compression"></a>Compression</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> compression=lz4 tank</span><br><span class="line">zfs <span class="built_in">set</span> compression=zstd tank</span><br></pre></td></tr></table></figure>
<h4 id="Xattr"><a href="#Xattr" class="headerlink" title="Xattr"></a>Xattr</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> xattr=sa tank</span><br></pre></td></tr></table></figure>

<h4 id="ACL"><a href="#ACL" class="headerlink" title="ACL"></a>ACL</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> acltype=posixacl tank/gentoo/home</span><br><span class="line">zfs get acltype tank/gentoo/home</span><br></pre></td></tr></table></figure>

<h4 id="Rmount"><a href="#Rmount" class="headerlink" title="Rmount"></a>Rmount</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> mountpoint=/var/lib/mysql tank/mysql</span><br><span class="line">zfs umount tank/mysql</span><br><span class="line">zfs mount tank/mysql</span><br></pre></td></tr></table></figure>
<h4 id="Zpool-rename"><a href="#Zpool-rename" class="headerlink" title="Zpool rename"></a>Zpool rename</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zpool <span class="built_in">export</span> app</span><br><span class="line">zpool import app apps</span><br></pre></td></tr></table></figure>

<h4 id="Detach-hdd-from-mirror"><a href="#Detach-hdd-from-mirror" class="headerlink" title="Detach hdd from mirror"></a>Detach hdd from mirror</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">zpool detach tank sdz <span class="comment">#detach zda from mirror-17</span></span><br><span class="line">only sdal <span class="keyword">in</span> mirror-17, and mirror-17 is missing</span><br><span class="line"></span><br><span class="line">          mirror-16  ONLINE       0     0     0</span><br><span class="line">            sdai     ONLINE       0     0     0</span><br><span class="line">            sdaj     ONLINE       0     0     0</span><br><span class="line">          sdal       ONLINE       0     0     0</span><br><span class="line">          mirror-18  ONLINE       0     0     0</span><br><span class="line">            sdam     ONLINE       0     0     0</span><br><span class="line">            sdan     ONLINE       0     0     0</span><br></pre></td></tr></table></figure>

<h4 id="Attach-one-hdd-to-mirror"><a href="#Attach-one-hdd-to-mirror" class="headerlink" title="Attach one hdd to mirror"></a>Attach one hdd to mirror</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">zpool attach tank sdal sdz</span><br><span class="line">          mirror-17  ONLINE       0     0     0</span><br><span class="line">            sdal     ONLINE       0     0     0</span><br><span class="line">            sdz      ONLINE       0     0     0  (resilvering)</span><br></pre></td></tr></table></figure>

<h4 id="sub-zpool"><a href="#sub-zpool" class="headerlink" title="sub zpool"></a>sub zpool</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zfs create tank/test</span><br></pre></td></tr></table></figure>

<h4 id="Quota"><a href="#Quota" class="headerlink" title="Quota"></a>Quota</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> quota=1024G tank/test</span><br><span class="line"><span class="comment"># disable</span></span><br><span class="line">zfs <span class="built_in">set</span> quota=none tank</span><br></pre></td></tr></table></figure>

<h4 id="user-quota"><a href="#user-quota" class="headerlink" title="user quota"></a>user quota</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">zfs <span class="built_in">set</span> userquota@nagios=2G tank</span><br><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=/tank01/test/nagios bs=1M</span><br><span class="line"><span class="built_in">dd</span>: writing /tank/test/nagios:Disk quota exceeded</span><br><span class="line"></span><br><span class="line"><span class="comment"># zfs get userquota@nagios</span></span><br><span class="line">NAME            PROPERTY          VALUE             SOURCE</span><br><span class="line">tank01          userquota@nagios  2G                <span class="built_in">local</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># group</span></span><br><span class="line">zfs <span class="built_in">set</span> groupquota@staff=10G tank/staff/admins</span><br></pre></td></tr></table></figure>
<h4 id="zpool-Add"><a href="#zpool-Add" class="headerlink" title="zpool Add"></a>zpool Add</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># add mirror</span></span><br><span class="line">$ zpool add tank mirror newdisk1 newdisk2</span><br><span class="line"></span><br><span class="line"><span class="comment"># add</span></span><br><span class="line">$ zpool add tank spare c1t4d0 c1t5d0</span><br><span class="line">$ zpool remove tank c1t5d0</span><br></pre></td></tr></table></figure>

<h4 id="Clear-zfs-label"><a href="#Clear-zfs-label" class="headerlink" title="Clear zfs label"></a><a target="_blank" rel="noopener" href="https://icesquare.com/wordpress/freebsdhow-to-remove-zfs-meta-data/">Clear zfs label</a></h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ zpool labelclear /dev/sdxxx</span><br><span class="line">or</span><br><span class="line">$ dd if=/dev/zero of=/dev/sdXX bs=512 count=10</span><br><span class="line">$ dd if=/dev/zero of=/dev/sdXX bs=512 seek=$(( $(blockdev --getsz /dev/sdXX) - 4096 ))</span><br></pre></td></tr></table></figure>

<h4 id="iostat"><a href="#iostat" class="headerlink" title="iostat"></a>iostat</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=/dev/shm/zil_cache bs=1M count=4096</span><br><span class="line">$ losetup -v -f /dev/shm/zil_cache</span><br><span class="line">$ losetup -a</span><br><span class="line">/dev/loop0: [0018]:55156303 (/dev/shm/zil_cache)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Add zil by mirror</span></span><br><span class="line">$ zpool add tank <span class="built_in">log</span> mirror /dev/loop0p1 /dev/loop0p2, because it <span class="string">&#x27;s test file from /dev/shm</span></span><br><span class="line"><span class="string">$ zpool add tank log /dev/loop0p1</span></span><br><span class="line"><span class="string">$ zpool iostat -v 2</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pool                                            alloc   free   read  write   read  write</span></span><br><span class="line"><span class="string">----------------------------------------------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank                                            24.4G  1.30T      0  8.74K      0   575M</span></span><br><span class="line"><span class="string">  scsi-3600605b005811bf01db36f957f619f26-part1  24.4G  1.30T      0  5.50K      0   448M</span></span><br><span class="line"><span class="string">logs                                                -      -      -      -      -      -</span></span><br><span class="line"><span class="string">  loop0p1                                        102M  3.87G      0  3.23K      0   127M</span></span><br><span class="line"><span class="string">----------------------------------------------  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">$ zpool iostat -lvyq 2</span></span><br><span class="line"><span class="string">  capacity     operations     bandwidth    total_wait     disk_wait    syncq_wait    asyncq_wait  scrub   trim   syncq_read    syncq_write   asyncq_read  asyncq_write   scrubq_read   trimq_write</span></span><br><span class="line"><span class="string">pool        alloc   free   read  write   read  write   read  write   read  write   read  write   read  write   wait   wait   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank        1.34M  54.6T      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">  raidz2    1.34M  54.6T      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdb         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdc         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdd         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sde         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdf         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdg         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdh         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdi         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdj         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdk         -      -      0      0      0      0      -      -      -      -      -      -      -      -      -      -      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -r 1</span></span><br><span class="line"><span class="string">tank_7         sync_read    sync_write    async_read    async_write      scrub</span></span><br><span class="line"><span class="string">req_size      ind    agg    ind    agg    ind    agg    ind    agg    ind    agg</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">512             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4K              0      0      8      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8K              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">32K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">64K             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">128K           63      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">256K            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">512K            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8M              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16M             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -L -v -q 2</span></span><br><span class="line"><span class="string">              capacity     operations     bandwidth    syncq_read    syncq_write   asyncq_read  asyncq_write   scrubq_read   trimq_write</span></span><br><span class="line"><span class="string">pool        alloc   free   read  write   read  write   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">tank         161G  90.8T      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">  raidz2     161G  90.8T      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sda         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdb         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdc         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdd         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sde         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdf         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdg         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdi         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdk         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">    sdl         -      -      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">## show the latency</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -vly 1 1</span></span><br><span class="line"><span class="string">                                     capacity     operations     bandwidth    total_wait     disk_wait    syncq_wait    asyncq_wait  scrub   trim</span></span><br><span class="line"><span class="string"> pool                              alloc   free   read  write   read  write   read  write   read  write   read  write   read  write   wait   wait</span></span><br><span class="line"><span class="string"> --------------------------------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"> banshee                            774G  1.01T      0     15      0   431K      -    1ms      -    1ms      -    1us      -      -      -      -</span></span><br><span class="line"><span class="string">   mirror                           774G  1.01T      0     15      0   431K      -    1ms      -    1ms      -    1us      -      -      -      -</span></span><br><span class="line"><span class="string">     wwn-0x5002538e4982fef1-part5      -      -      0      7      0   216K      -    1ms      -    1ms      -    2us      -      -      -      -</span></span><br><span class="line"><span class="string">     wwn-0x5002538e4982ff33-part5      -      -      0      7      0   216K      -    1ms      -    1ms      -    1us      -      -      -      -</span></span><br><span class="line"><span class="string"> --------------------------------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"> data                              2.82T   828G    904      0   145M      0   64ms      -   39ms      -   23ms      -      -      -      -      -</span></span><br><span class="line"><span class="string">   mirror                          1.40T   418G    374      0  72.0M      0  107ms      -   50ms      -   54ms      -      -      -      -      -</span></span><br><span class="line"><span class="string">     wwn-0x50014ee20d8e5e7f            -      -    153      0  35.4M      0  173ms      -   59ms      -  106ms      -      -      -      -      -</span></span><br><span class="line"><span class="string">     wwn-0x50014ee25d2510d4            -      -    220      0  36.6M      0   61ms      -   43ms      -   18ms      -      -      -      -      -</span></span><br><span class="line"><span class="string">   mirror                          1.41T   410G    529      0  72.7M      0   33ms      -   31ms      -    2ms      -      -      -      -      -</span></span><br><span class="line"><span class="string">     wwn-0x50014ee206fd9549            -      -    265      0  36.6M      0   32ms      -   30ms      -    2ms      -      -      -      -      -</span></span><br><span class="line"><span class="string">     wwn-0x5000c500c5dc8eca            -      -    264      0  36.1M      0   35ms      -   33ms      -    2ms      -      -      -      -      -</span></span><br><span class="line"><span class="string"> --------------------------------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  ----</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ zpool iostat -w 2</span></span><br><span class="line"><span class="string">tank         total_wait     disk_wait    syncq_wait    asyncq_wait</span></span><br><span class="line"><span class="string">latency      read  write   read  write   read  write   read  write  scrub   trim</span></span><br><span class="line"><span class="string">----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">1ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">3ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">7ns             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">15ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">31ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">63ns            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">127ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">255ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">511ns           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8us             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">32us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">65us            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">131us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">262us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">524us           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8ms             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">16ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">33ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">67ms            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">134ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">268ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">536ms           0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">1s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">2s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">4s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">8s              0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">17s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">34s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">68s             0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">137s            0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">--------------------------------------------------------------------------------</span></span><br><span class="line"><span class="string">$ cat /proc/spl/kstat/zfs/tank_86/dmu_tx_assign</span></span><br><span class="line"><span class="string">75 1 0x01 41 1968 259828173343194 262983153792140</span></span><br><span class="line"><span class="string">name                            type data</span></span><br><span class="line"><span class="string">1 ns                            4    0</span></span><br><span class="line"><span class="string">2 ns                            4    0</span></span><br><span class="line"><span class="string">4 ns                            4    0</span></span><br><span class="line"><span class="string">8 ns                            4    0</span></span><br><span class="line"><span class="string">16 ns                           4    0</span></span><br><span class="line"><span class="string">32 ns                           4    0</span></span><br><span class="line"><span class="string">64 ns                           4    0</span></span><br><span class="line"><span class="string">128 ns                          4    40</span></span><br><span class="line"><span class="string">256 ns                          4    40</span></span><br><span class="line"><span class="string">512 ns                          4    6</span></span><br><span class="line"><span class="string">1024 ns                         4    1</span></span><br><span class="line"><span class="string">2048 ns                         4    0</span></span><br><span class="line"><span class="string">4096 ns                         4    0</span></span><br><span class="line"><span class="string">8192 ns                         4    0</span></span><br><span class="line"><span class="string">16384 ns                        4    0</span></span><br><span class="line"><span class="string">32768 ns                        4    0</span></span><br><span class="line"><span class="string">65536 ns                        4    5</span></span><br><span class="line"><span class="string">131072 ns                       4    105</span></span><br><span class="line"><span class="string">262144 ns                       4    331</span></span><br><span class="line"><span class="string">524288 ns                       4    419</span></span><br><span class="line"><span class="string">1048576 ns                      4    325</span></span><br><span class="line"><span class="string">2097152 ns                      4    338</span></span><br><span class="line"><span class="string">4194304 ns                      4    188</span></span><br><span class="line"><span class="string">8388608 ns                      4    65</span></span><br><span class="line"><span class="string">16777216 ns                     4    59</span></span><br><span class="line"><span class="string">33554432 ns                     4    47</span></span><br><span class="line"><span class="string">67108864 ns                     4    34</span></span><br><span class="line"><span class="string">134217728 ns                    4    60</span></span><br><span class="line"><span class="string">268435456 ns                    4    55</span></span><br><span class="line"><span class="string">536870912 ns                    4    65</span></span><br><span class="line"><span class="string">1073741824 ns                   4    94</span></span><br><span class="line"><span class="string">2147483648 ns                   4    56</span></span><br><span class="line"><span class="string">4294967296 ns                   4    271</span></span><br><span class="line"><span class="string">8589934592 ns                   4    3454</span></span><br><span class="line"><span class="string">17179869184 ns                  4    4687</span></span><br><span class="line"><span class="string">34359738368 ns                  4    1970</span></span><br><span class="line"><span class="string">68719476736 ns                  4    642</span></span><br><span class="line"><span class="string">137438953472 ns                 4    380</span></span><br><span class="line"><span class="string">274877906944 ns                 4    440</span></span><br><span class="line"><span class="string">549755813888 ns                 4    299</span></span><br><span class="line"><span class="string">1099511627776 ns                4    36</span></span><br><span class="line"><span class="string">$ zpool iostat -v -y -l 2</span></span><br><span class="line"><span class="string">tank                           146T  13.8T  1.46K    345  49.0M   319K   24ms    2ms   24ms    1ms  741ns  627ns  610ns    1ms      -</span></span><br><span class="line"><span class="string">  raidz3                       146T  13.8T  1.46K    345  49.0M   319K   24ms    2ms   24ms    1ms  741ns  627ns  610ns    1ms      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a66cec97        -      -     88     18  2.76M  15.6K    8ms  840us    8ms  442us  664ns  474ns  758ns  407us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670caef        -      -     93     18  2.93M  15.1K   68ms   11ms   68ms   10ms  919ns    1us  758ns  281us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65a02fb        -      -     73     18  2.40M  14.8K   16ms  514us   16ms  335us  714ns  758ns  568ns  239us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670bbc7        -      -     79     17  2.74M  13.8K   10ms  556us   10ms  334us  679ns  474ns  568ns  244us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a6754b03        -      -     70     20  2.36M  15.6K   26ms  679us   26ms  360us  766ns  474ns  758ns  329us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675ae43        -      -     88     19  2.89M  15.1K   60ms    5ms   60ms    5ms  802ns  985ns  758ns  366us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c787        -      -     82     20  2.61M  16.8K   15ms  833us   15ms  468us  713ns  379ns  379ns  394us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a6754d7b        -      -     97     25  3.05M  43.9K    8ms   14ms    8ms    6ms  710ns  379ns  379ns    9ms      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a67540d7        -      -     74     19  2.50M  18.3K   23ms  796us   23ms  466us  727ns  474ns  379ns  371us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675455b        -      -     83     19  2.79M  17.3K   32ms  687us   32ms  393us  760ns  474ns  758ns  315us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670ca8f        -      -     71     19  2.38M  17.0K    9ms  771us    9ms  398us  644ns  474ns      -  361us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65b55bb        -      -     89     18  2.84M  16.0K    8ms  909us    8ms  567us  779ns  682ns      -  329us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65a7f57        -      -     82     20  2.60M  17.8K   32ms  527us   32ms  318us  744ns  695ns      -  289us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a65b2683        -      -     92     17  2.99M  15.3K   57ms  687us   57ms  443us  863ns    1us  758ns  242us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a675449f        -      -     74     16  2.55M  16.0K   11ms  625us   11ms  425us  633ns  474ns  379ns  257us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c30f        -      -     82     16  2.82M  15.6K    9ms  808us    9ms  539us  680ns  379ns  758ns  363us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670af17        -      -     77     19  2.65M  17.5K    8ms  876us    8ms  458us  695ns  474ns  758ns  380us      -</span></span><br><span class="line"><span class="string">    scsi-35000c500a670c2ab        -      -     95     18  3.10M  17.8K   18ms  786us   18ms  498us  770ns  758ns  379ns  333us      -</span></span><br><span class="line"><span class="string">----------------------------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br></pre></td></tr></table></figure>


<h4 id="Custom-script-show-temperature"><a href="#Custom-script-show-temperature" class="headerlink" title="Custom script ,show temperature"></a>Custom script ,show temperature</h4><p>&#x2F;etc&#x2F;zfs&#x2F;zpool.d<br>$ ZPOOL_SCRIPTS_AS_ROOT&#x3D;1 zpool iostat -c smart ## not support SCSI&#x2F;SAS<br>              capacity     operations     bandwidth<br>pool        alloc   free   read  write   read  write  temp  health  ata_err  realloc  rep_ucor  cmd_to  pend_sec  off_ucor</p>
<hr>
<p>testost_0    929G   116T  1.00K  1.59K  7.00M   151M<br>  mirror     107G  14.4T     23     30   179K  17.1M<br>    sda         -      -     11     15  89.1K  8.56M     -       -        -        -         -       -         -         -<br>    sdb         -      -     11     15  89.9K  8.56M     -       -        -        -         -       -         -         -<br>  mirror     107G  14.4T     23     29   179K  17.1M<br>    sdc         -      -     11     14  89.7K  8.56M     -       -        -        -         -       -         -         -<br>    sdd         -      -     11     14  89.3K  8.56M     -       -        -        -         -       -         -         -</p>
<p>$  ZPOOL_SCRIPTS_AS_ROOT&#x3D;1 zpool status -c temp<br>  pool: tank<br> state: ONLINE<br>  scan: scrub repaired 0B in 0 days 06:46:11 with 0 errors on Tue Oct 15 06:46:14 2019<br>config:</p>
<pre><code>    NAME        STATE     READ WRITE CKSUM  temp
    tank        ONLINE       0     0     0
      raidz2-0  ONLINE       0     0     0
        sdc     ONLINE       0     0     0    27
        sdd     ONLINE       0     0     0    28
        sde     ONLINE       0     0     0    27
        sdf     ONLINE       0     0     0    29
        sdg     ONLINE       0     0     0    27
        sdh     ONLINE       0     0     0    28
    logs
      mirror-1  ONLINE       0     0     0
        sda3    ONLINE       0     0     0    24
        sdb3    ONLINE       0     0     0    25
    cache
      sda4      ONLINE       0     0     0    24
      sdb4      ONLINE       0     0     0    25
</code></pre>
<p>errors: No known data errors</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">#### zdb</span><br><span class="line">```bash</span><br><span class="line">zdb -MM -P test_0</span><br><span class="line">        vdev          0         metaslabs  116          fragmentation 88%</span><br><span class="line">                          9: 31097798 ***********************</span><br><span class="line">                         10: 33383938 ************************</span><br><span class="line">                         11: 56520516 ****************************************</span><br><span class="line">                         12: 53907990 ***************************************</span><br><span class="line">                         13: 11130025 ********</span><br><span class="line">                         14: 4519568 ****</span><br><span class="line">                         15: 2751108 **</span><br><span class="line">                         16: 179324 *</span><br><span class="line">                         17:   2534 *</span><br><span class="line">                         18:    138 *</span><br><span class="line">                         19:     16 *</span><br><span class="line">                         20:      3 *</span><br><span class="line">                         21:      2 *</span><br><span class="line">        pool test_0     fragmentation    88%</span><br><span class="line">                          9: 31097798 ***********************</span><br><span class="line">                         10: 33383938 ************************</span><br><span class="line">                         11: 56520516 ****************************************</span><br><span class="line">                         12: 53907990 ***************************************</span><br><span class="line">                         13: 11130025 ********</span><br><span class="line">                         14: 4519568 ****</span><br><span class="line">                         15: 2751108 **</span><br><span class="line">                         16: 179324 *</span><br><span class="line">                         17:   2534 *</span><br><span class="line">                         18:    138 *</span><br><span class="line">                         19:     16 *</span><br><span class="line">                         20:      3 *</span><br><span class="line">                         21:      2 *</span><br><span class="line"></span><br><span class="line"># you could set debug level by zfs_flags</span><br><span class="line"># https://github.com/openzfs/zfs/issues/8192</span><br><span class="line">$ zdb -bb test_0</span><br><span class="line">$ zdb -b test_0</span><br><span class="line">$  sed -r &#x27;s/^[0-9]+  //&#x27; /proc/spl/kstat/zfs/dbgmsg | sort | uniq -c | sort -n</span><br><span class="line"></span><br><span class="line">Since you&#x27;re running a version with the &#x27;pool checkpoints&#x27; feature one thing you could try is to creating a checkpoint. Then you might be able to get zdb -bb -T &lt;checkpoint txg&gt; to run cleanly against that checkpoint since all the blocks will be stable. You can then remove the pool checkpoint.</span><br><span class="line"></span><br><span class="line">Traversing all blocks to verify nothing leaked ...</span><br><span class="line"></span><br><span class="line">loading space map for vdev 0 of 1, metaslab 115 of 116 ...</span><br><span class="line">39.0G completed (1569MB/s) estimated time remaining: 0hr 30min 43sec</span><br><span class="line">2.80T completed ( 553MB/s) estimated time remaining: 0hr 00min 00sec</span><br><span class="line">        No leaks (block sum matches space maps exactly)</span><br><span class="line"></span><br><span class="line">        bp count:      1154215521</span><br><span class="line">        ganged count:           0</span><br><span class="line">        bp logical:    3028205925888      avg:   2623</span><br><span class="line">        bp physical:   2695917303296      avg:   2335     compression:   1.12</span><br><span class="line">        bp allocated:  3076338462208      avg:   2665     compression:   0.98</span><br><span class="line">        bp deduped:             0    ref&gt;1:      0   deduplication:   1.00</span><br><span class="line">        SPA allocated: 3076338462208     used: 77.18%</span><br><span class="line"></span><br><span class="line">        additional, non-pointer bps of type 0:         40</span><br><span class="line">        Dittoed blocks on same vdev: 590073873</span><br></pre></td></tr></table></figure>
<p>ZDB Get file info</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br></pre></td><td class="code"><pre><span class="line">zdb &lt;dataset&gt; &lt;inode-number&gt;</span><br><span class="line">zdb tank/fs 14</span><br><span class="line"></span><br><span class="line"><span class="built_in">dd</span> <span class="keyword">if</span>=/dev/zero of=/tank/test/1 bs=1M count=1</span><br><span class="line">zdb -vv -bbbb -O tank/test test_1</span><br><span class="line">zdb -vv -bbbb -O tank  16K/16K.98274a32a967f971f8af.11597</span><br><span class="line">zdb -ddddd tank/test</span><br><span class="line">......</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 0:86bef7d8400:600 4000L/200P F=1 B=11629333/11629333</span><br><span class="line">            4000 L0 0:867fb11c400:1800 4000L/e00P F=1 B=11629333/11629333</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000008000) size   32K</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">       256    2   128K   128K  1.00M     512     1M  100.00  ZFS plain file</span><br><span class="line">                                               168   bonus  System attributes</span><br><span class="line">        dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED</span><br><span class="line">        dnode maxblkid: 7</span><br><span class="line">        path    /1</span><br><span class="line">        uid     0</span><br><span class="line">        gid     0</span><br><span class="line">        atime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        mtime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        ctime   Tue Dec 25 22:25:42 2018</span><br><span class="line">        crtime  Tue Dec 25 22:25:42 2018</span><br><span class="line">        gen     11630673</span><br><span class="line">        mode    100644</span><br><span class="line">        size    1048576</span><br><span class="line">        parent  34</span><br><span class="line">        links   1</span><br><span class="line">        pflags  40800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L1  0:86d5203a600:c00 20000L/400P F=8 B=11630673/11630673</span><br><span class="line">               0  L0 0:86e648c9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           20000  L0 0:86e648f9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           40000  L0 0:86e64929400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           60000  L0 0:86e64959400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           80000  L0 0:86e64989400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           a0000  L0 0:86e649b9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           c0000  L0 0:86e649e9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">           e0000  L0 0:86e64a19400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000100000) size    1M</span><br><span class="line"></span><br><span class="line">    Dnode slots:</span><br><span class="line">        Total used:             8</span><br><span class="line">        Max used:             256</span><br><span class="line">        Percent empty:  96.875000</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">ls</span> -l /tank/backup/test100/test.c</span><br><span class="line">-rw-r--r-- 1 root root 739 Mar  4 08:50 /tank/backup/test100/test.c</span><br><span class="line">$ zdb -vv -O tank/backup test100/test.c</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">   5526595    1   128K     1K     9K     512     1K  100.00  ZFS plain file</span><br><span class="line">                                               176   bonus  System attributes</span><br><span class="line">        dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED</span><br><span class="line">        dnode maxblkid: 0</span><br><span class="line">        uid     0</span><br><span class="line">        gid     0</span><br><span class="line">        atime   Wed Mar  4 08:50:52 2020</span><br><span class="line">        mtime   Wed Mar  4 08:50:50 2020</span><br><span class="line">        ctime   Wed Mar  4 08:50:50 2020</span><br><span class="line">        crtime  Wed Mar  4 08:50:50 2020</span><br><span class="line">        gen     1668705</span><br><span class="line">        mode    100644</span><br><span class="line">        size    739</span><br><span class="line">        parent  43</span><br><span class="line">        links   1</span><br><span class="line">        pflags  840800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 0:328e967a2000:3000 400L/400P F=1 B=1668705/1668705</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000000400) size    1K</span><br><span class="line"></span><br><span class="line">zdb -vv -bbbb -O tank/backup test100/test.c</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">   5526595    1   128K     1K     9K     512     1K  100.00  ZFS plain file</span><br><span class="line">                                               176   bonus  System attributes</span><br><span class="line">        dnode flags: USED_BYTES USERUSED_ACCOUNTED USEROBJUSED_ACCOUNTED</span><br><span class="line">        dnode maxblkid: 0</span><br><span class="line">        uid     0</span><br><span class="line">        gid     0</span><br><span class="line">        atime   Wed Mar  4 08:50:52 2020</span><br><span class="line">        mtime   Wed Mar  4 08:50:50 2020</span><br><span class="line">        ctime   Wed Mar  4 08:50:50 2020</span><br><span class="line">        crtime  Wed Mar  4 08:50:50 2020</span><br><span class="line">        gen     1668705</span><br><span class="line">        mode    100644</span><br><span class="line">        size    739</span><br><span class="line">        parent  43</span><br><span class="line">        links   1</span><br><span class="line">        pflags  840800000004</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L0 DVA[0]=&lt;0:328e967a2000:3000&gt; [L0 ZFS plain file] fletcher4 uncompressed unencrypted LE contiguous unique single size=400L/400P birth=1668705L/1668705P fill=1 <span class="built_in">cksum</span>=36e170f019:231ac0e9e639:c8f24370cf5ff:340704ee1157fd0</span><br><span class="line"></span><br><span class="line">                segment [0000000000000000, 0000000000000400) size    1K</span><br><span class="line"></span><br><span class="line"><span class="comment">## Got the data block</span></span><br><span class="line">$ zdb -R tank/backup 0:328e967a2000:3000</span><br><span class="line">Found vdev <span class="built_in">type</span>: raidz</span><br><span class="line"></span><br><span class="line">0:328e967a2000:3000</span><br><span class="line">          0 1 2 3 4 5 6 7   8 9 a b c d e f  0123456789abcdef</span><br><span class="line">000000:  23696e636c756465  203c737464696f2e  <span class="comment">#include &lt;stdio.</span></span><br><span class="line">000010:  683e0a23696e636c  756465203c737973  h&gt;.<span class="comment">#include &lt;sys</span></span><br><span class="line">000020:  2f73686d2e683e0a  23696e636c756465  /shm.h&gt;.<span class="comment">#include</span></span><br><span class="line">000030:  203c7379732f7374  61742e683e0a2369   &lt;sys/stat.h&gt;.<span class="comment">#i</span></span><br><span class="line">000040:  6e636c756465203c  7379732f6d6d616e  nclude &lt;sys/mman</span><br><span class="line">000050:  2e683e0a23696e63  6c756465203c6663  .h&gt;.<span class="comment">#include &lt;fc</span></span><br><span class="line">000060:  6e746c2e683e0a23  696e636c75646520  ntl.h&gt;.<span class="comment">#include</span></span><br><span class="line">000070:  3c7374646c69622e  683e0a23696e636c  &lt;stdlib.h&gt;.<span class="comment">#incl</span></span><br><span class="line">000080:  756465203c737472  696e672e683e0a23  ude &lt;string.h&gt;.<span class="comment">#</span></span><br><span class="line">000090:  696e636c75646520  3c756e697374642e  include &lt;unistd.</span><br><span class="line">0000a0:  683e0a0a696e7420  6d61696e28696e74  h&gt;..int main(int</span><br><span class="line">0000b0:  20617267632c2063  686172202a2a6172   argc, char **ar</span><br><span class="line">......</span><br><span class="line">002fd0:  0000000000000000  0000000000000000  ................</span><br><span class="line">002fe0:  0000000000000000  0000000000000000  ................</span><br><span class="line">002ff0:  0000000000000000  0000000000000000  ................</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">#recovery case</span></span><br><span class="line">zdb -eddddd tank/vol1 0</span><br><span class="line">zdb -eddddd tank/vol1 1</span><br><span class="line">zdb -eddddd tank/vol1 2</span><br><span class="line"><span class="built_in">cat</span> /proc/spl/kstat/zfs/dbgmsg</span><br><span class="line"></span><br><span class="line">zdb -dddddd tank/vol1 0</span><br><span class="line">Dataset tank/vol1 [ZPL], ID 263, cr_txg 70483, 157T, 18141424 objects, rootbp DVA[0]=&lt;0:3746b079f000:800&gt; DVA[1]=&lt;0:5052c05bb800:800&gt; [L0 DMU objset] fletcher4 lz4 unencrypted LE contiguous unique double size=800L/200P birth=17131459L/17131459P fill=18141424 <span class="built_in">cksum</span>=160cdc9d10:77cd8df278a:16197d88503fb:2e684de5796487</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  <span class="built_in">type</span></span><br><span class="line">         0    6   128K    16K  7.11G     512  13.3G   65.13  DMU dnode (K=inherit) (Z=inherit=lz4)</span><br><span class="line">        dnode flags: USED_BYTES</span><br><span class="line">        dnode maxblkid: 870399</span><br><span class="line">Indirect blocks:</span><br><span class="line">               0 L5      0:36dd2bdfb800:1000 0:500b619d3000:1000 20000L/400P F=18141424 B=17131459/17131459 <span class="built_in">cksum</span>=8682520015:5a4d8a7385fe:201930d2e8eaae:7fec6aa85516757</span><br><span class="line">               0  L4     0:36dd2bdfa800:1000 0:500b10b44000:1000 20000L/400P F=18141424 B=17131459/17131459 <span class="built_in">cksum</span>=858fb8735d:596caa3857cd:1fb07bb366bf30:7de1f51be6aceda</span><br><span class="line">               0   L3    0:36dd2bdf9800:1000 0:500add871000:1000 20000L/400P F=18141424 B=17131459/17131459 <span class="built_in">cksum</span>=85840a29ec:594cb64bd0ba:1f97557aa25dfa:7d2efb61eda54bd</span><br><span class="line">               0    L2   0:3746b078e800:10800 0:51fc3ba5c800:10800 20000L/d800P F=18141424 B=17131459/17131459 <span class="built_in">cksum</span>=13045a7b88b2:20616ea49a72e0e:9a5b0f9d48611342:6d7f667cde90cd03</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="built_in">cat</span> /sys/module/zfs/parameters/zfs_dbgmsg_enable</span><br><span class="line">zdb -edddddd tank/vol1 0</span><br><span class="line"></span><br><span class="line">zdb -eR vol1 0:3746b078e800:10800:r &gt;  vol1_obj0_file1</span><br><span class="line">zdb -eR vol1 0:51fc3ba5c800:10800:r &gt; vol1_obj0_file2</span><br><span class="line">cmp ./vol1_obj0_file11 ./vol1_obj0_file12</span><br><span class="line">cmp ./vol1_obj0_file1 ./vol1_obj0_file2</span><br><span class="line"></span><br><span class="line">vim vol1_obj0_file1</span><br><span class="line">vim -b vol1_obj0_file1</span><br><span class="line">vim -b vol1_obj0_file2</span><br><span class="line">zdb -eR vol1 0:36dd2bdf9800:1000:r &gt; vol1_obj0_l3_1</span><br><span class="line">zdb -eR vol1 0:500add871000:1000:r &gt; vol1_obj0_l3_2</span><br><span class="line">cmp vol1_obj0_l3_1 vol1_obj0_l3_2</span><br><span class="line">vim -b ./vol1_obj0_l3_1</span><br><span class="line">vim -b ./vol1_obj0_l3_2</span><br><span class="line">vim -b ./vol1_obj0_l3_1</span><br><span class="line">zdb -eR vol1 0:36dd2bdfa800:1000:r &gt; vol1_obj0_l4_1</span><br><span class="line">zdb -eR vol1 0:500b10b44000:1000:r &gt; vol1_obj0_l4_2</span><br><span class="line">cmp vol1_obj0_l4_1 vol1_obj0_l4_2</span><br><span class="line">zdb -edddd tank 34</span><br><span class="line">zdb -edddddd tank/vol1 34</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> -e <span class="string">&quot;/root/core-%e-%s-%u-%g-%p-%t&quot;</span> &gt; /proc/sys/kernel/core_pattern</span><br><span class="line"><span class="built_in">echo</span> 1 &gt;  /sys/module/zfs/parameters/zfs_recover</span><br><span class="line"><span class="built_in">cat</span> /sys/module/zfs/parameters/zil_replay_disable</span><br><span class="line"></span><br><span class="line">0:86e648c9400:30000 20000L/20000P F=1 B=11630673/11630673</span><br><span class="line">vdev:offset:size</span><br><span class="line"></span><br><span class="line">vdev=0</span><br><span class="line">offset=86e648c9400 (offset <span class="keyword">in</span> logic disk ?)</span><br><span class="line"></span><br><span class="line">20000L/20000P means size,  compressed size(2000L),the no compression file size(2000P)</span><br><span class="line">0x2000=131072</span><br><span class="line"></span><br><span class="line">B=11630673/11630673 means write operate transaction group</span><br></pre></td></tr></table></figure>

<h3 id="pin-CPU"><a href="#pin-CPU" class="headerlink" title="pin CPU"></a>pin CPU</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># number of z_wr_iss processes tunable</span></span><br><span class="line">zio_taskq_batch_pct</span><br><span class="line">(The bad setting will casue the performance issue, Many thanks <span class="keyword">for</span> Javen <span class="string">&#x27;s help)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">core=1</span></span><br><span class="line"><span class="string">[[ $(grep MHz /proc/cpuinfo -c) -gt 8 ]] &amp;&amp; maxcpu=8 &amp;&amp; for i in $(pgrep &#x27;</span>z_wr_iss$<span class="string">&#x27;)</span></span><br><span class="line"><span class="string">do</span></span><br><span class="line"><span class="string">   [[ $core -gt $maxcpu ]] &amp;&amp; core=0</span></span><br><span class="line"><span class="string">   taskset -p -c $core $i</span></span><br><span class="line"><span class="string">   ((core++))</span></span><br><span class="line"><span class="string">done</span></span><br></pre></td></tr></table></figure>

<h3 id="Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx"><a href="#Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx" class="headerlink" title="Disable AVX512 for scalable Xeon silver and gold 5xxx"></a>Disable AVX512 for scalable Xeon silver and gold 5xxx</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Gold 5115</span></span><br><span class="line">$ <span class="built_in">cat</span> /proc/spl/kstat/zfs/fletcher_4_bench</span><br><span class="line">0 0 0x01 -1 0 43875755056 2388098231282525</span><br><span class="line">implementation   native         byteswap</span><br><span class="line">scalar           5097645469     4106654089</span><br><span class="line">superscalar      6876819423     5086086480</span><br><span class="line">superscalar4     5926587517     4946863265</span><br><span class="line">sse2             11659567916    6572225291</span><br><span class="line">ssse3            11660661610    10379355285</span><br><span class="line">avx2             17865482202    16120833783</span><br><span class="line">avx512f          24802818495    8841728748</span><br><span class="line">fastest          avx512f        avx2</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> ssse3 &gt; /sys/module/zcommon/parameters/zfs_fletcher_4_impl</span><br><span class="line"><span class="built_in">echo</span> ssse3 &gt; /sys/module/zfs/parameters/zfs_vdev_raidz_impl</span><br></pre></td></tr></table></figure>
<p>Because I ‘m wrong about when avx2 or avx512 called, The cpu will reduce the frequency and it will impact the others (no avx system call) at the same time<br><code>In some fio mix IO case, I can &#39;t beleive the avx2 lower than 4% than the ssse3, the CPU was 1x Xeon E2000 </code></p>
<p><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/wiki/ZFS-on-Linux-Module-Parameters#zfs_vdev_raidz_impl">zfs_vdev_raidz_impl</a><br><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/wiki/ZFS-on-Linux-Module-Parameters#zfs_fletcher_4_impl">zfs_fletcher_4_impl</a>  </p>
<h3 id="zfs-dracut-cause-brain-split"><a href="#zfs-dracut-cause-brain-split" class="headerlink" title="zfs-dracut cause brain split"></a>zfs-dracut cause brain split</h3><p>In zfs old version 0.6.x, there is no multihost parameter.<br>When you upgrade to 0.7.x, it ‘s not enable by default !!!  but you can see the mmp history is worked !!!<br>So if you zfs-dracut , maybe you will be brain-split, lsinitrd show hostid &#x3D;&#x3D; 0 !!!  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">##not make sure</span></span><br><span class="line">lsinitrd</span><br><span class="line"></span><br><span class="line">You could unistall the package or remove it , zfs dracut is too dangerous <span class="keyword">for</span> HA <span class="built_in">arch</span></span><br><span class="line">rpm -ql zfs-dracut-0.7.9-1.el7.x86_64</span><br><span class="line">/usr/lib/dracut/modules.d/02zfsexpandknowledge</span><br><span class="line">/usr/lib/dracut/modules.d/02zfsexpandknowledge/module-setup.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/export-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/module-setup.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/mount-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/parse-zfs.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-generator.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-lib.sh</span><br><span class="line">/usr/lib/dracut/modules.d/90zfs/zfs-needshutdown.sh</span><br><span class="line">/usr/share/doc/zfs-dracut-0.7.9</span><br><span class="line">/usr/share/doc/zfs-dracut-0.7.9/README.dracut.markdown</span><br></pre></td></tr></table></figure>

<h3 id="Remove-slog"><a href="#Remove-slog" class="headerlink" title="Remove slog"></a>Remove slog</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## slog device has been destroy, skip/bypass import slog</span></span><br><span class="line">$ zpool import -m -a -f</span><br><span class="line"></span><br><span class="line">$ zpool status -x</span><br><span class="line">  pool: tank</span><br><span class="line"> state: DEGRADED</span><br><span class="line">status: One or more devices could not be used because the label is missing or</span><br><span class="line">        invalid.  Sufficient replicas exist <span class="keyword">for</span> the pool to <span class="built_in">continue</span></span><br><span class="line">        functioning <span class="keyword">in</span> a degraded state.</span><br><span class="line">action: Replace the device using <span class="string">&#x27;zpool replace&#x27;</span>.</span><br><span class="line">   see: http://zfsonlinux.org/msg/ZFS-8000-4J</span><br><span class="line">  scan: scrub repaired 0B <span class="keyword">in</span> 0 days 06:46:11 with 0 errors on Mon Oct 14 18:46:14 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">        NAME                      STATE     READ WRITE CKSUM</span><br><span class="line">        tank                      DEGRADED     0     0     0</span><br><span class="line">          raidz2-0                ONLINE       0     0     0</span><br><span class="line">            sdc                   ONLINE       0     0     0</span><br><span class="line">            sdd                   ONLINE       0     0     0</span><br><span class="line">            sde                   ONLINE       0     0     0</span><br><span class="line">            sdf                   ONLINE       0     0     0</span><br><span class="line">            sdg                   ONLINE       0     0     0</span><br><span class="line">            sdh                   ONLINE       0     0     0</span><br><span class="line">        logs</span><br><span class="line">          mirror-1                UNAVAIL      0     0     0  insufficient replicas</span><br><span class="line">            13378777246457412929  UNAVAIL      0     0     0  was /dev/sda3</span><br><span class="line">            1725913437883392852   UNAVAIL      0     0     0  was /dev/sdb</span><br><span class="line"></span><br><span class="line">$ zpool remove tank mirror-1</span><br><span class="line">that ok</span><br></pre></td></tr></table></figure>

<h3 id="About-SMR-HDD"><a href="#About-SMR-HDD" class="headerlink" title="About SMR HDD"></a><a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/4877">About SMR HDD</a></h3><p>Another thing you might try is to leave the block size set at 1M but increase the zfs_vdev_aggregation_limit to 16M. This way as long as your doing 1M aligned IO you should never write partial blocks and leave holes. ZFS will aggregate these 1M blocks in to larger 16M IOs to the disk.</p>
<p><a target="_blank" rel="noopener" href="https://devopstales.github.io/linux/speed_up_zfs/">speed-up zfs</a><br><a target="_blank" rel="noopener" href="http://broken.net/uncategorized/zfs-performance-tuning-for-scrubs-and-resilvers/">zfs-performance-tuning-for-scrubs-and-resilvers</a><br><a target="_blank" rel="noopener" href="https://forum.proxmox.com/threads/zfs-zil-log-txg_sync-causing-high-io-every-5-seconds-any-solution.24376/">zfs-zil-log-txg_sync-causing-high-io-every-5-seconds</a><br><a target="_blank" rel="noopener" href="https://www.svennd.be/tuning-of-zfs-module/">tuning zfs module</a><br><a target="_blank" rel="noopener" href="http://lfs.ornl.gov/ecosystem-2016/documents/tutorials/Stearman-LLNL-ZFS.pdf">Stearman-LLNL-ZFS</a><br><a target="_blank" rel="noopener" href="http://fibrevillage.com/storage/171-zfs-on-linux-performance-tuning">171-zfs-on-linux-performance</a><br><a target="_blank" rel="noopener" href="http://list.zfsonlinux.org/pipermail/zfs-discuss/2018-March/030666.html">Is the number of z_wr_iss processes tunable?</a><br><a target="_blank" rel="noopener" href="https://www.beegfs.io/wiki/StorageServerTuning#hn_59ca4f8bbb_16">Tips and Recommendations for Storage Server Tuning</a></p>
<h3 id="About-silent-error"><a href="#About-silent-error" class="headerlink" title="About silent error"></a><a target="_blank" rel="noopener" href="http://www.lieberbiber.de/2018/07/17/improving-data-safety-on-all-systems-using-zfs-and-btrfs/">About silent error</a></h3><p>Enterprise drives can store checksums in a separate space by using slightly larger on-disk blocks of 520 or 4104 bytes in size. The additional eight bytes can be accessed by the operating system using a feature called T10 Protection Information (T10-PI). On consumer drives the file system has to store the checksums with the rest of the data, slightly reducing the available space.</p>
<p>Even if you have just one drive, you now get to know immediately when the drive is acting up before something bad happens. The operating system signals read errors to the applications instead of passing corrupt data. The backup software doesn’t overwrite the good backup. You don’t end up with corrupted data on the second system when transferring data using that flaky USB drive.</p>
<p>Sun Microsystems invented ZFS as a modern file system with lots of features, among them checksumming for all data. It has its own storage layer, its own RAID level implementations and can perform all checks while the file system is mounted. It was open-sourced as part of OpenSolaris, but cannot be assimilated into the Linux kernel due to licensing constraints. Ubuntu is the only Linux distribution which ships a ready-made ZFSonLinux kernel module, with all other distribution things are more complicated. I recommend using ZFS if you can, but even I only use it in my Ubuntu-based NAS. It is also rather complicated to set up and was not designed for removable devices, making it less attractive for many use cases.</p>
<h4 id="Import-zpool-hang-single-dev-just-respond-very-slow-cause-all-zpool-command-hang"><a href="#Import-zpool-hang-single-dev-just-respond-very-slow-cause-all-zpool-command-hang" class="headerlink" title="Import zpool hang, single dev just respond very slow cause all zpool command hang"></a>Import zpool hang, single dev just respond very slow cause all zpool command hang</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/class/block/sddh/device/delete</span><br><span class="line"><span class="comment">### remove the dev, all has been imported</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## issue log</span></span><br><span class="line">$ lsscsi -git  | grep sg118</span><br><span class="line">[17:0:116:0] disk    sas:0x5000c500a65b61f1          /dev/sddh  35000c500a65b61f3  /dev/sg118</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: Attached scsi generic sg178 <span class="built_in">type</span> 0</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Enabling DIF Type 2 protection</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] 19134414848 512-byte logical blocks: (9.79 TB/8.91 TiB)</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] 4096-byte physical blocks</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Write Protect is off</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Mode Sense: d7 00 10 08</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Write cache: enabled, <span class="built_in">read</span> cache: enabled, supports DPO and FUA</span><br><span class="line">[Sun Sep 23 11:03:43 2018]  sdfm: sdfm1 sdfm9</span><br><span class="line">[Sun Sep 23 11:03:43 2018] sd 17:0:176:0: [sdfm] Attached SCSI disk</span><br><span class="line">[Sun Sep 23 11:03:44 2018] scsi 17:0:177:0: Attached scsi generic sg179 <span class="built_in">type</span> 3</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: attempting task abort! scmd(ffff88100abf2f40)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: [sg118] CDB: Inquiry 12 01 dc 01 52 00</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: handle(0x0087), sas_address(0x5000c500a65b61f1), phy(34)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: enclosure_logical_id(0x50050cc11ac01572), slot(56)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: task abort: SUCCESS scmd(ffff88100abf2f40)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: attempting task abort! scmd(ffff880fe9784ec0)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: [sg118] CDB: Inquiry 12 01 dc 01 52 00</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: handle(0x0087), sas_address(0x5000c500a65b61f1), phy(34)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] scsi target17:0:116: enclosure_logical_id(0x50050cc11ac01572), slot(56)</span><br><span class="line">[Sun Sep 23 11:07:09 2018] sd 17:0:116:0: task abort: SUCCESS scmd(ffff880fe9784ec0)</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task zpool:3155 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">&quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot;</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] zpool           D ffffffffc0ade588     0  3155   3152 0x00000084</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011267ce0 0000000000000082 ffff881025d0cf10 ffff881011267fd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011267fd8 ffff881011267fd8 ffff881025d0cf10 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff881025d0cf10 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d0744&gt;] spa_import+0x54/0x730 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc062cfc2&gt;] ? nvlist_lookup_common.part.71+0xa2/0xb0 [znvpair]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc090bc37&gt;] zfs_ioc_pool_import+0x147/0x160 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0910846&gt;] zfsdev_ioctl+0x606/0x650 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff812151bd&gt;] do_vfs_ioctl+0x33d/0x540</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b0091&gt;] ? __do_page_fault+0x171/0x450</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff81215461&gt;] SyS_ioctl+0xa1/0xc0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b5089&gt;] system_call_fastpath+0x16/0x1b</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task zpool:3157 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">&quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot;</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] zpool           D ffffffffc0ade588     0  3155   3152 0x00000084</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011267ce0 0000000000000082 ffff881025d0cf10 ffff881011267fd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011267fd8 ffff881011267fd8 ffff881025d0cf10 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff881025d0cf10 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08d0744&gt;] spa_import+0x54/0x730 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc062cfc2&gt;] ? nvlist_lookup_common.part.71+0xa2/0xb0 [znvpair]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc090bc37&gt;] zfs_ioc_pool_import+0x147/0x160 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0910846&gt;] zfsdev_ioctl+0x606/0x650 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff812151bd&gt;] do_vfs_ioctl+0x33d/0x540</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b0091&gt;] ? __do_page_fault+0x171/0x450</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff81215461&gt;] SyS_ioctl+0xa1/0xc0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b5089&gt;] system_call_fastpath+0x16/0x1b</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task zpool:3157 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">&quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot;</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] zpool           D ffffffffc0ade588     0  3157   3153 0x00000084</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011283c60 0000000000000086 ffff88103b1fdee0 ffff881011283fd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff881011283fd8 ffff881011283fd8 ffff88103b1fdee0 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff88103b1fdee0 00000000ffffffff ffffffffc0ade588</span><br><span class="line">[Sun Sep 23 11:07:58 2018] Call Trace:</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816aa489&gt;] schedule_preempt_disabled+0x29/0x70</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a83b7&gt;] __mutex_lock_slowpath+0xc7/0x1d0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816a77cf&gt;] mutex_lock+0x1f/0x2f</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cee41&gt;] spa_open_common+0x61/0x4d0 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff8118cb26&gt;] ? __alloc_pages_nodemask+0x176/0x420</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc08cf334&gt;] spa_get_stats+0x54/0x550 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc06021a0&gt;] ? spl_kmem_zalloc+0xc0/0x170 [spl]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff811ddbee&gt;] ? __kmalloc_node+0x27e/0x2b0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc090c019&gt;] zfs_ioc_pool_stats+0x39/0x90 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffffc0910846&gt;] zfsdev_ioctl+0x606/0x650 [zfs]</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff812151bd&gt;] do_vfs_ioctl+0x33d/0x540</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b0091&gt;] ? __do_page_fault+0x171/0x450</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff81215461&gt;] SyS_ioctl+0xa1/0xc0</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  [&lt;ffffffff816b5089&gt;] system_call_fastpath+0x16/0x1b</span><br><span class="line">[Sun Sep 23 11:07:58 2018] INFO: task zpool:3159 blocked <span class="keyword">for</span> more than 120 seconds.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] <span class="string">&quot;echo 0 &gt; /proc/sys/kernel/hung_task_timeout_secs&quot;</span> disables this message.</span><br><span class="line">[Sun Sep 23 11:07:58 2018] zpool           D ffffffffc0ade588     0  3159   3156 0x00000084</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88101128bc60 0000000000000086 ffff88102fb3af70 ffff88101128bfd8</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffff88101128bfd8 ffff88101128bfd8 ffff88102fb3af70 ffffffffc0ade580</span><br><span class="line">[Sun Sep 23 11:07:58 2018]  ffffffffc0ade584 ffff88102fb3af70 00000000ffffffff ffffffffc0ade588</span><br></pre></td></tr></table></figure>

<h3 id="SAS-signal-about-Invalid-DWORD-count"><a href="#SAS-signal-about-Invalid-DWORD-count" class="headerlink" title="SAS signal (about Invalid DWORD count)"></a>SAS signal (about Invalid DWORD count)</h3><pre><code>Invalid DWORD count = 124
Loss of DWORD synchronization = 31
</code></pre>
<p><a target="_blank" rel="noopener" href="//www.c0t0d0s0.org/archives/7601-Check-both-sides-or-Errors-on-the-SAN.html">The interesting part is the highlighted one. A massive increase in invalid tx word count. When you see an massively increased counter here, your HBA just received rubbish from the storage. I never saw a different reason for this than a problem between the interface to the optics on the HBA and the interface to the optics on the switch ranging from not properly seated transceivers to blatant cases of ignoring the minimum bending radius of the fibre optic cables.</a></p>
<p>Suggestions to the customer based on the rule “check cheapest solution first”:<br>reseat cables<br>reseat transceivers<br>use a new cable<br>use new transceivers</p>
<h3 id="direct-IO"><a href="#direct-IO" class="headerlink" title="direct IO"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/pull/10018">direct IO</a></h3><p>directio &lt;off,on,strict,legacy&gt;<br>off - Do not accept O_DIRECT flag<br>on - Accept O_DIRECT flag and possibly bypass ARC<br>(will bypass base on directio_write&#x2F;read_align SEE BELOW)<br>strict - Accept O_DIRECT flag and always bypass ARC<br>(may fail based on directio_write&#x2F;read_align SEE BELOW)<br>legacy - Accept O_DIRECT flag and always use ARC</p>
<p>directio&#x3D;standard | always | disabled<br>where standard means: if you request DIRECTIO, we’ll do it directly if we think it’s a good idea (e.g. writes are recordsize-aligned), and otherwise we’ll do the i&#x2F;o non-directly (we won’t fail it for poor alignment). This is the default.<br>always means act like DIRECTIO was always requested (may be actually direct or indirect depending on i&#x2F;o alignment, won’t fail for poor alignment).<br>disabled means act like DIRECTIO was never requested (which is the current behavior).</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">This means that DIRECTIO writes will be spread out among the vdevs using the old round-robin algorithm. This could potentially result <span class="keyword">in</span> poor performance due to allocating from the slowest / most fragmented vdev, and we could potentially make the vdevs even more imbalanced (at least <span class="keyword">in</span> terms of performance/fragmentation). @grwilson <span class="keyword">do</span> you have any thoughts on this? How big the impact could be, and potential ways to mitigate? Could we make this use the throttle?</span><br></pre></td></tr></table></figure>

<h3 id="Performance-tuning"><a href="#Performance-tuning" class="headerlink" title="Performance tuning"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/8381">Performance tuning</a></h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">In general, smaller max_active’s will lead to lower latency of synchronous operations.  Larger max_active’s may lead to higher overall throughput, depending on underlying storage.</span><br><span class="line">The ratio of the queues’ max_actives determines the balance of performance between reads, writes,  and  scrubs.</span><br><span class="line">E.g., increasing zfs_vdev_scrub_max_active will cause the scrub or resilver to complete more quickly, but reads and writes to have higher latency and lower throughput.</span><br><span class="line"></span><br><span class="line">Async Writes</span><br><span class="line">The number of concurrent operations issued for the async write I/O class follows a piece-wise  linear  function defined by a few adjustable points.</span><br></pre></td></tr></table></figure>

<pre><code>          |              o---------| &lt;-- zfs_vdev_async_write_max_active
     ^    |             /^         |
     |    |            / |         |
   active |           /  |         |
    I/O   |          /   |         |
   count  |         /    |         |
          |        /     |         |
          |-------o      |         | &lt;-- zfs_vdev_async_write_min_active
         0|_______^______|_________|
          0%      |      |       100% of zfs_dirty_data_max
                  |      |
                  |      ‘-- zfs_vdev_async_write_active_max_dirty_percent
                  ‘--------- zfs_vdev_async_write_active_min_dirty_percent
</code></pre>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Until  the  amount  of  dirty  data exceeds a minimum percentage of the dirty data allowed in the pool, the I/O scheduler will limit the number of concurrent operations to the minimum. As that threshold is crossed, the num-ber  of  concurrent  operations issued increases linearly to the maximum at the specified maximum percentage of the dirty data allowed in the pool.</span><br><span class="line"></span><br><span class="line">Ideally, the amount of dirty data on a busy pool  will  stay  in  the  sloped  part  of  the  function  between zfs_vdev_async_write_active_min_dirty_percent  and zfs_vdev_async_write_active_max_dirty_percent. If it exceeds the maximum percentage, this indicates that the rate of incoming data is greater than the rate that the backend storage can handle. In this case, we must further throttle incoming writes, as described in the next section.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">The minimum time for a transaction to take is calculated as:</span><br><span class="line">           min_time = zfs_delay_scale * (dirty - min) / (max - dirty)</span><br><span class="line">           min_time is then capped at 100 milliseconds.</span><br><span class="line"></span><br><span class="line">The  delay has two degrees of freedom that can be adjusted via tunables.  The percentage of dirty data at which we  start  to  delay  is  defined  by  zfs_delay_min_dirty_percent.  This  should  typically  be  at  or  above zfs_vdev_async_write_active_max_dirty_percent  so  that  we only start to delay after writing at full speed has failed to keep up with the incoming write rate.</span><br><span class="line">The scale of the curve is defined by  zfs_delay_scale.  Roughly speaking, this variable determines the amount of delay at the midpoint of the curve.</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>$ cat zfs_vdev_async_write_active_min_dirty_percent zfs_vdev_async_write_active_max_dirty_percent zfs_delay_min_dirty_percent zfs_delay_scale<br>30<br>60<br>60<br>500000<br>$ cat zfs_dirty_data_max<br>4294967296</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">note  that since the delay is added to the outstanding time remaining on the most recent transaction, the delay is effectively the inverse of IOPS.  Here the midpoint of 500us translates to 2000 IOPS. The shape of the curve was chosen such that small changes in the amount of accumulated dirty data in the first 3/4 of the curve yield relatively small differences in the amount of delay.</span><br><span class="line"></span><br><span class="line">```bash</span><br><span class="line">       delay</span><br><span class="line">       100ms +-------------------------------------------------------------++</span><br><span class="line">             +                                                              +</span><br><span class="line">             |                                                              |</span><br><span class="line">             +                                                             *+</span><br><span class="line">        10ms +                                                             *+</span><br><span class="line">             +                                                           ** +</span><br><span class="line">             |                                              (midpoint)  **  |</span><br><span class="line">             +                                                  |     **    +</span><br><span class="line">         1ms +                                                  v ****      +</span><br><span class="line">             +             zfs_delay_scale ----------&gt;        *****         +</span><br><span class="line">             |                                             ****             |</span><br><span class="line">             +                                          ****                +</span><br><span class="line">       100us +                                        **                    +</span><br><span class="line">             +                                       *                      +</span><br><span class="line">             |                                      *                       |</span><br><span class="line">             +                                     *                        +</span><br><span class="line">        10us +                                     *                        +</span><br><span class="line">             +                                                              +</span><br><span class="line">             |                                                              |</span><br><span class="line">             +                                                              +</span><br><span class="line">             +--------------------------------------------------------------+</span><br><span class="line">             0%                    &lt;- zfs_dirty_data_max -&gt;               100%</span><br></pre></td></tr></table></figure>

<p>Note here that only as the amount of dirty data approaches its limit does the delay start to increase  rapidly. The  goal  of  a  properly  tuned  system should be to keep the amount of dirty data out of that range by first ensuring that the appropriate limits are set for the I&#x2F;O scheduler to reach optimal throughput on  the  backend storage, and then by changing the value of zfs_delay_scale to increase the steepness of the curve.  </p>
<p>set spl_taskq_thread_dynamic to 0<br>This will disable all of the dynamic task queues and statistically allocate the threads.   </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1000 iops</span></span><br><span class="line">$ awk <span class="string">&#x27;BEGIN&#123;print 1/1000&#125;&#x27;</span></span><br><span class="line">0.001</span><br><span class="line">$ <span class="built_in">echo</span> 1000000 &gt; zfs_delay_scale</span><br></pre></td></tr></table></figure>

<h3 id="single-HDD-cause-the-zpool-import-failed"><a href="#single-HDD-cause-the-zpool-import-failed" class="headerlink" title="single HDD cause the zpool import failed"></a>single HDD cause the zpool import failed</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">$ zpool import -f -o  cachefile=none -o <span class="built_in">readonly</span>=on tank_61 <span class="comment"># or you could add echo 1 &gt; /sys/module/zfs/parameters/zfs_recover</span></span><br><span class="line">cannot import <span class="string">&#x27;tank_61&#x27;</span>: I/O error Destroy and re-create the pool from a backup <span class="built_in">source</span>.</span><br><span class="line"></span><br><span class="line">------------------------oh shit--------------------</span><br><span class="line"></span><br><span class="line">zpool import show</span><br><span class="line"></span><br><span class="line">     pool: tank_61</span><br><span class="line">     <span class="built_in">id</span>: 15498746923132605816</span><br><span class="line">  state: FAULTED</span><br><span class="line"> status: The pool metadata is corrupted.</span><br><span class="line"> action: The pool cannot be imported due to damaged devices or data.</span><br><span class="line">        The pool may be active on another system, but can be imported using</span><br><span class="line">        the <span class="string">&#x27;-f&#x27;</span> flag.</span><br><span class="line">   see: http://zfsonlinux.org/msg/ZFS-8000-72</span><br><span class="line"> config:        tank_61                      FAULTED  corrupted data</span><br><span class="line">          raidz3-0                  ONLINE</span><br><span class="line">            sddd                    ONLINE</span><br><span class="line">            sdde                    ONLINE</span><br><span class="line">            sddf                    ONLINE</span><br><span class="line">            sddg                    ONLINE</span><br><span class="line">            sddh                    ONLINE</span><br><span class="line">            sddi                    ONLINE</span><br><span class="line">            scsi-35000c500a66c86f7  ONLINE</span><br><span class="line">            scsi-35000c500a675b907  ONLINE</span><br><span class="line">            scsi-35000c500a665afe7  ONLINE</span><br><span class="line">            scsi-35000c500a670c7a3  ONLINE</span><br><span class="line">            scsi-35000c500a66d665f  ONLINE</span><br><span class="line">            scsi-35000c500a65a86b7  ONLINE</span><br><span class="line">            scsi-35000c500a664a617  ONLINE</span><br><span class="line">            scsi-35000c500a665b353  ONLINE</span><br><span class="line">            scsi-35000c500a670c9fb  ONLINE</span><br><span class="line">            scsi-35000c500a670b10f  ONLINE</span><br><span class="line">            scsi-35000c500a65a9027  ONLINE</span><br><span class="line">            scsi-35000c500a65a02af  ONLINE</span><br><span class="line">            scsi-35000c500a670b213  ONLINE</span><br><span class="line">            scsi-35000c500a670b157  ONLINE</span><br><span class="line">            scsi-35000c500a665b1b3  ONLINE</span><br><span class="line">            scsi-35000c500a670c5fb  ONLINE</span><br><span class="line">$ zpool import -f -o  cachefile=none -F -n tank_61</span><br><span class="line">$ <span class="built_in">echo</span> $?</span><br><span class="line">1</span><br><span class="line">$ zdb -e tank_61</span><br><span class="line">Configuration <span class="keyword">for</span> import:</span><br><span class="line">        vdev_children: 1</span><br><span class="line">        version: 5000</span><br><span class="line">        pool_guid: 15498746923132605816</span><br><span class="line">        name: <span class="string">&#x27;tank_61&#x27;</span></span><br><span class="line">        state: 0</span><br><span class="line">        <span class="built_in">hostid</span>: 3219179115</span><br><span class="line">        hostname: <span class="string">&#x27;cngb-oss-c25-4.cngb.sz.hpc&#x27;</span></span><br><span class="line">        vdev_tree:</span><br><span class="line">            <span class="built_in">type</span>: <span class="string">&#x27;root&#x27;</span></span><br><span class="line">            <span class="built_in">id</span>: 0</span><br><span class="line">            guid: 15498746923132605816</span><br><span class="line">            children[0]:</span><br><span class="line">                <span class="built_in">type</span>: <span class="string">&#x27;raidz&#x27;</span></span><br><span class="line">                <span class="built_in">id</span>: 0</span><br><span class="line">                guid: 18120440563767675975</span><br><span class="line">                nparity: 3</span><br><span class="line">                metaslab_array: 65</span><br><span class="line">                metaslab_shift: 40</span><br><span class="line">                ashift: 9</span><br><span class="line">                asize: 215529714352128</span><br><span class="line">                is_log: 0</span><br><span class="line">                create_txg: 4</span><br><span class="line">                children[0]:</span><br><span class="line">                    <span class="built_in">type</span>: <span class="string">&#x27;disk&#x27;</span></span><br><span class="line">                    <span class="built_in">id</span>: 0</span><br><span class="line">                    guid: 691489041564037318</span><br><span class="line">                    whole_disk: 1</span><br><span class="line">                    DTL: 277</span><br><span class="line">                    create_txg: 4</span><br><span class="line">                    path: <span class="string">&#x27;/dev/sddd1&#x27;</span></span><br><span class="line">                    devid: <span class="string">&#x27;scsi-35000c500a670b7bf-part1&#x27;</span></span><br><span class="line">                    phys_path: <span class="string">&#x27;pci-0000:b3:00.0-sas-0x5000c500a670b7bd-lun-0&#x27;</span></span><br><span class="line"></span><br><span class="line">......</span><br><span class="line"></span><br><span class="line"> children[20]:</span><br><span class="line">                    <span class="built_in">type</span>: <span class="string">&#x27;disk&#x27;</span></span><br><span class="line">                    <span class="built_in">id</span>: 20</span><br><span class="line">                    guid: 3278057294213455851</span><br><span class="line">                    whole_disk: 1</span><br><span class="line">                    DTL: 257</span><br><span class="line">                    create_txg: 4</span><br><span class="line">                    path: <span class="string">&#x27;/dev/disk/by-id/scsi-35000c500a665b1b3-part1&#x27;</span></span><br><span class="line">                    devid: <span class="string">&#x27;scsi-35000c500a665b1b3-part1&#x27;</span></span><br><span class="line">                    phys_path: <span class="string">&#x27;pci-0000:b3:00.0-sas-0x5000c500a665b1b1-lun-0&#x27;</span></span><br><span class="line">                children[21]:</span><br><span class="line">                    <span class="built_in">type</span>: <span class="string">&#x27;disk&#x27;</span></span><br><span class="line">                    <span class="built_in">id</span>: 21</span><br><span class="line">                    guid: 10749955888611927037</span><br><span class="line">                    whole_disk: 1</span><br><span class="line">                    DTL: 256</span><br><span class="line">                    create_txg: 4</span><br><span class="line">                    path: <span class="string">&#x27;/dev/disk/by-id/scsi-35000c500a670c5fb-part1&#x27;</span></span><br><span class="line">                    devid: <span class="string">&#x27;scsi-35000c500a670c5fb-part1&#x27;</span></span><br><span class="line">                    phys_path: <span class="string">&#x27;pci-0000:b3:00.0-sas-0x5000c500a670c5f9-lun-0&#x27;</span></span><br><span class="line">        rewind-policy:</span><br><span class="line">            rewind-request-txg: 18446744073709551615</span><br><span class="line">            rewind-request: 2</span><br><span class="line">zdb: can<span class="string">&#x27;t open &#x27;</span>tank_61<span class="string">&#x27;: Input/output error</span></span><br></pre></td></tr></table></figure>

<p>Try it one by one</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">$ zdb -e tank_61 -d scsi-35000c500a665afe7</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 509M, 284 objects</span><br><span class="line"></span><br><span class="line">    Object  lvl   iblk   dblk  dsize  dnsize  lsize   %full  type</span><br><span class="line">         0    2   128K    16K   736K     512   592K   23.99  DMU dnode</span><br><span class="line"></span><br><span class="line">$ zdb -e tank_61 -d sddi</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line"></span><br><span class="line">$ zdb -e tank_61 -d -t 1274825</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset tank_61/tank_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset tank_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">----sddh--finish-----</span><br><span class="line">----sddi-----</span><br><span class="line">zdb -e tank_61 -d -t 1414018</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line">zdb -e tank_61 -d -t 1414019</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line">zdb -e tank_61 -d -t 1414020</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line">zdb -e tank_61 -d -t 1414021</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line">zdb -e tank_61 -d -t 1414022</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line">zdb -e tank_61 -d -t 1414023</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line">zdb -e tank_61 -d -t 1414025</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line"></span><br><span class="line">......</span><br><span class="line">zdb -e tank_61 -d -t 1274813</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset tank_61/tank_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset tank_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">zdb -e tank_61 -d -t 1274814</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset tank_61/tank_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset tank_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">zdb -e tank_61 -d -t 1414007</span><br><span class="line">zdb: can&#x27;t open &#x27;tank_61&#x27;: Input/output error</span><br><span class="line">.....</span><br><span class="line"></span><br><span class="line">----sddi--finish-----</span><br><span class="line">----scsi-35000c500a66c86f7-----</span><br><span class="line">zdb -e tank_61 -d -t 1274826</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset tank_61/tank_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset tank_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br><span class="line">zdb -e tank_61 -d -t 1274827</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset tank_61/tank_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">......</span><br><span class="line">Dataset mos [META], ID 0, cr_txg 4, 492M, 284 objects</span><br><span class="line">Dataset tank_61/tank_61 [ZPL], ID 155, cr_txg 1744, 59.3T, 662232 objects</span><br><span class="line">Dataset tank_61 [ZPL], ID 51, cr_txg 1, 68.1K, 6 objects</span><br><span class="line">Verified large_blocks feature refcount of 1 is correct</span><br><span class="line">Verified large_dnode feature refcount of 0 is correct</span><br><span class="line">Verified sha512 feature refcount of 0 is correct</span><br><span class="line">Verified skein feature refcount of 0 is correct</span><br><span class="line">Verified edonr feature refcount of 0 is correct</span><br><span class="line">Verified userobj_accounting feature refcount of 2 is correct</span><br></pre></td></tr></table></figure>
<p>Only sddi show input&#x2F;output error<br>This is the <a target="_blank" rel="noopener" href="https://github.com/zfsonlinux/zfs/issues/7184">zfs bug</a>, the easy way is use zdb -lu &#x2F;dev&#x2F;sd(all zpool devices), you could compare txg number, single device has the wrong txg number with the others.</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ diff sdcu sdcv # it &#x27;s ok</span><br><span class="line">$ diff sdcu sddi # found the wrong device</span><br><span class="line">25,37c25,37</span><br><span class="line">&lt;       txg = 2674112</span><br><span class="line">&lt;       txg = 2673986</span><br><span class="line">&lt;       txg = 2674114</span><br><span class="line">&lt;       txg = 2674115</span><br><span class="line">&lt;       txg = 2674116</span><br><span class="line">&lt;       txg = 2674117</span><br><span class="line">&lt;       txg = 2674118</span><br><span class="line">&lt;       txg = 2674119</span><br><span class="line">&lt;       txg = 2674120</span><br><span class="line">&lt;       txg = 2674121</span><br><span class="line">&lt;       txg = 2674122</span><br><span class="line">&lt;       txg = 2674123</span><br><span class="line">&lt;       txg = 2674124</span><br><span class="line">---</span><br><span class="line">&gt;       txg = 7727315</span><br><span class="line">&gt;       txg = 7727443</span><br><span class="line">&gt;       txg = 7727317</span><br><span class="line">&gt;       txg = 7727318</span><br><span class="line">&gt;       txg = 7727319</span><br><span class="line">&gt;       txg = 7727320</span><br><span class="line">&gt;       txg = 7727321</span><br><span class="line">&gt;       txg = 7727322</span><br><span class="line">&gt;       txg = 7727323</span><br><span class="line">&gt;       txg = 7727324</span><br><span class="line">&gt;       txg = 7727325</span><br><span class="line">&gt;       txg = 7727453</span><br><span class="line"></span><br><span class="line">echo 1 &gt; /sys/class/block/sddi/device/delete</span><br></pre></td></tr></table></figure>
<p>After ban the sddi, the zpool could be imported……wipe the sweat, wipe the sweat……<br>The issue because this device (sddi) has been in two diff zpool(server A and server B) at the same time<br>The 2 OSS resilver different infomation to the single device   </p>
<h3 id="Error-ICRC-ABRT-at-LBA-in-SATA-device"><a href="#Error-ICRC-ABRT-at-LBA-in-SATA-device" class="headerlink" title="Error: ICRC, ABRT at LBA in SATA device"></a>Error: ICRC, ABRT at LBA in SATA device</h3><p>zfs show a lot of cksum errors<br>fix it by upgrade IO expander firmware</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">Error 17 occurred at disk power-on lifetime: 1903 hours (79 days + 7 hours)</span><br><span class="line">  When the <span class="built_in">command</span> that caused the error occurred, the device was doing SMART Offline or Self-<span class="built_in">test</span>.</span><br><span class="line"></span><br><span class="line">  After <span class="built_in">command</span> completion occurred, registers were:</span><br><span class="line">  ER ST SC SN CL CH DH</span><br><span class="line">  -- -- -- -- -- -- --</span><br><span class="line">  84 41 00 00 00 00 00  Error: ICRC, ABRT at LBA = 0x00000000 = 0</span><br><span class="line"></span><br><span class="line">  Commands leading to the <span class="built_in">command</span> that caused the error were:</span><br><span class="line">  CR FR SC SN CL CH DH DC   Powered_Up_Time  Command/Feature_Name</span><br><span class="line">  -- -- -- -- -- -- -- --  ----------------  --------------------</span><br><span class="line">  61 05 10 b2 8e 78 40 00  23d+05:55:53.717  WRITE FPDMA QUEUED</span><br><span class="line">  61 01 38 b1 8e 78 40 00  23d+05:55:53.716  WRITE FPDMA QUEUED</span><br><span class="line">  61 01 30 d9 8e 78 40 00  23d+05:55:53.716  WRITE FPDMA QUEUED</span><br><span class="line">  61 01 28 d8 8e 78 40 00  23d+05:55:53.716  WRITE FPDMA QUEUED</span><br><span class="line">  61 0a 20 68 ac 53 40 00  23d+05:55:53.715  WRITE FPDMA QUEUED</span><br><span class="line"></span><br><span class="line">  pool: tank</span><br><span class="line"> state: ONLINE</span><br><span class="line">status: One or more devices has experienced an unrecoverable error.  An</span><br><span class="line">        attempt was made to correct the error.  Applications are unaffected.</span><br><span class="line">action: Determine <span class="keyword">if</span> the device needs to be replaced, and clear the errors</span><br><span class="line">        using <span class="string">&#x27;zpool clear&#x27;</span> or replace the device with <span class="string">&#x27;zpool replace&#x27;</span>.</span><br><span class="line">   see: http://zfsonlinux.org/msg/ZFS-8000-9P</span><br><span class="line">  scan: scrub repaired 55.5K <span class="keyword">in</span> 11h11m with 0 errors on Mon Mar 18 09:57:08 2019</span><br><span class="line">config:</span><br><span class="line"></span><br><span class="line">        NAME        STATE     READ WRITE CKSUM</span><br><span class="line">        tank        ONLINE       0     0     0</span><br><span class="line">          raidz3-0  ONLINE       0     0     0</span><br><span class="line">            sdh     ONLINE       0     0    51</span><br><span class="line">            sdi     ONLINE       0     0    13</span><br><span class="line">            sdj     ONLINE       0     0    19</span><br><span class="line">            sdk     ONLINE       0     0    10</span><br><span class="line">            sdl     ONLINE       0     0    28</span><br><span class="line">            sdm     ONLINE       0     0    39</span><br><span class="line">            sdn     ONLINE       0     0    70</span><br><span class="line">            sdo     ONLINE       0     0    25</span><br></pre></td></tr></table></figure>

<h3 id="MMP-mmp-timeout-calculate"><a href="#MMP-mmp-timeout-calculate" class="headerlink" title="MMP(mmp) timeout calculate"></a>MMP(mmp) timeout <a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/7045">calculate</a></h3><p><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/pull/10873">(gethrtime() - mmp_last_write) &gt; mmp_fail_ns</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">max_fail_ns = mmp_interval * mmp_fail_intervals</span><br><span class="line">rather than <span class="string">&quot;max_fail_ns = MAX(zfs_multihost_interval, mmp_delay * MAX(vdev_count_leaves(spa), 1)) * mmp_fail_intervals that the peer node would wait before considering the pool to be inactive. The actual interval that a peer would wait is typically even longer, since there is a separate zfs_multihost_import_intervals parameter from the zfs_mmp_fail_intervals that is larger by default (10x vs. 5x).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">## if you want MMP go to suspend</span></span><br><span class="line"><span class="string">options zfs zfs_multihost_fail_intervals=100</span></span><br><span class="line"><span class="string">options zfs zfs_multihost_import_intervals=200</span></span><br><span class="line"><span class="string">options zfs zfs_multihost_interval=6000</span></span><br><span class="line"><span class="string"># Don&#x27;t allow fail_intervals larger than import_intervals</span></span><br><span class="line"><span class="string"># When zfs_multihost_fail_intervals &gt; 0 then sequential mul‐tihost write failures will cause the pool to be  suspended. This occurs when zfs_multihost_fail_intervals * zfs_multi‐htank_interval milliseconds have passed since the last successful multihost write.  This guarantees the activity test will see multihost writes if the pool is imported.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#dangerous !!!!</span></span><br><span class="line"><span class="string">#zfs_multihost_fail_intervals &gt;= zfs_multihost_import_intervals which could allow a pool to be imported on two nodes without any warnings</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">multihost = on</span></span><br><span class="line"><span class="string">       while (!mmp-&gt;mmp_thread_exiting) &#123;</span></span><br><span class="line"><span class="string">                uint64_t mmp_fail_intervals = zfs_multihost_fail_intervals; # default = 5</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        hrtime_t max_fail_ns = zfs_multihost_fail_intervals *</span></span><br><span class="line"><span class="string">            MSEC2NSEC(MAX(zfs_multihost_interval, MMP_MIN_INTERVAL));</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                if ((mmp_interval * mmp_fail_intervals) &lt; max_fail_ns) &#123;</span></span><br><span class="line"><span class="string">                        max_fail_ns = ((31 * max_fail_ns) + (mmp_interval *</span></span><br><span class="line"><span class="string">                            mmp_fail_intervals)) / 32;</span></span><br><span class="line"><span class="string">                &#125; else &#123;</span></span><br><span class="line"><span class="string">                        max_fail_ns = mmp_interval * mmp_fail_intervals; ### there is no 43 seconds</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                if ((!last_spa_multihost &amp;&amp; multihost) ||</span></span><br><span class="line"><span class="string">                    (last_spa_suspended &amp;&amp; !suspended)) &#123;</span></span><br><span class="line"><span class="string">                        mutex_enter(&amp;mmp-&gt;mmp_io_lock);</span></span><br><span class="line"><span class="string">                        mmp-&gt;mmp_last_write = gethrtime();</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">                max_fail_ns= 5 * 5000</span></span><br><span class="line"><span class="string">                                        5                  on</span></span><br><span class="line"><span class="string">                if (!suspended &amp;&amp; mmp_fail_intervals &amp;&amp; multihost &amp;&amp;</span></span><br><span class="line"><span class="string">                    (gethrtime() - mmp-&gt;mmp_last_write) &gt; max_fail_ns) &#123;</span></span><br><span class="line"><span class="string">                        cmn_err(CE_WARN, &quot;</span>MMP writes to pool <span class="string">&#x27;%s&#x27;</span> have not <span class="string">&quot;</span></span><br><span class="line"><span class="string">                            &quot;</span>succeeded <span class="keyword">in</span> over %llus; suspending pool<span class="string">&quot;,</span></span><br><span class="line"><span class="string">                            spa_name(spa),</span></span><br><span class="line"><span class="string">                            NSEC2SEC(gethrtime() - mmp-&gt;mmp_last_write));</span></span><br><span class="line"><span class="string">                        zio_suspend(spa, NULL, ZIO_SUSPEND_MMP);</span></span><br><span class="line"><span class="string">                &#125;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] scsi target13:0:10: _scsih_tm_display_info: handle(0x0015), sas_address(0x5000cca251b4462e), phy(37)</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] scsi target13:0:10: enclosurelogical id(0x500304800928dc3f), slot(10)</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] scsi target13:0:10: enclosure level(0x0000), connector name(     )</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] sd 13:0:10:0: task abort: SUCCESS scmd(ffff99cb3306a300)</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] sd 13:0:10:0: [sdff] tag#2 FAILED Result: hostbyte=DID_TIME_OUT driverbyte=DRIVER_OK</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] sd 13:0:10:0: [sdff] tag#2 CDB: Read(16) 88 00 00 00 00 04 6a 25 dc 13 00 00 00 01 00 00</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] blk_update_request: I/O error, dev sdff, sector 18960735251</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] WARNING: MMP writes to pool &#x27;tank_64&#x27; have not succeeded in over 43s; suspending pool</span></span><br><span class="line"><span class="string">[Wed May 20 11:27:44 2020] WARNING: Pool &#x27;tank_64&#x27; has encountered an uncorrectable I/O failure and has been suspended.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">$ mmp_interval=9000;mmp_fail_interval=5</span></span><br><span class="line"><span class="string">$ echo <span class="subst">$(($((31*$((mmp_interval*mmp_fail_interval)</span>)+((mmp_interval*mmp_fail_interva))))/32))</span></span><br><span class="line"><span class="string">43593</span></span><br></pre></td></tr></table></figure>
<p><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/blob/c449d4b06d1a6f26e3e422a845bff99a52b70000/include/os/linux/kernel/linux/blkdev_compat.h#L357">bio_set_flush - Set the appropriate flags in a bio to guarantee data are on non-volatile media on completion.</a><br>FUA - Force Unit Access<br>FUA is a caching hint that indicates the data should be referenced directly from the media of the device. That is cache should be bypassed for this command.</p>
<p>The Linux block layer provides two simple mechanisms that let filesystems control the caching behavior of the storage device. These mechanisms are a forced cache flush, and the Force Unit Access (FUA) flag for requests.<br>The REQ_FUA flag can be OR ed into the r&#x2F;w flags of a bio submitted from the filesystem and will make sure that I&#x2F;O completion for this request is only signaled after the data has been committed to non-volatile storage.</p>
<p>Filesystems can simply set the REQ_PREFLUSH and REQ_FUA bits and do not have to worry if the underlying devices need any explicit cache flushing and how the Forced Unit Access is implemented. The REQ_PREFLUSH and REQ_FUA flags may both be set on a single bio.</p>
<h3 id="zpool-block"><a href="#zpool-block" class="headerlink" title="zpool block"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/7734">zpool block</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ zfs create -V 4G -b <span class="string">&quot;<span class="subst">$(getconf PAGESIZE)</span>&quot;</span> -o logbias=throughput -o <span class="built_in">sync</span>=always  -o primarycache=metadata rpool/swap</span><br><span class="line">$ mkswap -f /dev/zvol/rpool/swap</span><br><span class="line">$ swapon /dev/zvol/rpool/swap</span><br></pre></td></tr></table></figure>

<h3 id="case-2246"><a href="#case-2246" class="headerlink" title="case 2246"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/2246">case 2246</a></h3><ul>
<li>Drive sector remapping is supposed to be a transparent process to the operating system. Though with a bit of knowledge you can coerce the drive into doing so ZFS is not explicitly trying anything<ul>
<li>not support linux until now, driver, firmware</li>
</ul>
</li>
<li>Read error during normal operation &#x3D; data gets reconstructed from parity and we continue</li>
<li>Read error during scrub &#x3D; data gets reconstructed from parity plus fixed by overwriting the data on the disk? The rewrite will then rely on the disk remapping capability of the hard disk.</li>
</ul>
<h3 id="case-reslivering-error-restart-dead-loop"><a href="#case-reslivering-error-restart-dead-loop" class="headerlink" title="case reslivering error restart dead loop"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/pull/10291">case reslivering error restart dead loop</a></h3><h3 id="zfs-uberblock-info"><a href="#zfs-uberblock-info" class="headerlink" title="zfs uberblock info"></a>zfs uberblock info</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/sda of=sda_first64 bs=512 count=$((<span class="number">512</span>*<span class="number">256</span>))</span><br><span class="line">$ <span class="built_in">dd</span> <span class="keyword">if</span>=/dev/sda of=sda_last_64 bs=512 skip=$(($(($(blockdev --getsize64 /dev/sda)/<span class="number">512</span>))-$((<span class="number">256</span>*<span class="number">512</span>))))</span><br><span class="line"></span><br><span class="line">$ hexdump sda_first64   | grep <span class="string">&quot;b10c 00ba&quot;</span> -A 2</span><br><span class="line">......</span><br><span class="line">--</span><br><span class="line">017f800 b10c 00ba 0000 0000 1388 0000 0000 0000</span><br><span class="line">017f810 ed93 0023 0000 0000 6011 8852 6417 8bc2</span><br><span class="line">017f820 64e3 625e 0000 0000 0001 0000 0000 0000</span><br><span class="line">--</span><br><span class="line">            -------------------magic num: 0x00bab10c</span><br><span class="line">            |</span><br><span class="line">017fc00 b10c 00ba 0000 0000 1388 0000 0000 0000-----0x1388=version: 5000</span><br><span class="line">            -------------------txg <span class="built_in">id</span>: 0x00276319=2581273</span><br><span class="line">            |</span><br><span class="line">017fc10 6319 0027 0000 0000 6011 8852 6417 8bc2-----guid <span class="built_in">sum</span> 0x8bc2641788526011=10070721768987975697</span><br><span class="line">            -------------------timestamp: 0x626fafdd=1651486685</span><br><span class="line">            |</span><br><span class="line">017fc20 afdd 626f 0000 0000 0001 0000 0000 0000</span><br><span class="line"></span><br><span class="line">zdb -lu /dev/sda1 | grep -E <span class="string">&#x27;5000|2581273|10070721768987975697&#x27;</span></span><br><span class="line">        version = 5000</span><br><span class="line">        txg = 2581273</span><br><span class="line">        guid_sum = 10070721768987975697</span><br><span class="line"></span><br><span class="line"><span class="comment">#the first 8 bytes are the magic uberblock number (b10c 00ba 0000 0000)</span></span><br><span class="line"><span class="comment">#the second 8 bytes are the version number (1388 0000 0000 0000)</span></span><br><span class="line"><span class="comment">#the third 8 bytes are the transaction group a.k.a txg (ed93 0023 0000 0000)</span></span><br><span class="line"><span class="comment">#the fourth 8 bytes are the guid sum (6011 8852 6417 8bc2)</span></span><br><span class="line"><span class="comment">#the fifth 8 bytes are the timestamp (64e3 625e 0000 0000)</span></span><br></pre></td></tr></table></figure>
<h3 id="zpool-status-show-the-read-x2F-write-x2F-chsum-error"><a href="#zpool-status-show-the-read-x2F-write-x2F-chsum-error" class="headerlink" title="zpool status show the read&#x2F;write&#x2F;chsum error"></a>zpool status show the read&#x2F;write&#x2F;chsum error</h3><p><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/1256">1256</a><br><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/4149">4149</a><br><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/4851">4851</a><br>If you’re reading a file and hit a read error, the data is re-constructed from the other drive and re-written to the bad sectors, but the read error counter is not incremented since it “self-healed”. However, unlike a checksum error, a self-healed read error will generate an IO event in zpool events.</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">zio_err = <span class="number">0x5</span> (EIO)</span><br><span class="line">zio_flags = <span class="number">0xc0880</span> (ZIO_FLAG_CANFAIL|ZIO_FLAG_DONT_CACHE|ZIO_FLAG_OPTIONAL|ZIO_FLAG_DONT_QUEUE)</span><br><span class="line"></span><br><span class="line">vdev_stat_update(<span class="type">zio_t</span> *zio, <span class="type">uint64_t</span> psize)</span><br><span class="line">...</span><br><span class="line">         <span class="comment">/*</span></span><br><span class="line"><span class="comment">          * If this is an I/O error that is going to be retried, then ignore the</span></span><br><span class="line"><span class="comment">          * error.  Otherwise, the user may interpret B_FAILFAST I/O errors as</span></span><br><span class="line"><span class="comment">          * hard errors, when in reality they can happen for any number of</span></span><br><span class="line"><span class="comment">          * innocuous reasons (bus resets, MPxIO link failure, etc).</span></span><br><span class="line"><span class="comment">          */</span></span><br><span class="line">         <span class="keyword">if</span> (zio-&gt;io_error == EIO &amp;&amp;</span><br><span class="line">             !(zio-&gt;io_flags &amp; ZIO_FLAG_IO_RETRY)) &#123;</span><br><span class="line">                 <span class="keyword">return</span>;</span><br><span class="line">         &#125;</span><br></pre></td></tr></table></figure>

<p>Looks like the self-healed not show in zpool status when you’re not enable ZIO_FLAG_IO_RETRY(0x00008000) in the zio_flags<br><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/blob/c26045b435408ce30ffae86f8a65670cc1e97590/module/os/linux/zfs/vdev_disk.c#L640">here is zfs bio retry</a></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">retry:</span><br><span class="line">        dr = vdev_disk_dio_alloc(bio_count);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (zio &amp;&amp; !(zio-&gt;io_flags &amp; (ZIO_FLAG_IO_RETRY | ZIO_FLAG_TRYHARD)))</span><br><span class="line">                bio_set_flags_failfast(bdev, &amp;flags);</span><br><span class="line"></span><br><span class="line">        dr-&gt;dr_zio = zio;</span><br><span class="line"></span><br><span class="line">        /*</span><br><span class="line">         * Since bio<span class="string">&#x27;s can have up to BIO_MAX_PAGES=256 iovec&#x27;</span>s, each of <span class="built_in">which</span></span><br><span class="line">         * is at least 512 bytes and at most PAGESIZE (typically 4K), one bio</span><br><span class="line">         * can cover at least 128KB and at most 1MB.  When the required number</span><br><span class="line">         * of iovec<span class="string">&#x27;s exceeds this, we are forced to break the IO in multiple</span></span><br><span class="line"><span class="string">         * bio&#x27;</span>s and <span class="built_in">wait</span> <span class="keyword">for</span> them all to complete.  This is likely <span class="keyword">if</span> the</span><br><span class="line">         * recordsize property is increased beyond 1MB.  The default</span><br><span class="line">         * bio_count=16 should typically accommodate the maximum-size zio of</span><br><span class="line">         * 16MB.</span><br><span class="line">         */</span><br></pre></td></tr></table></figure>
<p>make sure the read&#x2F;write error have been reconstructted</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">          mirror-1  ONLINE       0     0     0</span><br><span class="line">            sda     ONLINE       0     0     0</span><br><span class="line">            sdb     ONLINE       1     0     0</span><br><span class="line"></span><br><span class="line">scan: scrub repaired 0B in 1h35m with 0 errors on Thu Sep 15 11:56:39 2022</span><br><span class="line"></span><br><span class="line">xxxxxxxxxxxxxx ereport.fs.zfs.io</span><br><span class="line">        class = &quot;ereport.fs.zfs.io&quot;</span><br><span class="line">        ena = 0x92fbdfe0de302001</span><br><span class="line">        detector = (embedded nvlist)</span><br><span class="line">                version = 0x0</span><br><span class="line">                scheme = &quot;zfs&quot;</span><br><span class="line">                pool = 0x5275d6d385f8fbb5</span><br><span class="line">                vdev = 0xca1877dc3ff80838</span><br><span class="line">        (end detector)</span><br><span class="line">        pool = &quot;ost_0&quot;</span><br><span class="line">        pool_guid = 0x5275d6d385f8fbb5</span><br><span class="line">        pool_state = 0x0</span><br><span class="line">        pool_context = 0x0</span><br><span class="line">        pool_failmode = &quot;wait&quot;</span><br><span class="line">        vdev_guid = 0xca1877dc3ff80838</span><br><span class="line">        vdev_type = &quot;disk&quot;</span><br><span class="line">        vdev_path = &quot;/dev/sdx1&quot;</span><br><span class="line">        vdev_devid = &quot;scsi-35000cca25d727c30-part1&quot;</span><br><span class="line">        vdev_enc_sysfs_path = &quot;/sys/class/enclosure/12:0:52:0/Slot10&quot;</span><br><span class="line">        vdev_ashift = 0xc</span><br><span class="line">        vdev_complete_ts = 0x14f92fbdfdeb13</span><br><span class="line">        vdev_delta_ts = 0x1042310</span><br><span class="line">        vdev_read_errors = 0x1  &lt;----------- the block dev orgi addr read failed</span><br><span class="line">        vdev_write_errors = 0x0</span><br><span class="line">        vdev_cksum_errors = 0x0</span><br></pre></td></tr></table></figure>
<p>When the zfs write the leaf io failed and retry write the same LBA address, if all retry failed again, write error count increased and the vdev will reassign the new LBA address to write.</p>
<p>The error count ++</p>
<figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">https:<span class="comment">//github.com/openzfs/zfs/blob/zfs-2.0.7-staging/module/zfs/zio.c#L4575</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (zio-&gt;io_error) &#123;</span><br><span class="line">                <span class="comment">/*</span></span><br><span class="line"><span class="comment">                 * If this I/O is attached to a particular vdev,</span></span><br><span class="line"><span class="comment">                 * generate an error message describing the I/O failure</span></span><br><span class="line"><span class="comment">                 * at the block level.  We ignore these errors if the</span></span><br><span class="line"><span class="comment">                 * device is currently unavailable.</span></span><br><span class="line"><span class="comment">                 */</span></span><br><span class="line">                <span class="keyword">if</span> (zio-&gt;io_error != ECKSUM &amp;&amp; zio-&gt;io_vd != <span class="literal">NULL</span> &amp;&amp;</span><br><span class="line">                    !vdev_is_dead(zio-&gt;io_vd)) &#123;</span><br><span class="line">                        <span class="type">int</span> ret = zfs_ereport_post(FM_EREPORT_ZFS_IO,</span><br><span class="line">                            zio-&gt;io_spa, zio-&gt;io_vd, &amp;zio-&gt;io_bookmark, zio, <span class="number">0</span>);</span><br><span class="line">                        <span class="keyword">if</span> (ret != EALREADY) &#123;      &lt;-------------------------------------- <span class="comment">// if it &#x27;s not EALREADY(ereport: duplicate error report), add error count</span></span><br><span class="line">                                mutex_enter(&amp;zio-&gt;io_vd-&gt;vdev_stat_lock);</span><br><span class="line">                                <span class="keyword">if</span> (zio-&gt;io_type == ZIO_TYPE_READ)</span><br><span class="line">                                        zio-&gt;io_vd-&gt;vdev_stat.vs_read_errors++;</span><br><span class="line">                                <span class="keyword">else</span> <span class="keyword">if</span> (zio-&gt;io_type == ZIO_TYPE_WRITE)</span><br><span class="line">                                        zio-&gt;io_vd-&gt;vdev_stat.vs_write_errors++;</span><br><span class="line">                                mutex_exit(&amp;zio-&gt;io_vd-&gt;vdev_stat_lock);</span><br><span class="line">                        &#125;</span><br><span class="line">                &#125;</span><br><span class="line">......</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">https:<span class="comment">//github.com/openzfs/zfs/blob/zfs-2.0.7-staging/module/zfs/zfs_fm.c#L250</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment"> * Check if an ereport would be a duplicate of one recently posted.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * An ereport is considered a duplicate if the set of criteria in</span></span><br><span class="line"><span class="comment"> * recent_events_node_t all match.</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * Only FM_EREPORT_ZFS_IO, FM_EREPORT_ZFS_DATA, and FM_EREPORT_ZFS_CHECKSUM</span></span><br><span class="line"><span class="comment"> * are candidates for duplicate checking.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="type">static</span> <span class="type">boolean_t</span></span><br><span class="line"><span class="title function_">zfs_ereport_is_duplicate</span><span class="params">(<span class="type">const</span> <span class="type">char</span> *subclass, <span class="type">spa_t</span> *spa, <span class="type">vdev_t</span> *vd,</span></span><br><span class="line"><span class="params">    <span class="type">const</span> <span class="type">zbookmark_phys_t</span> *zb, <span class="type">zio_t</span> *zio, <span class="type">uint64_t</span> offset, <span class="type">uint64_t</span> size)</span></span><br><span class="line">&#123;</span><br></pre></td></tr></table></figure>

<h3 id="enable-debug"><a href="#enable-debug" class="headerlink" title="enable-debug"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/commit/d148e95156b98ac226013d24962d1afa4a51c712">enable-debug</a></h3><p>build with –enable-debug</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Finally, as a debugging aid when zfs is build with --enable-debug all</span><br><span class="line">errors from the backing block devices will be reported to the console</span><br><span class="line">with an error message like this:</span><br><span class="line"></span><br><span class="line">        ZFS: zio error=5 <span class="built_in">type</span>=1 offset=4217856 size=8192 flags=60440</span><br></pre></td></tr></table></figure>

<h3 id="zfs-read-perfromance"><a href="#zfs-read-perfromance" class="headerlink" title="zfs read perfromance"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/9375#issuecomment-538780029">zfs read perfromance</a></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line">In general I found that increasing:</span><br><span class="line"></span><br><span class="line">zfetch_array_rd_sz</span><br><span class="line">zfetch_max_distance &lt;---increase more performance <span class="keyword">for</span> the version less than 2.12.7, 2.12.7 is ok <span class="keyword">if</span> you <span class="string">&#x27;re not modify</span></span><br><span class="line"><span class="string">zfs_pd_bytes_max</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#not modify zfetch_max_distance</span></span><br><span class="line"><span class="string">$ zpool iostat -q 1 </span></span><br><span class="line"><span class="string">               capacity     operations     bandwidth    syncq_read    syncq_write   asyncq_read  asyncq_write   scrubq_read   trimq_write</span></span><br><span class="line"><span class="string">pool         alloc   free   read  write   read  write   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  8.69K    484   237M  69.8M      0      0      0      0     59     30      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   866G  86.5T      0    420  2.59K   267M      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  32.8K    325   490M  1.56M      0      0      0      0      8     21      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   866G  86.5T      0  1.42K      0  1.08G      0      0      0      0      0      0      0      1      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  44.9K      0   654M  3.98K      0      0      0      0    103     24      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   867G  86.5T      0    956      0   383M      0      0      0      0      0      0  14.4K     75      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  42.0K      0   622M  3.94K      0      0      0      0     96     29      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   868G  86.5T      0  1.13K      0   734M      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  41.7K      0   621M  3.96K      0      1      0      0      0      2      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   869G  86.5T      0  1.66K      0  1.09G      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span></span><br><span class="line"><span class="string">sdab              0.00     0.00 1183.00    0.00    17.06     0.00    29.53     0.51    0.42    0.42    0.00   0.20  23.20</span></span><br><span class="line"><span class="string">sdaa              0.00     0.00  942.00    0.00    18.88     0.00    41.04     0.61    0.65    0.65    0.00   0.27  25.30</span></span><br><span class="line"><span class="string">sdac              0.00     0.00  934.00    0.00    17.62     0.00    38.63     0.68    0.71    0.71    0.00   0.27  25.50</span></span><br><span class="line"><span class="string">sdad              0.00     0.00 1077.00    0.00    21.40     0.00    40.69     0.80    0.71    0.71    0.00   0.28  30.00</span></span><br><span class="line"><span class="string">sdae              0.00     0.00 1165.00    0.00    18.71     0.00    32.89     0.52    0.43    0.43    0.00   0.20  23.10</span></span><br><span class="line"><span class="string">sdaf              0.00     0.00 1239.00    0.00    16.95     0.00    28.02     0.42    0.33    0.33    0.00   0.16  19.80</span></span><br><span class="line"><span class="string">sdag              0.00     0.00 1009.00    0.00    17.90     0.00    36.33     0.60    0.59    0.59    0.00   0.25  25.20</span></span><br><span class="line"><span class="string">sdah              0.00     0.00 1144.00    0.00    22.18     0.00    39.70     0.69    0.60    0.60    0.00   0.25  28.40</span></span><br><span class="line"><span class="string">sdaj              0.00     0.00 1187.00    0.00    17.53     0.00    30.25     0.52    0.43    0.43    0.00   0.19  22.70</span></span><br><span class="line"><span class="string">sdai              0.00     0.00 1167.00    0.00    18.07     0.00    31.71     0.51    0.41    0.41    0.00   0.20  23.10</span></span><br><span class="line"><span class="string">sdak              0.00     0.00 1112.00    0.00    23.67     0.00    43.59     3.44    3.69    3.69    0.00   0.71  79.00</span></span><br><span class="line"><span class="string">sdal              0.00     0.00  857.00    0.00    22.06     0.00    52.72     0.92    1.04    1.04    0.00   0.37  31.40</span></span><br><span class="line"><span class="string">sdam              0.00     0.00 1267.00    0.00    18.80     0.00    30.39     0.46    0.36    0.36    0.00   0.18  22.60</span></span><br><span class="line"><span class="string">sdan              0.00     0.00 1325.00    0.00    16.92     0.00    26.15     0.45    0.34    0.34    0.00   0.16  21.70</span></span><br><span class="line"><span class="string">sdao              0.00     0.00 1129.00    0.00    17.98     0.00    32.61     0.59    0.52    0.52    0.00   0.22  24.50</span></span><br><span class="line"><span class="string">sdap              0.00     0.00  955.00    1.00    21.79     0.00    46.68     0.82    0.81    0.82    0.00   0.33  31.50</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">#modify zfetch_max_distance</span></span><br><span class="line"><span class="string">$ echo $((8388608*8)) &gt; /sys/module/zfs/parameters/zfetch_max_distance</span></span><br><span class="line"><span class="string">$ zpool iostat -q 1 </span></span><br><span class="line"><span class="string">               capacity     operations     bandwidth    syncq_read    syncq_write   asyncq_read  asyncq_write   scrubq_read   trimq_write</span></span><br><span class="line"><span class="string">pool         alloc   free   read  write   read  write   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ   pend  activ</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  8.95K    480   242M  69.1M      0      0      0      0    428     49      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   894G  86.5T      0    429  2.57K   273M      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  8.00K    201  1.07G  1.01M      0      0      0      0      1     12      0      1      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   895G  86.5T      0  1.59K      0  1.09G      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  5.65K    134  1.13G   596K      0      0      0      0    274     43      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   897G  86.5T      0  1.64K      0  1.09G      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  8.81K      0  1.10G  3.98K      0      0      0      0    927     50      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   898G  86.5T      0  1.74K      0  1.10G      0      0      0      0      0      0    571     32      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string">test_tank_0   215G  87.1T  7.76K      0  1.04G  3.98K      0     12      0      0  1.29K     49      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">test_tank_1   899G  86.5T      0  1.60K      0  1.09G      0      0      0      0      0      0      0      0      0      0      0      0</span></span><br><span class="line"><span class="string">-----------  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----  -----</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Device:         rrqm/s   wrqm/s     r/s     w/s    rMB/s    wMB/s avgrq-sz avgqu-sz   await r_await w_await  svctm  %util</span></span><br><span class="line"><span class="string">sdab              0.00     0.00  853.00   25.00    61.99     0.10   144.82     3.47    3.98    3.92    6.12   0.94  82.50</span></span><br><span class="line"><span class="string">sdaa              0.00     0.00  753.00   23.00    63.48     0.10   167.78     3.59    4.66    4.42   12.57   1.09  84.70</span></span><br><span class="line"><span class="string">sdac              0.00     0.00  931.00   23.00    61.45     0.10   132.13     3.41    3.59    3.31   14.70   0.83  79.50</span></span><br><span class="line"><span class="string">sdad              0.00     0.00 1024.00   25.00    64.56     0.10   126.23     3.61    3.43    3.28    9.84   0.80  84.40</span></span><br><span class="line"><span class="string">sdae              0.00     0.00  921.00   25.00    62.59     0.11   135.75     3.53    3.74    3.53   11.48   0.89  83.80</span></span><br><span class="line"><span class="string">sdaf              0.00     0.00 1002.00   25.00    61.12     0.11   122.11     3.43    3.34    3.09   13.36   0.79  80.90</span></span><br><span class="line"><span class="string">sdag              0.00     0.00 1061.00   26.00    60.49     0.12   114.19     3.56    3.29    2.90   18.85   0.72  78.50</span></span><br><span class="line"><span class="string">sdah              0.00     0.00  746.00   25.00    64.89     0.11   172.67     3.79    4.92    4.15   27.72   1.04  79.90</span></span><br><span class="line"><span class="string">sdaj              0.00     0.00 1166.00   20.00    59.50     0.09   102.89     2.73    2.30    2.27    4.40   0.58  69.20</span></span><br><span class="line"><span class="string">sdai              0.00     0.00  951.00   20.00    61.67     0.09   130.26     3.09    3.17    2.93   14.65   0.81  78.60</span></span><br><span class="line"><span class="string">sdak              0.00     0.00  707.00   20.00    61.95     0.09   174.76     3.47    4.77    4.66    8.40   1.10  80.00</span></span><br><span class="line"><span class="string">sdal              0.00     0.00  879.00   20.00    65.12     0.09   148.54     3.77    4.21    4.09    9.65   0.97  86.90</span></span><br><span class="line"><span class="string">sdam              0.00     0.00  769.00   28.00    63.43     0.14   163.35     4.06    5.11    4.84   12.57   1.08  86.40</span></span><br><span class="line"><span class="string">sdan              0.00     0.00 1094.00   28.00    60.80     0.14   111.23     3.14    2.78    2.58   10.32   0.68  76.50</span></span><br><span class="line"><span class="string">sdao              0.00     0.00  762.00   28.00    61.63     0.14   160.13     3.66    4.63    4.45    9.50   1.08  85.10</span></span><br><span class="line"><span class="string">sdap              0.00     0.00 1020.00   27.00    65.86     0.14   129.09     3.63    3.50    3.38    8.04   0.81  85.00</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> 65,224  6     2893    11.910617927 23469  A   R 40765488 + 1024 &lt;- (65,225) 40763440</span></span><br><span class="line"><span class="string"> 65,224  6     2894    11.910618184 23469  Q   R 40765488 + 1024 [z_rd_int]</span></span><br><span class="line"><span class="string"> 65,224  6     2895    11.910619109 23469  G   R 40765488 + 1024 [z_rd_int]</span></span><br><span class="line"><span class="string"> 65,224  6     2900    11.910653697 23469  I   R 40765488 + 1024 [z_rd_int]</span></span><br><span class="line"><span class="string"> 65,224  6     2902    11.910654374 23469  D   R 40765488 + 1024 [z_rd_int]</span></span><br><span class="line"><span class="string"> 65,224  9     5597    11.918826837 23493  A   R 40767536 + 1024 &lt;- (65,225) 40765488</span></span><br><span class="line"><span class="string"> 65,224  6     2904    11.934245390 16444  C   R 40765488 + 1024 [0]</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"> - Allow early reuse of inactive prefetch streams: streams that never</span></span><br><span class="line"><span class="string">saw hits can be reused immediately if there is a demand, while others</span></span><br><span class="line"><span class="string">can be reused after 1s of inactivity, starting with the oldest.  After</span></span><br><span class="line"><span class="string">2s of inactivity streams are deleted to free resources same as before.</span></span><br><span class="line"><span class="string">This allows by several times increase strided read performance on HDD</span></span><br><span class="line"><span class="string">pool in presence of simultaneous random reads, previously filling the</span></span><br><span class="line"><span class="string">zfetch_max_streams limit for seconds and so blocking most of prefetch.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">https://github.com/openzfs/zfs/commit/6aa8c21a2ad29ddd4564cdfd4c99048c891b717a</span></span><br><span class="line"><span class="string">https://www.hpss-collaboration.org/documents/HPSS_Tech-Topic_ZFS-ZVOL_LLNL_IM-1063127-2.pdf</span></span><br><span class="line"><span class="string">Ultimately, LLNL’s ongoing tests led us to deploy to production with these settings:</span></span><br><span class="line"><span class="string">options zfs zfetch_array_rd_sz=67108864</span></span><br><span class="line"><span class="string">options zfs zfetch_max_distance=67108864</span></span><br><span class="line"><span class="string">options zfs zfetch_max_streams=500</span></span><br></pre></td></tr></table></figure>

<h3 id="upgrade-SCSI-device-firmware-online"><a href="#upgrade-SCSI-device-firmware-online" class="headerlink" title="upgrade SCSI device firmware online"></a>upgrade SCSI device firmware online</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">[1:0:13:0] /dev/sdn 35000cca2c70557b0 C540</span><br><span class="line">sdn 5PH46N5D 0 13 0 1 C540</span><br><span class="line">zpool offline test_tank_0 scsi-35000cca2c70557b0</span><br><span class="line"></span><br><span class="line">hugo u --serial 5PH46N5D -f ./PQGNC54K.bin | grep -i <span class="string">&#x27;Update Successful&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> 1 &gt; /sys/block/sdn/device/delete</span><br><span class="line"><span class="built_in">echo</span> 0 13 0 &gt; /sys/class/scsi_host/host1/scan</span><br><span class="line"></span><br><span class="line">zpool events -c</span><br><span class="line"><span class="comment">#online scsi-35000cca2c70557b0</span></span><br><span class="line">zpool online test_tank_0 scsi-35000cca2c70557b0 || <span class="built_in">exit</span> 1</span><br><span class="line"><span class="built_in">sleep</span> 6</span><br><span class="line"><span class="comment">#check test_tank_0 resilver_finish</span></span><br><span class="line">zpool events | grep resilver_finish || <span class="built_in">exit</span> 1</span><br></pre></td></tr></table></figure>

<h3 id="zpool-write-full"><a href="#zpool-write-full" class="headerlink" title="zpool write full"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/5843">zpool write full</a></h3><p>In the old version, the remove will be hang because cow filesystem</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">https://github.com/openzfs/zfs/commit/19d55079aecb5c022c1c09e0eace4f7da7381a62</span><br><span class="line"></span><br><span class="line">ZFS: Loaded module v2.1.7-1, ZFS pool version 5000, ZFS filesystem version </span><br><span class="line"></span><br><span class="line">test_tank_0     zfs       166T  166T     0 100% /test_tank_0</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">rm</span> -f test_1 </span><br><span class="line">test_tank_0      166T  166T  254G 100% /test_tank_0</span><br></pre></td></tr></table></figure>

<h3 id="About-the-cksum-error"><a href="#About-the-cksum-error" class="headerlink" title="About the cksum error"></a><a target="_blank" rel="noopener" href="https://serverfault.com/questions/789194/zfs-checksum-errors-when-do-i-replace-the-drive">About the cksum error</a></h3><p>I have the same issue under the openzfs 0.7.13 and the ashift&#x3D;9, In my large-scale env,the ashift&#x3D;12 + openzfs 0.7.13 only little cksum errors compare with the ashfit&#x3D;9<br>Does the 512e RMW issue ? I’m not sure<br>Looks like it time to upgrade 4KiB native device<br>Replace The SAS HBA&#x2F;internal and external cable&#x2F;firmware&#x2F;backend board, there is no any help    </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Update 9: Still trying to track this one down. I came across this question which had some similarities to my situation. So, I went ahead and rebuilt the zpool using ashift=12 to see if that would resolve the issue (no luck). Then, I bit the bullet and bought a new controller. I just installed a Supermicro AOC-SAS2LP-MV8 HBA card. I&#x27;ll give it a week or two to see if this solves the problem.</span><br><span class="line"></span><br><span class="line">Update 10: Just to close this out. It&#x27;s been about 2 weeks since the new HBA card went in and, at the risk of jinxing it, I&#x27;ve had no checksum errors since. A huge thanks to everyone who helped me sort this one out.</span><br></pre></td></tr></table></figure>

<p><a target="_blank" rel="noopener" href="https://learn.microsoft.com/en-us/windows/win32/win7appqual/512-byte-emulation--512e--disk-compatibility-update">512e HDD issues</a>   </p>
<ul>
<li><p>Because most hard disk drives update in place, the physical sector – that is, the portion of the media where the physical sector was located – could have been corrupted with incomplete information due to a partial overwrite. Put another way, you can think of it as potentially having lost all 8 logical sectors (which the physical sector logically contains)</p>
</li>
<li><p>While most applications with a data store are designed with the capability to recover from media errors, the loss of eight sectors, or put another way, the loss of eight commit records, can potentially make it impossible for the data store to recover gracefully. An administrator may need to manually restore the database from a backup or may even need to perform a lengthy rebuild</p>
</li>
<li><p>One more important impact is that the act of another application causing a Read-Modify-Write cycle can potentially cause your data to be lost – even if your application is not running! This is simply because your data and the other application’s data could be located within the same physical sector</p>
</li>
</ul>
<h3 id="zfs-receive-stalls-x2F-slowdown-with-dnodesize-x3D-legacy-8458"><a href="#zfs-receive-stalls-x2F-slowdown-with-dnodesize-x3D-legacy-8458" class="headerlink" title="zfs receive stalls&#x2F;slowdown with dnodesize!&#x3D;legacy #8458"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/8458">zfs receive stalls&#x2F;slowdown with dnodesize!&#x3D;legacy #8458</a></h3><h3 id="zfs-0-7-X-zfs-receive-slow"><a href="#zfs-0-7-X-zfs-receive-slow" class="headerlink" title="zfs 0.7.X zfs receive slow"></a><a target="_blank" rel="noopener" href="https://github.com/openzfs/zfs/issues/8067">zfs 0.7.X zfs receive slow</a></h3>
  </div>
</article>


    <div class="blog-post-comments">
        <div id="utterances_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>


        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Why-ZFS"><span class="toc-number">1.</span> <span class="toc-text">Why ZFS</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Create-zpool"><span class="toc-number">2.</span> <span class="toc-text">Create zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#parameters"><span class="toc-number">3.</span> <span class="toc-text">parameters</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#intel-%E2%80%98s-zfs-solution-just-record-Ashamed"><span class="toc-number">4.</span> <span class="toc-text">intel ‘s zfs solution, just record ,Ashamed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#udev-setting"><span class="toc-number">5.</span> <span class="toc-text">udev setting</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NVME-zpool-throughput-not-test"><span class="toc-number">6.</span> <span class="toc-text">NVME zpool throughput, not test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-0-7-import-slow"><span class="toc-number">7.</span> <span class="toc-text">[ zfs 0.7 import slow]</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-schduler"><span class="toc-number">8.</span> <span class="toc-text">zfs schduler</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Monitor"><span class="toc-number">9.</span> <span class="toc-text">Monitor</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#L2ARC-and-slog"><span class="toc-number">9.1.</span> <span class="toc-text">L2ARC and slog</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#txg-timeout"><span class="toc-number">9.2.</span> <span class="toc-text">txg_timeout</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zpool-sync-mode"><span class="toc-number">9.3.</span> <span class="toc-text">zpool sync mode</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-import-failed"><span class="toc-number">10.</span> <span class="toc-text">zpool import failed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-command"><span class="toc-number">11.</span> <span class="toc-text">zfs command</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#special-class-vdev"><span class="toc-number">11.1.</span> <span class="toc-text">special class vdev</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#single-device-no-raid-too-a-mirror"><span class="toc-number">12.</span> <span class="toc-text">single device(no raid) too a mirror</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Add-special-device"><span class="toc-number">12.1.</span> <span class="toc-text">Add special device</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zpool-replace"><span class="toc-number">12.2.</span> <span class="toc-text">zpool replace</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Online-upgrade-SAS-device-firmware"><span class="toc-number">12.3.</span> <span class="toc-text">Online upgrade SAS device firmware</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Custom-packages"><span class="toc-number">12.4.</span> <span class="toc-text">Custom packages</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Replication-zpool"><span class="toc-number">13.</span> <span class="toc-text">Replication zpool</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#checkpoint"><span class="toc-number">14.</span> <span class="toc-text">checkpoint</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-test"><span class="toc-number">15.</span> <span class="toc-text">zfs test</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Work-with-gdb"><span class="toc-number">16.</span> <span class="toc-text">Work with gdb</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#snapshot"><span class="toc-number">16.1.</span> <span class="toc-text">snapshot</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-vol"><span class="toc-number">16.2.</span> <span class="toc-text">zfs vol</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zfs-event"><span class="toc-number">16.3.</span> <span class="toc-text">zfs event</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Compression"><span class="toc-number">16.4.</span> <span class="toc-text">Compression</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Xattr"><span class="toc-number">16.5.</span> <span class="toc-text">Xattr</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#ACL"><span class="toc-number">16.6.</span> <span class="toc-text">ACL</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Rmount"><span class="toc-number">16.7.</span> <span class="toc-text">Rmount</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Zpool-rename"><span class="toc-number">16.8.</span> <span class="toc-text">Zpool rename</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Detach-hdd-from-mirror"><span class="toc-number">16.9.</span> <span class="toc-text">Detach hdd from mirror</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Attach-one-hdd-to-mirror"><span class="toc-number">16.10.</span> <span class="toc-text">Attach one hdd to mirror</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#sub-zpool"><span class="toc-number">16.11.</span> <span class="toc-text">sub zpool</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Quota"><span class="toc-number">16.12.</span> <span class="toc-text">Quota</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#user-quota"><span class="toc-number">16.13.</span> <span class="toc-text">user quota</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#zpool-Add"><span class="toc-number">16.14.</span> <span class="toc-text">zpool Add</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Clear-zfs-label"><span class="toc-number">16.15.</span> <span class="toc-text">Clear zfs label</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#iostat"><span class="toc-number">16.16.</span> <span class="toc-text">iostat</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Custom-script-show-temperature"><span class="toc-number">16.17.</span> <span class="toc-text">Custom script ,show temperature</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#pin-CPU"><span class="toc-number">17.</span> <span class="toc-text">pin CPU</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Disable-AVX512-for-scalable-Xeon-silver-and-gold-5xxx"><span class="toc-number">18.</span> <span class="toc-text">Disable AVX512 for scalable Xeon silver and gold 5xxx</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-dracut-cause-brain-split"><span class="toc-number">19.</span> <span class="toc-text">zfs-dracut cause brain split</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Remove-slog"><span class="toc-number">20.</span> <span class="toc-text">Remove slog</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-SMR-HDD"><span class="toc-number">21.</span> <span class="toc-text">About SMR HDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-silent-error"><span class="toc-number">22.</span> <span class="toc-text">About silent error</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Import-zpool-hang-single-dev-just-respond-very-slow-cause-all-zpool-command-hang"><span class="toc-number">22.1.</span> <span class="toc-text">Import zpool hang, single dev just respond very slow cause all zpool command hang</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SAS-signal-about-Invalid-DWORD-count"><span class="toc-number">23.</span> <span class="toc-text">SAS signal (about Invalid DWORD count)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#direct-IO"><span class="toc-number">24.</span> <span class="toc-text">direct IO</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Performance-tuning"><span class="toc-number">25.</span> <span class="toc-text">Performance tuning</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#single-HDD-cause-the-zpool-import-failed"><span class="toc-number">26.</span> <span class="toc-text">single HDD cause the zpool import failed</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Error-ICRC-ABRT-at-LBA-in-SATA-device"><span class="toc-number">27.</span> <span class="toc-text">Error: ICRC, ABRT at LBA in SATA device</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MMP-mmp-timeout-calculate"><span class="toc-number">28.</span> <span class="toc-text">MMP(mmp) timeout calculate</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-block"><span class="toc-number">29.</span> <span class="toc-text">zpool block</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#case-2246"><span class="toc-number">30.</span> <span class="toc-text">case 2246</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#case-reslivering-error-restart-dead-loop"><span class="toc-number">31.</span> <span class="toc-text">case reslivering error restart dead loop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-uberblock-info"><span class="toc-number">32.</span> <span class="toc-text">zfs uberblock info</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-status-show-the-read-x2F-write-x2F-chsum-error"><span class="toc-number">33.</span> <span class="toc-text">zpool status show the read&#x2F;write&#x2F;chsum error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#enable-debug"><span class="toc-number">34.</span> <span class="toc-text">enable-debug</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-read-perfromance"><span class="toc-number">35.</span> <span class="toc-text">zfs read perfromance</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#upgrade-SCSI-device-firmware-online"><span class="toc-number">36.</span> <span class="toc-text">upgrade SCSI device firmware online</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zpool-write-full"><span class="toc-number">37.</span> <span class="toc-text">zpool write full</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#About-the-cksum-error"><span class="toc-number">38.</span> <span class="toc-text">About the cksum error</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-receive-stalls-x2F-slowdown-with-dnodesize-x3D-legacy-8458"><span class="toc-number">39.</span> <span class="toc-text">zfs receive stalls&#x2F;slowdown with dnodesize!&#x3D;legacy #8458</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#zfs-0-7-X-zfs-receive-slow"><span class="toc-number">40.</span> <span class="toc-text">zfs 0.7.X zfs receive slow</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=http://example.com/2016/12/04/zfs/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=http://example.com/2016/12/04/zfs/&text=openzfs in the production"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=http://example.com/2016/12/04/zfs/&is_video=false&description=openzfs in the production"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=openzfs in the production&body=Check out this article: http://example.com/2016/12/04/zfs/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=http://example.com/2016/12/04/zfs/&title=openzfs in the production"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=http://example.com/2016/12/04/zfs/&name=openzfs in the production&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=http://example.com/2016/12/04/zfs/&t=openzfs in the production"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2016-2023
    John Doe
  </div>
  <div class="footer-right">
    <nav>
      <ul>
        <!--
       --><li><a href="/">Home</a></li><!--
     --><!--
       --><li><a href="/about/">About</a></li><!--
     --><!--
       --><li><a href="/archives/">Writing</a></li><!--
     --><!--
       --><li><a target="_blank" rel="noopener" href="http://github.com/probberechts">Projects</a></li><!--
     -->
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->

<!-- utterances Comments -->

    <script type="text/javascript">
      var utterances_repo = '67e8c052/67e8c052.github.io';
      var utterances_issue_term = 'pathname';
      var utterances_label = 'blog-comments';
      var utterances_theme = 'github-dark';

      (function(){
          var script = document.createElement('script');

          script.src = 'https://utteranc.es/client.js';
          script.setAttribute('repo', utterances_repo);
          script.setAttribute('issue-term', 'pathname');
          script.setAttribute('label', utterances_label);
          script.setAttribute('theme', utterances_theme);
          script.setAttribute('crossorigin', 'anonymous');
          script.async = true;
          (document.getElementById('utterances_thread')).appendChild(script);
      }());
  </script>

</body>
</html>
